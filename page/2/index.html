<!DOCTYPE html>
<html lang="zh-Hans">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta property="og:type" content="website">
<meta property="og:title" content="Guadazi">
<meta property="og:url" content="http://example.com/page/2/index.html">
<meta property="og:site_name" content="Guadazi">
<meta property="og:locale">
<meta property="article:author" content="aaronzhang">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://example.com/page/2/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'zh-Hans'
  };
</script>

  <title>Guadazi</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Guadazi</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://example.com/bigdata/Flink/Flink-backpress/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="aaronzhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Guadazi">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/bigdata/Flink/Flink-backpress/" class="post-title-link" itemprop="url">Untitled</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-06-01 19:57:43" itemprop="dateCreated datePublished" datetime="2021-06-01T19:57:43+08:00">2021-06-01</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="实时流处理系统反压机制（BackPressure）综述-转"><a href="#实时流处理系统反压机制（BackPressure）综述-转" class="headerlink" title="实时流处理系统反压机制（BackPressure）综述[转]"></a>实时流处理系统反压机制（BackPressure）综述[转]</h1><p> 发表于 2018-11-15 |  更新于 2018-12-03 |  分类于 <a target="_blank" rel="noopener" href="http://ileaf.tech/category/#/BigData">BigData </a>|  阅读次数 333</p>
<p>本文主要关于实时流处理系统反压机制，最近看到反压问题看到此文章很好，在此分享并mark一下。</p>
<blockquote>
<p>本文主要关于实时流处理系统反压机制，最近看到反压问题看到此文章很好，在此分享并mark一下。<br>(￢_￢)ﾉ最近菜叶子没自己写见谅。<br>本文转自 <a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_21125183/article/details/80708142">实时流处理系统反压机制（BackPressure）综述</a><br><a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_21125183/article/details/80708142">https://blog.csdn.net/qq_21125183/article/details/80708142</a><br><a target="_blank" rel="noopener" href="https://www.jianshu.com/p/28fcd51d4edd">开启Back Pressure使生产环境的Spark Streaming应用更稳定、有效</a></p>
</blockquote>
<p>反压机制（BackPressure）被广泛应用到实时流处理系统中，流处理系统需要能优雅地处理反压（backpressure）问题。<br>反压通常产生于这样的场景：短时负载高峰导致系统接收数据的速率远高于它处理数据的速率。<br>许多日常问题都会导致反压，例如，垃圾回收停顿可能会导致流入的数据快速堆积，或者遇到大促或秒杀活动导致流量陡增。<br>反压如果不能得到正确的处理，可能会导致资源耗尽甚至系统崩溃。反压机制就是指系统能够自己检测到被阻塞的Operator，然后系统自适应地降低源头或者上游的发送速率。</p>
<p>目前主流的流处理系统 Apache Storm、JStorm、Spark Streaming、S4、Apache Flink、Twitter Heron都采用反压机制解决这个问题，不过他们的实现各自不同。</p>
<p><img src="_v_images/20201030212807758_57452691.png" alt="实时流处理系统反压机制01"></p>
<p>不同的组件可以不同的速度执行（并且每个组件中的处理速度随时间改变）。 例如，考虑一个工作流程，或由于数据倾斜或任务调度而导致数据被处理十分缓慢。<br>在这种情况下，如果上游阶段不减速，将导致缓冲区建立长队列(队列占用内存、硬盘空间，节点负载加重)，或导致系统丢弃元组。<br>如果元组在中途丢弃，那么效率可能会有损失，因为已经为这些元组产生的计算被浪费了。<br>并且在一些流处理系统中比如Strom，会将这些丢失的元组重新发送，这样会导致数据的一致性问题(at least once语义)，并且还会导致某些Operator状态叠加。<br>进而整个程序输出结果不准确。第二由于系统接收数据的速率是随着时间改变的，短时负载高峰导致系统接收数据的速率远高于它处理数据的速率的情况，也会导致Tuple在中途丢失。<br>所以实时流处理系统必须能够解决发送速率远大于系统能处理速率这个问题，大多数实时流处理系统采用反压（BackPressure）机制解决这个问题。</p>
<p>下面我们就来介绍一下不同的实时流处理系统采用的反压机制：</p>
<h1 id="Strom-反压机制"><a href="#Strom-反压机制" class="headerlink" title="Strom 反压机制"></a>Strom 反压机制</h1><h2 id="Storm-1-0-以前的反压机制"><a href="#Storm-1-0-以前的反压机制" class="headerlink" title="Storm 1.0 以前的反压机制"></a>Storm 1.0 以前的反压机制</h2><p>对于开启了acker机制的storm程序，可以通过设置conf.setMaxSpoutPending参数来实现反压效果，<strong>如果下游组件(bolt)处理速度跟不上导致spout发送的tuple没有及时确认的数超过了参数设定的值，spout会停止发送数据</strong>，这种方式的缺点是很难调优conf.setMaxSpoutPending参数的设置以达到最好的反压效果，设小了会导致吞吐上不去，设大了会导致worker OOM；有震荡，数据流会处于一个颠簸状态，效果不如逐级反压；另外对于关闭acker机制的程序无效；</p>
<h2 id="Storm-Automatic-Backpressure"><a href="#Storm-Automatic-Backpressure" class="headerlink" title="Storm Automatic Backpressure"></a>Storm Automatic Backpressure</h2><p>新的storm自动反压机制(Automatic Back Pressure)通过监控bolt中的接收队列的情况，当超过高水位值时专门的线程会将反压信息写到 Zookeeper ，Zookeeper上的watch会通知该拓扑的所有Worker都进入反压状态，最后Spout降低tuple发送的速度。</p>
<p><img src="_v_images/20201030212806451_178512803.png" alt="实时流处理系统反压机制02"></p>
<p>每个Executor都有一个接受队列和发送队列用来接收Tuple和发送Spout或者Bolt生成的Tuple元组。每个Worker进程都有一个单的的接收线程监听接收端口。<br>它从每个网络上进来的消息发送到Executor的接收队列中。Executor接收队列存放Worker或者Worker内部其他Executor发过来的消息。<br>Executor工作线程从接收队列中拿出数据，然后调用execute方法，发送Tuple到Executor的发送队列。<br>Executor的发送线程从发送队列中获取消息，按照消息目的地址选择发送到Worker的传输队列中或者其他Executor的接收队列中。<br>最后Worker的发送线程从传输队列中读取消息，然后将Tuple元组发送到网络中。</p>
<ol>
<li>当Worker进程中的Executor线程发现自己的接收队列满了时，也就是接收队列达到<code>high watermark</code>的阈值后，因此它会发送通知消息到背压线程。</li>
<li>背压线程将当前worker进程的信息注册到Zookeeper的Znode节点中。具体路径就是 <code>/Backpressure/topo1/wk1</code>下</li>
<li>Zookeepre的Znode Watcher监视/Backpreesure/topo1下的节点目录变化情况，如果发现目录增加了znode节点说明或者其他变化。这就说明该Topo1需要反压控制，然后它会通知Topo1所有的Worker进入反压状态。</li>
<li>最终Spout降低tuple发送的速度。</li>
</ol>
<h1 id="JStorm-反压机制"><a href="#JStorm-反压机制" class="headerlink" title="JStorm 反压机制"></a>JStorm 反压机制</h1><p>JStorm做了两级的反压，第一级和Jstorm类似，通过执行队列来监测，但是不会通过ZK来协调，而是通过Topology Master来协调。<br>在队列中会标记high water mark和low water mark，当执行队列超过high water mark时，就认为bolt来不及处理，则向TM发一条控制消息，上游开始减慢发送速率，直到下游低于low water mark时解除反压。</p>
<p>此外，在Netty层也做了一级反压，由于每个Worker Task都有自己的发送和接收的缓冲区，可以对缓冲区设定限额、控制大小，如果spout数据量特别大，缓冲区填满会导致下游bolt的接收缓冲区填满，造成了反压。</p>
<p><img src="_v_images/20201030212805000_1695102842.png" alt="实时流处理系统反压机制03"></p>
<p>限流机制：jstorm的限流机制， 当下游bolt发生阻塞时， 并且阻塞task的比例超过某个比例时（现在默认设置为0.1），触发反压</p>
<p>限流方式：计算阻塞Task的地方执行线程执行时间，Spout每发送一个tuple等待相应时间，然后讲这个时间发送给Spout， 于是， spout每发送一个tuple，就会等待这个执行时间。</p>
<p>Task阻塞判断方式：在jstorm 连续4次采样周期中采样，队列情况，当队列超过80%（可以设置）时，即可认为该task处在阻塞状态。</p>
<h1 id="SparkStreaming-反压机制"><a href="#SparkStreaming-反压机制" class="headerlink" title="SparkStreaming 反压机制"></a>SparkStreaming 反压机制</h1><h2 id="为什么引入反压机制Backpressure"><a href="#为什么引入反压机制Backpressure" class="headerlink" title="为什么引入反压机制Backpressure"></a>为什么引入反压机制Backpressure</h2><p>默认情况下，Spark Streaming通过Receiver以生产者生产数据的速率接收数据，计算过程中会出现<code>batch processing time &gt; batch interval</code>的情况，其中<code>batch processing time</code> 为实际计算一个批次花费时间， <code>batch interval</code>为Streaming应用设置的批处理间隔。<br>这意味着Spark Streaming的数据接收速率高于Spark从队列中移除数据的速率，也就是数据处理能力低，在设置间隔内不能完全处理当前接收速率接收的数据。如果这种情况持续过长的时间，会造成数据在内存中堆积，导致Receiver所在Executor内存溢出等问题（如果设置StorageLevel包含disk, 则内存存放不下的数据会溢写至disk, 加大延迟）。<br>Spark 1.5以前版本，用户如果要限制Receiver的数据接收速率，可以通过设置静态配制参数“<code>spark.streaming.receiver.maxRate</code>”的值来实现，此举虽然可以通过限制接收速率，来适配当前的处理能力，防止内存溢出，但也会引入其它问题。比如：producer数据生产高于maxRate，当前集群处理能力也高于maxRate，这就会造成资源利用率下降等问题。为了更好的协调数据接收速率与资源处理能力，Spark Streaming 从v1.5开始引入反压机制（back-pressure）,通过动态控制数据接收速率来适配集群数据处理能力。</p>
<h2 id="反压机制Backpressure"><a href="#反压机制Backpressure" class="headerlink" title="反压机制Backpressure"></a>反压机制Backpressure</h2><p><code>Spark Streaming Backpressure</code>: 根据JobScheduler反馈作业的执行信息来动态调整Receiver数据接收率。通过属性“spark.streaming.backpressure.enabled”来控制是否启用backpressure机制，默认值false，即不启用。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sparkConf.set(<span class="string">&quot;spark.streaming.backpressure.enabled&quot;</span>,”<span class="keyword">true</span>”)</span><br></pre></td></tr></table></figure>
<p>SparkStreaming 架构图如下所示:</p>
<p><img src="_v_images/20201030212803393_1135733829.png" alt="实时流处理系统反压机制04"></p>
<p>SparkStreaming 反压过程执行如下图所示：</p>
<p>在原架构的基础上加上一个新的组件RateController,这个组件负责监听“OnBatchCompleted”事件，然后从中抽取<code>processingDelay</code> 及<code>schedulingDelay</code>信息. <code>Estimator</code>依据这些信息估算出最大处理速度（rate），最后由基于<code>Receiver</code>的Input Stream将rate通过ReceiverTracker与ReceiverSupervisorImpl转发给BlockGenerator（继承自RateLimiter）.</p>
<p><img src="_v_images/20201030212801986_428923565.png" alt="实时流处理系统反压机制05"></p>
<h2 id="direct模式-BackPressure-此部分详细说明了direct模式接收：转自-开启Back-Pressure使生产环境的Spark-Streaming应用更稳定、有效"><a href="#direct模式-BackPressure-此部分详细说明了direct模式接收：转自-开启Back-Pressure使生产环境的Spark-Streaming应用更稳定、有效" class="headerlink" title="direct模式-BackPressure(此部分详细说明了direct模式接收：转自-开启Back Pressure使生产环境的Spark Streaming应用更稳定、有效)"></a>direct模式-BackPressure(此部分详细说明了direct模式接收：转自-<a target="_blank" rel="noopener" href="https://www.jianshu.com/p/28fcd51d4edd">开启Back Pressure使生产环境的Spark Streaming应用更稳定、有效</a>)</h2><p>当Spark Streaming与Kafka使用Direct API集群时，我们可以很方便的去控制最大数据摄入量–通过一个被称作spark.streaming.kafka.maxRatePerPartition的参数。根据文档描述，他的含义是：Direct API读取每一个Kafka partition数据的最大速率（每秒读取的消息量）。<br>配置项spark.streaming.kafka.maxRatePerPartition，对防止流式应用在下边两种情况下出现流量过载时尤其重要：<br>1.Kafka Topic中有大量未处理的消息，并且我们设置是Kafka auto.offset.reset参数值为smallest，他可以防止第一个批次出现数据流量过载情况。<br>2.当Kafka 生产者突然飙升流量的时候，他可以防止批次处理出现数据流量过载情况。</p>
<p>但是，配置Kafka每个partition每批次最大的摄入量是个静态值，也算是个缺点。随着时间的变化，在生产环境运行了一段时间的Spark Streaming应用，每批次每个Kafka partition摄入数据最大量的最优值也是变化的。有时候，是因为消息的大小会变，导致数据处理时间变化。有时候，是因为流计算所使用的多租户集群会变得非常繁忙，比如在白天时候，一些其他的数据应用（例如Impala/Hive/MR作业）竞争共享的系统资源时（CPU/内存/网络/磁盘IO）。<br>背压机制可以解决该问题。背压机制是呼声比较高的功能，他允许根据前一批次数据的处理情况，动态、自动的调整后续数据的摄入量，这样的反馈回路使得我们可以应对流式应用流量波动的问题。<br>Spark Streaming的背压机制是在Spark1.5版本引进的，我们可以添加如下代码启用改功能：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sparkConf.set(&quot;spark.streaming.backpressure.enabled&quot;,”true”)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>那应用启动后的第一个批次流量怎么控制呢？因为他没有前面批次的数据处理时间，所以没有参考的数据去评估这一批次最优的摄入量。在Spark官方文档中有个被称作spark.streaming.backpressure.initialRate的配置，看起来是控制开启背压机制时初始化的摄入量。其实不然，该参数只对receiver模式起作用，并不适用于direct模式。推荐的方法是使用spark.streaming.kafka.maxRatePerPartition控制背压机制起作用前的第一批次数据的最大摄入量。我通常建议设置spark.streaming.kafka.maxRatePerPartition的值为最优估计值的1.5到2倍，让背压机制的算法去调整后续的值。请注意，spark.streaming.kafka.maxRatePerPartition的值会一直控制最大的摄入量，所以背压机制的算法值不会超过他。<br>另一个需要注意的是，在第一个批次处理完成前，紧接着的批次都将使用spark.streaming.kafka.maxRatePerPartition的值作为摄入量。通过Spark UI可以看到，批次间隔为5s，当批次调度延迟31秒时候，前7个批次的摄入量是20条记录。直到第八个批次，背压机制起作用时，摄入量变为5条记录。</p>
<h1 id="Heron-反压机制"><a href="#Heron-反压机制" class="headerlink" title="Heron 反压机制"></a>Heron 反压机制</h1><p><img src="_v_images/20201030212800445_1650579597.png" alt="实时流处理系统反压机制06"></p>
<p>当下游处理速度跟不上上游发送速度时，一旦StreamManager 发现一个或多个Heron Instance 速度变慢，立刻对本地spout进行降级，降低本地Spout发送速度, 停止从这些spout读取数据。并且受影响的StreamManager 会发送一个特殊的start backpressure message 给其他的StreamManager ，要求他们对spout进行本地降级。 当其他StreamManager 接收到这个特殊消息时，他们通过不读取当地Spout中的Tuple来进行降级。一旦出问题的Heron Instance 恢复速度后，本地的SM 会发送stop backpressure message 解除降级。</p>
<p>很多Socket Channel与应用程序级别的Buffer相关联，该缓冲区由high watermark 和low watermark组成。 当缓冲区大小达到high watermark时触发反压，并保持有效，直到缓冲区大小低于low watermark。 此设计的基本原理是防止拓扑在进入和退出背压缓解模式之间快速振荡。</p>
<h1 id="Flink-反压机制"><a href="#Flink-反压机制" class="headerlink" title="Flink 反压机制"></a>Flink 反压机制</h1><p>Flink 没有使用任何复杂的机制来解决反压问题，因为根本不需要那样的方案！它利用自身作为纯数据流引擎的优势来优雅地响应反压问题。下面我们会深入分析 Flink 是如何在 Task 之间传输数据的，以及数据流如何实现自然降速的。 Flink 在运行时主要由 operators 和 streams 两大组件构成。每个 operator 会消费中间态的流，并在流上进行转换，然后生成新的流。对于 Flink 的网络机制一种形象的类比是，Flink 使用了高效有界的分布式阻塞队列，就像 Java 通用的阻塞队列（BlockingQueue）一样。还记得经典的线程间通信案例：生产者消费者模型吗？使用 BlockingQueue 的话，一个较慢的接受者会降低发送者的发送速率，因为一旦队列满了（有界队列）发送者会被阻塞。Flink 解决反压的方案就是这种感觉。 在 Flink 中，这些分布式阻塞队列就是这些逻辑流，而队列容量是通过缓冲池来（LocalBufferPool）实现的。每个被生产和被消费的流都会被分配一个缓冲池。缓冲池管理着一组缓冲(Buffer)，缓冲在被消费后可以被回收循环利用。这很好理解：你从池子中拿走一个缓冲，填上数据，在数据消费完之后，又把缓冲还给池子，之后你可以再次使用它。</p>
<h2 id="Flink-网络传输中的内存管理"><a href="#Flink-网络传输中的内存管理" class="headerlink" title="Flink 网络传输中的内存管理"></a>Flink 网络传输中的内存管理</h2><p>如下图所示展示了 Flink 在网络传输场景下的内存管理。网络上传输的数据会写到 Task 的 InputGate（IG） 中，经过 Task 的处理后，再由 Task 写到 ResultPartition（RS） 中。每个 Task 都包括了输入和输入，输入和输出的数据存在 Buffer 中（都是字节数据）。Buffer 是 MemorySegment 的包装类。</p>
<p><img src="_v_images/20201030212758832_1369878662.png" alt="实时流处理系统反压机制07"></p>
<ol>
<li>TaskManager（TM）在启动时，会先初始化NetworkEnvironment对象，TM 中所有与网络相关的东西都由该类来管理（如 Netty 连接），其中就包括NetworkBufferPool。根据配置，Flink 会在 NetworkBufferPool 中生成一定数量（默认2048个）的内存块 MemorySegment（关于 Flink 的内存管理，后续文章会详细谈到），内存块的总数量就代表了网络传输中所有可用的内存。NetworkEnvironment 和 NetworkBufferPool 是 Task 之间共享的，每个 TM 只会实例化一个。</li>
<li>Task 线程启动时，会向 NetworkEnvironment 注册，NetworkEnvironment 会为 Task 的 InputGate（IG）和 ResultPartition（RP） 分别创建一个 LocalBufferPool（缓冲池）并设置可申请的 MemorySegment（内存块）数量。IG 对应的缓冲池初始的内存块数量与 IG 中 InputChannel 数量一致，RP 对应的缓冲池初始的内存块数量与 RP 中的 ResultSubpartition 数量一致。不过，每当创建或销毁缓冲池时，NetworkBufferPool 会计算剩余空闲的内存块数量，并平均分配给已创建的缓冲池。注意，这个过程只是指定了缓冲池所能使用的内存块数量，并没有真正分配内存块，只有当需要时才分配。为什么要动态地为缓冲池扩容呢？因为内存越多，意味着系统可以更轻松地应对瞬时压力（如GC），不会频繁地进入反压状态，所以我们要利用起那部分闲置的内存块。</li>
<li>在 Task 线程执行过程中，当 Netty 接收端收到数据时，为了将 Netty 中的数据拷贝到 Task 中，InputChannel（实际是 RemoteInputChannel）会向其对应的缓冲池申请内存块（上图中的①）。如果缓冲池中也没有可用的内存块且已申请的数量还没到池子上限，则会向 NetworkBufferPool 申请内存块（上图中的②）并交给 InputChannel 填上数据（上图中的③和④）。如果缓冲池已申请的数量达到上限了呢？或者 NetworkBufferPool 也没有可用内存块了呢？这时候，Task 的 Netty Channel 会暂停读取，上游的发送端会立即响应停止发送，拓扑会进入反压状态。当 Task 线程写数据到 ResultPartition 时，也会向缓冲池请求内存块，如果没有可用内存块时，会阻塞在请求内存块的地方，达到暂停写入的目的。</li>
<li>当一个内存块被消费完成之后（在输入端是指内存块中的字节被反序列化成对象了，在输出端是指内存块中的字节写入到 Netty Channel 了），会调用 Buffer.recycle() 方法，会将内存块还给 LocalBufferPool （上图中的⑤）。如果LocalBufferPool中当前申请的数量超过了池子容量（由于上文提到的动态容量，由于新注册的 Task 导致该池子容量变小），则LocalBufferPool会将该内存块回收给 NetworkBufferPool（上图中的⑥）。如果没超过池子容量，则会继续留在池子中，减少反复申请的开销。</li>
</ol>
<h2 id="Flink-反压机制-1"><a href="#Flink-反压机制-1" class="headerlink" title="Flink 反压机制"></a>Flink 反压机制</h2><p>下面这张图简单展示了两个 Task 之间的数据传输以及 Flink 如何感知到反压的：</p>
<p><img src="_v_images/20201030212757424_78076714.png" alt="实时流处理系统反压机制08"></p>
<ol>
<li>记录“A”进入了 Flink 并且被 Task 1 处理。（这里省略了 Netty 接收、反序列化等过程）</li>
<li>记录被序列化到 buffer 中。</li>
<li>该 buffer 被发送到 Task 2，然后 Task 2 从这个 buffer 中读出记录。</li>
</ol>
<p><strong>不要忘了：记录能被 Flink 处理的前提是，必须有空闲可用的 Buffer。</strong></p>
<p>结合上面两张图看：Task 1 在输出端有一个相关联的 LocalBufferPool（称缓冲池1），Task 2 在输入端也有一个相关联的 LocalBufferPool（称缓冲池2）。如果缓冲池1中有空闲可用的 buffer 来序列化记录 “A”，我们就序列化并发送该 buffer。</p>
<p>这里我们需要注意两个场景：</p>
<ul>
<li>本地传输：如果 Task 1 和 Task 2 运行在同一个 worker 节点（TaskManager），该 buffer 可以直接交给下一个 Task。一旦 Task 2 消费了该 buffer，则该 buffer 会被缓冲池1回收。如果 Task 2 的速度比 1 慢，那么 buffer 回收的速度就会赶不上 Task 1 取 buffer 的速度，导致缓冲池1无可用的 buffer，Task 1 等待在可用的 buffer 上。最终形成 Task 1 的降速。</li>
<li>远程传输：如果 Task 1 和 Task 2 运行在不同的 worker 节点上，那么 buffer 会在发送到网络（TCP Channel）后被回收。在接收端，会从 LocalBufferPool 中申请 buffer，然后拷贝网络中的数据到 buffer 中。如果没有可用的 buffer，会停止从 TCP 连接中读取数据。在输出端，通过 Netty 的水位值机制来保证不往网络中写入太多数据（后面会说）。如果网络中的数据（Netty输出缓冲中的字节数）超过了高水位值，我们会等到其降到低水位值以下才继续写入数据。这保证了网络中不会有太多的数据。如果接收端停止消费网络中的数据（由于接收端缓冲池没有可用 buffer），网络中的缓冲数据就会堆积，那么发送端也会暂停发送。另外，这会使得发送端的缓冲池得不到回收，writer 阻塞在向 LocalBufferPool 请求 buffer，阻塞了 writer 往 ResultSubPartition 写数据。</li>
</ul>
<p>这种固定大小缓冲池就像阻塞队列一样，保证了 Flink 有一套健壮的反压机制，使得 Task 生产数据的速度不会快于消费的速度。我们上面描述的这个方案可以从两个 Task 之间的数据传输自然地扩展到更复杂的 pipeline 中，保证反压机制可以扩散到整个 pipeline。</p>
<h2 id="反压实验"><a href="#反压实验" class="headerlink" title="反压实验"></a>反压实验</h2><p>另外，官方博客中为了展示反压的效果，给出了一个简单的实验。下面这张图显示了：随着时间的改变，生产者（黄色线）和消费者（绿色线）每5秒的平均吞吐与最大吞吐（在单一JVM中每秒达到8百万条记录）的百分比。我们通过衡量task每5秒钟处理的记录数来衡量平均吞吐。该实验运行在单 JVM 中，不过使用了完整的 Flink 功能栈。</p>
<p><img src="_v_images/20201030212755917_1236027751.png" alt="实时流处理系统反压机制09"></p>
<p>首先，我们运行生产task到它最大生产速度的60%（我们通过Thread.sleep()来模拟降速）。消费者以同样的速度处理数据。然后，我们将消费task的速度降至其最高速度的30%。你就会看到背压问题产生了，正如我们所见，生产者的速度也自然降至其最高速度的30%。接着，停止消费task的人为降速，之后生产者和消费者task都达到了其最大的吞吐。接下来，我们再次将消费者的速度降至30%，pipeline给出了立即响应：生产者的速度也被自动降至30%。最后，我们再次停止限速，两个task也再次恢复100%的速度。总而言之，我们可以看到：生产者和消费者在 pipeline 中的处理都在跟随彼此的吞吐而进行适当的调整，这就是我们希望看到的反压的效果。</p>
<h2 id="Flink-反压监控"><a href="#Flink-反压监控" class="headerlink" title="Flink 反压监控"></a>Flink 反压监控</h2><p>在 Storm/JStorm 中，只要监控到队列满了，就可以记录下拓扑进入反压了。但是 Flink 的反压太过于天然了，导致我们无法简单地通过监控队列来监控反压状态。Flink 在这里使用了一个 trick 来实现对反压的监控。如果一个 Task 因为反压而降速了，那么它会卡在向 LocalBufferPool 申请内存块上。那么这时候，该 Task 的 stack trace 就会长下面这样：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">java.lang.Object.wait(Native Method)</span><br><span class="line">o.a.f.[...].LocalBufferPool.requestBuffer(LocalBufferPool.java:<span class="number">163</span>)</span><br><span class="line">o.a.f.[...].LocalBufferPool.requestBufferBlocking(LocalBufferPool.java:<span class="number">133</span>) &lt;--- BLOCKING request</span><br><span class="line">[...]</span><br></pre></td></tr></table></figure>
<p>那么事情就简单了。通过不断地采样每个 task 的 stack trace 就可以实现反压监控。</p>
<p><img src="_v_images/20201030212753993_1098219801.png" alt="实时流处理系统反压机制10"></p>
<p>Flink 的实现中，只有当 Web 页面切换到某个 Job 的 Backpressure 页面，才会对这个 Job 触发反压检测，因为反压检测还是挺昂贵的。JobManager 会通过 Akka 给每个 TaskManager 发送TriggerStackTraceSample消息。默认情况下，TaskManager 会触发100次 stack trace 采样，每次间隔 50ms（也就是说一次反压检测至少要等待5秒钟）。并将这 100 次采样的结果返回给 JobManager，由 JobManager 来计算反压比率（反压出现的次数/采样的次数），最终展现在 UI 上。UI 刷新的默认周期是一分钟，目的是不对 TaskManager 造成太大的负担。</p>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>Flink不需要一种特殊的机制来处理反压，因为Flink 中的数据传输相当于已经提供了应对反压的机制。因此，Flink 所能获得的最大吞吐量由其 pipeline 中最慢的组件决定。相对于 Storm/JStorm 的实现，Flink 的实现更为简洁优雅，源码中也看不见与反压相关的代码，无需 Zookeeper/TopologyMaster 的参与也降低了系统的负载，也利于对反压更迅速的响应。</p>
<blockquote>
<p>本文转自 <a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_21125183/article/details/80708142">实时流处理系统反压机制（BackPressure）综述</a><br><a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_21125183/article/details/80708142">https://blog.csdn.net/qq_21125183/article/details/80708142</a><br><a target="_blank" rel="noopener" href="https://www.jianshu.com/p/28fcd51d4edd">开启Back Pressure使生产环境的Spark Streaming应用更稳定、有效</a></p>
</blockquote>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://example.com/bigdata/Flink/actions/FrundDetectedSystem/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="aaronzhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Guadazi">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/bigdata/Flink/actions/FrundDetectedSystem/" class="post-title-link" itemprop="url">Untitled</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-06-01 19:57:43" itemprop="dateCreated datePublished" datetime="2021-06-01T19:57:43+08:00">2021-06-01</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="Flink-实时规则引擎实现与应用"><a href="#Flink-实时规则引擎实现与应用" class="headerlink" title="Flink 实时规则引擎实现与应用"></a>Flink 实时规则引擎实现与应用</h1><p>Rule engine</p>
<p>欺诈检测<br>实时标签工厂<br>ABT<br>CEP有什么问题？</p>
<p>Frund detected system</p>
<p>秒级规则生效<br>自定义分发，运行时动态切换<br>自定义窗口<br>实时多维聚合</p>
<p>样例:</p>
<ol>
<li>欺诈检测: 同一个用户连续往两一个账户三天内连续支付超过</li>
<li>实时标签工厂: 标签-用户近三十天访问次数、近三十天访问某个页面的次数</li>
<li>ABT</li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://example.com/bigdata/Flink/actions/01.%20%E5%AE%9E%E6%97%B6%E5%8F%8D%E6%AC%BA%E8%AF%88/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="aaronzhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Guadazi">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/bigdata/Flink/actions/01.%20%E5%AE%9E%E6%97%B6%E5%8F%8D%E6%AC%BA%E8%AF%88/" class="post-title-link" itemprop="url">Untitled</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-06-01 19:57:43" itemprop="dateCreated datePublished" datetime="2021-06-01T19:57:43+08:00">2021-06-01</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="玖富实时反欺诈"><a href="#玖富实时反欺诈" class="headerlink" title="玖富实时反欺诈"></a>玖富实时反欺诈</h1><h2 id="业务场景"><a href="#业务场景" class="headerlink" title="业务场景"></a>业务场景</h2><ul>
<li>低延迟</li>
<li>大数据</li>
<li>多维度</li>
</ul>
<p>系统获取用户产生数据最简单有效的方法就是流水式数据，单个数据包里包含了发生时间点的各个维度的所有信息量，这种场景的特性之一就是数据高并发，因此对时效要求比较高的数据分析来说是一个非常巨大的挑战</p>
<blockquote>
<p>在哪个场景下，实时宽表都是一个门槛</p>
</blockquote>
<ul>
<li><p>高并发</p>
</li>
<li><p>规则实时下发</p>
</li>
</ul>
<p>数据加工提速</p>
<p>在大量数据中做快速预查，利用Flink并发能力进行数据覆盖，最后在缓存里命中结果，从而不必重新进行网络I/O 查询、等待返回的过程。经过部分计算框架升级，最终系统实现了p99 延迟由1s 降为100ms 的优化</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://example.com/bigdata/Flink/actions/02.%20%E5%AE%9E%E6%97%B6%E7%94%A8%E6%88%B7%E7%94%BB%E5%83%8F/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="aaronzhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Guadazi">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/bigdata/Flink/actions/02.%20%E5%AE%9E%E6%97%B6%E7%94%A8%E6%88%B7%E7%94%BB%E5%83%8F/" class="post-title-link" itemprop="url">Untitled</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-06-01 19:57:43" itemprop="dateCreated datePublished" datetime="2021-06-01T19:57:43+08:00">2021-06-01</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="实时用户画像"><a href="#实时用户画像" class="headerlink" title="实时用户画像"></a>实时用户画像</h1><h2 id="场景"><a href="#场景" class="headerlink" title="场景"></a>场景</h2><p>企业面对不断增加的海量信息，其信息筛选和处理效率低下的困扰与日俱增。由于用户营销不够细化，企业App 中许多不合时宜或不合偏好的消息推送很大程度上影响了用户体验，甚至引发了用户流失</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://example.com/bigdata/Flink/Flink-SQL-codeGen/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="aaronzhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Guadazi">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/bigdata/Flink/Flink-SQL-codeGen/" class="post-title-link" itemprop="url">Untitled</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-06-01 19:57:43" itemprop="dateCreated datePublished" datetime="2021-06-01T19:57:43+08:00">2021-06-01</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="Flink-SQL-代码生成"><a href="#Flink-SQL-代码生成" class="headerlink" title="Flink SQL 代码生成"></a>Flink SQL 代码生成</h1><p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/rossiXYZ/p/12773123.html">从”UDF不应有状态” 切入来剖析Flink SQL代码生成</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://example.com/bigdata/Flink/Flink-watermark/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="aaronzhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Guadazi">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/bigdata/Flink/Flink-watermark/" class="post-title-link" itemprop="url">Untitled</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-06-01 19:57:43" itemprop="dateCreated datePublished" datetime="2021-06-01T19:57:43+08:00">2021-06-01</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="watermark"><a href="#watermark" class="headerlink" title="watermark"></a>watermark</h1><p>a window operator registers a timer for every active window, which cleans up the window’s state when the event time passes the window’s ending time.</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://example.com/bigdata/Flink/Flink-join/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="aaronzhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Guadazi">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/bigdata/Flink/Flink-join/" class="post-title-link" itemprop="url">Untitled</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-06-01 19:57:43" itemprop="dateCreated datePublished" datetime="2021-06-01T19:57:43+08:00">2021-06-01</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="Flink-join"><a href="#Flink-join" class="headerlink" title="Flink join"></a>Flink join</h1><p>Flink DataStream API为用户提供了3个算子来实现双流join，分别是：</p>
<ul>
<li>join():   inner join，on window</li>
<li>coGroup():   custom join, on window</li>
<li>intervalJoin():   inner join, on time range, keyed stream</li>
</ul>
<p>另外，还提供了broadcast join来关联较小的</p>
<h2 id="准备数据"><a href="#准备数据" class="headerlink" title="准备数据"></a>准备数据</h2><p>从Kafka分别接入点击流和订单流，并转化为POJO。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">DataStream&lt;String&gt; clickSourceStream = env</span><br><span class="line">  .addSource(<span class="keyword">new</span> FlinkKafkaConsumer011&lt;&gt;(</span><br><span class="line">    <span class="string">&quot;ods_analytics_access_log&quot;</span>,</span><br><span class="line">    <span class="keyword">new</span> SimpleStringSchema(),</span><br><span class="line">    kafkaProps</span><br><span class="line">  ).setStartFromLatest());</span><br><span class="line">DataStream&lt;String&gt; orderSourceStream = env</span><br><span class="line">  .addSource(<span class="keyword">new</span> FlinkKafkaConsumer011&lt;&gt;(</span><br><span class="line">    <span class="string">&quot;ods_ms_order_done&quot;</span>,</span><br><span class="line">    <span class="keyword">new</span> SimpleStringSchema(),</span><br><span class="line">    kafkaProps</span><br><span class="line">  ).setStartFromLatest());</span><br><span class="line"></span><br><span class="line">DataStream&lt;AnalyticsAccessLogRecord&gt; clickRecordStream = clickSourceStream</span><br><span class="line">  .map(message -&gt; JSON.parseObject(message, AnalyticsAccessLogRecord.class));</span><br><span class="line">DataStream&lt;OrderDoneLogRecord&gt; orderRecordStream = orderSourceStream</span><br><span class="line">  .map(message -&gt; JSON.parseObject(message, OrderDoneLogRecord.class));</span><br></pre></td></tr></table></figure>
<h2 id="join"><a href="#join" class="headerlink" title="join()"></a>join()</h2><p>join()算子提供的语义为”<strong>Window join</strong>“，即按照指定字段和（滚动/滑动/会话）窗口进行<strong>inner join</strong>，支持处理时间和事件时间两种时间特征。</p>
<p>以下示例以10秒滚动窗口，将两个流通过商品ID关联，取得订单流中的售价相关字段。</p>
<p><img src="//upload-images.jianshu.io/upload_images/195230-05b05dd1da1e6025.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1000/format/webp"></p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">clickRecordStream</span><br><span class="line">  .join(orderRecordStream)</span><br><span class="line">  .where(record -&gt; record.getMerchandiseId())</span><br><span class="line">  .equalTo(record -&gt; record.getMerchandiseId())</span><br><span class="line">  .window(TumblingProcessingTimeWindows.of(Time.seconds(<span class="number">10</span>)))</span><br><span class="line">  .apply(<span class="keyword">new</span> JoinFunction&lt;AnalyticsAccessLogRecord, OrderDoneLogRecord, String&gt;() &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">join</span><span class="params">(AnalyticsAccessLogRecord accessRecord, OrderDoneLogRecord orderRecord)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">      <span class="keyword">return</span> StringUtils.join(Arrays.asList(</span><br><span class="line">        accessRecord.getMerchandiseId(),</span><br><span class="line">        orderRecord.getPrice(),</span><br><span class="line">        orderRecord.getCouponMoney(),</span><br><span class="line">        orderRecord.getRebateAmount()</span><br><span class="line">      ), <span class="string">&#x27;\t&#x27;</span>);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;)</span><br><span class="line">  .print().setParallelism(<span class="number">1</span>);</span><br></pre></td></tr></table></figure>
<p>简单易用。</p>
<h2 id="coGroup"><a href="#coGroup" class="headerlink" title="coGroup()"></a>coGroup()</h2><p>只有inner join肯定还不够，如何实现left/right outer join呢？答案就是利用coGroup()算子。它的调用方式类似于join()算子，也需要开窗，但是CoGroupFunction比JoinFunction更加灵活，可以按照用户指定的逻辑匹配左流和/或右流的数据并输出。</p>
<p>以下的例子就实现了点击流left join订单流的功能，是很朴素的nested loop join思想（二重循环）。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">clickRecordStream</span><br><span class="line">  .coGroup(orderRecordStream)</span><br><span class="line">  .where(record -&gt; record.getMerchandiseId())</span><br><span class="line">  .equalTo(record -&gt; record.getMerchandiseId())</span><br><span class="line">  .window(TumblingProcessingTimeWindows.of(Time.seconds(<span class="number">10</span>)))</span><br><span class="line">  .apply(<span class="keyword">new</span> CoGroupFunction&lt;AnalyticsAccessLogRecord, OrderDoneLogRecord, Tuple2&lt;String, Long&gt;&gt;() &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">coGroup</span><span class="params">(Iterable&lt;AnalyticsAccessLogRecord&gt; accessRecords, Iterable&lt;OrderDoneLogRecord&gt; orderRecords, Collector&lt;Tuple2&lt;String, Long&gt;&gt; collector)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">      <span class="keyword">for</span> (AnalyticsAccessLogRecord accessRecord : accessRecords) &#123;</span><br><span class="line">        <span class="keyword">boolean</span> isMatched = <span class="keyword">false</span>;</span><br><span class="line">        <span class="keyword">for</span> (OrderDoneLogRecord orderRecord : orderRecords) &#123;</span><br><span class="line">          <span class="comment">// 右流中有对应的记录</span></span><br><span class="line">          collector.collect(<span class="keyword">new</span> Tuple2&lt;&gt;(accessRecord.getMerchandiseName(), orderRecord.getPrice()));</span><br><span class="line">          isMatched = <span class="keyword">true</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> (!isMatched) &#123;</span><br><span class="line">          <span class="comment">// 右流中没有对应的记录</span></span><br><span class="line">          collector.collect(<span class="keyword">new</span> Tuple2&lt;&gt;(accessRecord.getMerchandiseName(), <span class="keyword">null</span>));</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;)</span><br><span class="line">  .print().setParallelism(<span class="number">1</span>);</span><br></pre></td></tr></table></figure>
<p>CoGroupFunction中会返回所有数据，不管有没有匹配上</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line">DataStream&lt;Tuple3&lt;Long, String, String&gt;&gt; input1 = ...;</span><br><span class="line">input1 = input1.assignTimestampsAndWatermarks(<span class="keyword">new</span> AscendingTimestampExtractor&lt;Tuple3&lt;Long, String, String&gt;&gt;() &#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">long</span> <span class="title">extractAscendingTimestamp</span><span class="params">(Tuple3&lt;Long, String, String&gt; arg0)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> arg0.f0;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line">DataStream&lt;Tuple2&lt;Long, String&gt;&gt; input2 = ...;</span><br><span class="line">input2 = input2.assignTimestampsAndWatermarks(<span class="keyword">new</span> AscendingTimestampExtractor&lt;Tuple2&lt;Long, String&gt;&gt;() &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">long</span> <span class="title">extractAscendingTimestamp</span><span class="params">(Tuple2&lt;Long, String&gt; stringStringTuple2)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> stringStringTuple2.f0;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line">input1.coGroup(input2).where(<span class="keyword">new</span> KeySelector&lt;Tuple3&lt;Long, String, String&gt;, String&gt;() &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">getKey</span><span class="params">(Tuple3&lt;Long, String, String&gt; itemEntity)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> itemEntity.f1;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;)</span><br><span class="line">.equalTo(<span class="keyword">new</span> KeySelector&lt;Tuple2&lt;Long, String&gt;, String&gt;() &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">getKey</span><span class="params">(Tuple2&lt;Long, String&gt; value)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> value.f1;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;)</span><br><span class="line">.window(TumblingEventTimeWindows.of(Time.minutes(<span class="number">1</span>)))</span><br><span class="line">.apply(<span class="keyword">new</span> CoGroupFunction&lt;Tuple3&lt;Long, String, String&gt;, Tuple2&lt;Long, String&gt;, String&gt;() &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">coGroup</span><span class="params">(Iterable&lt;Tuple3&lt;Long, String, String&gt;&gt; first,</span></span></span><br><span class="line"><span class="function"><span class="params">            Iterable&lt;Tuple2&lt;Long, String&gt;&gt; second, Collector&lt;String&gt; collector)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        StringBuilder buffer = <span class="keyword">new</span> StringBuilder();</span><br><span class="line">        buffer.append(<span class="string">&quot;DataStream first:\n&quot;</span>);</span><br><span class="line">        <span class="keyword">for</span> (Tuple3&lt;Long, String, String&gt; value : first) &#123;</span><br><span class="line">            buffer.append(value).append(<span class="string">&quot;\n&quot;</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        buffer.append(<span class="string">&quot;DataStream second:\n&quot;</span>);</span><br><span class="line">        <span class="keyword">for</span> (Tuple2&lt;Long, String&gt; value : second) &#123;</span><br><span class="line">            buffer.append(value.f0).append(<span class="string">&quot;=&gt;&quot;</span>).append(value.f1).append(<span class="string">&quot;\n&quot;</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        collector.collect(buffer.toString());</span><br><span class="line">    &#125;</span><br><span class="line">&#125;)</span><br><span class="line">.print();</span><br></pre></td></tr></table></figure>
<p>上面的例子，左流有三个元素 <code>Tuple3&lt;String,String,String&gt;</code>，右流有两个元素<code>Tuple2&lt;String,String&gt;</code><br>两个流第一个元素相互关联。分别指定两个流的事件时间字段。<br>两个流关联后，按照EventTime划分窗口。与单流类似。<br>不管元素是否可以关联上，都会输出</p>
<p>用户可以定义CoGroupFunction函数, 可以实现在窗口内，任意组合，如笛卡尔积</p>
<h2 id="intervalJoin"><a href="#intervalJoin" class="headerlink" title="intervalJoin()"></a>intervalJoin()</h2><p>join()和coGroup()都是基于窗口做关联的。但是在某些情况下，两条流的数据步调未必一致。例如，订单流的数据有可能在点击流的购买动作发生之后很久才被写入，如果用窗口来圈定，很容易join不上。所以Flink又提供了”<strong>Interval join</strong>“的语义，按照指定字段以及<strong>右流相对左流偏移的时间区间</strong>进行关联，即：</p>
<blockquote>
<p>$right.timestamp ∈ [left.timestamp + lowerBound; left.timestamp + upperBound]$</p>
</blockquote>
<p><img src="vx_images/4903841188593.png"></p>
<p>interval join也是inner join，虽然不需要开窗，但是需要用户指定偏移区间的上下界，并且<strong>只支持事件时间</strong>。</p>
<p>示例代码如下。注意在运行之前，需要分别在两个流上应用assignTimestampsAndWatermarks()方法获取事件时间戳和水印。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">clickRecordStream</span><br><span class="line">  .keyBy(record -&gt; record.getMerchandiseId())</span><br><span class="line">  .intervalJoin(orderRecordStream.keyBy(record -&gt; record.getMerchandiseId()))</span><br><span class="line">  .between(Time.seconds(-<span class="number">30</span>), Time.seconds(<span class="number">30</span>))</span><br><span class="line">  .process(<span class="keyword">new</span> ProcessJoinFunction&lt;AnalyticsAccessLogRecord, OrderDoneLogRecord, String&gt;() &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">processElement</span><span class="params">(AnalyticsAccessLogRecord accessRecord, OrderDoneLogRecord orderRecord, Context context, Collector&lt;String&gt; collector)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">      collector.collect(StringUtils.join(Arrays.asList(</span><br><span class="line">        accessRecord.getMerchandiseId(),</span><br><span class="line">        orderRecord.getPrice(),</span><br><span class="line">        orderRecord.getCouponMoney(),</span><br><span class="line">        orderRecord.getRebateAmount()</span><br><span class="line">      ), <span class="string">&#x27;\t&#x27;</span>));</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;)</span><br><span class="line">  .print().setParallelism(<span class="number">1</span>);</span><br></pre></td></tr></table></figure>
<p>由上可见，interval join与window join不同，是两个KeyedStream之上的操作，并且需要调用between()方法指定偏移区间的上下界。如果想令上下界是开区间，可以调用upperBoundExclusive()/lowerBoundExclusive()方法。</p>
<h3 id="interval-join的实现原理"><a href="#interval-join的实现原理" class="headerlink" title="interval join的实现原理"></a>interval join的实现原理</h3><p>以下是KeyedStream.process(ProcessJoinFunction)方法调用的重载方法的逻辑。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> &lt;OUT&gt; <span class="function">SingleOutputStreamOperator&lt;OUT&gt; <span class="title">process</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">        ProcessJoinFunction&lt;IN1, IN2, OUT&gt; processJoinFunction,</span></span></span><br><span class="line"><span class="function"><span class="params">        TypeInformation&lt;OUT&gt; outputType)</span> </span>&#123;</span><br><span class="line">    Preconditions.checkNotNull(processJoinFunction);</span><br><span class="line">    Preconditions.checkNotNull(outputType);</span><br><span class="line">    <span class="keyword">final</span> ProcessJoinFunction&lt;IN1, IN2, OUT&gt; cleanedUdf = left.getExecutionEnvironment().clean(processJoinFunction);</span><br><span class="line">    <span class="keyword">final</span> IntervalJoinOperator&lt;KEY, IN1, IN2, OUT&gt; operator =</span><br><span class="line">        <span class="keyword">new</span> IntervalJoinOperator&lt;&gt;(</span><br><span class="line">            lowerBound,</span><br><span class="line">            upperBound,</span><br><span class="line">            lowerBoundInclusive,</span><br><span class="line">            upperBoundInclusive,</span><br><span class="line">            left.getType().createSerializer(left.getExecutionConfig()),</span><br><span class="line">            right.getType().createSerializer(right.getExecutionConfig()),</span><br><span class="line">            cleanedUdf</span><br><span class="line">        );</span><br><span class="line">    <span class="keyword">return</span> left</span><br><span class="line">        .connect(right)</span><br><span class="line">        .keyBy(keySelector1, keySelector2)</span><br><span class="line">        .transform(<span class="string">&quot;Interval Join&quot;</span>, outputType, operator);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>可见是先对两条流执行connect()和keyBy()操作，然后利用IntervalJoinOperator算子进行转换。在IntervalJoinOperator中，会利用两个MapState分别缓存左流和右流的数据。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="keyword">transient</span> MapState&lt;Long, List&lt;BufferEntry&lt;T1&gt;&gt;&gt; leftBuffer;</span><br><span class="line"><span class="keyword">private</span> <span class="keyword">transient</span> MapState&lt;Long, List&lt;BufferEntry&lt;T2&gt;&gt;&gt; rightBuffer;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">initializeState</span><span class="params">(StateInitializationContext context)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    <span class="keyword">super</span>.initializeState(context);</span><br><span class="line">    <span class="keyword">this</span>.leftBuffer = context.getKeyedStateStore().getMapState(<span class="keyword">new</span> MapStateDescriptor&lt;&gt;(</span><br><span class="line">        LEFT_BUFFER,</span><br><span class="line">        LongSerializer.INSTANCE,</span><br><span class="line">        <span class="keyword">new</span> ListSerializer&lt;&gt;(<span class="keyword">new</span> BufferEntrySerializer&lt;&gt;(leftTypeSerializer))</span><br><span class="line">    ));</span><br><span class="line">    <span class="keyword">this</span>.rightBuffer = context.getKeyedStateStore().getMapState(<span class="keyword">new</span> MapStateDescriptor&lt;&gt;(</span><br><span class="line">        RIGHT_BUFFER,</span><br><span class="line">        LongSerializer.INSTANCE,</span><br><span class="line">        <span class="keyword">new</span> ListSerializer&lt;&gt;(<span class="keyword">new</span> BufferEntrySerializer&lt;&gt;(rightTypeSerializer))</span><br><span class="line">    ));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>其中<code>Long</code>表示事件时间戳，<code>List&lt;BufferEntry&lt;T&gt;&gt;</code>表示该时刻到来的数据记录。</p>
<p>当左流和右流有数据到达时，会分别调用<code>processElement1()</code>和<code>processElement2()</code>方法，它们都调用了<code>processElement()</code>方法，代码如下。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">processElement1</span><span class="params">(StreamRecord&lt;T1&gt; record)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    processElement(record, leftBuffer, rightBuffer, lowerBound, upperBound, <span class="keyword">true</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">processElement2</span><span class="params">(StreamRecord&lt;T2&gt; record)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    processElement(record, rightBuffer, leftBuffer, -upperBound, -lowerBound, <span class="keyword">false</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@SuppressWarnings(&quot;unchecked&quot;)</span></span><br><span class="line"><span class="keyword">private</span> &lt;THIS, OTHER&gt; <span class="function"><span class="keyword">void</span> <span class="title">processElement</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">        <span class="keyword">final</span> StreamRecord&lt;THIS&gt; record,</span></span></span><br><span class="line"><span class="function"><span class="params">        <span class="keyword">final</span> MapState&lt;Long, List&lt;IntervalJoinOperator.BufferEntry&lt;THIS&gt;&gt;&gt; ourBuffer,</span></span></span><br><span class="line"><span class="function"><span class="params">        <span class="keyword">final</span> MapState&lt;Long, List&lt;IntervalJoinOperator.BufferEntry&lt;OTHER&gt;&gt;&gt; otherBuffer,</span></span></span><br><span class="line"><span class="function"><span class="params">        <span class="keyword">final</span> <span class="keyword">long</span> relativeLowerBound,</span></span></span><br><span class="line"><span class="function"><span class="params">        <span class="keyword">final</span> <span class="keyword">long</span> relativeUpperBound,</span></span></span><br><span class="line"><span class="function"><span class="params">        <span class="keyword">final</span> <span class="keyword">boolean</span> isLeft)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    <span class="keyword">final</span> THIS ourValue = record.getValue();</span><br><span class="line">    <span class="keyword">final</span> <span class="keyword">long</span> ourTimestamp = record.getTimestamp();</span><br><span class="line">    <span class="keyword">if</span> (ourTimestamp == Long.MIN_VALUE) &#123;</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> FlinkException(<span class="string">&quot;Long.MIN_VALUE timestamp: Elements used in &quot;</span> +</span><br><span class="line">                <span class="string">&quot;interval stream joins need to have timestamps meaningful timestamps.&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> (isLate(ourTimestamp)) &#123;</span><br><span class="line">        <span class="keyword">return</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    addToBuffer(ourBuffer, ourValue, ourTimestamp);</span><br><span class="line">    <span class="keyword">for</span> (Map.Entry&lt;Long, List&lt;BufferEntry&lt;OTHER&gt;&gt;&gt; bucket: otherBuffer.entries()) &#123;</span><br><span class="line">        <span class="keyword">final</span> <span class="keyword">long</span> timestamp  = bucket.getKey();</span><br><span class="line">        <span class="keyword">if</span> (timestamp &lt; ourTimestamp + relativeLowerBound ||</span><br><span class="line">                timestamp &gt; ourTimestamp + relativeUpperBound) &#123;</span><br><span class="line">            <span class="keyword">continue</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">for</span> (BufferEntry&lt;OTHER&gt; entry: bucket.getValue()) &#123;</span><br><span class="line">            <span class="keyword">if</span> (isLeft) &#123;</span><br><span class="line">                collect((T1) ourValue, (T2) entry.element, ourTimestamp, timestamp);</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                collect((T1) entry.element, (T2) ourValue, timestamp, ourTimestamp);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">long</span> cleanupTime = (relativeUpperBound &gt; <span class="number">0L</span>) ? ourTimestamp + relativeUpperBound : ourTimestamp;</span><br><span class="line">    <span class="keyword">if</span> (isLeft) &#123;</span><br><span class="line">        internalTimerService.registerEventTimeTimer(CLEANUP_NAMESPACE_LEFT, cleanupTime);</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        internalTimerService.registerEventTimeTimer(CLEANUP_NAMESPACE_RIGHT, cleanupTime);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这段代码的思路是：</p>
<ol>
<li> 取得当前流<code>StreamRecord</code>的时间戳，调用<code>isLate()</code>方法判断它是否是迟到数据（即时间戳小于当前水印值），如是则丢弃。</li>
<li> 调用<code>addToBuffer()</code>方法，将时间戳和数据一起插入当前流对应的<code>MapState</code>。</li>
<li> 遍历另外一个流的<code>MapState</code>，如果数据满足前述的时间区间条件，则调用<code>collect()</code>方法将该条数据投递给用户定义的<code>ProcessJoinFunction</code>进行处理。<br><code>collect()</code>方法的代码如下，注意结果对应的时间戳是左右流时间戳里较大的那个。</li>
</ol>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">collect</span><span class="params">(T1 left, T2 right, <span class="keyword">long</span> leftTimestamp, <span class="keyword">long</span> rightTimestamp)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    <span class="keyword">final</span> <span class="keyword">long</span> resultTimestamp = Math.max(leftTimestamp, rightTimestamp);</span><br><span class="line">    collector.setAbsoluteTimestamp(resultTimestamp);</span><br><span class="line">    context.updateTimestamps(leftTimestamp, rightTimestamp, resultTimestamp);</span><br><span class="line">    userFunction.processElement(left, right, context, collector);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ol start="4">
<li> 调用<code>TimerService.registerEventTimeTimer()</code>注册时间戳为<code>timestamp + relativeUpperBound</code>的定时器，该定时器负责在水印超过区间的上界时执行状态的清理逻辑，防止数据堆积。注意左右流的定时器所属的<code>namespace</code>是不同的，具体逻辑则位于<code>onEventTime()</code>方法中。</li>
</ol>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onEventTime</span><span class="params">(InternalTimer&lt;K, String&gt; timer)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    <span class="keyword">long</span> timerTimestamp = timer.getTimestamp();</span><br><span class="line">    String namespace = timer.getNamespace();</span><br><span class="line">    logger.trace(<span class="string">&quot;onEventTime @ &#123;&#125;&quot;</span>, timerTimestamp);</span><br><span class="line">    <span class="keyword">switch</span> (namespace) &#123;</span><br><span class="line">        <span class="keyword">case</span> CLEANUP_NAMESPACE_LEFT: &#123;</span><br><span class="line">            <span class="keyword">long</span> timestamp = (upperBound &lt;= <span class="number">0L</span>) ? timerTimestamp : timerTimestamp - upperBound;</span><br><span class="line">            logger.trace(<span class="string">&quot;Removing from left buffer @ &#123;&#125;&quot;</span>, timestamp);</span><br><span class="line">            leftBuffer.remove(timestamp);</span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">case</span> CLEANUP_NAMESPACE_RIGHT: &#123;</span><br><span class="line">            <span class="keyword">long</span> timestamp = (lowerBound &lt;= <span class="number">0L</span>) ? timerTimestamp + lowerBound : timerTimestamp;</span><br><span class="line">            logger.trace(<span class="string">&quot;Removing from right buffer @ &#123;&#125;&quot;</span>, timestamp);</span><br><span class="line">            rightBuffer.remove(timestamp);</span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">default</span>:</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> RuntimeException(<span class="string">&quot;Invalid namespace &quot;</span> + namespace);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="broadcast-join"><a href="#broadcast-join" class="headerlink" title="broadcast join"></a>broadcast join</h2><h2 id="interval-join"><a href="#interval-join" class="headerlink" title="interval join"></a>interval join</h2>
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://example.com/bigdata/Flink/Flink-SQL-Catalog/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="aaronzhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Guadazi">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/bigdata/Flink/Flink-SQL-Catalog/" class="post-title-link" itemprop="url">Untitled</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-06-01 19:57:43" itemprop="dateCreated datePublished" datetime="2021-06-01T19:57:43+08:00">2021-06-01</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>在Flink SQL中， 元数据的管理分为三层: catalog-&gt; database-&gt; table，<br>我们知道Flink SQL是依托<strong>calcite</strong>框架来进行SQL执行树生产，校验，优化等等， 所以本文讲介绍FlinkSQL是如何来结合<strong>Calcite</strong>来进行元数据管理的.</p>
<h2 id="calcite开放的接口"><a href="#calcite开放的接口" class="headerlink" title="calcite开放的接口"></a><strong>calcite</strong>开放的接口</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">Schema</span> </span>&#123;</span><br><span class="line">    <span class="function">Table <span class="title">getTable</span><span class="params">(String name)</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="function">Schema <span class="title">getSubSchema</span><span class="params">(String name)</span></span>;</span><br><span class="line"></span><br><span class="line">    ....</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>如接口所示， <strong>Schema</strong>接口，可以通过table名来获得一张表， 可以通过<strong>schema</strong>名来获得一个子<strong>schema</strong>.</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">Table</span> </span>&#123;</span><br><span class="line">    <span class="function">RelDataType <span class="title">getRowType</span><span class="params">(RelDataTypeFactory typeFactory)</span></span>;</span><br><span class="line">    ....</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>看Table的接口， 主要就是返回table的RelDataType.</p>
<h2 id="Flink的相关实现"><a href="#Flink的相关实现" class="headerlink" title="Flink的相关实现"></a>Flink的相关实现</h2><p>接下来，我们来看下Flink是如何实现这些接口的:</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CatalogManagerCalciteSchema</span> <span class="keyword">extends</span> <span class="title">FlinkSchema</span> </span>&#123;</span><br><span class="line">	<span class="meta">@Override</span></span><br><span class="line">	<span class="function"><span class="keyword">public</span> Schema <span class="title">getSubSchema</span><span class="params">(String schemaName)</span> </span>&#123;</span><br><span class="line">		<span class="keyword">if</span> (catalogManager.schemaExists(name)) &#123;</span><br><span class="line">			<span class="keyword">return</span> <span class="keyword">new</span> CatalogCalciteSchema(name, catalogManager, isStreamingMode);</span><br><span class="line">		&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">			<span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CatalogCalciteSchema</span> <span class="keyword">extends</span> <span class="title">FlinkSchema</span> </span>&#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Schema <span class="title">getSubSchema</span><span class="params">(String schemaName)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (catalogManager.schemaExists(catalogName, schemaName)) &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">new</span> DatabasecalciteSchema(schemaName, catalogNmae, catalogManager, isStreamingMode);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">DatabaseCalciteSchema</span> <span class="keyword">extends</span> <span class="title">FlinkSchema</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> String databaseName;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> String catalogName;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> CatalogManager catalogManager;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Table <span class="title">getTable</span><span class="params">(String tableName)</span> </span>&#123;</span><br><span class="line">		ObjectIdentifier identifier = ObjectIdentifier.of(catalogName, databaseName, tableName);</span><br><span class="line">		<span class="keyword">return</span> catalogManager.getTable(identifier)</span><br><span class="line">			.map(result -&gt; &#123;</span><br><span class="line">				CatalogBaseTable table = result.getTable();</span><br><span class="line">				FlinkStatistic statistic = getStatistic(result.isTemporary(), table, identifier);</span><br><span class="line">				<span class="keyword">return</span> <span class="keyword">new</span> CatalogSchemaTable(identifier,</span><br><span class="line">					table,</span><br><span class="line">					statistic,</span><br><span class="line">					catalogManager.getCatalog(catalogName)</span><br><span class="line">						.flatMap(Catalog::getTableFactory)</span><br><span class="line">						.orElse(<span class="keyword">null</span>),</span><br><span class="line">					isStreamingMode,</span><br><span class="line">					result.isTemporary());</span><br><span class="line">			&#125;)</span><br><span class="line">			.orElse(<span class="keyword">null</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Schema <span class="title">getSubSchema</span><span class="params">(String name)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>很容易发现，CatalogSchema返回DatabaseSchema， DatabaseSchema返回Table,<br>这样就容易理解，Flink的三层结构是怎样的了. 同时， 具体的元数据实际上都是在catalogManager中。</p>
<p>DatabaseSchema中返回的Table类型为CatalogSchemaTable，我们来看下具体的结结构是怎样的，<br>上文中也提到了，Table接口主为getRowType函数， 用于返回某个table的type信息。<br>TableSchema是Flink内部用于保存各个字段的类型信息的类， 通过相关的转化函数，转换为<strong>calcite</strong>的type类型.</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CatalogSchemaTable</span> <span class="keyword">extends</span> <span class="title">AbstractTable</span> <span class="keyword">implements</span> <span class="title">TemporalTable</span> </span>&#123;</span><br><span class="line">    </span><br><span class="line">	<span class="keyword">private</span> <span class="keyword">final</span> ObjectIdentifier tableIdentifier;</span><br><span class="line">	<span class="keyword">private</span> <span class="keyword">final</span> CatalogBaseTable catalogBaseTable;</span><br><span class="line">	<span class="keyword">private</span> <span class="keyword">final</span> FlinkStatistic statistic;</span><br><span class="line">	<span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">boolean</span> isStreamingMode;</span><br><span class="line">	<span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">boolean</span> isTemporary;</span><br><span class="line">    ...</span><br><span class="line">	<span class="function"><span class="keyword">private</span> <span class="keyword">static</span> RelDataType <span class="title">getRowType</span><span class="params">(RelDataTypeFactory typeFactory,</span></span></span><br><span class="line"><span class="function"><span class="params">			CatalogBaseTable catalogBaseTable,</span></span></span><br><span class="line"><span class="function"><span class="params">			<span class="keyword">boolean</span> isStreamingMode)</span> </span>&#123;</span><br><span class="line">		<span class="keyword">final</span> FlinkTypeFactory flinkTypeFactory = (FlinkTypeFactory) typeFactory;</span><br><span class="line">		TableSchema tableSchema = catalogBaseTable.getSchema();</span><br><span class="line">		<span class="keyword">final</span> DataType[] fieldDataTypes = tableSchema.getFieldDataTypes();</span><br><span class="line">		<span class="keyword">if</span> (!isStreamingMode</span><br><span class="line">			&amp;&amp; catalogBaseTable <span class="keyword">instanceof</span> ConnectorCatalogTable</span><br><span class="line">			&amp;&amp; ((ConnectorCatalogTable) catalogBaseTable).getTableSource().isPresent()) &#123;</span><br><span class="line">			<span class="comment">// If the table source is bounded, materialize the time attributes to normal TIMESTAMP type.</span></span><br><span class="line">			<span class="comment">// Now for ConnectorCatalogTable, there is no way to</span></span><br><span class="line">			<span class="comment">// deduce if it is bounded in the table environment, so the data types in TableSchema</span></span><br><span class="line">			<span class="comment">// always patched with TimeAttribute.</span></span><br><span class="line">			<span class="comment">// See ConnectorCatalogTable#calculateSourceSchema</span></span><br><span class="line">			<span class="comment">// for details.</span></span><br><span class="line"></span><br><span class="line">			<span class="comment">// Remove the patched time attributes type to let the TableSourceTable handle it.</span></span><br><span class="line">			<span class="comment">// We should remove this logic if the isBatch flag in ConnectorCatalogTable is fixed.</span></span><br><span class="line">			<span class="comment">// <span class="doctag">TODO:</span> Fix FLINK-14844.</span></span><br><span class="line">			<span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; fieldDataTypes.length; i++) &#123;</span><br><span class="line">				LogicalType lt = fieldDataTypes[i].getLogicalType();</span><br><span class="line">				<span class="keyword">if</span> (lt <span class="keyword">instanceof</span> TimestampType</span><br><span class="line">					&amp;&amp; (((TimestampType) lt).getKind() == TimestampKind.PROCTIME</span><br><span class="line">					|| ((TimestampType) lt).getKind() == TimestampKind.ROWTIME)) &#123;</span><br><span class="line">					<span class="keyword">int</span> precision = ((TimestampType) lt).getPrecision();</span><br><span class="line">					fieldDataTypes[i] = DataTypes.TIMESTAMP(precision);</span><br><span class="line">				&#125;</span><br><span class="line">			&#125;</span><br><span class="line">		&#125;</span><br><span class="line">		<span class="keyword">return</span> TableSourceUtil.getSourceRowType(flinkTypeFactory,</span><br><span class="line">			tableSchema,</span><br><span class="line">			scala.Option.empty(),</span><br><span class="line">			isStreamingMode);</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>CatalogBaseTable接口定义如下， Flink的Table的参数(<strong>schema</strong>参数,connector参数)都可以最终表示为一个map.</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">CatalogBaseTable</span> </span>&#123;</span><br><span class="line">	<span class="comment">/**</span></span><br><span class="line"><span class="comment">	 * Get the properties of the table.</span></span><br><span class="line"><span class="comment">	 *</span></span><br><span class="line"><span class="comment">	 * <span class="doctag">@return</span> property map of the table/view</span></span><br><span class="line"><span class="comment">	 */</span></span><br><span class="line">	<span class="function">Map&lt;String, String&gt; <span class="title">getProperties</span><span class="params">()</span></span>;</span><br><span class="line"></span><br><span class="line">	<span class="comment">/**</span></span><br><span class="line"><span class="comment">	 * Get the schema of the table.</span></span><br><span class="line"><span class="comment">	 *</span></span><br><span class="line"><span class="comment">	 * <span class="doctag">@return</span> schema of the table/view.</span></span><br><span class="line"><span class="comment">	 */</span></span><br><span class="line">	<span class="function">TableSchema <span class="title">getSchema</span><span class="params">()</span></span>;</span><br><span class="line"></span><br><span class="line">	<span class="comment">/**</span></span><br><span class="line"><span class="comment">	 * Get comment of the table or view.</span></span><br><span class="line"><span class="comment">	 *</span></span><br><span class="line"><span class="comment">	 * <span class="doctag">@return</span> comment of the table/view.</span></span><br><span class="line"><span class="comment">	 */</span></span><br><span class="line">	<span class="function">String <span class="title">getComment</span><span class="params">()</span></span>;</span><br><span class="line"></span><br><span class="line">	<span class="comment">/**</span></span><br><span class="line"><span class="comment">	 * Get a deep copy of the CatalogBaseTable instance.</span></span><br><span class="line"><span class="comment">	 *</span></span><br><span class="line"><span class="comment">	 * <span class="doctag">@return</span> a copy of the CatalogBaseTable instance</span></span><br><span class="line"><span class="comment">	 */</span></span><br><span class="line">	<span class="function">CatalogBaseTable <span class="title">copy</span><span class="params">()</span></span>;</span><br><span class="line"></span><br><span class="line">	<span class="comment">/**</span></span><br><span class="line"><span class="comment">	 * Get a brief description of the table or view.</span></span><br><span class="line"><span class="comment">	 *</span></span><br><span class="line"><span class="comment">	 * <span class="doctag">@return</span> an optional short description of the table/view</span></span><br><span class="line"><span class="comment">	 */</span></span><br><span class="line">	<span class="function">Optional&lt;String&gt; <span class="title">getDescription</span><span class="params">()</span></span>;</span><br><span class="line"></span><br><span class="line">	<span class="comment">/**</span></span><br><span class="line"><span class="comment">	 * Get a detailed description of the table or view.</span></span><br><span class="line"><span class="comment">	 *</span></span><br><span class="line"><span class="comment">	 * <span class="doctag">@return</span> an optional long description of the table/view</span></span><br><span class="line"><span class="comment">	 */</span></span><br><span class="line">	<span class="function">Optional&lt;String&gt; <span class="title">getDetailedDescription</span><span class="params">()</span></span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="FlinkSchema的使用"><a href="#FlinkSchema的使用" class="headerlink" title="FlinkSchema的使用"></a>FlinkSchema的使用</h2><p>上面都是的相关接口都是Flink用于适配<strong>calcite</strong>框架元数据的相关实现。<br>那么这些类具体是在哪里调用的？ 已经什么时候会被调用到？<br><strong>calcite</strong>中的<strong>schema</strong>，主要是在validate过程中， 获得对应table的字段信息， 对应的function的返回值信息，<br>确保SQL的字段名， 字段类型是正确的.<br>类的依赖关系为:<br>validator —&gt; schemaReader —&gt; <strong>schema</strong></p>
<p>FlinkPlannerImpl.scala中</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> def <span class="title">createSqlValidator</span><span class="params">(catalogReader: CatalogReader)</span> </span>= &#123;</span><br><span class="line">  val validator = <span class="keyword">new</span> FlinkCalciteSqlValidator(</span><br><span class="line">    operatorTable,</span><br><span class="line">    catalogReader,</span><br><span class="line">    typeFactory)</span><br><span class="line">  validator.setIdentifierExpansion(<span class="keyword">true</span>)</span><br><span class="line">  <span class="comment">// Disable implicit type coercion for now.</span></span><br><span class="line">  validator.setEnableTypeCoercion(<span class="keyword">false</span>)</span><br><span class="line">  validator</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>PlanningConfigurationBuilder.java</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> CatalogReader <span class="title">createCatalogReader</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">		<span class="keyword">boolean</span> lenientCaseSensitivity,</span></span></span><br><span class="line"><span class="function"><span class="params">		String currentCatalog,</span></span></span><br><span class="line"><span class="function"><span class="params">		String currentDatabase)</span> </span>&#123;</span><br><span class="line">	SqlParser.Config sqlParserConfig = getSqlParserConfig();</span><br><span class="line">	<span class="keyword">final</span> <span class="keyword">boolean</span> caseSensitive;</span><br><span class="line">	<span class="keyword">if</span> (lenientCaseSensitivity) &#123;</span><br><span class="line">		caseSensitive = <span class="keyword">false</span>;</span><br><span class="line">	&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">		caseSensitive = sqlParserConfig.caseSensitive();</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	SqlParser.Config parserConfig = SqlParser.configBuilder(sqlParserConfig)</span><br><span class="line">		.setCaseSensitive(caseSensitive)</span><br><span class="line">		.build();</span><br><span class="line"></span><br><span class="line">	<span class="keyword">return</span> <span class="keyword">new</span> CatalogReader(</span><br><span class="line">		rootSchema,</span><br><span class="line">		asList(</span><br><span class="line">			asList(currentCatalog, currentDatabase),</span><br><span class="line">			singletonList(currentCatalog)</span><br><span class="line">		),</span><br><span class="line">		typeFactory,</span><br><span class="line">		CalciteConfig.connectionConfig(parserConfig));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>综上所诉， 我们就知道了Flink是如何来利用<strong>calcite</strong>的<strong>schema</strong>来管理Flink的table信息的.</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://example.com/bigdata/Flink/Flink-checkpoint%E8%AF%8A%E6%96%AD/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="aaronzhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Guadazi">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/bigdata/Flink/Flink-checkpoint%E8%AF%8A%E6%96%AD/" class="post-title-link" itemprop="url">Untitled</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-06-01 19:57:43" itemprop="dateCreated datePublished" datetime="2021-06-01T19:57:43+08:00">2021-06-01</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="Flink-checkpoint诊断"><a href="#Flink-checkpoint诊断" class="headerlink" title="Flink-checkpoint诊断"></a>Flink-checkpoint诊断</h1>
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://example.com/bigdata/Flink/Flink-SQL-TableFactory/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="aaronzhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Guadazi">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/bigdata/Flink/Flink-SQL-TableFactory/" class="post-title-link" itemprop="url">Untitled</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-06-01 19:57:43" itemprop="dateCreated datePublished" datetime="2021-06-01T19:57:43+08:00">2021-06-01</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="Flink-SQL-TableFactory"><a href="#Flink-SQL-TableFactory" class="headerlink" title="Flink SQL TableFactory"></a>Flink SQL TableFactory</h1>
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/"><i class="fa fa-angle-left" aria-label="Previous page"></i></a><a class="page-number" href="/">1</a><span class="page-number current">2</span><a class="page-number" href="/page/3/">3</a><span class="space">&hellip;</span><a class="page-number" href="/page/25/">25</a><a class="extend next" rel="next" href="/page/3/"><i class="fa fa-angle-right" aria-label="Next page"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">aaronzhang</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">242</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">16</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">128</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">aaronzhang</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  

</body>
</html>
