<!DOCTYPE html>
<html lang="zh-Hans">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta property="og:type" content="website">
<meta property="og:title" content="Guadazi">
<meta property="og:url" content="http://example.com/page/4/index.html">
<meta property="og:site_name" content="Guadazi">
<meta property="og:locale">
<meta property="article:author" content="aaronzhang">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://example.com/page/4/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'zh-Hans'
  };
</script>

  <title>Guadazi</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Guadazi</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://example.com/bigdata/Flink/Flink-sink/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="aaronzhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Guadazi">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/bigdata/Flink/Flink-sink/" class="post-title-link" itemprop="url">Flink-sink</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-04-11 14:58:00" itemprop="dateCreated datePublished" datetime="2021-04-11T14:58:00+08:00">2021-04-11</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-04-23 13:04:00" itemprop="dateModified" datetime="2021-04-23T13:04:00+08:00">2021-04-23</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/bigdata/" itemprop="url" rel="index"><span itemprop="name">bigdata</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="Sink的三种模式"><a href="#Sink的三种模式" class="headerlink" title="Sink的三种模式"></a>Sink的三种模式</h2><p><a target="_blank" rel="noopener" href="http://www.whitewood.me/2020/02/26/Flink-Table-%E7%9A%84%E4%B8%89%E7%A7%8D-Sink-%E6%A8%A1%E5%BC%8F/">Flink table的三种sink模式</a></p>
<p>Sink有INSERT、UPDATE 和 DELETE 三类，Table的sink模式有append、upsert和retract三种</p>
<table>
<thead>
<tr>
<th>Sink模式</th>
<th>Insert</th>
<th>Update</th>
<th>Delete</th>
<th>支持的存储</th>
</tr>
</thead>
<tbody><tr>
<td>Append(追加)</td>
<td>支持</td>
<td>不支持</td>
<td>不支持</td>
<td></td>
</tr>
<tr>
<td>Upsert(重复时更新)</td>
<td>支持</td>
<td>支持</td>
<td>支持</td>
<td>KV: HBase JDBC</td>
</tr>
<tr>
<td>Retract(允许撤销)</td>
<td>支持</td>
<td>支持</td>
<td>支持</td>
<td></td>
</tr>
</tbody></table>
<h3 id="Upsert模式与Retract模式的区别"><a href="#Upsert模式与Retract模式的区别" class="headerlink" title="Upsert模式与Retract模式的区别"></a>Upsert模式与Retract模式的区别</h3><p>Upsert模式需要唯一的key来传递更新消息，外部连接器需要明确知道这个唯一key的属性</p>
<p>Upsert模式和Retract模式</p>
<p>消息: 都为(Boolean,Row)二元组, 第一个元素代表操作类型:</p>
<table>
<thead>
<tr>
<th>模式</th>
<th>操作类型</th>
<th>插入</th>
<th>更新</th>
<th>删除</th>
</tr>
</thead>
<tbody><tr>
<td>Upsert模式</td>
<td>true 为 UPSERT消息(不存在则INSERT, 存在则UPDATE)<br/> false 为 DELETE消息</td>
<td>upsert消息</td>
<td>upsert消息</td>
<td>delete消息</td>
</tr>
<tr>
<td>Retract模式</td>
<td>true为添加消息<br/>false为撤回消息</td>
<td>添加消息</td>
<td>已更新行(上一行)的撤回消息<br/>更新行(新行)的添加消息</td>
<td>撤回消息</td>
</tr>
</tbody></table>
<h3 id="Append模式-窗口聚合中的应用"><a href="#Append模式-窗口聚合中的应用" class="headerlink" title="Append模式-窗口聚合中的应用"></a>Append模式-窗口聚合中的应用</h3><p>在实时聚合统计中，聚合统计的结果输出是由 Trigger 决定的，而 Append-Only 则意味着对于每个窗口实例（Pane，窗格）Trigger 只能触发一次，则就导致无法在迟到数据到达时再刷新结果。</p>
<p>通常来说，我们可以给 Watermark 设置一个较大的延迟容忍阈值来避免这种刷新（再有迟到数据则丢弃），但代价是却会引入较大的延迟。</p>
<h3 id="Upsert模式"><a href="#Upsert模式" class="headerlink" title="Upsert模式"></a>Upsert模式</h3><p>支持 Append-Only 的操作和在有主键的前提下的 Update 和 Delete 操作.</p>
<p><strong>重复时更新</strong></p>
<p>Upsert 模式依赖业务主键来实现输出结果的更新和删除，因此非常适合 KV 数据库，比如 HBase、JDBC 的 TableSink 都使用了这种方式。</p>
<p>Upsert 模式是目前来说比较实用的模式，因为大部分业务都会提供原子或复合类型的主键，而在支持 KV 的存储系统也非常多，但要注意的是不要变更主键，具体原因会在下一节谈到。</p>
<h3 id="Retract模式"><a href="#Retract模式" class="headerlink" title="Retract模式"></a>Retract模式</h3><p><strong>允许撤销</strong></p>
<p>举个例子，假设我们将电商订单按照承运快递公司进行分类计数，有如下的结果表。</p>
<table>
<thead>
<tr>
<th align="left">公司</th>
<th align="left">订单数</th>
</tr>
</thead>
<tbody><tr>
<td align="left">中通</td>
<td align="left">2</td>
</tr>
<tr>
<td align="left">圆通</td>
<td align="left">1</td>
</tr>
<tr>
<td align="left">顺丰</td>
<td align="left">3</td>
</tr>
</tbody></table>
<p>那么如果原本一单为中通的快递，后续更新为用顺丰发货，对于 Upsert 模式会产生 <code>(true, (顺丰, 4))</code> 这样一条 changelog，但中通的订单数没有被修正。相比之下，Retract 模式产出 <code>(false, (中通, 1))</code> 和 <code>(true, (顺丰, 1))</code> 两条数据，则可以正确地更新数据。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://example.com/bigdata/hive/hive-sample/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="aaronzhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Guadazi">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/bigdata/hive/hive-sample/" class="post-title-link" itemprop="url">Hive SQL优化样例</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-03-12 00:00:00" itemprop="dateCreated datePublished" datetime="2021-03-12T00:00:00+08:00">2021-03-12</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-04-23 13:04:00" itemprop="dateModified" datetime="2021-04-23T13:04:00+08:00">2021-04-23</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/BigData/" itemprop="url" rel="index"><span itemprop="name">BigData</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="comment">/*+ MAPJOIN(hj3,hj1,hj0)*/</span> <span class="number">20210324</span> <span class="keyword">as</span> fdate, TO_CHAR(SYSTIMESTAMP(), <span class="string">&#x27;yyyymmddhh24miss&#x27;</span>) <span class="keyword">as</span> fetl_time,hj2.fuin <span class="keyword">as</span> fuin,<span class="built_in">COUNT</span>(<span class="keyword">DISTINCT</span>(SUBSTR(<span class="built_in">CAST</span>(if (((hj2.fevent_time <span class="keyword">between</span> <span class="number">20210318000000</span> <span class="keyword">and</span> <span class="number">20210324235959</span>) <span class="keyword">and</span> (hj2.fscn <span class="keyword">in</span> (<span class="string">&#x27;1&#x27;</span>))) , hj2.fevent_time, <span class="keyword">null</span>) <span class="keyword">AS</span> STRING),<span class="number">1</span>,<span class="number">8</span>))) <span class="keyword">as</span> flabel_25942,   <span class="built_in">COUNT</span>(<span class="keyword">DISTINCT</span>(SUBSTR(<span class="built_in">CAST</span>(if (((hj2.fevent_time <span class="keyword">between</span> <span class="number">20210318000000</span> <span class="keyword">and</span> <span class="number">20210324235959</span>)  <span class="keyword">and</span> (hj2.fscn <span class="keyword">in</span> (<span class="string">&#x27;1&#x27;</span>))) , hj2.fevent_time, <span class="keyword">null</span>) <span class="keyword">AS</span> STRING),<span class="number">1</span>,<span class="number">8</span>))) <span class="keyword">as</span> flabel_25970, <span class="built_in">COUNT</span>(<span class="keyword">DISTINCT</span>(SUBSTR(<span class="built_in">CAST</span>(if (((hj2.furl_orig <span class="operator">=</span> <span class="string">&#x27;/mb/v5/fund/list/steady.shtml&#x27;</span>)  <span class="keyword">or</span> (hj2.furl_orig <span class="operator">=</span> <span class="string">&#x27;/mb/v4/fundlist/fund_all.shtml&#x27;</span>)    <span class="keyword">or</span> (hj2.furl_orig <span class="operator">=</span> <span class="string">&#x27;/mb/v4/fundlist/fund_all_v5.shtml&#x27;</span>))  <span class="keyword">and</span> ((hj2.fevent_time <span class="keyword">between</span> <span class="number">20210318000000</span> <span class="keyword">and</span> <span class="number">20210324235959</span>)) , hj2.fevent_time, <span class="keyword">null</span>) <span class="keyword">AS</span> STRING),<span class="number">1</span>,<span class="number">8</span>))) <span class="keyword">as</span> flabel_25972</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> dml_base::dml_evt_lct_cft_label_factory_mta_access_dd hj2</span><br><span class="line"></span><br><span class="line"><span class="keyword">left</span> <span class="keyword">join</span> dim_base::dim_lct_hq_beacon_report_manage hj1 <span class="keyword">on</span> if(hj2.furl_orig <span class="keyword">is</span> <span class="keyword">null</span></span><br><span class="line">                                                              <span class="keyword">or</span> <span class="built_in">trim</span>(hj2.furl_orig) <span class="operator">=</span> <span class="string">&#x27;&#x27;</span>, concat(<span class="string">&#x27;null_&#x27;</span>, <span class="built_in">floor</span>(rand() <span class="operator">*</span> <span class="number">10000</span>)), hj2.furl_orig) <span class="operator">=</span> hj1.fevent_code</span><br><span class="line"></span><br><span class="line"><span class="keyword">left</span> <span class="keyword">join</span> dim_base::dim_lct_hq_virtual_event_info hj3 <span class="keyword">on</span> if(hj2.furl_orig <span class="keyword">is</span> <span class="keyword">null</span></span><br><span class="line">                                                      <span class="keyword">or</span> <span class="built_in">trim</span>(hj2.furl_orig) <span class="operator">=</span> <span class="string">&#x27;&#x27;</span>, concat(<span class="string">&#x27;null_&#x27;</span>, <span class="built_in">floor</span>(rand() <span class="operator">*</span> <span class="number">10000</span>)), hj2.furl_orig) <span class="operator">=</span> hj3.fevent_code</span><br><span class="line"><span class="keyword">left</span> <span class="keyword">join</span> dim_base::dim_prd_lct_cft_fund_type_conf hj0 <span class="keyword">on</span> if(hj2.fspid_fundcode <span class="keyword">is</span> <span class="keyword">null</span></span><br><span class="line">                                                             <span class="keyword">or</span> <span class="built_in">trim</span>(hj2.fspid_fundcode) <span class="operator">=</span> <span class="string">&#x27;&#x27;</span>, concat(<span class="string">&#x27;null_&#x27;</span>, <span class="built_in">floor</span>(rand() <span class="operator">*</span> <span class="number">10000</span>)), hj2.fspid_fundcode) <span class="operator">=</span> hj0.fspid_fundcode</span><br><span class="line"></span><br><span class="line"><span class="keyword">where</span> hj2.fuin <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">null</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">and</span> <span class="built_in">trim</span>(hj2.fuin) <span class="operator">&lt;&gt;</span> <span class="string">&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">and</span> hj2.fdate <span class="operator">&gt;=</span> <span class="number">20210318</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">and</span> hj2.fdate <span class="operator">&lt;=</span> <span class="number">20210324</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> hj2.fuin</span><br></pre></td></tr></table></figure>




<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br></pre></td><td class="code"><pre><span class="line">ABSTRACT SYNTAX TREE:</span><br><span class="line">  (TOK_QUERY (TOK_FROM (TOK_LEFTOUTERJOIN (TOK_LEFTOUTERJOIN (TOK_LEFTOUTERJOIN (TOK_TABREF (TOK_TAB dml_evt_lct_cft_label_factory_mta_access_dd dml_base) hj2) (TOK_TABREF (TOK_TAB dim_lct_hq_beacon_report_manage dim_base) hj1) (= (<span class="function">TOK_FUNCTION <span class="title">if</span> <span class="params">(or (TOK_FUNCTION TOK_ISNULL (. (TOK_TABLE_OR_COL hj2)</span> furl_orig)) <span class="params">(= (TOK_FUNCTION trim (. (TOK_TABLE_OR_COL hj2)</span> furl_orig)) &#x27;&#x27;)) <span class="params">(TOK_FUNCTION concat <span class="string">&#x27;null_&#x27;</span> (TOK_FUNCTION floor (* (TOK_FUNCTION rand)</span> 10000))) <span class="params">(. (TOK_TABLE_OR_COL hj2)</span> furl_orig)) <span class="params">(. (TOK_TABLE_OR_COL hj1)</span> fevent_code))) <span class="params">(TOK_TABREF (TOK_TAB dim_lct_hq_virtual_event_info dim_base)</span> hj3) <span class="params">(= (TOK_FUNCTION <span class="keyword">if</span> (or (TOK_FUNCTION TOK_ISNULL (. (TOK_TABLE_OR_COL hj2)</span> furl_orig)) <span class="params">(= (TOK_FUNCTION trim (. (TOK_TABLE_OR_COL hj2)</span> furl_orig)) &#x27;&#x27;)) <span class="params">(TOK_FUNCTION concat <span class="string">&#x27;null_&#x27;</span> (TOK_FUNCTION floor (* (TOK_FUNCTION rand)</span> 10000))) <span class="params">(. (TOK_TABLE_OR_COL hj2)</span> furl_orig)) <span class="params">(. (TOK_TABLE_OR_COL hj3)</span> fevent_code))) <span class="params">(TOK_TABREF (TOK_TAB dim_prd_lct_cft_fund_type_conf dim_base)</span> hj0) <span class="params">(= (TOK_FUNCTION <span class="keyword">if</span> (or (TOK_FUNCTION TOK_ISNULL (. (TOK_TABLE_OR_COL hj2)</span> fspid_fundcode)) <span class="params">(= (TOK_FUNCTION trim (. (TOK_TABLE_OR_COL hj2)</span> fspid_fundcode)) &#x27;&#x27;)) <span class="params">(TOK_FUNCTION concat <span class="string">&#x27;null_&#x27;</span> (TOK_FUNCTION floor (* (TOK_FUNCTION rand)</span> 10000))) <span class="params">(. (TOK_TABLE_OR_COL hj2)</span> fspid_fundcode)) <span class="params">(. (TOK_TABLE_OR_COL hj0)</span> fspid_fundcode)))) <span class="params">(TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)</span>) <span class="params">(TOK_SELECT (TOK_HINTLIST (TOK_HINT TOK_MAPJOIN (TOK_HINTARGLIST (TOK_TABLE_OR_COL hj3)</span> <span class="params">(TOK_TABLE_OR_COL hj1)</span> <span class="params">(TOK_TABLE_OR_COL hj0)</span>))) <span class="params">(TOK_SELEXPR <span class="number">20210324</span> fdate)</span> <span class="params">(TOK_SELEXPR (TOK_FUNCTION TO_CHAR (TOK_FUNCTION SYSTIMESTAMP)</span> &#x27;yyyymmddhh24miss&#x27;) fetl_time) <span class="params">(TOK_SELEXPR (. (TOK_TABLE_OR_COL hj2)</span> fuin) fuin) <span class="params">(TOK_SELEXPR (TOK_FUNCTIONDI COUNT (TOK_FUNCTION SUBSTR (TOK_FUNCTION TOK_STRING (TOK_FUNCTION <span class="keyword">if</span> (and (and (&gt;= (. (TOK_TABLE_OR_COL hj2)</span> fevent_time) 20210318000000) <span class="params">(&lt;= (. (TOK_TABLE_OR_COL hj2)</span> fevent_time) 20210324235959)) <span class="params">(in (. (TOK_TABLE_OR_COL hj2)</span> fscn) &#x27;1&#x27;)) <span class="params">(. (TOK_TABLE_OR_COL hj2)</span> fevent_time) TOK_NULL)) 1 8)) flabel_25942) <span class="params">(TOK_SELEXPR (TOK_FUNCTIONDI COUNT (TOK_FUNCTION SUBSTR (TOK_FUNCTION TOK_STRING (TOK_FUNCTION <span class="keyword">if</span> (and (and (&gt;= (. (TOK_TABLE_OR_COL hj2)</span> fevent_time) 20210318000000) <span class="params">(&lt;= (. (TOK_TABLE_OR_COL hj2)</span> fevent_time) 20210324235959)) <span class="params">(in (. (TOK_TABLE_OR_COL hj2)</span> fscn) &#x27;1&#x27;)) <span class="params">(. (TOK_TABLE_OR_COL hj2)</span> fevent_time) TOK_NULL)) 1 8)) flabel_25970) <span class="params">(TOK_SELEXPR (TOK_FUNCTIONDI COUNT (TOK_FUNCTION SUBSTR (TOK_FUNCTION TOK_STRING (TOK_FUNCTION <span class="keyword">if</span> (and (or (or (= (. (TOK_TABLE_OR_COL hj2)</span> furl_orig) &#x27;/mb/v5/fund/list/steady.shtml&#x27;) <span class="params">(= (. (TOK_TABLE_OR_COL hj2)</span> furl_orig) &#x27;/mb/v4/fundlist/fund_all.shtml&#x27;)) <span class="params">(= (. (TOK_TABLE_OR_COL hj2)</span> furl_orig) &#x27;/mb/v4/fundlist/fund_all_v5.shtml&#x27;)) <span class="params">(and (&gt;= (. (TOK_TABLE_OR_COL hj2)</span> fevent_time) 20210318000000) <span class="params">(&lt;= (. (TOK_TABLE_OR_COL hj2)</span> fevent_time) 20210324235959))) <span class="params">(. (TOK_TABLE_OR_COL hj2)</span> fevent_time) TOK_NULL)) 1 8)) flabel_25972)) <span class="params">(TOK_WHERE (and (and (and (TOK_FUNCTION TOK_ISNOTNULL (. (TOK_TABLE_OR_COL hj2)</span> fuin)) <span class="params">(&lt;&gt; (TOK_FUNCTION trim (. (TOK_TABLE_OR_COL hj2)</span> fuin)) &#x27;&#x27;)) <span class="params">(&gt;= (. (TOK_TABLE_OR_COL hj2)</span> fdate) 20210318)) <span class="params">(&lt;= (. (TOK_TABLE_OR_COL hj2)</span> fdate) 20210324))) <span class="params">(TOK_GROUPBY (. (TOK_TABLE_OR_COL hj2)</span> fuin))))</span></span><br><span class="line"><span class="function"></span></span><br><span class="line"><span class="function">STAGE DEPENDENCIES:</span></span><br><span class="line"><span class="function">  Stage-1</span></span><br><span class="line"><span class="function">    type:root stage</span>;</span><br><span class="line">  Stage-<span class="number">2</span></span><br><span class="line">    type:;depends on:Stage-<span class="number">1</span>;</span><br><span class="line">  Stage-<span class="number">3</span></span><br><span class="line">    type:;depends on:Stage-<span class="number">2</span>;</span><br><span class="line">  Stage-<span class="number">0</span></span><br><span class="line">    type:root stage;</span><br><span class="line"></span><br><span class="line">STAGE PLANS:</span><br><span class="line">  Stage: Stage-<span class="number">1</span></span><br><span class="line">    Map Reduce</span><br><span class="line">      Alias -&gt; Map Operator Tree:</span><br><span class="line">        dml_base/dml_evt_lct_cft_label_factory_mta_access_dd#hj2 </span><br><span class="line">          Operator:          TableScan</span><br><span class="line">            alias: dml_base/dml_evt_lct_cft_label_factory_mta_access_dd#hj2</span><br><span class="line">            Operator:            Filter Operator</span><br><span class="line">              predicate:</span><br><span class="line">                  expr: (((<span class="function">fuin is not <span class="keyword">null</span> <span class="title">and</span> <span class="params">(trim(fuin)</span> &lt;&gt; &#x27;&#x27;)) <span class="title">and</span> <span class="params">(fdate &gt;= <span class="number">20210318</span>)</span>) <span class="title">and</span> <span class="params">(fdate &lt;= <span class="number">20210324</span>)</span>)</span></span><br><span class="line"><span class="function">                  type: <span class="keyword">boolean</span></span></span><br><span class="line"><span class="function">              Operator:              Common Join Operator</span></span><br><span class="line"><span class="function">                condition map:</span></span><br><span class="line"><span class="function">                     Left Outer Join0 to 1</span></span><br><span class="line"><span class="function">                     Left Outer Join0 to 2</span></span><br><span class="line"><span class="function">                condition expressions:</span></span><br><span class="line"><span class="function">                  0 </span>&#123;fdate&#125; &#123;fuin&#125; &#123;furl_orig&#125; &#123;fevent_time&#125; &#123;fspid_fundcode&#125; &#123;fscn&#125;</span><br><span class="line">                  <span class="number">1</span> </span><br><span class="line">                  <span class="number">2</span> </span><br><span class="line">                handleSkewJoin: <span class="keyword">false</span></span><br><span class="line">                keys:</span><br><span class="line">                  0 [class com.tencent.tdw_udf_cloud.hive.udf.generic.GenericUDFIf(class org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPOr(class org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPNull(Column[furl_orig](), class org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPEqual(class org.apache.hadoop.hive.ql.udf.generic.GenericUDFBridge(Column[furl_orig](), Const string ()(), class org.apache.hadoop.hive.ql.udf.generic.GenericUDFBridge(Const string null_, class org.apache.hadoop.hive.ql.udf.generic.GenericUDFBridge(class org.apache.hadoop.hive.ql.udf.generic.GenericUDFBridge(class org.apache.hadoop.hive.ql.udf.generic.GenericUDFBridge((), Const int 10000()()(), Column[furl_orig]()]</span><br><span class="line">                  <span class="number">1</span> [Column[fevent_code]]</span><br><span class="line">                  <span class="number">2</span> [Column[fevent_code]]</span><br><span class="line">                outputColumnNames: _col0, _col42, _col85, _col88, _col103, _col104</span><br><span class="line">                Position of Big Table: <span class="number">0</span></span><br><span class="line">                Operator:                File Output Operator</span><br><span class="line">                  compressed: <span class="keyword">false</span></span><br><span class="line">                  GlobalTableId: <span class="number">0</span></span><br><span class="line">                  table:</span><br><span class="line">                    table descs</span><br><span class="line">                      input format: org.apache.hadoop.mapred.SequenceFileInputFormat</span><br><span class="line">                      output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat</span><br><span class="line">      Local Work:</span><br><span class="line">        Map Reduce Local Work</span><br><span class="line">          Alias -&gt; Map Local Tables:</span><br><span class="line">            dim_base/dim_lct_hq_beacon_report_manage#hj1 </span><br><span class="line">              Fetch Operator</span><br><span class="line">                limit: -<span class="number">1</span></span><br><span class="line">            dim_base/dim_lct_hq_virtual_event_info#hj3 </span><br><span class="line">              Fetch Operator</span><br><span class="line">                limit: -<span class="number">1</span></span><br><span class="line">          Alias -&gt; Map Local Operator Tree:</span><br><span class="line">            dim_base/dim_lct_hq_beacon_report_manage#hj1 </span><br><span class="line">              Operator:              TableScan</span><br><span class="line">                alias: dim_base/dim_lct_hq_beacon_report_manage#hj1</span><br><span class="line">                Operator:                Common Join Operator</span><br><span class="line">                  condition map:</span><br><span class="line">                       Left Outer Join0 to <span class="number">1</span></span><br><span class="line">                       Left Outer Join0 to <span class="number">2</span></span><br><span class="line">                  condition expressions:</span><br><span class="line">                    <span class="number">0</span> &#123;fdate&#125; &#123;fuin&#125; &#123;furl_orig&#125; &#123;fevent_time&#125; &#123;fspid_fundcode&#125; &#123;fscn&#125;</span><br><span class="line">                    <span class="number">1</span> </span><br><span class="line">                    <span class="number">2</span> </span><br><span class="line">                  handleSkewJoin: <span class="keyword">false</span></span><br><span class="line">                  keys:</span><br><span class="line">                    0 [class com.tencent.tdw_udf_cloud.hive.udf.generic.GenericUDFIf(class org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPOr(class org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPNull(Column[furl_orig](), class org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPEqual(class org.apache.hadoop.hive.ql.udf.generic.GenericUDFBridge(Column[furl_orig](), Const string ()(), class org.apache.hadoop.hive.ql.udf.generic.GenericUDFBridge(Const string null_, class org.apache.hadoop.hive.ql.udf.generic.GenericUDFBridge(class org.apache.hadoop.hive.ql.udf.generic.GenericUDFBridge(class org.apache.hadoop.hive.ql.udf.generic.GenericUDFBridge((), Const int 10000()()(), Column[furl_orig]()]</span><br><span class="line">                    <span class="number">1</span> [Column[fevent_code]]</span><br><span class="line">                    <span class="number">2</span> [Column[fevent_code]]</span><br><span class="line">                  outputColumnNames: _col0, _col42, _col85, _col88, _col103, _col104</span><br><span class="line">                  Position of Big Table: <span class="number">0</span></span><br><span class="line">                  Operator:                  File Output Operator</span><br><span class="line">                    compressed: <span class="keyword">false</span></span><br><span class="line">                    GlobalTableId: <span class="number">0</span></span><br><span class="line">                    table:</span><br><span class="line">                      table descs</span><br><span class="line">                        input format: org.apache.hadoop.mapred.SequenceFileInputFormat</span><br><span class="line">                        output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat</span><br><span class="line">            dim_base/dim_lct_hq_virtual_event_info#hj3 </span><br><span class="line">              Operator:              TableScan</span><br><span class="line">                alias: dim_base/dim_lct_hq_virtual_event_info#hj3</span><br><span class="line">                Operator:                Common Join Operator</span><br><span class="line">                  condition map:</span><br><span class="line">                       Left Outer Join0 to <span class="number">1</span></span><br><span class="line">                       Left Outer Join0 to <span class="number">2</span></span><br><span class="line">                  condition expressions:</span><br><span class="line">                    <span class="number">0</span> &#123;fdate&#125; &#123;fuin&#125; &#123;furl_orig&#125; &#123;fevent_time&#125; &#123;fspid_fundcode&#125; &#123;fscn&#125;</span><br><span class="line">                    <span class="number">1</span> </span><br><span class="line">                    <span class="number">2</span> </span><br><span class="line">                  handleSkewJoin: <span class="keyword">false</span></span><br><span class="line">                  keys:</span><br><span class="line">                    0 [class com.tencent.tdw_udf_cloud.hive.udf.generic.GenericUDFIf(class org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPOr(class org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPNull(Column[furl_orig](), class org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPEqual(class org.apache.hadoop.hive.ql.udf.generic.GenericUDFBridge(Column[furl_orig](), Const string ()(), class org.apache.hadoop.hive.ql.udf.generic.GenericUDFBridge(Const string null_, class org.apache.hadoop.hive.ql.udf.generic.GenericUDFBridge(class org.apache.hadoop.hive.ql.udf.generic.GenericUDFBridge(class org.apache.hadoop.hive.ql.udf.generic.GenericUDFBridge((), Const int 10000()()(), Column[furl_orig]()]</span><br><span class="line">                    <span class="number">1</span> [Column[fevent_code]]</span><br><span class="line">                    <span class="number">2</span> [Column[fevent_code]]</span><br><span class="line">                  outputColumnNames: _col0, _col42, _col85, _col88, _col103, _col104</span><br><span class="line">                  Position of Big Table: <span class="number">0</span></span><br><span class="line">                  Operator:                  File Output Operator</span><br><span class="line">                    compressed: <span class="keyword">false</span></span><br><span class="line">                    GlobalTableId: <span class="number">0</span></span><br><span class="line">                    table:</span><br><span class="line">                      table descs</span><br><span class="line">                        input format: org.apache.hadoop.mapred.SequenceFileInputFormat</span><br><span class="line">                        output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat</span><br><span class="line">      Path -&gt; Alias:</span><br><span class="line">        hdfs:<span class="comment">//&lt;hive_hdfs_base_path&gt;/warehouse/dml_base.db/dml_evt_lct_cft_label_factory_mta_access_dd/par_20210318 [dml_base/dml_evt_lct_cft_label_factory_mta_access_dd#hj2]</span></span><br><span class="line">        hdfs:<span class="comment">//&lt;hive_hdfs_base_path&gt;/warehouse/dml_base.db/dml_evt_lct_cft_label_factory_mta_access_dd/par_20210319 [dml_base/dml_evt_lct_cft_label_factory_mta_access_dd#hj2]</span></span><br><span class="line">        hdfs:<span class="comment">//&lt;hive_hdfs_base_path&gt;/warehouse/dml_base.db/dml_evt_lct_cft_label_factory_mta_access_dd/par_20210320 [dml_base/dml_evt_lct_cft_label_factory_mta_access_dd#hj2]</span></span><br><span class="line">        hdfs:<span class="comment">//&lt;hive_hdfs_base_path&gt;/warehouse/dml_base.db/dml_evt_lct_cft_label_factory_mta_access_dd/par_20210321 [dml_base/dml_evt_lct_cft_label_factory_mta_access_dd#hj2]</span></span><br><span class="line">        hdfs:<span class="comment">//&lt;hive_hdfs_base_path&gt;/warehouse/dml_base.db/dml_evt_lct_cft_label_factory_mta_access_dd/par_20210322 [dml_base/dml_evt_lct_cft_label_factory_mta_access_dd#hj2]</span></span><br><span class="line">        hdfs:<span class="comment">//&lt;hive_hdfs_base_path&gt;/warehouse/dml_base.db/dml_evt_lct_cft_label_factory_mta_access_dd/par_20210323 [dml_base/dml_evt_lct_cft_label_factory_mta_access_dd#hj2]</span></span><br><span class="line">        hdfs:<span class="comment">//&lt;hive_hdfs_base_path&gt;/warehouse/dml_base.db/dml_evt_lct_cft_label_factory_mta_access_dd/par_20210324 [dml_base/dml_evt_lct_cft_label_factory_mta_access_dd#hj2]</span></span><br><span class="line"></span><br><span class="line">  Stage: Stage-<span class="number">2</span></span><br><span class="line">    Map Reduce</span><br><span class="line">      Alias -&gt; Map Operator Tree:</span><br><span class="line">        hdfs:<span class="comment">//&lt;tmp_hive_hdfs_base_path&gt;/20210325/&lt;hdfs_user&gt;_tdwadmin_20210325195415346_29895878_7_699597773/10002 </span></span><br><span class="line">          Operator:          Select Operator</span><br><span class="line">            expressions:</span><br><span class="line">                  expr: _col0</span><br><span class="line">                  type: bigint</span><br><span class="line">                  expr: _col42</span><br><span class="line">                  type: string</span><br><span class="line">                  expr: _col85</span><br><span class="line">                  type: string</span><br><span class="line">                  expr: _col88</span><br><span class="line">                  type: bigint</span><br><span class="line">                  expr: _col103</span><br><span class="line">                  type: string</span><br><span class="line">                  expr: _col104</span><br><span class="line">                  type: string</span><br><span class="line">            outputColumnNames: _col0, _col42, _col85, _col88, _col103, _col104</span><br><span class="line">            Operator:            Common Join Operator</span><br><span class="line">              condition map:</span><br><span class="line">                   Left Outer Join0 to <span class="number">1</span></span><br><span class="line">              condition expressions:</span><br><span class="line">                <span class="number">0</span> &#123;_col0&#125; &#123;_col42&#125; &#123;_col85&#125; &#123;_col88&#125; &#123;_col104&#125;</span><br><span class="line">                <span class="number">1</span> </span><br><span class="line">              handleSkewJoin: <span class="keyword">false</span></span><br><span class="line">              keys:</span><br><span class="line">                0 [class com.tencent.tdw_udf_cloud.hive.udf.generic.GenericUDFIf(class org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPOr(class org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPNull(Column[_col103](), class org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPEqual(class org.apache.hadoop.hive.ql.udf.generic.GenericUDFBridge(Column[_col103](), Const string ()(), class org.apache.hadoop.hive.ql.udf.generic.GenericUDFBridge(Const string null_, class org.apache.hadoop.hive.ql.udf.generic.GenericUDFBridge(class org.apache.hadoop.hive.ql.udf.generic.GenericUDFBridge(class org.apache.hadoop.hive.ql.udf.generic.GenericUDFBridge((), Const int 10000()()(), Column[_col103]()]</span><br><span class="line">                <span class="number">1</span> [Column[fspid_fundcode]]</span><br><span class="line">              outputColumnNames: _col0, _col42, _col85, _col88, _col104</span><br><span class="line">              Position of Big Table: <span class="number">0</span></span><br><span class="line">              Operator:              File Output Operator</span><br><span class="line">                compressed: <span class="keyword">false</span></span><br><span class="line">                GlobalTableId: <span class="number">0</span></span><br><span class="line">                table:</span><br><span class="line">                  table descs</span><br><span class="line">                    input format: org.apache.hadoop.mapred.SequenceFileInputFormat</span><br><span class="line">                    output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat</span><br><span class="line">      Local Work:</span><br><span class="line">        Map Reduce Local Work</span><br><span class="line">          Alias -&gt; Map Local Tables:</span><br><span class="line">            dim_base/dim_prd_lct_cft_fund_type_conf#hj0 </span><br><span class="line">              Fetch Operator</span><br><span class="line">                limit: -<span class="number">1</span></span><br><span class="line">          Alias -&gt; Map Local Operator Tree:</span><br><span class="line">            dim_base/dim_prd_lct_cft_fund_type_conf#hj0 </span><br><span class="line">              Operator:              TableScan</span><br><span class="line">                alias: dim_base/dim_prd_lct_cft_fund_type_conf#hj0</span><br><span class="line">                Operator:                Common Join Operator</span><br><span class="line">                  condition map:</span><br><span class="line">                       Left Outer Join0 to <span class="number">1</span></span><br><span class="line">                  condition expressions:</span><br><span class="line">                    <span class="number">0</span> &#123;_col0&#125; &#123;_col42&#125; &#123;_col85&#125; &#123;_col88&#125; &#123;_col104&#125;</span><br><span class="line">                    <span class="number">1</span> </span><br><span class="line">                  handleSkewJoin: <span class="keyword">false</span></span><br><span class="line">                  keys:</span><br><span class="line">                    0 [class com.tencent.tdw_udf_cloud.hive.udf.generic.GenericUDFIf(class org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPOr(class org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPNull(Column[_col103](), class org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPEqual(class org.apache.hadoop.hive.ql.udf.generic.GenericUDFBridge(Column[_col103](), Const string ()(), class org.apache.hadoop.hive.ql.udf.generic.GenericUDFBridge(Const string null_, class org.apache.hadoop.hive.ql.udf.generic.GenericUDFBridge(class org.apache.hadoop.hive.ql.udf.generic.GenericUDFBridge(class org.apache.hadoop.hive.ql.udf.generic.GenericUDFBridge((), Const int 10000()()(), Column[_col103]()]</span><br><span class="line">                    <span class="number">1</span> [Column[fspid_fundcode]]</span><br><span class="line">                  outputColumnNames: _col0, _col42, _col85, _col88, _col104</span><br><span class="line">                  Position of Big Table: <span class="number">0</span></span><br><span class="line">                  Operator:                  File Output Operator</span><br><span class="line">                    compressed: <span class="keyword">false</span></span><br><span class="line">                    GlobalTableId: <span class="number">0</span></span><br><span class="line">                    table:</span><br><span class="line">                      table descs</span><br><span class="line">                        input format: org.apache.hadoop.mapred.SequenceFileInputFormat</span><br><span class="line">                        output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat</span><br><span class="line">      Path -&gt; Alias:</span><br><span class="line">        hdfs:<span class="comment">//&lt;tmp_hive_hdfs_base_path&gt;/20210325/&lt;hdfs_user&gt;_tdwadmin_20210325195415346_29895878_7_699597773/10002 [hdfs://&lt;tmp_hive_hdfs_base_path&gt;/20210325/&lt;hdfs_user&gt;_tdwadmin_20210325195415346_29895878_7_699597773/10002]</span></span><br><span class="line"></span><br><span class="line">  Stage: Stage-<span class="number">3</span></span><br><span class="line">    Map Reduce</span><br><span class="line">      Alias -&gt; Map Operator Tree:</span><br><span class="line">        hdfs:<span class="comment">//&lt;tmp_hive_hdfs_base_path&gt;/20210325/&lt;hdfs_user&gt;_tdwadmin_20210325195415346_29895878_7_699597773/10002 </span></span><br><span class="line">          Operator:          Select Operator</span><br><span class="line">            expressions:</span><br><span class="line">                  expr: _col0</span><br><span class="line">                  type: bigint</span><br><span class="line">                  expr: _col42</span><br><span class="line">                  type: string</span><br><span class="line">                  expr: _col85</span><br><span class="line">                  type: string</span><br><span class="line">                  expr: _col88</span><br><span class="line">                  type: bigint</span><br><span class="line">                  expr: _col103</span><br><span class="line">                  type: string</span><br><span class="line">                  expr: _col104</span><br><span class="line">                  type: string</span><br><span class="line">            outputColumnNames: _col0, _col42, _col85, _col88, _col103, _col104</span><br><span class="line">            Operator:            Common Join Operator</span><br><span class="line">              condition map:</span><br><span class="line">                   Left Outer Join0 to <span class="number">1</span></span><br><span class="line">              condition expressions:</span><br><span class="line">                <span class="number">0</span> &#123;_col0&#125; &#123;_col42&#125; &#123;_col85&#125; &#123;_col88&#125; &#123;_col104&#125;</span><br><span class="line">                <span class="number">1</span> </span><br><span class="line">              handleSkewJoin: <span class="keyword">false</span></span><br><span class="line">              keys:</span><br><span class="line">                0 [class com.tencent.tdw_udf_cloud.hive.udf.generic.GenericUDFIf(class org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPOr(class org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPNull(Column[_col103](), class org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPEqual(class org.apache.hadoop.hive.ql.udf.generic.GenericUDFBridge(Column[_col103](), Const string ()(), class org.apache.hadoop.hive.ql.udf.generic.GenericUDFBridge(Const string null_, class org.apache.hadoop.hive.ql.udf.generic.GenericUDFBridge(class org.apache.hadoop.hive.ql.udf.generic.GenericUDFBridge(class org.apache.hadoop.hive.ql.udf.generic.GenericUDFBridge((), Const int 10000()()(), Column[_col103]()]</span><br><span class="line">                <span class="number">1</span> [Column[fspid_fundcode]]</span><br><span class="line">              outputColumnNames: _col0, _col42, _col85, _col88, _col104</span><br><span class="line">              Position of Big Table: <span class="number">0</span></span><br><span class="line">              Operator:              File Output Operator</span><br><span class="line">                compressed: <span class="keyword">false</span></span><br><span class="line">                GlobalTableId: <span class="number">0</span></span><br><span class="line">                table:</span><br><span class="line">                  table descs</span><br><span class="line">                    input format: org.apache.hadoop.mapred.SequenceFileInputFormat</span><br><span class="line">                    output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat</span><br><span class="line">      Path -&gt; Alias:</span><br><span class="line">        hdfs:<span class="comment">//&lt;tmp_hive_hdfs_base_path&gt;/20210325/&lt;hdfs_user&gt;_tdwadmin_20210325195415346_29895878_7_699597773/10002 [hdfs://&lt;tmp_hive_hdfs_base_path&gt;/20210325/&lt;hdfs_user&gt;_tdwadmin_20210325195415346_29895878_7_699597773/10002]</span></span><br><span class="line">      Reduce Operator Tree:</span><br><span class="line">        Operator:        Group By Operator</span><br><span class="line">          aggregations:</span><br><span class="line">                expr: count(DISTINCT KEY._col1:<span class="number">1.</span>_col0)</span><br><span class="line">                expr: count(DISTINCT KEY._col1:<span class="number">2.</span>_col0)</span><br><span class="line">          keys:</span><br><span class="line">                expr: KEY._col0</span><br><span class="line">                type: string</span><br><span class="line">          mode: mergepartial</span><br><span class="line">          outputColumnNames: _col0, _col1, _col2</span><br><span class="line">          UseNewGroupBy: <span class="keyword">true</span></span><br><span class="line">          Operator:          Select Operator</span><br><span class="line">            expressions:</span><br><span class="line">                  expr: <span class="number">20210324</span></span><br><span class="line">                  type: <span class="keyword">int</span></span><br><span class="line">                  expr: (systimestamp to_char <span class="string">&#x27;yyyymmddhh24miss&#x27;</span>)</span><br><span class="line">                  type: string</span><br><span class="line">                  expr: _col0</span><br><span class="line">                  type: string</span><br><span class="line">                  expr: _col1</span><br><span class="line">                  type: bigint</span><br><span class="line">                  expr: _col1</span><br><span class="line">                  type: bigint</span><br><span class="line">                  expr: _col2</span><br><span class="line">                  type: bigint</span><br><span class="line">            outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5</span><br><span class="line">            Operator:            File Output Operator</span><br><span class="line">              compressed: <span class="keyword">false</span></span><br><span class="line">              GlobalTableId: <span class="number">0</span></span><br><span class="line">              table:</span><br><span class="line">                table descs</span><br><span class="line">                  input format: org.apache.hadoop.mapred.TextInputFormat</span><br><span class="line">                  output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat</span><br><span class="line"></span><br><span class="line">  Stage: Stage-<span class="number">0</span></span><br><span class="line">    Fetch Operator</span><br><span class="line">      limit: <span class="number">100000000</span></span><br></pre></td></tr></table></figure>




<p>error</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">submitSql has error: </span><br><span class="line">org.apache.spark.sql.AnalysisException: nondeterministic expressions are only allowed in</span><br><span class="line">Project, Filter, Aggregate or Window, found:</span><br><span class="line"> ((IF(((hj2.`furl_orig` IS NULL) OR (com.tencent.tdw_udf_cloud.hive.udf.UDFTrim(hj2.`furl_orig`) = <span class="string">&#x27;&#x27;</span>)), com.tencent.tdw_udf_cloud.hive.udf.UDFConcat(<span class="string">&#x27;null_&#x27;</span>, com.tencent.tdw_udf_cloud.hive.udf.UDFFloor((org.apache.hadoop.hive.ql.udf.UDFRand() * CAST(<span class="number">10000</span> AS DOUBLE)))), hj2.`furl_orig`)) = hj1.`fevent_code`)</span><br><span class="line">in operator Join LeftOuter, (if ((isnull(furl_orig#92) || (HiveSimpleUDF#com.tencent.tdw_udf_cloud.hive.udf.UDFTrim(furl_orig#92) = ))) HiveSimpleUDF#com.tencent.tdw_udf_cloud.hive.udf.UDFConcat(null_,HiveSimpleUDF#com.tencent.tdw_udf_cloud.hive.udf.UDFFloor((HiveSimpleUDF#org.apache.hadoop.hive.ql.udf.UDFRand() * cast(10000 as double)))) else furl_orig#92 = fevent_code#132)</span><br><span class="line">               ;;</span><br><span class="line">Aggregate [fuin#49], [20210324 AS fdate#0, HiveSimpleUDF#com.tencent.tdw_udf_cloud.hive.udf.UDFToChar(HiveSimpleUDF#org.apache.hadoop.hive.ql.udf.UDFSysTimestamp(),yyyymmddhh24miss) AS fetl_time#1, fuin#49 AS fuin#2, count(distinct HiveSimpleUDF#com.tencent.tdw_udf_cloud.hive.udf.UDFSubstr(cast(if ((((fevent_time#95L &gt;= 20210318000000) &amp;&amp; (fevent_time#95L &lt;= 20210324235959)) &amp;&amp; fscn#111 IN (1))) fevent_time#95L else cast(null as bigint) as string),1,8)) AS flabel_25942#3L, count(distinct HiveSimpleUDF#com.tencent.tdw_udf_cloud.hive.udf.UDFSubstr(cast(if ((((fevent_time#95L &gt;= 20210318000000) &amp;&amp; (fevent_time#95L &lt;= 20210324235959)) &amp;&amp; fscn#111 IN (1))) fevent_time#95L else cast(null as bigint) as string),1,8)) AS flabel_25970#4L, count(distinct HiveSimpleUDF#com.tencent.tdw_udf_cloud.hive.udf.UDFSubstr(cast(if (((((furl_orig#92 = /mb/v5/fund/list/steady.shtml) || (furl_orig#92 = /mb/v4/fundlist/fund_all.shtml)) || (furl_orig#92 = /mb/v4/fundlist/fund_all_v5.shtml)) &amp;&amp; ((fevent_time#95L &gt;= 20210318000000) &amp;&amp; (fevent_time#95L &lt;= 20210324235959)))) fevent_time#95L else cast(null as bigint) as string),1,8)) AS flabel_25972#5L]</span><br><span class="line">+- Filter ((isnotnull(fuin#49) &amp;&amp; NOT (HiveSimpleUDF#com.tencent.tdw_udf_cloud.hive.udf.UDFTrim(fuin#49) = )) &amp;&amp; ((fdate#7L &gt;= cast(20210318 as bigint)) &amp;&amp; (fdate#7L &lt;= cast(20210324 as bigint))))</span><br><span class="line">   +- Join LeftOuter, (if ((isnull(fspid_fundcode#110) || (HiveSimpleUDF#com.tencent.tdw_udf_cloud.hive.udf.UDFTrim(fspid_fundcode#110) = ))) HiveSimpleUDF#com.tencent.tdw_udf_cloud.hive.udf.UDFConcat(null_,HiveSimpleUDF#com.tencent.tdw_udf_cloud.hive.udf.UDFFloor((HiveSimpleUDF#org.apache.hadoop.hive.ql.udf.UDFRand() * cast(10000 as double)))) else fspid_fundcode#110 = fspid_fundcode#209)</span><br><span class="line">      :- Join LeftOuter, (if ((isnull(furl_orig#92) || (HiveSimpleUDF#com.tencent.tdw_udf_cloud.hive.udf.UDFTrim(furl_orig#92) = ))) HiveSimpleUDF#com.tencent.tdw_udf_cloud.hive.udf.UDFConcat(null_,HiveSimpleUDF#com.tencent.tdw_udf_cloud.hive.udf.UDFFloor((HiveSimpleUDF#org.apache.hadoop.hive.ql.udf.UDFRand() * cast(10000 as double)))) else furl_orig#92 = fevent_code#201)</span><br><span class="line">      :  :- Join LeftOuter, (if ((isnull(furl_orig#92) || (HiveSimpleUDF#com.tencent.tdw_udf_cloud.hive.udf.UDFTrim(furl_orig#92) = ))) HiveSimpleUDF#com.tencent.tdw_udf_cloud.hive.udf.UDFConcat(null_,HiveSimpleUDF#com.tencent.tdw_udf_cloud.hive.udf.UDFFloor((HiveSimpleUDF#org.apache.hadoop.hive.ql.udf.UDFRand() * cast(10000 as double)))) else furl_orig#92 = fevent_code#132)</span><br><span class="line">      :  :  :- SubqueryAlias hj2</span><br><span class="line">      :  :  :  +- MetastoreRelation dml_base, dml_evt_lct_cft_label_factory_mta_access_dd</span><br><span class="line">      :  :  +- BroadcastHint</span><br><span class="line">      :  :     +- SubqueryAlias hj1</span><br><span class="line">      :  :        +- MetastoreRelation dim_base, dim_lct_hq_beacon_report_manage</span><br><span class="line">      :  +- BroadcastHint</span><br><span class="line">      :     +- SubqueryAlias hj3</span><br><span class="line">      :        +- MetastoreRelation dim_base, dim_lct_hq_virtual_event_info</span><br><span class="line">      +- BroadcastHint</span><br><span class="line">         +- SubqueryAlias hj0</span><br><span class="line">            +- MetastoreRelation dim_base, dim_prd_lct_cft_fund_type_conf</span><br></pre></td></tr></table></figure>




<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">java.lang.RuntimeException: Map local work failed at org.apache.hadoop.hive.ql.exec.ExecMapper.processOldMapLocalWork(ExecMapper.java:<span class="number">317</span>) at org.apache.hadoop.hive.ql.exec.ExecMapper.map(ExecMapper.java:<span class="number">151</span>) at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:<span class="number">54</span>) at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:<span class="number">453</span>) at org.apache.hadoop.mapred.MapTask.run(MapTask.java:<span class="number">343</span>) at org.apache.hadoop.mapred.YarnChild$<span class="number">2.</span>run(YarnChild.java:<span class="number">175</span>) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:<span class="number">422</span>) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:<span class="number">2286</span>) at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:<span class="number">169</span>) Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: java.io.IOException: 没有那个文件或目录 at org.apache.hadoop.hive.ql.exec.persistence.HashMapWrapper.getPersistentHash(HashMapWrapper.java:<span class="number">189</span>) at org.apache.hadoop.hive.ql.exec.persistence.HashMapWrapper.put(HashMapWrapper.java:<span class="number">155</span>) at org.apache.hadoop.hive.ql.exec.MapJoinOperator.process(MapJoinOperator.java:<span class="number">474</span>) at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:<span class="number">471</span>) at org.apache.hadoop.hive.ql.exec.TableScanOperator.process(TableScanOperator.java:<span class="number">37</span>) at org.apache.hadoop.hive.ql.exec.ExecMapper.processOldMapLocalWork(ExecMapper.java:<span class="number">302</span>) ... <span class="number">9</span> more Caused by: java.io.IOException: 没有那个文件或目录 at java.io.UnixFileSystem.createFileExclusively(Native Method) at java.io.File.createTempFile(File.java:<span class="number">2024</span>) at org.apache.hadoop.hive.ql.exec.persistence.HashMapWrapper.getPersistentHash(HashMapWrapper.java:<span class="number">176</span>) ... <span class="number">14</span> more	N/A</span><br></pre></td></tr></table></figure>


<p>定位出错误原因是：强制指定了mapjoin，内存溢出了</p>
<p>Hive的自动join策略选择：</p>
<p>由于开启了hive.auto.convert.join，但是实际小表大小是hive.mapjoin.smalltable.filesize（默认25M，小表不会超过25M）。由于使用的是orc压缩，解压缩后可能大小到了250M，存放到内存大小可能就会超过1G。mapjoin的时候，hive orcfile 放到内存中会放大40倍<br> 可以看到JVM Max Heap Size大小为：1013645312 （大约1G）</p>
<h2 id="一句话总结"><a href="#一句话总结" class="headerlink" title="一句话总结"></a>一句话总结</h2><p>由于使用了hive.auto.convert.join,对小表进行广播，但是原表是orc的，存放到内存可能膨胀到大于localtask的堆内存大小，导致sql执行失败。</p>
<h2 id="解决措施"><a href="#解决措施" class="headerlink" title="解决措施"></a>解决措施</h2><h6 id="方案一"><a href="#方案一" class="headerlink" title="方案一"></a>方案一</h6><p>调大localtask的内存，set hive.mapred.local.mem=XX ，默认1G，调大到4G</p>
<h6 id="方案二"><a href="#方案二" class="headerlink" title="方案二"></a>方案二</h6><p>直接关表autojoin，将hive.auto.convert.join设置成false</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://example.com/bigdata/Spark/Spark-streaming-runtime/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="aaronzhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Guadazi">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/bigdata/Spark/Spark-streaming-runtime/" class="post-title-link" itemprop="url">Spark streaming runtime</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-01-15 19:47:00" itemprop="dateCreated datePublished" datetime="2021-01-15T19:47:00+08:00">2021-01-15</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-04-23 13:04:00" itemprop="dateModified" datetime="2021-04-23T13:04:00+08:00">2021-04-23</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/bigdata/" itemprop="url" rel="index"><span itemprop="name">bigdata</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="Spark-streaming-runtime"><a href="#Spark-streaming-runtime" class="headerlink" title="Spark-streaming-runtime"></a>Spark-streaming-runtime</h1><p><img src="_v_images/20210113163649916_346932316.jpg"></p>
<p> <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/159041276">https://zhuanlan.zhihu.com/p/159041276</a></p>
<p><strong>spark vs storm</strong></p>
<p><img src="_v_images/20210113163812387_1035142961.png"></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://example.com/bigdata/Spark/Spark%E8%B5%84%E6%BA%90%E8%AF%84%E4%BC%B0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="aaronzhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Guadazi">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/bigdata/Spark/Spark%E8%B5%84%E6%BA%90%E8%AF%84%E4%BC%B0/" class="post-title-link" itemprop="url">Spark资源评估</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-01-15 19:47:00" itemprop="dateCreated datePublished" datetime="2021-01-15T19:47:00+08:00">2021-01-15</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-04-23 13:04:00" itemprop="dateModified" datetime="2021-04-23T13:04:00+08:00">2021-04-23</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/bigdata/" itemprop="url" rel="index"><span itemprop="name">bigdata</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="Spark资源评估"><a href="#Spark资源评估" class="headerlink" title="Spark资源评估"></a>Spark资源评估</h1><table>
<thead>
<tr>
<th>机器机型</th>
<th>内存</th>
<th>硬盘</th>
<th>核数</th>
</tr>
</thead>
<tbody><tr>
<td>M10</td>
<td>128G</td>
<td>3.6T</td>
<td>48</td>
</tr>
<tr>
<td>BX1</td>
<td>16G×16 256G</td>
<td>4T×12=48T</td>
<td>80</td>
</tr>
<tr>
<td>CG3</td>
<td>256G</td>
<td>3.6T</td>
<td>96</td>
</tr>
</tbody></table>
<h2 id="Spark-On-Yarn-内存计算"><a href="#Spark-On-Yarn-内存计算" class="headerlink" title="Spark On Yarn 内存计算"></a>Spark On Yarn 内存计算</h2><p>在介绍了，spark任务在yarn运行时需要的Continer数量，以及内存大小之后，我们再来看spark on yarn的时候整体任务在yarn中占用资源大小。</p>
<p>Core： yarn中Core指的是Continer数量，所以Core = ContinerNum</p>
<p>而内存的计算则较为复杂了，设单个Continer向集群申请的资源经我们上面公式算出来的需要申请的内存大小为：excutorTotalMemory ，则该Continer在yarn集群上占用的最终资源为continerMemory。<br>minContiner = yarn.scheduler.minimum-allocation-mb（continer分配资源的最小值，目前是128）<br>Increment = yarn.scheduler.increment-allocation-mb（yarn分配资源的增量，也叫规整化参数，默认值为1024 mb）<br>resultMemory的计算方式如下所示：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">If（totalMemory&lt;=minContiner）&#123;</span><br><span class="line">	continerMemory = minContiner</span><br><span class="line">&#125;<span class="keyword">else</span>&#123;</span><br><span class="line">	continerMemory = minContiner + Math.ceil（(excutorTotalMemory - minContiner)/increment） * increment</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p><strong>总结</strong></p>
<p>例如某个spark任务的提交参数为，driverMemory=2G，executorMemory=2G，executorNum = 1<br>minContiner=512m<br>Increment =1024m<br>则该任务<br>executorContinerMemory计算过程如下</p>
<pre><code>申请资源数：executor = Max(executorMemory*0.1，384M)+executorMemory=2432M
ContinerMymory = 512+Math.ceil（(2432-512)/1024.0）*1024 = 2.5G</code></pre>
<p>driverContinerMemory计算过程同上：2.5G<br>最终该任务在yarn消耗资源为5G<br>可以看出来，spark任务最终消耗资源并非为初始化资源数。</p>
<p>需要join 75张表，每张表的主键分布不同：</p>
<ul>
<li>直接join会造成数据倾斜，某个节点撑爆</li>
<li>所有的表都shuffle，会造成shuffle数据量太多，撑爆硬盘</li>
</ul>
<p>申请的资源:</p>
<p><img src="_v_images/20201012172033562_1858655038.png"></p>
<p>策略一:</p>
<ul>
<li>join后的表，每隔join20次则repartition一次</li>
<li>待join的子表，partition个数超过30，或行数超过1.5亿，则repartition一次</li>
</ul>
<p><img src="_v_images/20201012170200410_989497495.png"></p>
<p>宽表数据量:<br><img src="_v_images/20201012190000429_1118094404.png"></p>
<p><img src="_v_images/20201012204407377_1330736778.png"></p>
<h2 id="问题点"><a href="#问题点" class="headerlink" title="问题点"></a>问题点</h2><ol>
<li>dag排布的规则是什么？</li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://example.com/bigdata/Spark/Spark-task_split_block/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="aaronzhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Guadazi">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/bigdata/Spark/Spark-task_split_block/" class="post-title-link" itemprop="url">Spark task split block</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-01-15 19:47:00" itemprop="dateCreated datePublished" datetime="2021-01-15T19:47:00+08:00">2021-01-15</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-04-23 13:04:00" itemprop="dateModified" datetime="2021-04-23T13:04:00+08:00">2021-04-23</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/bigdata/" itemprop="url" rel="index"><span itemprop="name">bigdata</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="Spark-task-split-block"><a href="#Spark-task-split-block" class="headerlink" title="Spark-task_split_block"></a>Spark-task_split_block</h1><p>梳理一下Spark中关于并发度涉及的几个概念File，Block，Split，Task，Partition，RDD以及节点数、Executor数、core数目的关系。</p>
<p><img src="_v_images/20201015163856847_762442646.jpg"></p>
<ol>
<li>用户设置了numSplit，那么goalSize=totalSize/numSplit</li>
<li>minSize=max(1,minSplitSize)</li>
<li>splitSize=max(minSplitSize, min(goalSize,blockSize))</li>
<li>task个数=totalSize除以splitSize</li>
</ol>
<p>输入可能以多个文件的形式存储在HDFS上，每个File都包含了很多块，称为<strong>Block</strong>。<br>当Spark读取这些文件作为输入时，会根据具体数据格式对应的InputFormat进行解析，一般是将若干个Block合并成一个输入分片，称为<strong>InputSplit</strong>，注意InputSplit不能跨越文件。<br>随后将为这些输入分片生成具体的<strong>Task</strong>。InputSplit与Task是<strong>一一对应</strong>的关系。<br>随后这些具体的Task每个都会被分配到集群上的某个节点的某个<strong>Executor</strong>去执行。</p>
<ul>
<li>每个节点可以起一个或多个Executor。</li>
<li>每个Executor由若干<strong>core</strong>组成，每个Executor的每个core<strong>一次只能执行一个</strong>Task。</li>
<li>每个Task执行的结果就是生成了目标<strong>RDD</strong>的一个<strong>partiton</strong>。</li>
</ul>
<p><strong>注意:</strong> 这里的core是虚拟的core而不是机器的物理CPU核，可以理解为就是Executor的一个工作线程。</p>
<p>而 Task被执行的并发度 = Executor数目 * 每个Executor核数。</p>
<p>至于partition的数目：</p>
<ul>
<li>对于数据读入阶段，例如sc.textFile，输入文件被划分为多少InputSplit就会需要多少初始Task。</li>
<li>在Map阶段partition数目保持不变。</li>
<li>在Reduce阶段，RDD的聚合会触发shuffle操作，聚合后的RDD的partition数目跟具体操作有关，例如repartition操作会聚合成指定分区数，还有一些算子是可配置的。</li>
</ul>
<h3 id="1，Application"><a href="#1，Application" class="headerlink" title="1，Application"></a>1，Application</h3><p>application（应用）其实就是用spark-submit提交的程序。比方说spark examples中的计算pi的SparkPi。一个application通常包含三部分：从数据源（比方说HDFS）取数据形成RDD，通过RDD的transformation和action进行计算，将结果输出到console或者外部存储（比方说collect收集输出到console）。</p>
<h3 id="2，Driver"><a href="#2，Driver" class="headerlink" title="2，Driver"></a>2，Driver</h3><p> Spark中的driver感觉其实和yarn中Application Master的功能相类似。主要完成任务的调度以及和executor和cluster manager进行协调。有client和cluster联众模式。client模式driver在任务提交的机器上运行，而cluster模式会随机选择机器中的一台机器启动driver。从spark官网截图的一张图可以大致了解driver的功能。</p>
<p><img src="_v_images/20201015163856640_926148743.png"></p>
<h3 id="3，Job"><a href="#3，Job" class="headerlink" title="3，Job"></a>3，Job</h3><p> Spark中的Job和MR中Job不一样不一样。MR中Job主要是Map或者Reduce Job。而Spark的Job其实很好区别，一个action算子就算一个Job，比方说count，first等。</p>
<h3 id="4-Task"><a href="#4-Task" class="headerlink" title="4, Task"></a>4, Task</h3><p>Task是Spark中最新的执行单元。RDD一般是带有partitions的，每个partition的在一个executor上的执行可以任务是一个Task。 </p>
<h3 id="5-Stage"><a href="#5-Stage" class="headerlink" title="5, Stage"></a>5, Stage</h3><p>Stage概念是spark中独有的。一般而言一个Job会切换成一定数量的stage。各个stage之间按照顺序执行。至于stage是怎么切分的，首选得知道spark论文中提到的narrow dependency(窄依赖)和wide dependency（ 宽依赖）的概念。其实很好区分，看一下父RDD中的数据是否进入不同的子RDD，如果只进入到一个子RDD则是窄依赖，否则就是宽依赖。宽依赖和窄依赖的边界就是stage的划分点</p>
<p><img src="_v_images/20201015163856436_1360238478.png"></p>
<p><img src="_v_images/20201015163856129_654594465.png"></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://example.com/bigdata/Flink/01.Release_log/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="aaronzhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Guadazi">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/bigdata/Flink/01.Release_log/" class="post-title-link" itemprop="url">Flink-release</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-01-15 19:47:00" itemprop="dateCreated datePublished" datetime="2021-01-15T19:47:00+08:00">2021-01-15</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-04-23 13:04:00" itemprop="dateModified" datetime="2021-04-23T13:04:00+08:00">2021-04-23</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/bigdata/" itemprop="url" rel="index"><span itemprop="name">bigdata</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="10-12-0"><a href="#10-12-0" class="headerlink" title="10.12.0"></a>10.12.0</h1><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_44904816/article/details/111027068">reference</a></p>
<ul>
<li>在 DataStream API 上添加了高效的批执行模式的支持。这是批处理和流处理实现真正统一的运行时的一个重要里程碑。</li>
<li>实现了基于Kubernetes的高可用性（HA）方案，作为生产环境中，ZooKeeper方案之外的另外一种选择。</li>
<li>扩展了 Kafka SQL connector，使其可以在 upsert 模式下工作，并且支持在 SQL DDL 中处理 connector 的 metadata。现在，时态表 Join 可以完全用 SQL 来表示，不再依赖于 Table API 了。</li>
<li>PyFlink 中添加了对于 DataStream API 的支持，将 PyFlink 扩展到了更复杂的场景，比如需要对状态或者定时器 timer 进行细粒度控制的场景。除此之外，现在原生支持将 PyFlink 作业部署到 Kubernetes上。</li>
</ul>
<h2 id="DataStream-API支持批量"><a href="#DataStream-API支持批量" class="headerlink" title="DataStream API支持批量"></a>DataStream API支持批量</h2><p>可复用性：作业可以在流和批这两种执行模式之间自由地切换，而无需重写任何代码。因此，用户可以复用同一个作业，来处理实时数据和历史数据。</p>
<p>维护简单：统一的 API 意味着流和批可以共用同一组 connector，维护同一套代码，并能够轻松地实现流批混合执行，例如 backfilling 之类的场景。</p>
<h2 id="Data-Sink-API"><a href="#Data-Sink-API" class="headerlink" title="Data Sink API"></a>Data Sink API</h2><h2 id="Sort-Merge-Shuffle"><a href="#Sort-Merge-Shuffle" class="headerlink" title="Sort-Merge Shuffle"></a>Sort-Merge Shuffle</h2><h2 id="SQL-中-支持-Temporal-Table-Join"><a href="#SQL-中-支持-Temporal-Table-Join" class="headerlink" title="SQL 中 支持 Temporal Table Join"></a>SQL 中 支持 Temporal Table Join</h2>
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://example.com/bigdata/Flink/RealTimeSystem-backpress/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="aaronzhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Guadazi">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/bigdata/Flink/RealTimeSystem-backpress/" class="post-title-link" itemprop="url">【转】实时流处理系统反压机制</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-01-15 19:47:00" itemprop="dateCreated datePublished" datetime="2021-01-15T19:47:00+08:00">2021-01-15</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-04-23 13:04:00" itemprop="dateModified" datetime="2021-04-23T13:04:00+08:00">2021-04-23</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/bigdata/" itemprop="url" rel="index"><span itemprop="name">bigdata</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="实时流处理系统反压机制（BackPressure）综述-转"><a href="#实时流处理系统反压机制（BackPressure）综述-转" class="headerlink" title="实时流处理系统反压机制（BackPressure）综述[转]"></a>实时流处理系统反压机制（BackPressure）综述[转]</h1><p> 发表于 2018-11-15 |  更新于 2018-12-03 |  分类于 <a target="_blank" rel="noopener" href="http://ileaf.tech/category/#/BigData">BigData </a>|  阅读次数 333</p>
<p>本文主要关于实时流处理系统反压机制，最近看到反压问题看到此文章很好，在此分享并mark一下。</p>
<blockquote>
<p>本文主要关于实时流处理系统反压机制，最近看到反压问题看到此文章很好，在此分享并mark一下。<br>(￢_￢)ﾉ最近菜叶子没自己写见谅。<br>本文转自 <a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_21125183/article/details/80708142">实时流处理系统反压机制（BackPressure）综述</a><br><a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_21125183/article/details/80708142">https://blog.csdn.net/qq_21125183/article/details/80708142</a><br><a target="_blank" rel="noopener" href="https://www.jianshu.com/p/28fcd51d4edd">开启Back Pressure使生产环境的Spark Streaming应用更稳定、有效</a></p>
</blockquote>
<p>反压机制（BackPressure）被广泛应用到实时流处理系统中，流处理系统需要能优雅地处理反压（backpressure）问题。<br>反压通常产生于这样的场景：短时负载高峰导致系统接收数据的速率远高于它处理数据的速率。<br>许多日常问题都会导致反压，例如，垃圾回收停顿可能会导致流入的数据快速堆积，或者遇到大促或秒杀活动导致流量陡增。<br>反压如果不能得到正确的处理，可能会导致资源耗尽甚至系统崩溃。反压机制就是指系统能够自己检测到被阻塞的Operator，然后系统自适应地降低源头或者上游的发送速率。</p>
<p>目前主流的流处理系统 Apache Storm、JStorm、Spark Streaming、S4、Apache Flink、Twitter Heron都采用反压机制解决这个问题，不过他们的实现各自不同。</p>
<p><img src="_v_images/20201030212807758_57452691.png" alt="实时流处理系统反压机制01"></p>
<p>不同的组件可以不同的速度执行（并且每个组件中的处理速度随时间改变）。 例如，考虑一个工作流程，或由于数据倾斜或任务调度而导致数据被处理十分缓慢。<br>在这种情况下，如果上游阶段不减速，将导致缓冲区建立长队列(队列占用内存、硬盘空间，节点负载加重)，或导致系统丢弃元组。<br>如果元组在中途丢弃，那么效率可能会有损失，因为已经为这些元组产生的计算被浪费了。<br>并且在一些流处理系统中比如Strom，会将这些丢失的元组重新发送，这样会导致数据的一致性问题(at least once语义)，并且还会导致某些Operator状态叠加。<br>进而整个程序输出结果不准确。第二由于系统接收数据的速率是随着时间改变的，短时负载高峰导致系统接收数据的速率远高于它处理数据的速率的情况，也会导致Tuple在中途丢失。<br>所以实时流处理系统必须能够解决发送速率远大于系统能处理速率这个问题，大多数实时流处理系统采用反压（BackPressure）机制解决这个问题。</p>
<p>下面我们就来介绍一下不同的实时流处理系统采用的反压机制：</p>
<h1 id="Strom-反压机制"><a href="#Strom-反压机制" class="headerlink" title="Strom 反压机制"></a>Strom 反压机制</h1><h2 id="Storm-1-0-以前的反压机制"><a href="#Storm-1-0-以前的反压机制" class="headerlink" title="Storm 1.0 以前的反压机制"></a>Storm 1.0 以前的反压机制</h2><p>对于开启了acker机制的storm程序，可以通过设置conf.setMaxSpoutPending参数来实现反压效果，<strong>如果下游组件(bolt)处理速度跟不上导致spout发送的tuple没有及时确认的数超过了参数设定的值，spout会停止发送数据</strong>，这种方式的缺点是很难调优conf.setMaxSpoutPending参数的设置以达到最好的反压效果，设小了会导致吞吐上不去，设大了会导致worker OOM；有震荡，数据流会处于一个颠簸状态，效果不如逐级反压；另外对于关闭acker机制的程序无效；</p>
<h2 id="Storm-Automatic-Backpressure"><a href="#Storm-Automatic-Backpressure" class="headerlink" title="Storm Automatic Backpressure"></a>Storm Automatic Backpressure</h2><p>新的storm自动反压机制(Automatic Back Pressure)通过监控bolt中的接收队列的情况，当超过高水位值时专门的线程会将反压信息写到 Zookeeper ，Zookeeper上的watch会通知该拓扑的所有Worker都进入反压状态，最后Spout降低tuple发送的速度。</p>
<p><img src="_v_images/20201030212806451_178512803.png" alt="实时流处理系统反压机制02"></p>
<p>每个Executor都有一个接受队列和发送队列用来接收Tuple和发送Spout或者Bolt生成的Tuple元组。每个Worker进程都有一个单的的接收线程监听接收端口。<br>它从每个网络上进来的消息发送到Executor的接收队列中。Executor接收队列存放Worker或者Worker内部其他Executor发过来的消息。<br>Executor工作线程从接收队列中拿出数据，然后调用execute方法，发送Tuple到Executor的发送队列。<br>Executor的发送线程从发送队列中获取消息，按照消息目的地址选择发送到Worker的传输队列中或者其他Executor的接收队列中。<br>最后Worker的发送线程从传输队列中读取消息，然后将Tuple元组发送到网络中。</p>
<ol>
<li>当Worker进程中的Executor线程发现自己的接收队列满了时，也就是接收队列达到<code>high watermark</code>的阈值后，因此它会发送通知消息到背压线程。</li>
<li>背压线程将当前worker进程的信息注册到Zookeeper的Znode节点中。具体路径就是 <code>/Backpressure/topo1/wk1</code>下</li>
<li>Zookeepre的Znode Watcher监视/Backpreesure/topo1下的节点目录变化情况，如果发现目录增加了znode节点说明或者其他变化。这就说明该Topo1需要反压控制，然后它会通知Topo1所有的Worker进入反压状态。</li>
<li>最终Spout降低tuple发送的速度。</li>
</ol>
<h1 id="JStorm-反压机制"><a href="#JStorm-反压机制" class="headerlink" title="JStorm 反压机制"></a>JStorm 反压机制</h1><p>JStorm做了两级的反压，第一级和Jstorm类似，通过执行队列来监测，但是不会通过ZK来协调，而是通过Topology Master来协调。<br>在队列中会标记high water mark和low water mark，当执行队列超过high water mark时，就认为bolt来不及处理，则向TM发一条控制消息，上游开始减慢发送速率，直到下游低于low water mark时解除反压。</p>
<p>此外，在Netty层也做了一级反压，由于每个Worker Task都有自己的发送和接收的缓冲区，可以对缓冲区设定限额、控制大小，如果spout数据量特别大，缓冲区填满会导致下游bolt的接收缓冲区填满，造成了反压。</p>
<p><img src="_v_images/20201030212805000_1695102842.png" alt="实时流处理系统反压机制03"></p>
<p>限流机制：jstorm的限流机制， 当下游bolt发生阻塞时， 并且阻塞task的比例超过某个比例时（现在默认设置为0.1），触发反压</p>
<p>限流方式：计算阻塞Task的地方执行线程执行时间，Spout每发送一个tuple等待相应时间，然后讲这个时间发送给Spout， 于是， spout每发送一个tuple，就会等待这个执行时间。</p>
<p>Task阻塞判断方式：在jstorm 连续4次采样周期中采样，队列情况，当队列超过80%（可以设置）时，即可认为该task处在阻塞状态。</p>
<h1 id="SparkStreaming-反压机制"><a href="#SparkStreaming-反压机制" class="headerlink" title="SparkStreaming 反压机制"></a>SparkStreaming 反压机制</h1><h2 id="为什么引入反压机制Backpressure"><a href="#为什么引入反压机制Backpressure" class="headerlink" title="为什么引入反压机制Backpressure"></a>为什么引入反压机制Backpressure</h2><p>默认情况下，Spark Streaming通过Receiver以生产者生产数据的速率接收数据，计算过程中会出现<code>batch processing time &gt; batch interval</code>的情况，其中<code>batch processing time</code> 为实际计算一个批次花费时间， <code>batch interval</code>为Streaming应用设置的批处理间隔。<br>这意味着Spark Streaming的数据接收速率高于Spark从队列中移除数据的速率，也就是数据处理能力低，在设置间隔内不能完全处理当前接收速率接收的数据。如果这种情况持续过长的时间，会造成数据在内存中堆积，导致Receiver所在Executor内存溢出等问题（如果设置StorageLevel包含disk, 则内存存放不下的数据会溢写至disk, 加大延迟）。<br>Spark 1.5以前版本，用户如果要限制Receiver的数据接收速率，可以通过设置静态配制参数“<code>spark.streaming.receiver.maxRate</code>”的值来实现，此举虽然可以通过限制接收速率，来适配当前的处理能力，防止内存溢出，但也会引入其它问题。比如：producer数据生产高于maxRate，当前集群处理能力也高于maxRate，这就会造成资源利用率下降等问题。为了更好的协调数据接收速率与资源处理能力，Spark Streaming 从v1.5开始引入反压机制（back-pressure）,通过动态控制数据接收速率来适配集群数据处理能力。</p>
<h2 id="反压机制Backpressure"><a href="#反压机制Backpressure" class="headerlink" title="反压机制Backpressure"></a>反压机制Backpressure</h2><p><code>Spark Streaming Backpressure</code>: 根据JobScheduler反馈作业的执行信息来动态调整Receiver数据接收率。通过属性“spark.streaming.backpressure.enabled”来控制是否启用backpressure机制，默认值false，即不启用。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sparkConf.set(<span class="string">&quot;spark.streaming.backpressure.enabled&quot;</span>,”<span class="keyword">true</span>”)</span><br></pre></td></tr></table></figure>
<p>SparkStreaming 架构图如下所示:</p>
<p><img src="_v_images/20201030212803393_1135733829.png" alt="实时流处理系统反压机制04"></p>
<p>SparkStreaming 反压过程执行如下图所示：</p>
<p>在原架构的基础上加上一个新的组件RateController,这个组件负责监听“OnBatchCompleted”事件，然后从中抽取<code>processingDelay</code> 及<code>schedulingDelay</code>信息. <code>Estimator</code>依据这些信息估算出最大处理速度（rate），最后由基于<code>Receiver</code>的Input Stream将rate通过ReceiverTracker与ReceiverSupervisorImpl转发给BlockGenerator（继承自RateLimiter）.</p>
<p><img src="_v_images/20201030212801986_428923565.png" alt="实时流处理系统反压机制05"></p>
<h2 id="direct模式-BackPressure-此部分详细说明了direct模式接收：转自-开启Back-Pressure使生产环境的Spark-Streaming应用更稳定、有效"><a href="#direct模式-BackPressure-此部分详细说明了direct模式接收：转自-开启Back-Pressure使生产环境的Spark-Streaming应用更稳定、有效" class="headerlink" title="direct模式-BackPressure(此部分详细说明了direct模式接收：转自-开启Back Pressure使生产环境的Spark Streaming应用更稳定、有效)"></a>direct模式-BackPressure(此部分详细说明了direct模式接收：转自-<a target="_blank" rel="noopener" href="https://www.jianshu.com/p/28fcd51d4edd">开启Back Pressure使生产环境的Spark Streaming应用更稳定、有效</a>)</h2><p>当Spark Streaming与Kafka使用Direct API集群时，我们可以很方便的去控制最大数据摄入量–通过一个被称作spark.streaming.kafka.maxRatePerPartition的参数。根据文档描述，他的含义是：Direct API读取每一个Kafka partition数据的最大速率（每秒读取的消息量）。<br>配置项spark.streaming.kafka.maxRatePerPartition，对防止流式应用在下边两种情况下出现流量过载时尤其重要：<br>1.Kafka Topic中有大量未处理的消息，并且我们设置是Kafka auto.offset.reset参数值为smallest，他可以防止第一个批次出现数据流量过载情况。<br>2.当Kafka 生产者突然飙升流量的时候，他可以防止批次处理出现数据流量过载情况。</p>
<p>但是，配置Kafka每个partition每批次最大的摄入量是个静态值，也算是个缺点。随着时间的变化，在生产环境运行了一段时间的Spark Streaming应用，每批次每个Kafka partition摄入数据最大量的最优值也是变化的。有时候，是因为消息的大小会变，导致数据处理时间变化。有时候，是因为流计算所使用的多租户集群会变得非常繁忙，比如在白天时候，一些其他的数据应用（例如Impala/Hive/MR作业）竞争共享的系统资源时（CPU/内存/网络/磁盘IO）。<br>背压机制可以解决该问题。背压机制是呼声比较高的功能，他允许根据前一批次数据的处理情况，动态、自动的调整后续数据的摄入量，这样的反馈回路使得我们可以应对流式应用流量波动的问题。<br>Spark Streaming的背压机制是在Spark1.5版本引进的，我们可以添加如下代码启用改功能：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sparkConf.set(&quot;spark.streaming.backpressure.enabled&quot;,”true”)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>那应用启动后的第一个批次流量怎么控制呢？因为他没有前面批次的数据处理时间，所以没有参考的数据去评估这一批次最优的摄入量。在Spark官方文档中有个被称作spark.streaming.backpressure.initialRate的配置，看起来是控制开启背压机制时初始化的摄入量。其实不然，该参数只对receiver模式起作用，并不适用于direct模式。推荐的方法是使用spark.streaming.kafka.maxRatePerPartition控制背压机制起作用前的第一批次数据的最大摄入量。我通常建议设置spark.streaming.kafka.maxRatePerPartition的值为最优估计值的1.5到2倍，让背压机制的算法去调整后续的值。请注意，spark.streaming.kafka.maxRatePerPartition的值会一直控制最大的摄入量，所以背压机制的算法值不会超过他。<br>另一个需要注意的是，在第一个批次处理完成前，紧接着的批次都将使用spark.streaming.kafka.maxRatePerPartition的值作为摄入量。通过Spark UI可以看到，批次间隔为5s，当批次调度延迟31秒时候，前7个批次的摄入量是20条记录。直到第八个批次，背压机制起作用时，摄入量变为5条记录。</p>
<h1 id="Heron-反压机制"><a href="#Heron-反压机制" class="headerlink" title="Heron 反压机制"></a>Heron 反压机制</h1><p><img src="_v_images/20201030212800445_1650579597.png" alt="实时流处理系统反压机制06"></p>
<p>当下游处理速度跟不上上游发送速度时，一旦StreamManager 发现一个或多个Heron Instance 速度变慢，立刻对本地spout进行降级，降低本地Spout发送速度, 停止从这些spout读取数据。并且受影响的StreamManager 会发送一个特殊的start backpressure message 给其他的StreamManager ，要求他们对spout进行本地降级。 当其他StreamManager 接收到这个特殊消息时，他们通过不读取当地Spout中的Tuple来进行降级。一旦出问题的Heron Instance 恢复速度后，本地的SM 会发送stop backpressure message 解除降级。</p>
<p>很多Socket Channel与应用程序级别的Buffer相关联，该缓冲区由high watermark 和low watermark组成。 当缓冲区大小达到high watermark时触发反压，并保持有效，直到缓冲区大小低于low watermark。 此设计的基本原理是防止拓扑在进入和退出背压缓解模式之间快速振荡。</p>
<h1 id="Flink-反压机制"><a href="#Flink-反压机制" class="headerlink" title="Flink 反压机制"></a>Flink 反压机制</h1><p>Flink 没有使用任何复杂的机制来解决反压问题，因为根本不需要那样的方案！它利用自身作为纯数据流引擎的优势来优雅地响应反压问题。下面我们会深入分析 Flink 是如何在 Task 之间传输数据的，以及数据流如何实现自然降速的。 Flink 在运行时主要由 operators 和 streams 两大组件构成。每个 operator 会消费中间态的流，并在流上进行转换，然后生成新的流。对于 Flink 的网络机制一种形象的类比是，Flink 使用了高效有界的分布式阻塞队列，就像 Java 通用的阻塞队列（BlockingQueue）一样。还记得经典的线程间通信案例：生产者消费者模型吗？使用 BlockingQueue 的话，一个较慢的接受者会降低发送者的发送速率，因为一旦队列满了（有界队列）发送者会被阻塞。Flink 解决反压的方案就是这种感觉。 在 Flink 中，这些分布式阻塞队列就是这些逻辑流，而队列容量是通过缓冲池来（LocalBufferPool）实现的。每个被生产和被消费的流都会被分配一个缓冲池。缓冲池管理着一组缓冲(Buffer)，缓冲在被消费后可以被回收循环利用。这很好理解：你从池子中拿走一个缓冲，填上数据，在数据消费完之后，又把缓冲还给池子，之后你可以再次使用它。</p>
<h2 id="Flink-网络传输中的内存管理"><a href="#Flink-网络传输中的内存管理" class="headerlink" title="Flink 网络传输中的内存管理"></a>Flink 网络传输中的内存管理</h2><p>如下图所示展示了 Flink 在网络传输场景下的内存管理。网络上传输的数据会写到 Task 的 InputGate（IG） 中，经过 Task 的处理后，再由 Task 写到 ResultPartition（RS） 中。每个 Task 都包括了输入和输入，输入和输出的数据存在 Buffer 中（都是字节数据）。Buffer 是 MemorySegment 的包装类。</p>
<p><img src="_v_images/20201030212758832_1369878662.png" alt="实时流处理系统反压机制07"></p>
<ol>
<li>TaskManager（TM）在启动时，会先初始化NetworkEnvironment对象，TM 中所有与网络相关的东西都由该类来管理（如 Netty 连接），其中就包括NetworkBufferPool。根据配置，Flink 会在 NetworkBufferPool 中生成一定数量（默认2048个）的内存块 MemorySegment（关于 Flink 的内存管理，后续文章会详细谈到），内存块的总数量就代表了网络传输中所有可用的内存。NetworkEnvironment 和 NetworkBufferPool 是 Task 之间共享的，每个 TM 只会实例化一个。</li>
<li>Task 线程启动时，会向 NetworkEnvironment 注册，NetworkEnvironment 会为 Task 的 InputGate（IG）和 ResultPartition（RP） 分别创建一个 LocalBufferPool（缓冲池）并设置可申请的 MemorySegment（内存块）数量。IG 对应的缓冲池初始的内存块数量与 IG 中 InputChannel 数量一致，RP 对应的缓冲池初始的内存块数量与 RP 中的 ResultSubpartition 数量一致。不过，每当创建或销毁缓冲池时，NetworkBufferPool 会计算剩余空闲的内存块数量，并平均分配给已创建的缓冲池。注意，这个过程只是指定了缓冲池所能使用的内存块数量，并没有真正分配内存块，只有当需要时才分配。为什么要动态地为缓冲池扩容呢？因为内存越多，意味着系统可以更轻松地应对瞬时压力（如GC），不会频繁地进入反压状态，所以我们要利用起那部分闲置的内存块。</li>
<li>在 Task 线程执行过程中，当 Netty 接收端收到数据时，为了将 Netty 中的数据拷贝到 Task 中，InputChannel（实际是 RemoteInputChannel）会向其对应的缓冲池申请内存块（上图中的①）。如果缓冲池中也没有可用的内存块且已申请的数量还没到池子上限，则会向 NetworkBufferPool 申请内存块（上图中的②）并交给 InputChannel 填上数据（上图中的③和④）。如果缓冲池已申请的数量达到上限了呢？或者 NetworkBufferPool 也没有可用内存块了呢？这时候，Task 的 Netty Channel 会暂停读取，上游的发送端会立即响应停止发送，拓扑会进入反压状态。当 Task 线程写数据到 ResultPartition 时，也会向缓冲池请求内存块，如果没有可用内存块时，会阻塞在请求内存块的地方，达到暂停写入的目的。</li>
<li>当一个内存块被消费完成之后（在输入端是指内存块中的字节被反序列化成对象了，在输出端是指内存块中的字节写入到 Netty Channel 了），会调用 Buffer.recycle() 方法，会将内存块还给 LocalBufferPool （上图中的⑤）。如果LocalBufferPool中当前申请的数量超过了池子容量（由于上文提到的动态容量，由于新注册的 Task 导致该池子容量变小），则LocalBufferPool会将该内存块回收给 NetworkBufferPool（上图中的⑥）。如果没超过池子容量，则会继续留在池子中，减少反复申请的开销。</li>
</ol>
<h2 id="Flink-反压机制-1"><a href="#Flink-反压机制-1" class="headerlink" title="Flink 反压机制"></a>Flink 反压机制</h2><p>下面这张图简单展示了两个 Task 之间的数据传输以及 Flink 如何感知到反压的：</p>
<p><img src="_v_images/20201030212757424_78076714.png" alt="实时流处理系统反压机制08"></p>
<ol>
<li>记录“A”进入了 Flink 并且被 Task 1 处理。（这里省略了 Netty 接收、反序列化等过程）</li>
<li>记录被序列化到 buffer 中。</li>
<li>该 buffer 被发送到 Task 2，然后 Task 2 从这个 buffer 中读出记录。</li>
</ol>
<p><strong>不要忘了：记录能被 Flink 处理的前提是，必须有空闲可用的 Buffer。</strong></p>
<p>结合上面两张图看：Task 1 在输出端有一个相关联的 LocalBufferPool（称缓冲池1），Task 2 在输入端也有一个相关联的 LocalBufferPool（称缓冲池2）。如果缓冲池1中有空闲可用的 buffer 来序列化记录 “A”，我们就序列化并发送该 buffer。</p>
<p>这里我们需要注意两个场景：</p>
<ul>
<li>本地传输：如果 Task 1 和 Task 2 运行在同一个 worker 节点（TaskManager），该 buffer 可以直接交给下一个 Task。一旦 Task 2 消费了该 buffer，则该 buffer 会被缓冲池1回收。如果 Task 2 的速度比 1 慢，那么 buffer 回收的速度就会赶不上 Task 1 取 buffer 的速度，导致缓冲池1无可用的 buffer，Task 1 等待在可用的 buffer 上。最终形成 Task 1 的降速。</li>
<li>远程传输：如果 Task 1 和 Task 2 运行在不同的 worker 节点上，那么 buffer 会在发送到网络（TCP Channel）后被回收。在接收端，会从 LocalBufferPool 中申请 buffer，然后拷贝网络中的数据到 buffer 中。如果没有可用的 buffer，会停止从 TCP 连接中读取数据。在输出端，通过 Netty 的水位值机制来保证不往网络中写入太多数据（后面会说）。如果网络中的数据（Netty输出缓冲中的字节数）超过了高水位值，我们会等到其降到低水位值以下才继续写入数据。这保证了网络中不会有太多的数据。如果接收端停止消费网络中的数据（由于接收端缓冲池没有可用 buffer），网络中的缓冲数据就会堆积，那么发送端也会暂停发送。另外，这会使得发送端的缓冲池得不到回收，writer 阻塞在向 LocalBufferPool 请求 buffer，阻塞了 writer 往 ResultSubPartition 写数据。</li>
</ul>
<p>这种固定大小缓冲池就像阻塞队列一样，保证了 Flink 有一套健壮的反压机制，使得 Task 生产数据的速度不会快于消费的速度。我们上面描述的这个方案可以从两个 Task 之间的数据传输自然地扩展到更复杂的 pipeline 中，保证反压机制可以扩散到整个 pipeline。</p>
<h2 id="反压实验"><a href="#反压实验" class="headerlink" title="反压实验"></a>反压实验</h2><p>另外，官方博客中为了展示反压的效果，给出了一个简单的实验。下面这张图显示了：随着时间的改变，生产者（黄色线）和消费者（绿色线）每5秒的平均吞吐与最大吞吐（在单一JVM中每秒达到8百万条记录）的百分比。我们通过衡量task每5秒钟处理的记录数来衡量平均吞吐。该实验运行在单 JVM 中，不过使用了完整的 Flink 功能栈。</p>
<p><img src="_v_images/20201030212755917_1236027751.png" alt="实时流处理系统反压机制09"></p>
<p>首先，我们运行生产task到它最大生产速度的60%（我们通过Thread.sleep()来模拟降速）。消费者以同样的速度处理数据。然后，我们将消费task的速度降至其最高速度的30%。你就会看到背压问题产生了，正如我们所见，生产者的速度也自然降至其最高速度的30%。接着，停止消费task的人为降速，之后生产者和消费者task都达到了其最大的吞吐。接下来，我们再次将消费者的速度降至30%，pipeline给出了立即响应：生产者的速度也被自动降至30%。最后，我们再次停止限速，两个task也再次恢复100%的速度。总而言之，我们可以看到：生产者和消费者在 pipeline 中的处理都在跟随彼此的吞吐而进行适当的调整，这就是我们希望看到的反压的效果。</p>
<h2 id="Flink-反压监控"><a href="#Flink-反压监控" class="headerlink" title="Flink 反压监控"></a>Flink 反压监控</h2><p>在 Storm/JStorm 中，只要监控到队列满了，就可以记录下拓扑进入反压了。但是 Flink 的反压太过于天然了，导致我们无法简单地通过监控队列来监控反压状态。Flink 在这里使用了一个 trick 来实现对反压的监控。如果一个 Task 因为反压而降速了，那么它会卡在向 LocalBufferPool 申请内存块上。那么这时候，该 Task 的 stack trace 就会长下面这样：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">java.lang.Object.wait(Native Method)</span><br><span class="line">o.a.f.[...].LocalBufferPool.requestBuffer(LocalBufferPool.java:<span class="number">163</span>)</span><br><span class="line">o.a.f.[...].LocalBufferPool.requestBufferBlocking(LocalBufferPool.java:<span class="number">133</span>) &lt;--- BLOCKING request</span><br><span class="line">[...]</span><br></pre></td></tr></table></figure>
<p>那么事情就简单了。通过不断地采样每个 task 的 stack trace 就可以实现反压监控。</p>
<p><img src="_v_images/20201030212753993_1098219801.png" alt="实时流处理系统反压机制10"></p>
<p>Flink 的实现中，只有当 Web 页面切换到某个 Job 的 Backpressure 页面，才会对这个 Job 触发反压检测，因为反压检测还是挺昂贵的。JobManager 会通过 Akka 给每个 TaskManager 发送TriggerStackTraceSample消息。默认情况下，TaskManager 会触发100次 stack trace 采样，每次间隔 50ms（也就是说一次反压检测至少要等待5秒钟）。并将这 100 次采样的结果返回给 JobManager，由 JobManager 来计算反压比率（反压出现的次数/采样的次数），最终展现在 UI 上。UI 刷新的默认周期是一分钟，目的是不对 TaskManager 造成太大的负担。</p>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>Flink不需要一种特殊的机制来处理反压，因为Flink 中的数据传输相当于已经提供了应对反压的机制。因此，Flink 所能获得的最大吞吐量由其 pipeline 中最慢的组件决定。相对于 Storm/JStorm 的实现，Flink 的实现更为简洁优雅，源码中也看不见与反压相关的代码，无需 Zookeeper/TopologyMaster 的参与也降低了系统的负载，也利于对反压更迅速的响应。</p>
<blockquote>
<p>本文转自 <a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_21125183/article/details/80708142">实时流处理系统反压机制（BackPressure）综述</a><br><a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_21125183/article/details/80708142">https://blog.csdn.net/qq_21125183/article/details/80708142</a><br><a target="_blank" rel="noopener" href="https://www.jianshu.com/p/28fcd51d4edd">开启Back Pressure使生产环境的Spark Streaming应用更稳定、有效</a></p>
</blockquote>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://example.com/bigdata/Flink/Flink-DataStream-join/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="aaronzhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Guadazi">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/bigdata/Flink/Flink-DataStream-join/" class="post-title-link" itemprop="url">Flink join</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-01-15 19:47:00" itemprop="dateCreated datePublished" datetime="2021-01-15T19:47:00+08:00">2021-01-15</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-04-23 13:04:00" itemprop="dateModified" datetime="2021-04-23T13:04:00+08:00">2021-04-23</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/bigdata/" itemprop="url" rel="index"><span itemprop="name">bigdata</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="flink-join"><a href="#flink-join" class="headerlink" title="flink join"></a>flink join</h1><h2 id="Cogroup"><a href="#Cogroup" class="headerlink" title="Cogroup"></a>Cogroup</h2><p>CoGroupFunction中会返回所有数据，不管有没有匹配上</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line">DataStream&lt;Tuple3&lt;Long, String, String&gt;&gt; input1 = ...;</span><br><span class="line">input1 = input1.assignTimestampsAndWatermarks(<span class="keyword">new</span> AscendingTimestampExtractor&lt;Tuple3&lt;Long, String, String&gt;&gt;() &#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">long</span> <span class="title">extractAscendingTimestamp</span><span class="params">(Tuple3&lt;Long, String, String&gt; arg0)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> arg0.f0;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line">DataStream&lt;Tuple2&lt;Long, String&gt;&gt; input2 = ...;</span><br><span class="line">input2 = input2.assignTimestampsAndWatermarks(<span class="keyword">new</span> AscendingTimestampExtractor&lt;Tuple2&lt;Long, String&gt;&gt;() &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">long</span> <span class="title">extractAscendingTimestamp</span><span class="params">(Tuple2&lt;Long, String&gt; stringStringTuple2)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> stringStringTuple2.f0;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line">input1.coGroup(input2).where(<span class="keyword">new</span> KeySelector&lt;Tuple3&lt;Long, String, String&gt;, String&gt;() &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">getKey</span><span class="params">(Tuple3&lt;Long, String, String&gt; itemEntity)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> itemEntity.f1;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;)</span><br><span class="line">.equalTo(<span class="keyword">new</span> KeySelector&lt;Tuple2&lt;Long, String&gt;, String&gt;() &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">getKey</span><span class="params">(Tuple2&lt;Long, String&gt; value)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> value.f1;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;)</span><br><span class="line">.window(TumblingEventTimeWindows.of(Time.minutes(<span class="number">1</span>)))</span><br><span class="line">.apply(<span class="keyword">new</span> CoGroupFunction&lt;Tuple3&lt;Long, String, String&gt;, Tuple2&lt;Long, String&gt;, String&gt;() &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">coGroup</span><span class="params">(Iterable&lt;Tuple3&lt;Long, String, String&gt;&gt; first,</span></span></span><br><span class="line"><span class="function"><span class="params">            Iterable&lt;Tuple2&lt;Long, String&gt;&gt; second, Collector&lt;String&gt; collector)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        StringBuilder buffer = <span class="keyword">new</span> StringBuilder();</span><br><span class="line">        buffer.append(<span class="string">&quot;DataStream first:\n&quot;</span>);</span><br><span class="line">        <span class="keyword">for</span> (Tuple3&lt;Long, String, String&gt; value : first) &#123;</span><br><span class="line">            buffer.append(value).append(<span class="string">&quot;\n&quot;</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        buffer.append(<span class="string">&quot;DataStream second:\n&quot;</span>);</span><br><span class="line">        <span class="keyword">for</span> (Tuple2&lt;Long, String&gt; value : second) &#123;</span><br><span class="line">            buffer.append(value.f0).append(<span class="string">&quot;=&gt;&quot;</span>).append(value.f1).append(<span class="string">&quot;\n&quot;</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        collector.collect(buffer.toString());</span><br><span class="line">    &#125;</span><br><span class="line">&#125;)</span><br><span class="line">.print();</span><br></pre></td></tr></table></figure>
<p>上面的例子，左流有三个元素 <code>Tuple3&lt;String,String,String&gt;</code>，右流有两个元素<code>Tuple2&lt;String,String&gt;</code><br>两个流第一个元素相互关联。分别指定两个流的事件时间字段。<br>两个流关联后，按照EventTime划分窗口。与单流类似。<br>不管元素是否可以关联上，都会输出</p>
<p>用户可以定义CoGroupFunction函数, 可以实现在窗口内，任意组合，如笛卡尔积</p>
<p>举例说明:</p>
<h2 id="window-Join"><a href="#window-Join" class="headerlink" title="window Join"></a>window Join</h2><h2 id="interval-join"><a href="#interval-join" class="headerlink" title="interval join"></a>interval join</h2><h2 id="broadcast-join"><a href="#broadcast-join" class="headerlink" title="broadcast join"></a>broadcast join</h2>
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://example.com/bigdata/storm/Storm-runtime/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="aaronzhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Guadazi">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/bigdata/storm/Storm-runtime/" class="post-title-link" itemprop="url">Storm runtime</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-01-15 19:47:00" itemprop="dateCreated datePublished" datetime="2021-01-15T19:47:00+08:00">2021-01-15</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-04-23 13:04:00" itemprop="dateModified" datetime="2021-04-23T13:04:00+08:00">2021-04-23</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/bigdata/" itemprop="url" rel="index"><span itemprop="name">bigdata</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="Storm-runtime"><a href="#Storm-runtime" class="headerlink" title="Storm-runtime"></a>Storm-runtime</h1><p><img src="_v_images/20210116161223695_155417172.jpg"></p>
<p>master-slave结构:</p>
<ul>
<li>Nimbus是主节点，负责分发用户代码，指派Supervisor上的worker进程，运行topology的(Spout/Bolt)Task</li>
<li>Supervisor是从节点，守护进程. 负责启动和终止worker进程. 通过Storm的配置文件中的 supervisor.slots.ports配置项，可以指定在一个Supervisor上最大允许多少个Slot，每个Slot通过端口号来唯一标识，一个端口号 对应一个Worker进程（如果该Worker进程被启动）。</li>
</ul>
<p><img src="_v_images/20210116162304428_695142382.jpg"></p>
<p>运行流程</p>
<p>1）户端提交拓扑到nimbus。</p>
<p>2） Nimbus针对该拓扑建立本地的目录根据topology的配置计算task，分配task，在zookeeper上建立assignments节点存储 task和supervisor机器节点中woker的对应关系；</p>
<p>在zookeeper上创建taskbeats节点来监控task的心跳；启动topology。</p>
<p>3） Supervisor去zookeeper上获取分配的tasks，启动多个woker进行，每个woker生成task，一个task一个线程；根据topology 信息初始化建立task之间的连接;Task和Task之间是通过zeroMQ管理的；后整个拓扑运行起来。</p>
<h2 id="内核-队列"><a href="#内核-队列" class="headerlink" title="内核-队列"></a>内核-队列</h2><p>在Storm中大量使用Disrupt Queue来解耦Storm内部的消息处理过程。分析这些Queue的分布，是分析Storm运行时态的基础。</p>
<p><img src="_v_images/20210116174558832_184704273.png"></p>
<p>在Storm的worker中，最小的执行单元是executor,一个executor只会有一个component。目前一个component只会有一个task。而一个executor会有两个Disruptor Queue, 一个用于接受数据的receive Disrupor Queue和一个用于发送send Disrupor Queue。然后在worker中有一个全局的send Disrupor Queue。分析完队列的分布，在分析topology的运行时状况。</p>
<p>当一个Topology提交到Storm集群后，task被分配到各个wrker中开始执行后，task是怎么执行的。Storm的Task分为两类，一类是消息的源头Spout,它负责在源头产生消息；然后就是Bolt，它是执行单元。但是这两者各自的工作逻辑如下。</p>
<p>我们来分析Spout在运行时的工作状况。Spout的nextTuple用于发送数据，接口注释上说明它不能阻塞，因为它与active， deactive，ack, fail在一个处理线程里面被处理。但是nextTuple被阻塞会有什么副作用列？当nextTuple被阻塞，应用代码中另起线程调用SpoutCollector会有什么后果？下图是SpoutExecutor的执行逻辑。</p>
<p><img src="_v_images/20210116174558730_1417384061.png"></p>
<p>在SpoutExecutor处理循环里面，第一步做的事情是从receive Disrupor Queue里面消费里面的消息。这里面的就是SpoutExecutor所收到的消息。处理逻辑如下图所示</p>
<p><img src="_v_images/20210116174558627_554147077.png"></p>
<p>然后SpoutExecutor会将overflow中的数据再次发送。overflow是用于接受SpoutCollector.emit()无发及时发送的数据的，具体SpoutCollector发送数据的逻辑见下文分析。但是这里在发送overflow的数据时，与SpoutCollector.emit()有个区别，就是数据还是无法被正常发送时，数据会丢弃，也就是不会被再次写入overflow中。当overflow中没有数据，以及pending中的数据量小于TOPOLOGY_MAX_SPOUT_PENDING时，判断topology的状态。当topology不是deactive状态时，如果topology有触发active命令，会调用spout的active接口，然后调用spout的nextTuple接口。否则调用deactive接口。所以这里当Spout的nextTuple被阻塞时，spout没办法处理acker回报的消息，回报的消息会阻塞在executor的receive DisruptorQueue中，当receive DisruptorQueue塞满后，是会阻塞在对应的网络处理模块中，storm中经典的是zeroMQ,老版本的zeroMQ是没有设置水位，这样会大量堆积到内存中，因为zeroMQ是C++的，占用的是堆外内存，JVM无法管理，最坏就是把机器的内存耗光。如果这个是另起线程调用SpoutCollector的emit，当emit数据速度过快，会导致overflow中堆积数据，导致worker内存消耗。</p>
<p>上面讲到SpoutCollector在emit数据会写入overflow,那什么情况下会写入overflow。SpoutCollector.emit是否真的就把数据发送到了网络。下面是SpoutCollector的处理逻辑。</p>
<p><img src="_v_images/20210116174558522_1112615304.png"></p>
<p>Spout当调用SpoutCollect.emit()发送时，首先的逻辑是根据根据消息的STREAM ID,和对应的values,得到目的端task id。当topology开启acker机制时，会生成一个随机的rootId，否则使用一个默认值作为rootId。然后为消息生成一个随机的messageId，由rootId和messageId组成对应的tuple Id。这里tuple Id是有rootId为key,messageId为value的一个HashMap。这里采用HashMap的作用会在下面Spout的ack机制是做说明。在将生成的tuple Id和用会的values组成一个tuple。并判断overflower是否有数据，让overflow中有数据时，数据会直接放入overflow中，而不会放入executor  的send Disraptor Queue中。当overflow为空时，会将tuple放入send Disraptor Queue中。当捕获Disraptor Queue的InsufficientCapacityException时，数据就放入了overflow中。然后处理ack。当开启了ack机制，会在pending中加入对应的tuple数据，然后项acker发送ack init消息。</p>
<p>根据上面的发送过程，数据emit只是被写入了executor的send Disraptor Queue。而数据在Spout端的丢失多是数据在overflow中被SpoutExecutor在次处理时，send Disraptor Queue满导致。</p>
<p>关于pending，首先它是一个RotatingMap。它通过定时旋转，以达到定时器的目的。在SpoutExecutor的RotatingMap中有两个桶，然后executor有个定时线程，会按照用户设定的message timeout second，定期向executor的receive DisruptorQueue写入SYSTEM_TICK_STREAM_ID消息，然后SpoutExecutor处理SYSTEM_TICK_STREAM_ID消息时就旋转RotatingMap，当被清除的桶中有数据时，被清除的桶中的数据会调用fail接口，通知业务逻辑fail。这就是当超时间设置比业务逻辑短时，导致数据重复的原因。还有一种是acker消息丢失，导致数据重复。由于RotatingMap的底层是HashMap，中间没有锁，overflow是个LinkedList，所以SpoutCollector和SpoutExecutor不能在两个线程中并发 处理。</p>
<p>BoltExecutor的处理逻辑非常简单，就是消费receive Disrupor Queue中数据，然后调用bolt的excue。但是这里也是单线程处理的，阻塞或者处理速度不匹配，就会导致数据在Disrupor Queue或者网络模块中堆积，其中使用zeroMQ的副作用最大。</p>
<p>BoltCollector的emit与SpoutCollector的emit处理相比，首先是少了overflow承接无法发送的数据，会直接丢弃。其次是没有pending和acker消息的发送。</p>
<p>接下来分析一下Storm的Acker机制。Storm的Ack机制在Storm刚刚开源时被大书特书，处理原理也确实非常的精彩。由于这里ID都是随机数，所以这里不会在讨论随机数的唯一问题。</p>
<p><img src="_v_images/20210116174558419_714266215.png"></p>
<p>如上图所示，spout发送t1给bolt a, bolt a 在t1的基础上生成t2,t3,t4给bolt b，bolt b ack所收到的数据。下面来追踪整个id变化的过程。</p>
<p>Spout 发送t1， message id为&lt;r_1, m_1&gt;,发送ack</p>
<p>Acker收到ack init, map中缓存&lt;r_1,m_1&gt;</p>
<p>Bolt a 收到t1, messageId为&lt;r_1,m_1&gt;,生成t2，t3, t4</p>
<p>Bolt a 发送t2, anchor t1, t2,messageId为&lt;r_1,m_2&gt;, 更新t1的ackVal为m_2</p>
<p>Bolt a 发送t3, anchor t1, t3,messageId为&lt;r_1,m_3&gt;, 更新t1的ackVal为m_2^m_3</p>
<p>Bolt a 发送t4, anchor t1, t4,messageId为&lt;r_1,m_4&gt;, 更新t1的ackVal为m_2^m_3^m_4</p>
<p>Bolt a ack t1, 向acker发送ack消息&lt;r_1, m_1^ m_2^m_3^m_4&gt;</p>
<p>Acker 收到bolt a的ack消息，更新缓存为&lt;r_1, m_1^ m_1^ m_2^m_3^m_4&gt;即&lt;r_1,  m_2^m_3^m_4&gt;</p>
<p>Bolt b ack t2, 向acker发送ack消息&lt;r_1, m_2&gt;</p>
<p>Acker 收到bolt b的ack消息，更新缓存为&lt;r_1, m_2^ m_2^m_3^m_4&gt;即&lt;r_1, m_3^m_4&gt;</p>
<p>Bolt b ack t3, 向acker发送ack消息&lt;r_1, m_3&gt;</p>
<p>Acker 收到bolt b的ack消息，更新缓存为&lt;r_1, m_3^m_3^m_4&gt;即&lt;r_1, m_4&gt;</p>
<p>Bolt b ack t4, 向acker发送ack消息&lt;r_1, m_4&gt;</p>
<p>Acker 收到bolt b的ack消息，更新缓存为&lt;r_1, m_4^m_4&gt;即&lt;r_1, 0&gt;</p>
<p>messageId确认完毕，向Spout发送ack消息。当消息没有被ack,会一直在spout的pending队列中，知道被ack或者超时。</p>
<p>它基本上使用两个long值就跟踪了一个消息在整个流中的处理过程。</p>
<p>【参考文献】</p>
<hr>
<ol>
<li><a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s/PRDSu-qOxb17qjdfpO1IYA">Apache storm内核原理</a></li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://example.com/bigdata/Spark/Spark-join/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="aaronzhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Guadazi">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/bigdata/Spark/Spark-join/" class="post-title-link" itemprop="url">Spark join</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-01-15 19:47:00" itemprop="dateCreated datePublished" datetime="2021-01-15T19:47:00+08:00">2021-01-15</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-04-23 13:04:00" itemprop="dateModified" datetime="2021-04-23T13:04:00+08:00">2021-04-23</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/bigdata/" itemprop="url" rel="index"><span itemprop="name">bigdata</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="Spark-Join"><a href="#Spark-Join" class="headerlink" title="Spark-Join"></a>Spark-Join</h1><table>
<thead>
<tr>
<th>参数</th>
<th>默认值</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>spark.sql.shuffle.partitions</td>
<td>200</td>
<td>Configures the number of partitions to use when shuffling data for joins or aggregations.</td>
</tr>
<tr>
<td>spark.default.parallelism</td>
<td>For distributed shuffle operations like reduceByKey and join, the largest number of partitions in a parent RDD. For operations like parallelize with no parent RDDs, it depends on the cluster manager:Local mode: number of cores on the local machineMesos fine grained mode: 8 Others: total number of cores on all executor nodes or 2, whichever is larger</td>
<td>Default number of partitions in RDDs returned by transformations like join, reduceByKey, and parallelize when not set by user.</td>
</tr>
</tbody></table>
<p>上面两个参数都是设置默认的并行度，但是适用的场景不同：</p>
<p>spark.sql.shuffle.partitions是对sparkSQL进行shuffle操作的时候生效，比如 join或者aggregation等操作的时候，之前有个同学设置了spark.default.parallelism 这个并行度为2000，结果还是产生200的stage，排查了很久才发现，是这个原因。<br>spark.default.parallelism这个参数只是针对rdd的shuffle操作才生效，比如join，reduceByKey。</p>
<p>作者：pcqlegend<br>链接：<a target="_blank" rel="noopener" href="https://www.jianshu.com/p/c5914126ef98">https://www.jianshu.com/p/c5914126ef98</a><br>来源：简书<br>著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。</p>
<h2 id="shuffle-join-VS-map-join"><a href="#shuffle-join-VS-map-join" class="headerlink" title="shuffle join VS map join"></a>shuffle join VS map join</h2><p>在对大表与大表之间进行连接操作时，通常都会触发 <code>Shuffle Join</code>，两表的所有分区节点会进行 <code>All-to-All</code> 的通讯，这种查询通常比较昂贵，会对网络 IO 会造成比较大的负担。</p>
<p><img src="_v_images/20201009194634406_1569670303.png"></p>
<p>而对于大表和小表的连接操作，Spark 会在一定程度上进行优化，如果小表的数据量小于 Worker Node 的内存空间，Spark 会考虑将小表的数据广播到每一个 Worker Node，在每个工作节点内部执行连接计算，这可以降低网络的 IO，但会加大每个 Worker Node 的 CPU 负担。</p>
<p><img src="_v_images/20201009194633499_170639920"></p>
<p>是否采用广播方式进行 <code>Join</code> 取决于程序内部对小表的判断，如果想明确使用广播方式进行 <code>Join</code>，则可以在 DataFrame API 中使用 <code>broadcast</code> 方法指定需要广播的小表：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">empDF.join(broadcast(deptDF), joinExpression).show()</span><br><span class="line">复制代码</span><br></pre></td></tr></table></figure>

<p>作者：heibaiying<br>链接：<a target="_blank" rel="noopener" href="https://juejin.im/post/6844903950349500430">https://juejin.im/post/6844903950349500430</a><br>来源：掘金<br>著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。</p>
<h2 id="map-join小表误判"><a href="#map-join小表误判" class="headerlink" title="map-join小表误判"></a>map-join小表误判</h2><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="operator">/</span><span class="operator">/</span> a是一个几亿行的大表，b是一个只有几十行的小表。a和b都是由hive创建的表</span><br><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> a <span class="keyword">where</span> id <span class="keyword">not</span> <span class="keyword">in</span> (<span class="keyword">select</span> id <span class="keyword">from</span> b)</span><br></pre></td></tr></table></figure>
<p>在spark ui中看到了该sql的执行计划，该sql语句执行了Map-side Join操作，但是spark把a表当成了小表，准备把a表broadcast到其他的节点，然后就是一直卡在这步broadcast操作上。 造成上述问题的原因就是spark认为a表是一个小表，但是在spark ui上明显可以看到a表读了很多的行。但是为什么spark还会认为a表是一个小表呢？原因是spark判断一个hive表的大小会用hive的metastore数据来判断，因为我们的a表没有执行过ANALYZE TABLE，自然a表的metastore里面的数据就不准确了。</p>
<h3 id="解决方法"><a href="#解决方法" class="headerlink" title="解决方法"></a>解决方法</h3><ol>
<li><p>设置<code>spark.sql.statistics.fallBackToHdfs=True</code><br>该参数能让spark直接读取hdfs的文件大小来判断一个表达大小，从而代替从metastore里面的获取的关于表的信息。这样spark自然能正确的判断出表的大小，从而使用b表来进行broadcast。</p>
</li>
<li><p>使用hint<br>在使用sql语句执行的时候在sql语句里面加上mapjoin的注释，也能够达到相应的效果，比如把上述的sql语句改成:</p>
</li>
</ol>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="comment">/*+ BROADCAST (b) */</span> <span class="operator">*</span> <span class="keyword">from</span> a <span class="keyword">where</span> id <span class="keyword">not</span> <span class="keyword">in</span> (<span class="keyword">select</span> id <span class="keyword">from</span> b)</span><br></pre></td></tr></table></figure>
<p>这样spark也会使用b表来进行broadcast。</p>
<ol start="3">
<li>使用spark代码的方式<br>使用broadcast函数就能达到此效果：</li>
</ol>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql.functions import broadcast</span><br><span class="line">broadcast(spark.table(&quot;b&quot;)).<span class="keyword">join</span>(spark.table(&quot;a&quot;), &quot;id&quot;).<span class="keyword">show</span>()</span><br></pre></td></tr></table></figure>
<ol start="4">
<li>备注</li>
</ol>
<ul>
<li><p>只有当要进行join的表的大小小于spark.sql.autoBroadcastJoinThreshold（默认是10M）的时候，才会进行mapjoin。</p>
</li>
<li><p>Impala通过hint和执行表的位置调整也能够优化join操作，通过explain也可以查看sql的执行计划，然后再进行优化。</p>
</li>
</ul>
<p>Join背景  </p>
<p>当前SparkSQL支持三种join算法：Shuffle Hash Join、Broadcast Hash Join以及Sort Merge Join。其中前两者归根到底都属于Hash Join，只不过载Hash Join之前需要先Shuffle还是先Broadcast。其实，Hash Join算法来自于传统数据库，而Shuffle和Broadcast是大数据在分布式情况下的概念，两者结合的产物。因此可以说，大数据的根就是传统数据库。Hash Join是内核。</p>
<h4 id="Spark-Join的分类和实现机制"><a href="#Spark-Join的分类和实现机制" class="headerlink" title="Spark Join的分类和实现机制"></a>Spark Join的分类和实现机制</h4><p><img src="vx_images/4410808926830" alt="图片"></p>
<p>上图是Spark Join的分类和使用。</p>
<h5 id="Hash-Join"><a href="#Hash-Join" class="headerlink" title="Hash Join"></a>Hash Join</h5><p>先来看看这样一条SQL语句：select * from order,item where item.id = order.i_id，参与join的两张表是order和item，join key分别是item.id以及order.i_id。现在假设Join采用的是hash join算法，整个过程会经历三步：</p>
<ul>
<li><p>  确定Build Table以及Probe Table：这个概念比较重要，Build Table会被构建成以join key为key的hash table，而Probe Table使用join key在这张hash table表中寻找符合条件的行，然后进行join链接。Build表和Probe表是Spark决定的。通常情况下，小表会被作为Build Table，较大的表会被作为Probe Table。</p>
</li>
<li><p>  构建Hash Table：依次读取Build Table(item)的数据，对于每一条数据根据Join Key(item.id)进行hash，hash到对应的bucket中(类似于HashMap的原理)，最后会生成一张HashTable，HashTable会缓存在内存中，如果内存放不下会dump到磁盘中。</p>
</li>
<li><p>  匹配：生成Hash Table后，在依次扫描Probe Table(order)的数据，使用相同的hash函数(在spark中，实际上就是要使用相同的partitioner)在Hash Table中寻找hash(join key)相同的值，如果匹配成功就将两者join在一起。</p>
</li>
</ul>
<h5 id="Broadcast-Hash-Join"><a href="#Broadcast-Hash-Join" class="headerlink" title="Broadcast Hash Join"></a>Broadcast Hash Join</h5><p>当Join的一张表很小的时候，使用broadcast hash join。</p>
<p>Broadcast Hash Join的条件有以下几个：</p>
<ul>
<li><p>  被广播的表需要小于spark.sql.autoBroadcastJoinThreshold所配置的信息，默认是10M；</p>
</li>
<li><p>  基表不能被广播，比如left outer join时，只能广播右表。</p>
</li>
</ul>
<p><img src="vx_images/4390860112486" alt="图片"></p>
<p>broadcast hash join可以分为两步：</p>
<ul>
<li><p>  broadcast阶段：将小表广播到所有的executor上，广播的算法有很多，最简单的是先发给driver，driver再统一分发给所有的executor，要不就是基于bittorrete的p2p思路；</p>
</li>
<li><p>  hash join阶段：在每个executor上执行 hash join，小表构建为hash table，大表的分区数据匹配hash table中的数据。</p>
</li>
</ul>
<h5 id="Sort-Merge-Join"><a href="#Sort-Merge-Join" class="headerlink" title="Sort Merge Join"></a>Sort Merge Join</h5><p><img src="vx_images/4199861566008" alt="图片"></p>
<p>当两个表都非常大时，SparkSQL采用了一种全新的方案来对表进行Join，即Sort Merge Join。这种方式不用将一侧数据全部加载后再进行hash join，但需要在join前将数据进行排序。</p>
<p>首先将两张表按照join key进行重新shuffle，保证join key值相同的记录会被分在相应的分区，分区后对每个分区内的数据进行排序，排序后再对相应的分区内的记录进行连接。可以看出，无论分区有多大，Sort Merge Join都不用把一侧的数据全部加载到内存中，而是即用即丢；因为两个序列都有有序的，从头遍历，碰到key相同的就输出，如果不同，左边小就继续取左边，反之取右边。从而大大提高了大数据量下sql join的稳定性。</p>
<p>整个过程分为三个步骤：</p>
<ul>
<li><p>  shuffle阶段：将两张大表根据join key进行重新分区，两张表数据会分布到整个集群，以便分布式并行处理</p>
</li>
<li><p>  sort阶段：对单个分区节点的两表数据，分别进行排序</p>
</li>
<li><p>  merge阶段：对排好序的两张分区表数据执行join操作。join操作很简单，分别遍历两个有序序列，碰到相同join key就merge输出，否则继续取更小一边的key。</p>
</li>
</ul>
<p><img src="vx_images/4141432879366" alt="图片"></p>
<p>经过上文的分析，很明显可以得出这几种join的代价关系：cost(Broadcast Hash Join)&lt; cost(Shuffle Hash Join) &lt; cost(Sort Merge Join)，数据仓库设计时最好避免大表与大表的join查询，SparkSQL也可以根据内存资源、带宽资源适量将参数spark.sql. autoBroadcastJoinThreshold调大，让更多join实际执行为Broadcast Hash Join。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/3/"><i class="fa fa-angle-left" aria-label="Previous page"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/3/">3</a><span class="page-number current">4</span><a class="page-number" href="/page/5/">5</a><span class="space">&hellip;</span><a class="page-number" href="/page/24/">24</a><a class="extend next" rel="next" href="/page/5/"><i class="fa fa-angle-right" aria-label="Next page"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">aaronzhang</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">236</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">16</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">127</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">aaronzhang</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  

</body>
</html>
