<!DOCTYPE html>
<html lang="zh-Hans">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta property="og:type" content="website">
<meta property="og:title" content="Guadazi">
<meta property="og:url" content="http://example.com/page/4/index.html">
<meta property="og:site_name" content="Guadazi">
<meta property="og:locale">
<meta property="article:author" content="aaronzhang">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://example.com/page/4/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'zh-Hans'
  };
</script>

  <title>Guadazi</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Guadazi</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://example.com/Java/collection/ConcurrentSkipListMap/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="aaronzhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Guadazi">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/Java/collection/ConcurrentSkipListMap/" class="post-title-link" itemprop="url">Untitled</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-04-24 12:58:51" itemprop="dateCreated datePublished" datetime="2021-04-24T12:58:51+08:00">2021-04-24</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="基于跳跃表的-ConcurrentSkipListMap-内部实现（Java-8）"><a href="#基于跳跃表的-ConcurrentSkipListMap-内部实现（Java-8）" class="headerlink" title="基于跳跃表的 ConcurrentSkipListMap 内部实现（Java 8）"></a>基于跳跃表的 ConcurrentSkipListMap 内部实现（Java 8）</h1><p>我们知道 HashMap 是一种键值对形式的数据存储容器，但是它有一个缺点是，元素内部无序。由于它内部根据键的 hash 值取模表容量来得到元素的存储位置，所以整体上说 HashMap 是无序的一种容器。当然，jdk 中也为我们提供了基于红黑树的存储的 TreeMap 容器，它的内部元素是有序的，但是由于它内部通过红黑结点的各种变换来维持二叉搜索树的平衡，相对复杂，并且在并发环境下碍于 rebalance 操作，性能会受到一定的影响。</p>
<p>跳表（SkipList）是一种随机化的数据结构，通过“空间来换取时间”的一个算法，建立多级索引，实现以二分查找遍历一个有序链表。时间复杂度等同于红黑树，O(log n)。但实现却远远比红黑树要简单，本篇我们主要从以下几个方面来对这种并发版本的数据结构进行学习：</p>
<ul>
<li>跳跃表的数据结构介绍</li>
<li>ConcurrentSkipListMap 的前导知识预备</li>
<li>基本的成员属性介绍</li>
<li>put 方法并发添加</li>
<li>remove 方法的并发删除</li>
<li>get 方法获取指定结点的 value</li>
<li>其它的一些方法的简单描述</li>
</ul>
<h2 id="一、跳跃表的数据结构介绍"><a href="#一、跳跃表的数据结构介绍" class="headerlink" title="一、跳跃表的数据结构介绍"></a>一、跳跃表的数据结构介绍</h2><p><img src="_v_images/20200819194446142_966009886" alt="这里写图片描述"></p>
<p>跳跃表具有以下几个必备的性质：</p>
<ul>
<li>最底层包含所有节点的一个有序的链表</li>
<li>每一层都是一个有序的链表</li>
<li>每个节点都有两个指针，一个指向右侧节点（没有则为空），一个指向下层节点（没有则为空）</li>
<li>必备一个头节点指向最高层的第一个节点，通过它可以遍历整张表</li>
</ul>
<p>当我们查找一个元素的时候就是这样的：</p>
<p><img src="_v_images/20200819194445937_815191891" alt="这里写图片描述"></p>
<p>查找的过程有点像我们的二分查找，不过这里我们是通过为链表建立多级索引，以空间换时间来实现二分查找。所以，跳表的查询操作的时间复杂度为 O(logN)。</p>
<p>接着我们看看跳表的插入操作：<br>首先，跳表的插入必然会在底层增加一个节点，但是往上的层次是否需要增加节点则完全是随机的，SkipList 通过概率保证整张表的节点分布均匀，它不像红黑树是通过人为的 rebalance 操作来保证二叉树的平衡性。（数学对于计算机还是很重要的）。</p>
<p>通过概率算法得到新插入节点的一个 level 值，如果小于当前表的最大 level，从最底层到 level 层都添加一个该节点。例如：</p>
<p><img src="_v_images/20200819194445733_1944248029" alt="这里写图片描述"></p>
<p>如图，首先 119 节点会被添加到最底层链表的合适位置，然后通过概率算法得到 level 为 2，于是 1—level 层中的每一层都添加了 119 节点。</p>
<p>如果概率算法得到的 level 大于当前表的最大 level 值的话，那么将会新增一个 level，并且将新节点添加到该 level 上。</p>
<p><img src="_v_images/20200819194445529_52135211" alt="这里写图片描述"></p>
<p>跳表的删除操作其实就是一个查找加删除节点的操作</p>
<p><img src="_v_images/20200819194445324_1213153640" alt="这里写图片描述"></p>
<p>好了，有关跳表这种数据结构的基本理论知识已经简单的介绍了，下面我们看 jdk 中对该数据结构的基本实现情况，并了解它的并发版本是如何实现的。</p>
<h2 id="二、ConcurrentSkipListMap-的前导知识预备"><a href="#二、ConcurrentSkipListMap-的前导知识预备" class="headerlink" title="二、ConcurrentSkipListMap 的前导知识预备"></a>二、ConcurrentSkipListMap 的前导知识预备</h2><p>在实际分析 put 方法之前，有一些预备的知识我们需要先有个大致的了解，否则在实际分析源码的时候会感觉吃力些。</p>
<p>首先是删除操作，在我们上述的跳表数据结构中谈及的删除操作主要是定位待删结点+删除该结点的一个复合操作。而在我们的并发跳表中，删除操作相对复杂点，需要分为以下三个步骤：</p>
<ul>
<li>找到待删结点并将其 value 属性值由 notnull 置为 null，整个过程是基于 CAS 无锁式算法的</li>
<li>向待删结点的 next 位置新增一个 marker 标记结点，整个过程也是基于 CAS 无锁式算法</li>
<li>CAS 式删除具体的结点，实际上也就是跳过该待删结点，让待删结点的前驱节点直接越过本身指向待删结点的后继结点即可</li>
</ul>
<p>例如我们有以下三个结点，n 为待删除的结点。</p>
<blockquote>
<p>+------+ +------+ +——+<br>… | b |——&gt;| n |—–&gt;| f | …<br>+------+ +------+ +——+</p>
</blockquote>
<p>第一步是找到 n ，然后 CAS 该结点的 value 值为 null。如果该步骤失败了，那么 ConcurrentSkipListMap 会通过循环再次尝试 CAS 将 n 的 value 属性赋值为 null。</p>
<p>第二步是建立在第一步成功的前提下的，n 的当前 value 属性的值为 null，ConcurrentSkipListMap 试图在 n 的后面增加一个空的 node 结点（marker）以分散下一步的并发冲突性。</p>
<blockquote>
<p>+------+ +------+ +------+ +——+<br>… | b |——&gt;| n |—–&gt;|marker|—-&gt;| f | …<br>+------+ +------+ +------+ +——+</p>
</blockquote>
<p>第三步，断链操作。如果 marker 添加失败，将不会有第三步，直接回重新回到第一步。如果成功添加，那么将试图断开 b 到 n 的链接，直接绕过 n，让 b 的 next 指向 f。那么，这个 n 结点将作为内存中的一个游离结点，最终被 GC 掉。断开失败的话，也将回到第一步。</p>
<blockquote>
<p>+------+ +——+<br>… | b |———————–&gt;| f | …<br>+------+ +——+</p>
</blockquote>
<p>主要还是有关删除这方面的预备知识，其它的信息点我们将从实际方法的源码中再进行分析。</p>
<h2 id="三、基本的成员属性介绍"><a href="#三、基本的成员属性介绍" class="headerlink" title="三、基本的成员属性介绍"></a>三、基本的成员属性介绍</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">static</span> <span class="keyword">final</span> <span class="class"><span class="keyword">class</span> <span class="title">Node</span>&lt;<span class="title">K</span>,<span class="title">V</span>&gt; </span>&#123;</span><br><span class="line">    <span class="keyword">final</span> K key;</span><br><span class="line">    <span class="keyword">volatile</span> Object value;</span><br><span class="line">    <span class="keyword">volatile</span> Node&lt;K,V&gt; next;</span><br><span class="line"></span><br><span class="line">    Node(K key, Object value, Node&lt;K,V&gt; next) &#123;</span><br><span class="line">        <span class="keyword">this</span>.key = key;</span><br><span class="line">        <span class="keyword">this</span>.value = value;</span><br><span class="line">        <span class="keyword">this</span>.next = next;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">//省略其它的一些基于当前结点的 CAS 方法</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>这是 node 结点类型的定义，是最基本的数据存储单元。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">Index</span>&lt;<span class="title">K</span>,<span class="title">V</span>&gt; </span>&#123;</span><br><span class="line">    <span class="keyword">final</span> Node&lt;K,V&gt; node;</span><br><span class="line">    <span class="keyword">final</span> Index&lt;K,V&gt; down;</span><br><span class="line">    <span class="keyword">volatile</span> Index&lt;K,V&gt; right;</span><br><span class="line"></span><br><span class="line">    Index(Node&lt;K,V&gt; node, Index&lt;K,V&gt; down, Index&lt;K,V&gt; right) &#123;</span><br><span class="line">        <span class="keyword">this</span>.node = node;</span><br><span class="line">        <span class="keyword">this</span>.down = down;</span><br><span class="line">        <span class="keyword">this</span>.right = right;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">//省略其它的一些基于当前结点的 CAS 方法</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>Index 结点封装了 node 结点，作为跳表的最基本组成单元。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">static</span> <span class="keyword">final</span> <span class="class"><span class="keyword">class</span> <span class="title">HeadIndex</span>&lt;<span class="title">K</span>,<span class="title">V</span>&gt; <span class="keyword">extends</span> <span class="title">Index</span>&lt;<span class="title">K</span>,<span class="title">V</span>&gt; </span>&#123;</span><br><span class="line">    <span class="keyword">final</span> <span class="keyword">int</span> level;</span><br><span class="line">    HeadIndex(Node&lt;K,V&gt; node, Index&lt;K,V&gt; down, Index&lt;K,V&gt; right, <span class="keyword">int</span> level) &#123;</span><br><span class="line">        <span class="keyword">super</span>(node, down, right);</span><br><span class="line">        <span class="keyword">this</span>.level = level;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>封装了 Index 结点，作为每层的头结点，level 属性用于标识当前层次的序号。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * The topmost head index of the skiplist.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">transient</span> <span class="keyword">volatile</span> HeadIndex&lt;K,V&gt; head;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>整个跳表的头结点，通过它可以遍历访问整张跳表。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//比较器，用于比较两个元素的键值大小，如果没有显式传入则默认为自然排序</span></span><br><span class="line"><span class="keyword">final</span> Comparator&lt;? <span class="keyword">super</span> K&gt; comparator;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Special value used to identify base-level header</span></span><br><span class="line"><span class="comment"> * 特殊的值，用于初始化跳表</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> Object BASE_HEADER = <span class="keyword">new</span> Object();</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>紧接着，我们看看它的几个构造器：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//未传入比较器，则为默认值</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">ConcurrentSkipListMap</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">this</span>.comparator = <span class="keyword">null</span>;</span><br><span class="line">    initialize();</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">ConcurrentSkipListMap</span><span class="params">(Comparator&lt;? <span class="keyword">super</span> K&gt; comparator)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">this</span>.comparator = comparator;</span><br><span class="line">    initialize();</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//所有的构造器都会调用这个初始化的方法</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">initialize</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    keySet = <span class="keyword">null</span>;</span><br><span class="line">    entrySet = <span class="keyword">null</span>;</span><br><span class="line">    values = <span class="keyword">null</span>;</span><br><span class="line">    descendingMap = <span class="keyword">null</span>;</span><br><span class="line">    head = <span class="keyword">new</span> HeadIndex&lt;K,V&gt;(<span class="keyword">new</span> Node&lt;K,V&gt;(<span class="keyword">null</span>, BASE_HEADER, <span class="keyword">null</span>),<span class="keyword">null</span>, <span class="keyword">null</span>, <span class="number">1</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>这个初始化方法主要完成的是对整张跳表的一个初始化操作，head 头指针指向这个并没有什么实际意义的头结点。</p>
<p>基本的成员属性就简单介绍到这，重点还是那三个内部类，都分别代表了什么样的结点类型，都使用在何种场景下，务必清晰。</p>
<h3 id="四、put-并发添加的内部实现"><a href="#四、put-并发添加的内部实现" class="headerlink" title="四、put 并发添加的内部实现"></a>四、put 并发添加的内部实现</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//基本的 put 方法，向跳表中添加一个节点</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> V <span class="title">put</span><span class="params">(K key, V value)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (value == <span class="keyword">null</span>)</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> NullPointerException();</span><br><span class="line">    <span class="keyword">return</span> doPut(key, value, <span class="keyword">false</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>put 方法的内部调用的是 doPut 方法来实现添加元素的，但是由于 doPut 方法的方法体很长，我们分几个部分进行分析。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//第一部分</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> V <span class="title">doPut</span><span class="params">(K key, V value, <span class="keyword">boolean</span> onlyIfAbsent)</span> </span>&#123;</span><br><span class="line">    Node&lt;K,V&gt; z;</span><br><span class="line">    <span class="comment">//边界值判断，空的 key 自然是不允许插入的</span></span><br><span class="line">    <span class="keyword">if</span> (key == <span class="keyword">null</span>)</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> NullPointerException();</span><br><span class="line">    <span class="comment">//拿到比较器的引用</span></span><br><span class="line">    Comparator&lt;? <span class="keyword">super</span> K&gt; cmp = comparator;</span><br><span class="line">    outer: <span class="keyword">for</span> (;;) &#123;</span><br><span class="line">	    <span class="comment">//根据 key，找到待插入的位置</span></span><br><span class="line">	    <span class="comment">//b 叫做前驱节点，将来作为新加入结点的前驱节点</span></span><br><span class="line">	    <span class="comment">//n 叫做后继结点，将来作为新加入结点的后继结点</span></span><br><span class="line">	    <span class="comment">//也就是说，新节点将插入在 b 和 n 之间</span></span><br><span class="line">        <span class="keyword">for</span> (Node&lt;K,V&gt; b = findPredecessor(key, cmp), n = b.next;;) &#123;</span><br><span class="line">	        <span class="comment">//如果 n 为 null，那么说明 b 是链表的最尾端的结点，这种情况比较简单，直接构建新节点插入即可</span></span><br><span class="line">	        <span class="comment">//否则走下面的判断体</span></span><br><span class="line">            <span class="keyword">if</span> (n != <span class="keyword">null</span>) &#123;</span><br><span class="line">                Object v; <span class="keyword">int</span> c;</span><br><span class="line">                Node&lt;K,V&gt; f = n.next;</span><br><span class="line">                <span class="comment">//如果 n 不再是 b 的后继结点了，说明有其他线程向 b 后面添加了新元素</span></span><br><span class="line">                <span class="comment">//那么我们直接退出内循环，重新计算新节点将要插入的位置</span></span><br><span class="line">                <span class="keyword">if</span> (n != b.next)</span><br><span class="line">                    <span class="keyword">break</span>;</span><br><span class="line">                <span class="comment">//value =0 说明 n 已经被标识位待删除，其他线程正在进行删除操作</span></span><br><span class="line">                <span class="comment">//调用 helpDelete 帮助删除，并退出内层循环重新计算待插入位置</span></span><br><span class="line">                <span class="keyword">if</span> ((v = n.value) == <span class="keyword">null</span>) &#123; </span><br><span class="line">                    n.helpDelete(b, f);</span><br><span class="line">                    <span class="keyword">break</span>;</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="comment">//b 已经被标记为待删除，前途结点 b 都丢了，可不得重新计算待插入位置吗</span></span><br><span class="line">                <span class="keyword">if</span> (b.value == <span class="keyword">null</span> || v == n) </span><br><span class="line">                    <span class="keyword">break</span>;</span><br><span class="line">				<span class="comment">//如果新节点的 key 大于 n 的 key 说明找到的前驱节点有误，按序往后挪一个位置即可</span></span><br><span class="line">				<span class="comment">//回到内层循环重新试图插入</span></span><br><span class="line">                <span class="keyword">if</span> ((c = cpr(cmp, key, n.key)) &gt; <span class="number">0</span>) &#123;</span><br><span class="line">                    b = n;</span><br><span class="line">                    n = f;</span><br><span class="line">                    <span class="keyword">continue</span>;</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="comment">//新节点的 key 等于 n 的 key，这是一次 update 操作，CAS 更新即可</span></span><br><span class="line">                <span class="comment">//如果更新失败，重新进循环再来一次</span></span><br><span class="line">                <span class="keyword">if</span> (c == <span class="number">0</span>) &#123;</span><br><span class="line">                    <span class="keyword">if</span> (onlyIfAbsent || n.casValue(v, value)) &#123;</span><br><span class="line">                        <span class="meta">@SuppressWarnings(&quot;unchecked&quot;)</span> V vv = (V)v;</span><br><span class="line">                        <span class="keyword">return</span> vv;</span><br><span class="line">                    &#125;</span><br><span class="line">                    <span class="keyword">break</span>; </span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">	    <span class="comment">//无论遇到何种问题，到这一步说明待插位置已经确定</span></span><br><span class="line">            z = <span class="keyword">new</span> Node&lt;K,V&gt;(key, value, n);</span><br><span class="line">            <span class="keyword">if</span> (!b.casNext(n, z))</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">            <span class="comment">//如果成功了，退出最外层循环，完成了底层的插入工作        </span></span><br><span class="line">            <span class="keyword">break</span> outer;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>以上这一部分主要完成了向底层链表插入一个节点，至于其中具体的怎么找前驱节点的方法稍后介绍。但这其实只不过才完成一小半的工作，就像红黑树在插入后需要 rebalance 一样，我们的跳表需要根据概率算法保证节点分布稳定，它的调节措施相对于红黑树来说就简单多了，通过往上层索引层添加相关引用即可，以空间换时间。具体的我们来看：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//第二部分</span></span><br><span class="line"><span class="comment">//获取一个线程无关的随机数，占四个字节，32 个比特位</span></span><br><span class="line"><span class="keyword">int</span> rnd = ThreadLocalRandom.nextSecondarySeed();</span><br><span class="line">	<span class="comment">//和 1000 0000 0000 0000 0000 0000 0000 0001 与</span></span><br><span class="line">	<span class="comment">//如果等于 0，说明这个随机数最高位和最低位都为 0，这种概率很大</span></span><br><span class="line">	<span class="comment">//如果不等于 0，那么将仅仅把新节点插入到最底层的链表中即可，不会往上层递归</span></span><br><span class="line">    <span class="keyword">if</span> ((rnd &amp; <span class="number">0x80000001</span>) == <span class="number">0</span>) &#123; </span><br><span class="line">        <span class="keyword">int</span> level = <span class="number">1</span>, max;</span><br><span class="line">        <span class="comment">//用低位连续为 1 的个数作为 level 的值，也是一种概率策略</span></span><br><span class="line">        <span class="keyword">while</span> (((rnd &gt;&gt;&gt;= <span class="number">1</span>) &amp; <span class="number">1</span>) != <span class="number">0</span>)</span><br><span class="line">            ++level;</span><br><span class="line">        Index&lt;K,V&gt; idx = <span class="keyword">null</span>;</span><br><span class="line">        HeadIndex&lt;K,V&gt; h = head;</span><br><span class="line">        <span class="comment">//如果概率算得的 level 在当前跳表 level 范围内</span></span><br><span class="line">        <span class="comment">//构建一个从 1 到 level 的纵列 index 结点引用</span></span><br><span class="line">        <span class="keyword">if</span> (level &lt;= (max = h.level)) &#123;</span><br><span class="line">            <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">1</span>; i &lt;= level; ++i)</span><br><span class="line">                idx = <span class="keyword">new</span> Index&lt;K,V&gt;(z, idx, <span class="keyword">null</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">//否则需要新增一个 level 层</span></span><br><span class="line">        <span class="keyword">else</span> &#123; </span><br><span class="line">            level = max + <span class="number">1</span>; </span><br><span class="line">            <span class="meta">@SuppressWarnings(&quot;unchecked&quot;)</span></span><br><span class="line">            Index&lt;K,V&gt;[] idxs =(Index&lt;K,V&gt;[])<span class="keyword">new</span> Index&lt;?,?&gt;[level+<span class="number">1</span>];</span><br><span class="line">            <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">1</span>; i &lt;= level; ++i)</span><br><span class="line">                idxs[i] = idx = <span class="keyword">new</span> Index&lt;K,V&gt;(z, idx, <span class="keyword">null</span>);</span><br><span class="line">            <span class="keyword">for</span> (;;) &#123;</span><br><span class="line">                h = head;</span><br><span class="line">                <span class="keyword">int</span> oldLevel = h.level;</span><br><span class="line">                <span class="comment">//level 肯定是比 oldLevel 大一的，如果小了说明其他线程更新过表了</span></span><br><span class="line">                <span class="keyword">if</span> (level &lt;= oldLevel) </span><br><span class="line">                    <span class="keyword">break</span>;</span><br><span class="line">                HeadIndex&lt;K,V&gt; newh = h;</span><br><span class="line">                Node&lt;K,V&gt; oldbase = h.node;</span><br><span class="line">                <span class="comment">//正常情况下，循环只会执行一次，如果由于其他线程的并发操作导致 oldLevel 的值不稳定，那么会执行多次循环体</span></span><br><span class="line">                <span class="keyword">for</span> (<span class="keyword">int</span> j = oldLevel+<span class="number">1</span>; j &lt;= level; ++j)</span><br><span class="line">                    newh = <span class="keyword">new</span> HeadIndex&lt;K,V&gt;(oldbase, newh, idxs[j], j);</span><br><span class="line">                <span class="comment">//更新头指针</span></span><br><span class="line">                <span class="keyword">if</span> (casHead(h, newh)) &#123;</span><br><span class="line">                    h = newh;</span><br><span class="line">                    idx = idxs[level = oldLevel];</span><br><span class="line">                    <span class="keyword">break</span>;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>这一部分的代码主要完成的是根据 level 的值，确认是否需要增加一层索引，如果不需要则构建好底层到 level 层的 index 结点的纵向引用。如果需要，则新创建一层索引，完成 head 结点的指针转移，并构建好纵向的 index 结点引用。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//第三部分</span></span><br><span class="line"><span class="keyword">if</span> ((rnd &amp; <span class="number">0x80000001</span>) == <span class="number">0</span>)&#123;</span><br><span class="line"><span class="comment">//省略第二部分的代码段</span></span><br><span class="line"><span class="comment">//第三部分的代码是紧接着第二部分代码段后面的</span></span><br><span class="line">	splice: <span class="keyword">for</span> (<span class="keyword">int</span> insertionLevel = level;;) &#123;</span><br><span class="line">            <span class="keyword">int</span> j = h.level;</span><br><span class="line">            <span class="keyword">for</span> (Index&lt;K,V&gt; q = h, r = q.right, t = idx;;) &#123;</span><br><span class="line">	            <span class="comment">//其他线程并发操作导致头结点被删除，直接退出外层循环</span></span><br><span class="line">	            <span class="comment">//这种情况发生的概率很小，除非并发量实在太大</span></span><br><span class="line">                <span class="keyword">if</span> (q == <span class="keyword">null</span> || t == <span class="keyword">null</span>)</span><br><span class="line">                    <span class="keyword">break</span> splice;</span><br><span class="line">                <span class="keyword">if</span> (r != <span class="keyword">null</span>) &#123;</span><br><span class="line">                    Node&lt;K,V&gt; n = r.node;</span><br><span class="line">                    <span class="keyword">int</span> c = cpr(cmp, key, n.key);</span><br><span class="line">                    <span class="comment">//如果 n 正在被其他线程删除，那么调用 unlink 去删除它</span></span><br><span class="line">                    <span class="keyword">if</span> (n.value == <span class="keyword">null</span>) &#123;</span><br><span class="line">                        <span class="keyword">if</span> (!q.unlink(r))</span><br><span class="line">                            <span class="keyword">break</span>;</span><br><span class="line">                        <span class="comment">//重新获取 q 的右结点，再次进入循环</span></span><br><span class="line">                        r = q.right;</span><br><span class="line">                        <span class="keyword">continue</span>;</span><br><span class="line">                    &#125;</span><br><span class="line">                    <span class="comment">//c &gt; 0 说明前驱结点定位有误，重新进入</span></span><br><span class="line">                    <span class="keyword">if</span> (c &gt; <span class="number">0</span>) &#123;</span><br><span class="line">                        q = r;</span><br><span class="line">                        r = r.right;</span><br><span class="line">                        <span class="keyword">continue</span>;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="keyword">if</span> (j == insertionLevel) &#123;</span><br><span class="line">		            <span class="comment">//尝试着将 t 插在 q 和 r 之间，如果失败了，退出内循环重试</span></span><br><span class="line">                    <span class="keyword">if</span> (!q.link(r, t))</span><br><span class="line">                        <span class="keyword">break</span>; <span class="comment">// restart</span></span><br><span class="line">                    <span class="comment">//如果插入完成后，t 结点被删除了，那么结束插入操作</span></span><br><span class="line">                    <span class="keyword">if</span> (t.node.value == <span class="keyword">null</span>) &#123;</span><br><span class="line">                        findNode(key);</span><br><span class="line">                        <span class="keyword">break</span> splice;</span><br><span class="line">                    &#125;</span><br><span class="line">                    <span class="comment">// insertionLevel-- 处理底层链接</span></span><br><span class="line">                    <span class="keyword">if</span> (--insertionLevel == <span class="number">0</span>)</span><br><span class="line">                        <span class="keyword">break</span> splice;</span><br><span class="line">                &#125;</span><br><span class="line">				<span class="comment">//--j，j 应该与 insertionLevel 同步，它代表着我们创建的那个纵向的结点数组的索引</span></span><br><span class="line">				<span class="comment">//并完成层次下移操作</span></span><br><span class="line">                <span class="keyword">if</span> (--j &gt;= insertionLevel &amp;&amp; j &lt; level)</span><br><span class="line">                    t = t.down;</span><br><span class="line">                <span class="comment">//至此，新节点在当前层次的前后引用关系已经被链接完成，现在处理下一层</span></span><br><span class="line">                q = q.down;</span><br><span class="line">                r = q.right;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>我们根据概率算法得到了一个 level 值，并且通过第二步创建了 level 个新节点并构成了一个纵向的引用关联，但是这些纵向的结点并没有链接到每层中。而我们的第三部分代码就是完成的这个工作，将我们的新节点在每个索引层都构建好前后的链接关系。下面用三张图描述着三个部分所完成的主要工作。</p>
<p>初始化的跳表如下：</p>
<p><img src="_v_images/20200819194445120_1704156511" alt="这里写图片描述"></p>
<p>第一部分，新增一个结点到最底层的链表上。</p>
<p><img src="_v_images/20200819194444915_223530245" alt="这里写图片描述"></p>
<p>第二部分，假设概率得出一个 level 值为 10，那么根据跳表的算法描述需要新建一层索引层。</p>
<p><img src="_v_images/20200819194444711_692580724" alt="这里写图片描述"></p>
<p>第三步，链接各个索引层次上的新节点。</p>
<p><img src="_v_images/20200819194444122_943848032" alt="这里写图片描述"></p>
<p>这样就完成了新增结点到跳表中的全部过程，大体上已如上图描述，至于 ConcurrentSkipListMap 中关于并发处理的细节之处，图中无法展示，大家可据此重新感受下源码的实现过程。下面我们着重描述下整个 doPut 方法中还涉及的其他几个方法的具体实现。</p>
<p><strong>首先是 findPredecessor 方法</strong>，我们说该方法将根据给定的 key，为我们返回最合适的前驱节点。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> Node&lt;K,V&gt; <span class="title">findPredecessor</span><span class="params">(Object key, Comparator&lt;? <span class="keyword">super</span> K&gt; cmp)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (key == <span class="keyword">null</span>)</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> NullPointerException(); </span><br><span class="line">    <span class="keyword">for</span> (;;) &#123;</span><br><span class="line">        <span class="keyword">for</span> (Index&lt;K,V&gt; q = head, r = q.right, d;;) &#123;</span><br><span class="line">            <span class="comment">//r 为空说明 head 后面并没有其他节点了</span></span><br><span class="line">            <span class="keyword">if</span> (r != <span class="keyword">null</span>) &#123;</span><br><span class="line">                Node&lt;K,V&gt; n = r.node;</span><br><span class="line">				<span class="comment">// r 节点处于待删除状态，那么尝试 unlink 它，失败了将重新进入循环再此尝试</span></span><br><span class="line">				<span class="comment">//否则重新获取 q 的右结点并重新进入循环查找前驱节点</span></span><br><span class="line">                <span class="keyword">if</span> (n.value == <span class="keyword">null</span>) &#123;</span><br><span class="line">                    <span class="keyword">if</span> (!q.unlink(r))</span><br><span class="line">                        <span class="keyword">break</span>;           <span class="comment">// restart</span></span><br><span class="line">                    r = q.right;         <span class="comment">// reread r</span></span><br><span class="line">                    <span class="keyword">continue</span>;</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="comment">//大于零说明当前位置上的 q 还不是我们要的前驱节点，继续往后找</span></span><br><span class="line">                <span class="keyword">if</span> (cpr(cmp, key, k) &gt; <span class="number">0</span>) &#123;</span><br><span class="line">                    q = r;</span><br><span class="line">                    r = r.right;</span><br><span class="line">                    <span class="keyword">continue</span>;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="comment">//如果当前的 level 结束了或者 cpr(cmp, key, k) &lt;= 0 会达到此位置</span></span><br><span class="line">            <span class="comment">//往低层递归，如果没有低层了，那么当前的 q 就是最合适的前驱节点</span></span><br><span class="line">            <span class="comment">//整个循环只有这一个出口，无论如何最终都会从此处结束方法</span></span><br><span class="line">            <span class="keyword">if</span> ((d = q.down) == <span class="keyword">null</span>)</span><br><span class="line">                <span class="keyword">return</span> q.node;</span><br><span class="line">           <span class="comment">//否则向低层递归并重置 q 和 r</span></span><br><span class="line">            q = d;</span><br><span class="line">            r = d.right;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>最后总结下 findPredecessor 方法的大体逻辑，首先程序会从 head 节点开始在当前的索引层上寻找最后一个比给定 key 小的结点，它就是我们需要的前驱节点（q），我们只需要返回它即可。</p>
<p><strong>其次我们看看 helpDelete 方法</strong>，当检测到某个结点的 value 属性值为 null 的时候，一般都会调用这个方法来删除该结点。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">   一般的调用形式如下：</span></span><br><span class="line"><span class="comment">   n.helpDelete(b, f);</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">helpDelete</span><span class="params">(Node&lt;K,V&gt; b, Node&lt;K,V&gt; f)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (f == next &amp;&amp; <span class="keyword">this</span> == b.next) &#123;</span><br><span class="line">       <span class="keyword">if</span> (f == <span class="keyword">null</span> || f.value != f) </span><br><span class="line">            casNext(f, <span class="keyword">new</span> Node&lt;K,V&gt;(f));</span><br><span class="line">        <span class="keyword">else</span></span><br><span class="line">            b.casNext(<span class="keyword">this</span>, f.next);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>该方法是 Node 结点的内部实例方法，逻辑相对简单，此处不再赘述。通过该方法可以完成将 b.next 指向 f，完成对 n 结点的删除。</p>
<p>至此，有关 put 方法的源码分析就简单到这，大部分的代码还是用于实现跳表这种数据结构的构建和插入，关于并发的处理，你会发现基本都是双层 for 循环+ CAS 无锁式更新，如果遇到竞争失利将退出里层循环重新进行尝试，否则成功的话就会直接 return 或者退出外层循环并结束 CAS 操作。下面我们看删除操作是如何实现的。</p>
<h2 id="五、remove-并发删除操作的内部实现"><a href="#五、remove-并发删除操作的内部实现" class="headerlink" title="五、remove 并发删除操作的内部实现"></a>五、remove 并发删除操作的内部实现</h2><p>remove 方法的部分内容我们在介绍相关预备知识中已经提及过，此处的理解想必会容易些。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> V <span class="title">remove</span><span class="params">(Object key)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> doRemove(key, <span class="keyword">null</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//代码比较多，建议读者结合自己的 jdk 源码共同来分析</span></span><br><span class="line"><span class="function"><span class="keyword">final</span> V <span class="title">doRemove</span><span class="params">(Object key, Object value)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (key == <span class="keyword">null</span>)</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> NullPointerException();</span><br><span class="line">    Comparator&lt;? <span class="keyword">super</span> K&gt; cmp = comparator;</span><br><span class="line">    outer: <span class="keyword">for</span> (;;) &#123;</span><br><span class="line">		<span class="comment">//找到 key 的前驱节点</span></span><br><span class="line">		<span class="comment">//因为删除不单单是根据 key 找到对应的结点，然后赋 null 就完事的，还要负责链接该结点前后的关联</span></span><br><span class="line">        <span class="keyword">for</span> (Node&lt;K,V&gt; b = findPredecessor(key, cmp), n = b.next;;) &#123;</span><br><span class="line">            Object v; <span class="keyword">int</span> c;</span><br><span class="line">            <span class="comment">//目前 n 基本上就是我们要删除的结点，它为 null，那自然不用继续了，已经被删除了</span></span><br><span class="line">            <span class="keyword">if</span> (n == <span class="keyword">null</span>)</span><br><span class="line">                <span class="keyword">break</span> outer;</span><br><span class="line">            Node&lt;K,V&gt; f = n.next;</span><br><span class="line">            <span class="comment">//再次确认 n 还是不是 b 的后继结点，如果不是将退出里层循环重新进入</span></span><br><span class="line">            <span class="keyword">if</span> (n != b.next)               </span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">            <span class="comment">//如果有人正在删除 n，那么帮助它删除</span></span><br><span class="line">            <span class="keyword">if</span> ((v = n.value) == <span class="keyword">null</span>) &#123;     </span><br><span class="line">                n.helpDelete(b, f);</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="comment">//b 被删除了，重新定位前驱节点</span></span><br><span class="line">            <span class="keyword">if</span> (b.value == <span class="keyword">null</span> || v == n)     </span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">            <span class="comment">//正常情况下，key 应该等于 n.key</span></span><br><span class="line">            <span class="comment">//key 大于 n.key 说明我们要找的结点可能在 n 的后面，往后递归即可</span></span><br><span class="line">            <span class="comment">//key 小于 n.key 说明 key 所代表的结点根本不存在</span></span><br><span class="line">            <span class="keyword">if</span> ((c = cpr(cmp, key, n.key)) &lt; <span class="number">0</span>)</span><br><span class="line">                <span class="keyword">break</span> outer;</span><br><span class="line">            <span class="keyword">if</span> (c &gt; <span class="number">0</span>) &#123;</span><br><span class="line">                b = n;</span><br><span class="line">                n = f;</span><br><span class="line">                <span class="keyword">continue</span>;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="comment">//如果删除是根据键和值两个参数来删除的话，value 是不为 null 的</span></span><br><span class="line">            <span class="comment">//这种情况下，如果 n 的 value 属性不等于我们传入的 value ，那么是不进行删除的</span></span><br><span class="line">            <span class="keyword">if</span> (value != <span class="keyword">null</span> &amp;&amp; !value.equals(v))</span><br><span class="line">                <span class="keyword">break</span> outer;</span><br><span class="line">			<span class="comment">//下面三个步骤才是整个删除操作的核心，大致的逻辑我们也在上文提及过了，此处想必会容易理解些</span></span><br><span class="line">			<span class="comment">//第一步，尝试将待删结点的 value 属性赋值 null，失败将退出重试</span></span><br><span class="line">            <span class="keyword">if</span> (!n.casValue(v, <span class="keyword">null</span>))</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">            <span class="comment">//第二步和第三步如果有一步由于竞争失败，将调用 findNode 方法根据我们第一步的成果，也就是删除所有 value 为 null 的结点</span></span><br><span class="line">            <span class="keyword">if</span> (!n.appendMarker(f) || !b.casNext(n, f))</span><br><span class="line">                findNode(key);  </span><br><span class="line">            <span class="comment">//否则说明三个步骤都成功完成了   </span></span><br><span class="line">            <span class="keyword">else</span> &#123;</span><br><span class="line">                findPredecessor(key, cmp);  </span><br><span class="line">                <span class="comment">//判断此次删除后是否导致某一索引层没有其他节点了，并适情况删除该层索引  </span></span><br><span class="line">                <span class="keyword">if</span> (head.right == <span class="keyword">null</span>)</span><br><span class="line">                    tryReduceLevel();</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="meta">@SuppressWarnings(&quot;unchecked&quot;)</span> V vv = (V)v;</span><br><span class="line">            <span class="keyword">return</span> vv;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>remove 方法其实从整体上来看，首先会有一堆的判断，根据给定的 key 和 value 会判断是否存在与 key 对应的一个节点，也会判断和待删结点相关的前后结点是否正在被删除，并适情况帮助删除。其次才是删除的三大步骤，核心步骤还是将待删结点的 value 属性赋 null 以标记该结点无用了，至于这个 marker 也是为了分散并发冲突的，最后通过 casNext 完成结点的删除。</p>
<h2 id="六、get-方法获取指定结点的-value"><a href="#六、get-方法获取指定结点的-value" class="headerlink" title="六、get 方法获取指定结点的 value"></a><strong>六、get 方法获取指定结点的 value</strong></h2><p>算上本小节将要介绍的 “查” 方法，我们就完成了对并发跳表 “增删改查” 的全部分析。 相对于“增”来说，其他的三种操作还是相对容易的，尤其是本小节的“查”操作，下面我们看看它的内部实现：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> V <span class="title">get</span><span class="params">(Object key)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> doGet(key);</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> V <span class="title">doGet</span><span class="params">(Object key)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (key == <span class="keyword">null</span>)</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> NullPointerException();</span><br><span class="line">    Comparator&lt;? <span class="keyword">super</span> K&gt; cmp = comparator;</span><br><span class="line">	<span class="comment">//依然是双层循环来处理并发</span></span><br><span class="line">    outer: <span class="keyword">for</span> (;;) &#123;</span><br><span class="line">        <span class="keyword">for</span> (Node&lt;K,V&gt; b = findPredecessor(key, cmp), n = b.next;;) &#123;</span><br><span class="line">            Object v; <span class="keyword">int</span> c;</span><br><span class="line">            <span class="comment">//以下的一些判断的作用已经描述了多次，此处不再赘述了</span></span><br><span class="line">            <span class="keyword">if</span> (n == <span class="keyword">null</span>)</span><br><span class="line">                <span class="keyword">break</span> outer;</span><br><span class="line">            Node&lt;K,V&gt; f = n.next;</span><br><span class="line">            <span class="keyword">if</span> (n != b.next)           </span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">            <span class="keyword">if</span> ((v = n.value) == <span class="keyword">null</span>) &#123;    </span><br><span class="line">                n.helpDelete(b, f);</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">if</span> (b.value == <span class="keyword">null</span> || v == n)  </span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">            <span class="comment">//c = 0 说明 n 就是我们要的结点</span></span><br><span class="line">            <span class="keyword">if</span> ((c = cpr(cmp, key, n.key)) == <span class="number">0</span>) &#123;</span><br><span class="line">                <span class="meta">@SuppressWarnings(&quot;unchecked&quot;)</span> V vv = (V)v;</span><br><span class="line">                <span class="keyword">return</span> vv;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="comment">//c &lt; 0 说明不存在这个 key 所对应的结点</span></span><br><span class="line">            <span class="keyword">if</span> (c &lt; <span class="number">0</span>)</span><br><span class="line">                <span class="keyword">break</span> outer;</span><br><span class="line">            b = n;</span><br><span class="line">            n = f;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>doGet 方法的实现相对还是比较简单的，所以并没有给出太多的注释，主要还是由于大量的并发判断的代码都是一样的，大多都已经在 doPut 方法中给予了详细的注释了。</p>
<h2 id="七、其它的一些方法的简单描述"><a href="#七、其它的一些方法的简单描述" class="headerlink" title="七、其它的一些方法的简单描述"></a><strong>七、其它的一些方法的简单描述</strong></h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//是否包含指定 key 的结点</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">containsKey</span><span class="params">(Object key)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> doGet(key) != <span class="keyword">null</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//根据 key 返回该 key 所代表的结点的 value 值，不存在该结点则返回默认的 defaultValue</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> V <span class="title">getOrDefault</span><span class="params">(Object key, V defaultValue)</span> </span>&#123;</span><br><span class="line">    V v;</span><br><span class="line">    <span class="keyword">return</span> (v = doGet(key)) == <span class="keyword">null</span> ? defaultValue : v;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//返回跳表的实际存储元素个数，采取遍历来进行统计</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">size</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">long</span> count = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">for</span> (Node&lt;K,V&gt; n = findFirst(); n != <span class="keyword">null</span>; n = n.next) &#123;</span><br><span class="line">        <span class="keyword">if</span> (n.getValidValue() != <span class="keyword">null</span>)</span><br><span class="line">            ++count;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> (count &gt;= Integer.MAX_VALUE) ? Integer.MAX_VALUE : (<span class="keyword">int</span>) count;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//返回所有键的集</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> NavigableSet&lt;K&gt; <span class="title">keySet</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    KeySet&lt;K&gt; ks = keySet;</span><br><span class="line">    <span class="keyword">return</span> (ks != <span class="keyword">null</span>) ? ks : (keySet = <span class="keyword">new</span> KeySet&lt;K&gt;(<span class="keyword">this</span>));</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//返回所有值的集</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> Collection&lt;V&gt; <span class="title">values</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    Values&lt;V&gt; vs = values;</span><br><span class="line">    <span class="keyword">return</span> (vs != <span class="keyword">null</span>) ? vs : (values = <span class="keyword">new</span> Values&lt;V&gt;(<span class="keyword">this</span>));</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>这里需要说明一点的是，虽然返回来的是键或者值的一个集合，但是无论你是通过这个集合获取键或者值，还是删除集合中的键或者值，都会直接映射到当前跳表实例中。原因是这个集合中没有一个方法是自己实现的，都是调用传入的跳表实例的内部方法，具体的大家查看源码即可知晓，此处不再贴出源码。</p>
<p>至此，有关 SkipList 这种跳表数据结构及其在 jdk 中的实现，以及它的并发版本 ConcurrentSkipListMap 的实现，我们都已经简单的分析完了，有理解错误之处，望指出，相互学习！</p>
<h4 id="参考的几篇优秀博文"><a href="#参考的几篇优秀博文" class="headerlink" title="参考的几篇优秀博文"></a><strong>参考的几篇优秀博文</strong></h4><p><a target="_blank" rel="noopener" href="http://xiaobaoqiu.github.io/blog/2014/12/19/javabing-fa-rong-qi-zhi-skiplist/">Java并发容器之SkipList（需要科学上网）  
</a></p>
<p><a target="_blank" rel="noopener" href="http://blog.csdn.net/lihui6636/article/details/48947407">深入Java集合学习系列：ConcurrentSkipListMap实现原理</a></p>
<p><a target="_blank" rel="noopener" href="http://blog.csdn.net/guangcigeyun/article/details/8278349">Java多线程（四）之ConcurrentSkipListMap深入分析</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://example.com/bigdata/Flink/Flink-SQL-Calcite/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="aaronzhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Guadazi">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/bigdata/Flink/Flink-SQL-Calcite/" class="post-title-link" itemprop="url">Flink-SQL原理之Calcite</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-04-18 07:58:00" itemprop="dateCreated datePublished" datetime="2021-04-18T07:58:00+08:00">2021-04-18</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-04-24 12:58:52" itemprop="dateModified" datetime="2021-04-24T12:58:52+08:00">2021-04-24</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/bigdata/" itemprop="url" rel="index"><span itemprop="name">bigdata</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <blockquote>
<p>Flink SQL是Flink API的最顶层抽象，在使用Flink SQL的时候，是否被其便捷和高效惊艳到？到底一条SQL语句是如何运行起来的？提到Flink SQL，离不开SQL引擎框架 – Calcite。Calcite是面向Hadoop的查询引擎，提供了SQL解析、优化、多重数据源查询的基础框架。Flink借助Calcite实现了SQL解析、优化和graph生成。</p>
</blockquote>
<h2 id="Flink-API与Flink-SQL简介"><a href="#Flink-API与Flink-SQL简介" class="headerlink" title="Flink API与Flink SQL简介"></a>Flink API与Flink SQL简介</h2><p><img src="https://gitee.com/averyzhang/pic-go/raw/master/img/20210418105609.png" alt="img"></p>
<p>Flink提供了三层API抽象，每一层API都是便捷性和变现力之前的权衡，来应对不同的计算场景。</p>
<p>Flink SQL是最顶层的抽象，这层抽象在语义和程序表达式上都类似于Table API，但是程序实现都是SQL表达式: </p>
<ol>
<li><p>SQL和Table API都是遵循关系模型：元数据和操作</p>
<ul>
<li>元数据类似于关系型数据库中的schema</li>
<li>操作类似于关系型数据库中的操作, 如select、project、join、group-by和aggregate</li>
</ul>
</li>
<li><p>SQL和Table API都是声明式定义，而不会执行执行的具体代码</p>
</li>
<li><p>简洁，通过UDF扩展，但比core API的表达能力差</p>
</li>
<li><p>执行之前，通过优化器中的优化规则对用户编写的表达式进行优化</p>
</li>
<li><p>TableEnvironment是Flink SQL和Table API的入口，可以无缝切换到DataStream/DataSet, 允许混用。</p>
</li>
</ol>
<h2 id="Calcite"><a href="#Calcite" class="headerlink" title="Calcite"></a>Calcite</h2><h3 id="Calcite简介"><a href="#Calcite简介" class="headerlink" title="Calcite简介"></a>Calcite简介</h3><p>是一个动态数据的管理框架，可以用来构建数据库系统的语法解析模块</p>
<ul>
<li>不包含数据存储、数据处理等功能</li>
<li>可以通过编写 Adaptor 来扩展功能，以支持不同的数据处理平台</li>
<li>Flink SQL 使用并对其扩展以支持 SQL 语句的解析和验证</li>
</ul>
<p>Calcite提供了SQL parser、SQL validation、Query optimizer、SQL generator和Data federator</p>
<h3 id="查询的执行过程"><a href="#查询的执行过程" class="headerlink" title="查询的执行过程"></a>查询的执行过程</h3><p>分四步: </p>
<p><img src="https://gitee.com/averyzhang/pic-go/raw/master/img/20210418225117.png" alt="img"></p>
<ol>
<li>Parse(SQL -&gt; SqlNode ):  使用JavaCC生成的parser来转换查询</li>
<li>validate(SqlNode -&gt; SqlNode ): 通过元数据验证查询</li>
<li>optimize: 逻辑计划优化和转换为物理计划<ul>
<li>语义分析(SqlNode -&gt; RelNode/RexNode ): 根据 SqlNode 及元信息构建 RelNode 树，也就是最初版本的逻辑计划（Logical Plan）；</li>
<li>逻辑计划优化(RelNode -&gt; RelNode ): 优化器的核心，根据前面生成的逻辑计划按照相应的规则（Rule）进行优化；</li>
</ul>
</li>
<li>execute: 物理计划转换为应用框架的执行逻辑(如Flink的graph)</li>
</ol>
<h3 id="Catalog"><a href="#Catalog" class="headerlink" title="Catalog"></a>Catalog</h3><p>定义Calcite查询的命名空间:</p>
<p><code>Schema</code> :  <code>schema</code>和<code>table</code>的集合，可以任意嵌套</p>
<p><code>Table</code>: 代表单个数据集，字段定义为<code>RelDataType</code></p>
<p><code>RelDataType</code>:  代表数据集中的字段，支持所有的SQL数据类型，包括结构体</p>
<p><code>Statistic</code>: 提供用于优化的表统计信息, 如行数、分布信息、是否为key</p>
<p><img src="https://gitee.com/averyzhang/pic-go/raw/master/img/20210418173928.png" alt="image-20210418173928391"></p>
<h3 id="SQL-parser"><a href="#SQL-parser" class="headerlink" title="SQL parser"></a>SQL parser</h3><ul>
<li>LL(K) parser通过JavaCC(Java Compiler Compiler)写的</li>
<li>输入的查询转换为AST(abstract syntax tree)</li>
<li><code>SqlNode</code>代表Token</li>
<li><code>SqlNode</code>可以通过<code>unparse</code>方法转换会SQL</li>
</ul>
<p><img src="https://gitee.com/averyzhang/pic-go/raw/master/img/20210418181320.png" alt="image-20210418181320343"></p>
<p><code>SqlNode</code>代表AST中的一个节点</p>
<p><img src="https://gitee.com/averyzhang/pic-go/raw/master/img/20210418181402.png" alt="image-20210418181402108"></p>
<p><code>SqlDialect</code>代表特定数据库的方言规则</p>
<h3 id="Query-optimizer"><a href="#Query-optimizer" class="headerlink" title="Query optimizer"></a>Query optimizer</h3><p>查询计划(Query plans)代表执行一个查询必须的步骤</p>
<p><img src="https://gitee.com/averyzhang/pic-go/raw/master/img/20210418184009.png" alt="image-20210418184009836"></p>
<p>Query优化:</p>
<ul>
<li>优化逻辑计划</li>
<li>目标通常是尽量减少计划中必须在早期处理的数据量</li>
<li>将逻辑计划转换为物理计划</li>
<li>物理计划与引擎有关，代表了物理执行过程</li>
</ul>
<p>常见的优化方法:</p>
<table>
<thead>
<tr>
<th><strong>RBO</strong></th>
<th><strong>规则名称</strong></th>
<th></th>
</tr>
</thead>
<tbody><tr>
<td>列裁剪</td>
<td>column_prune</td>
<td>Prune unused fields</td>
</tr>
<tr>
<td>子查询去关联</td>
<td>decorrelate</td>
<td></td>
</tr>
<tr>
<td>子查询转换为join</td>
<td></td>
<td>Convert subqueries to joins</td>
</tr>
<tr>
<td>聚合消除</td>
<td>aggregation_eliminate</td>
<td></td>
</tr>
<tr>
<td>投影消除</td>
<td>projection_eliminate</td>
<td></td>
</tr>
<tr>
<td>最大最小消除</td>
<td>max_min_eliminate</td>
<td></td>
</tr>
<tr>
<td>谓词下推</td>
<td>predicate_push_down</td>
<td></td>
</tr>
<tr>
<td>外连接消除</td>
<td>outer_join_eliminate</td>
<td></td>
</tr>
<tr>
<td>分区裁剪</td>
<td>partition_processor</td>
<td></td>
</tr>
<tr>
<td>聚合下推</td>
<td>aggregation_push_down</td>
<td></td>
</tr>
<tr>
<td>TopN 下推</td>
<td>topn_push_down</td>
<td></td>
</tr>
<tr>
<td>Join 重排序</td>
<td>join_reorder</td>
<td></td>
</tr>
</tbody></table>
<p><img src="https://gitee.com/averyzhang/pic-go/raw/master/img/20210418190336.png" alt="image-20210418190335990"></p>
<h3 id="核心概念"><a href="#核心概念" class="headerlink" title="核心概念"></a>核心概念</h3><ol>
<li><strong>关系代数</strong>（Relational algebra）：即关系表达式。它们通常以动词命名，例如 Sort, Join, Project, Filter, Scan, Sample.</li>
<li><strong>行表达式</strong>（Row expressions）：例如 RexLiteral (常量), RexVariable (变量), RexCall (调用) 等，例如投影列表（Project）、过滤规则列表（Filter）、JOIN 条件列表和 ORDER BY 列表、WINDOW 表达式、函数调用等。使用 RexBuilder 来构建行表达式。</li>
<li>表达式有各种<strong>特征</strong>（Trait）：使用 Trait 的 satisfies() 方法来测试某个表达式是否符合某 Trait 或 Convention.</li>
<li><strong>转化特征</strong>（Convention）：属于 Trait 的子类，用于转化 RelNode 到具体平台实现（可以将下文提到的 Planner 注册到 Convention 中）. 例如 JdbcConvention，FlinkConventions.DATASTREAM 等。同一个关系表达式的输入必须来自单个数据源，各表达式之间通过 Converter 生成的 Bridge 来连接。</li>
<li><strong>规则</strong>（Rules）：用于将一个表达式转换（Transform）为另一个表达式。它有一个由 RelOptRuleOperand 组成的列表来决定是否可将规则应用于树的某部分。</li>
</ol>
<h3 id="Planner-规划器"><a href="#Planner-规划器" class="headerlink" title="Planner(规划器)"></a>Planner(规划器)</h3><p><strong>规划器</strong>（Planner） ：即请求优化器，它可以根据一系列规则和成本模型（例如基于成本的优化模型 VolcanoPlanner、启发式优化模型 HepPlanner）来将一个表达式转为语义等价（但效率更优）的另一个表达式。</p>
<h4 id="HepPlanner-启发式优化模型"><a href="#HepPlanner-启发式优化模型" class="headerlink" title="HepPlanner(启发式优化模型)"></a>HepPlanner(启发式优化模型)</h4><ul>
<li>与Spark优化器类似的启发式优化器</li>
<li>启发式优化比CBO要快速</li>
<li>如果规则对计划做出相反的改变，则存在无限递归的风险</li>
</ul>
<h4 id=""><a href="#" class="headerlink" title=""></a></h4><h4 id="VolcanoPlanner-基于成本的优化模型"><a href="#VolcanoPlanner-基于成本的优化模型" class="headerlink" title="VolcanoPlanner(基于成本的优化模型)"></a>VolcanoPlanner(基于成本的优化模型)</h4><ul>
<li>遍历所有的规则，选择代价最小的计划</li>
<li>代价是通过关系表达式提供的</li>
<li>并不是所有可能的计划都会计算</li>
<li>当经过指定的迭代未显著提升将停止优化</li>
<li>代价包括行数、I/O和CPU</li>
<li>Statistics用来提高代价评估的准确性</li>
<li>Calcite提供了工具来在统计资源消耗</li>
</ul>
<h2 id="Flink-与-Calcite"><a href="#Flink-与-Calcite" class="headerlink" title="Flink 与 Calcite"></a>Flink 与 Calcite</h2><h3 id="Calcite中Flink中的重要作用"><a href="#Calcite中Flink中的重要作用" class="headerlink" title="Calcite中Flink中的重要作用"></a>Calcite中Flink中的重要作用</h3><p><img src="https://gitee.com/averyzhang/pic-go/raw/master/img/20210418224641.png" alt="在这里插入图片描述"></p>
<p>在Flink中，Calcite扮演着重要的角色：</p>
<ul>
<li>以Calcite Catalog为核心，上面承载了Table/SQL API</li>
<li>Flink SQL和Table的代码最后生成Calcite Logic Plan(SqlNode Tree)</li>
<li>随后验证、优化为 RelNode 树，</li>
<li>最终通过 Rules（规则）和 Convention（转化特征）生成具体的 DataSet Plan（批处理）或 DataStream Plan（流处理），即 Flink 算子构成的处理逻辑。</li>
</ul>
<p><img src="https://gitee.com/averyzhang/pic-go/raw/master/img/20210418225006.png" alt="在这里插入图片描述"></p>
<p>Table / SQL API 的编程框架如下：</p>
<ul>
<li><p>通过 TableEnvironment 配置 CalciteConfig 对象，自动设置 SQL &amp; Table API 默认处理参数。</p>
</li>
<li><p>使用 registerTableSource() 来将一个 TableSource 注册到 rootSchema. 后续可以通过 scan() 获取此 Table 并调用各种 Table API 进行处理。</p>
</li>
<li><p>接下可以调用 sqlQuery() 和 sqlUpdate() 方法来使用 SQL 语句进行数据处理。</p>
</li>
</ul>
<h3 id="Flink-SQL的执行流程"><a href="#Flink-SQL的执行流程" class="headerlink" title="Flink SQL的执行流程"></a>Flink SQL的执行流程</h3><p><code>Planner接口</code>: 解析SQL，转换为Transformation</p>
<p><code>Executor接口</code>: 将Planner转换的Transformation生成streamGraph并执行</p>
<p><code>Parser接口</code>: 负责SQL解析，parse方法将SQL语句转换为Operation数组</p>
<ul>
<li>通过Calcite将SQL解析为SqlNode</li>
<li>根据SqlNode的类型，将SqlNode转换为Operation数组</li>
</ul>
<p>DDL语句的转换过程:</p>
<p>SqlNode转换为RelNode:</p>
<ul>
<li>推断Table类型</li>
<li>推断计算列</li>
<li>推断watermark分配</li>
</ul>
<p>SQL转换:</p>
<ol>
<li>Operation-&gt;RelNode</li>
<li>优化RelNode</li>
<li>RelNode-&gt;ExecNode</li>
<li>ExecNode-&gt;Transformation算子</li>
</ol>
<p>[参考文献]</p>
<ol>
<li>[Parsing database Query with Apache Calcite](<a target="_blank" rel="noopener" href="https://blog.knoldus.com/parsing-database-query-with-apache-calcite/">Parsing database Query with Apache Calcite - Knoldus Blogs</a>)</li>
<li><a target="_blank" rel="noopener" href="https://www.slideshare.net/julianhyde/apache-calcite-one-planner-fits-all">Apache Calcite: One planner fits all (slideshare.net)</a></li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://example.com/bigdata/Flink/Flink-SQL-Window/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="aaronzhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Guadazi">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/bigdata/Flink/Flink-SQL-Window/" class="post-title-link" itemprop="url">Flink sql window</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-04-12 19:47:00" itemprop="dateCreated datePublished" datetime="2021-04-12T19:47:00+08:00">2021-04-12</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-04-24 12:58:51" itemprop="dateModified" datetime="2021-04-24T12:58:51+08:00">2021-04-24</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/bigdata/" itemprop="url" rel="index"><span itemprop="name">bigdata</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>flink窗口函数包含滚动窗口、滑动窗口、会话窗口和OVER窗口</p>
<h2 id="Flink-SQL-窗口的基本概念与使用"><a href="#Flink-SQL-窗口的基本概念与使用" class="headerlink" title="Flink SQL 窗口的基本概念与使用"></a>Flink SQL 窗口的基本概念与使用</h2><h3 id="滚动窗口"><a href="#滚动窗口" class="headerlink" title="滚动窗口"></a>滚动窗口</h3><p>滚动窗口（TUMBLE）将每个元素分配到一个指定大小的窗口中。通常，滚动窗口有一个固定的大小，并且不会出现重叠。例如，如果指定了一个5分钟大小的滚动窗口，无限流的数据会根据时间划分为<code>[0:00 - 0:05)</code>、<code>[0:05, 0:10)</code>、<code>[0:10, 0:15)</code>等窗口。下图展示了一个30秒的滚动窗口。<br><img src="_v_images/20210412125050690_701107302"><br>使用标识函数选出窗口的起始时间或者结束时间，窗口的时间属性用于下级Window的聚合。</p>
<table>
<thead>
<tr>
<th>窗口标识函数</th>
<th>返回类型</th>
<th>描述</th>
</tr>
</thead>
<tbody><tr>
<td><code>TUMBLE_START(time-attr, size-interval)</code></td>
<td>TIMESTAMP</td>
<td>返回窗口的起始时间（包含边界）。例如<code>[00:10, 00:15)</code> 窗口，返回<code>00:10</code> 。</td>
</tr>
<tr>
<td><code>TUMBLE_END(time-attr, size-interval)</code></td>
<td>TIMESTAMP</td>
<td>返回窗口的结束时间（包含边界）。例如<code>[00:00, 00:15]</code>窗口，返回<code>00:15</code>。</td>
</tr>
<tr>
<td><code>TUMBLE_ROWTIME(time-attr, size-interval)</code></td>
<td>TIMESTAMP(rowtime-attr)</td>
<td>返回窗口的结束时间（不包含边界）。例如<code>[00:00, 00:15]</code>窗口，返回<code>00:14:59.999</code> 。返回值是一个rowtime attribute，即可以基于该字段做时间属性的操作，例如，级联窗口只能用在基于Event Time的Window上</td>
</tr>
<tr>
<td><code>TUMBLE_PROCTIME(time-attr, size-interval)</code></td>
<td>TIMESTAMP(rowtime-attr)</td>
<td>返回窗口的结束时间（不包含边界）。例如<code>[00:00, 00:15]</code>窗口，返回<code>00:14:59.999</code>。返回值是一个proctime attribute，即可以基于该字段做时间属性的操作，例如，级联窗口只能用在基于Processing Time的Window上</td>
</tr>
</tbody></table>
<p>TUMBLE window示例</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.typeinfo.TypeHint;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.typeinfo.TypeInformation;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.tuple.Tuple3;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.TimeCharacteristic;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.datastream.DataStream;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.functions.timestamps.AscendingTimestampExtractor;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.EnvironmentSettings;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.Table;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.bridge.java.StreamTableEnvironment;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.sql.Timestamp;</span><br><span class="line"><span class="keyword">import</span> java.util.Arrays;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TumbleWindowExample</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">/**</span></span><br><span class="line"><span class="comment">         * 1 注册环境</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        EnvironmentSettings mySetting = EnvironmentSettings</span><br><span class="line">                .newInstance()</span><br><span class="line"><span class="comment">//                .useOldPlanner()</span></span><br><span class="line">                .useBlinkPlanner()</span><br><span class="line">                .inStreamingMode()</span><br><span class="line">                .build();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 获取 environment</span></span><br><span class="line">        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">        <span class="comment">// 指定系统时间概念为 event time</span></span><br><span class="line">        env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime);</span><br><span class="line"></span><br><span class="line">        StreamTableEnvironment tEnv = StreamTableEnvironment.create(env,mySetting);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment">// 初始数据</span></span><br><span class="line">        DataStream&lt;Tuple3&lt;Long, String,Integer&gt;&gt; log = env.fromCollection(Arrays.asList(</span><br><span class="line">                <span class="comment">//时间 14:53:00</span></span><br><span class="line">                <span class="keyword">new</span> Tuple3&lt;&gt;(<span class="number">1572591180_000L</span>,<span class="string">&quot;xiao_ming&quot;</span>,<span class="number">300</span>),</span><br><span class="line">                <span class="comment">//时间 14:53:09</span></span><br><span class="line">                <span class="keyword">new</span> Tuple3&lt;&gt;(<span class="number">1572591189_000L</span>,<span class="string">&quot;zhang_san&quot;</span>,<span class="number">303</span>),</span><br><span class="line">                <span class="comment">//时间 14:53:12</span></span><br><span class="line">                <span class="keyword">new</span> Tuple3&lt;&gt;(<span class="number">1572591192_000L</span>, <span class="string">&quot;xiao_li&quot;</span>,<span class="number">204</span>),</span><br><span class="line">                <span class="comment">//时间 14:53:21</span></span><br><span class="line">                <span class="keyword">new</span> Tuple3&lt;&gt;(<span class="number">1572591201_000L</span>,<span class="string">&quot;li_si&quot;</span>, <span class="number">208</span>)</span><br><span class="line">                ));</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 指定时间戳</span></span><br><span class="line">        SingleOutputStreamOperator&lt;Tuple3&lt;Long, String, Integer&gt;&gt; logWithTime = log.assignTimestampsAndWatermarks(<span class="keyword">new</span> AscendingTimestampExtractor&lt;Tuple3&lt;Long, String, Integer&gt;&gt;() &#123;</span><br><span class="line"></span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">long</span> <span class="title">extractAscendingTimestamp</span><span class="params">(Tuple3&lt;Long, String, Integer&gt; element)</span> </span>&#123;</span><br><span class="line">                <span class="keyword">return</span> element.f0;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 转换为 Table</span></span><br><span class="line">        Table logT = tEnv.fromDataStream(logWithTime, <span class="string">&quot;t.rowtime, name, v&quot;</span>);</span><br><span class="line"></span><br><span class="line">        Table result = tEnv.sqlQuery(<span class="string">&quot;SELECT TUMBLE_START(t, INTERVAL &#x27;10&#x27; SECOND) AS window_start,&quot;</span> +</span><br><span class="line">                <span class="string">&quot;TUMBLE_END(t, INTERVAL &#x27;10&#x27; SECOND) AS window_end, SUM(v) FROM &quot;</span></span><br><span class="line">                + logT + <span class="string">&quot; GROUP BY TUMBLE(t, INTERVAL &#x27;10&#x27; SECOND)&quot;</span>);</span><br><span class="line"></span><br><span class="line">        TypeInformation&lt;Tuple3&lt;Timestamp,Timestamp,Integer&gt;&gt; tpinf = <span class="keyword">new</span> TypeHint&lt;Tuple3&lt;Timestamp,Timestamp,Integer&gt;&gt;()&#123;&#125;.getTypeInfo();</span><br><span class="line">        tEnv.toAppendStream(result, tpinf).print();</span><br><span class="line"></span><br><span class="line">        env.execute();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>sql逻辑，每十秒钟聚合<br>执行结果：  </p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">(2019-11-01 06:53:00.0,2019-11-01 06:53:10.0,603)  </span><br><span class="line">(2019-11-01 06:53:20.0,2019-11-01 06:53:30.0,208)  </span><br><span class="line">(2019-11-01 06:53:10.0,2019-11-01 06:53:20.0,204)</span><br></pre></td></tr></table></figure>
<h3 id="滑动窗口"><a href="#滑动窗口" class="headerlink" title="滑动窗口"></a>滑动窗口</h3><p>滑动窗口（HOP），也被称作Sliding Window。不同于滚动窗口，滑动窗口的窗口可以重叠。</p>
<p>滑动窗口有两个参数：slide和size。slide为每次滑动的步长，size为窗口的大小。</p>
<ul>
<li>slide &lt; size，则窗口会重叠，每个元素会被分配到多个窗口。</li>
<li>slide = size，则等同于滚动窗口（TUMBLE）。</li>
<li>slide &gt; size，则为跳跃窗口，窗口之间不重叠且有间隙。</li>
</ul>
<p>通常，大部分元素符合多个窗口情景，窗口是重叠的。因此，滑动窗口在计算移动平均数（moving averages）时很实用。例如，计算过去5分钟数据的平均值，每10秒钟更新一次，可以设置slide为10秒，size为5分钟。下图为您展示间隔为30秒，窗口大小为1分钟的滑动窗口。</p>
<p><img src="_v_images/20210412125050182_1936746176" alt="滑动窗口" title="滑动窗口"></p>
<p>使用滑动窗口标识函数选出窗口的起始时间或者结束时间，窗口的时间属性用于下级Window的聚合。</p>
<table>
<thead>
<tr>
<th>窗口标识函数</th>
<th>返回类型</th>
<th>描述</th>
</tr>
</thead>
<tbody><tr>
<td><code>HOP_START（&lt;time-attr&gt;, &lt;slide-interval&gt;, &lt;size-interval&gt;）</code></td>
<td>TIMESTAMP</td>
<td>返回窗口的起始时间（包含边界）。例如<code>[00:10, 00:15)</code> 窗口，返回<code>00:10</code> 。</td>
</tr>
<tr>
<td><code>HOP_END（&lt;time-attr&gt;, &lt;slide-interval&gt;, &lt;size-interval&gt;）</code></td>
<td>TIMESTAMP</td>
<td>返回窗口的结束时间（包含边界）。例如<code>[00:00, 00:15)</code> 窗口，返回<code>00:15</code>。</td>
</tr>
<tr>
<td><code>HOP_ROWTIME（&lt;time-attr&gt;, &lt;slide-interval&gt;, &lt;size-interval&gt;）</code></td>
<td>TIMESTAMP（rowtime-attr）</td>
<td>返回窗口的结束时间（不包含边界）。例如<code>[00:00, 00:15)</code> 窗口，返回<code>00:14:59.999</code>。返回值是一个rowtime attribute，即可以基于该字段做时间类型的操作，只能用在基于event time的window上。</td>
</tr>
<tr>
<td><code>HOP_PROCTIME（&lt;time-attr&gt;, &lt;slide-interval&gt;, &lt;size-interval&gt;）</code></td>
<td>TIMESTAMP（rowtime-attr）</td>
<td>返回窗口的结束时间（不包含边界）。例如<code>[00:00, 00:15)</code> 窗口，返回<code>00:14:59.999</code> 。返回值是一个proctime attribute</td>
</tr>
</tbody></table>
<p>滑动窗口实例：<br>java代码同上，sql语句改为：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> HOP_START(t, <span class="type">INTERVAL</span> <span class="string">&#x27;5&#x27;</span> <span class="keyword">SECOND</span>, <span class="type">INTERVAL</span> <span class="string">&#x27;10&#x27;</span> <span class="keyword">SECOND</span>) <span class="keyword">AS</span> window_start, HOP_END(t, <span class="type">INTERVAL</span> <span class="string">&#x27;5&#x27;</span> <span class="keyword">SECOND</span>, <span class="type">INTERVAL</span> <span class="string">&#x27;10&#x27;</span> <span class="keyword">SECOND</span>) <span class="keyword">AS</span> window_end, <span class="built_in">SUM</span>(v) <span class="keyword">FROM</span>   logT   <span class="keyword">GROUP</span> <span class="keyword">BY</span> HOP(t, <span class="type">INTERVAL</span> <span class="string">&#x27;5&#x27;</span> <span class="keyword">SECOND</span>, <span class="type">INTERVAL</span> <span class="string">&#x27;10&#x27;</span> <span class="keyword">SECOND</span>)</span><br></pre></td></tr></table></figure>
<p>每间隔5秒统计10秒内的数据<br>sql结果如下：  </p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">(2019-11-01 06:53:15.0,2019-11-01 06:53:25.0,208)  </span><br><span class="line">(2019-11-01 06:53:10.0,2019-11-01 06:53:20.0,204)  </span><br><span class="line">(2019-11-01 06:53:05.0,2019-11-01 06:53:15.0,507)  </span><br><span class="line">(2019-11-01 06:53:20.0,2019-11-01 06:53:30.0,208)  </span><br><span class="line">(2019-11-01 06:53:00.0,2019-11-01 06:53:10.0,603)  </span><br><span class="line">(2019-11-01 06:52:55.0,2019-11-01 06:53:05.0,300)</span><br></pre></td></tr></table></figure>
<h3 id="会话窗口"><a href="#会话窗口" class="headerlink" title="会话窗口"></a>会话窗口</h3><p>会话窗口（SESSION）通过Session活动来对元素进行分组。会话窗口与滚动窗口和滑动窗口相比，没有窗口重叠，没有固定窗口大小。相反，当它在一个固定的时间周期内不再收到元素，即会话断开时，这个窗口就会关闭。</p>
<p>会话窗口通过一个间隔时间（Gap）来配置，这个间隔定义了非活跃周期的长度。例如，一个表示鼠标点击活动的数据流可能具有长时间的空闲时间，并在两段空闲之间散布着高浓度的点击。 如果数据在指定的间隔（Gap）之后到达，则会开始一个新的窗口。</p>
<p>会话窗口示例如下图。每个Key由于不同的数据分布，形成了不同的Window。</p>
<p><img src="_v_images/20210412125049773_363551252"><br>使用标识函数选出窗口的起始时间或者结束时间，窗口的时间属性用于下级Window的聚合。</p>
<table>
<thead>
<tr>
<th>窗口标识函数</th>
<th>返回类型</th>
<th>描述</th>
</tr>
</thead>
<tbody><tr>
<td><code>SESSION_START（&lt;time-attr&gt;, &lt;gap-interval&gt;）</code></td>
<td>Timestamp</td>
<td>返回窗口的起始时间（包含边界）。如<code>[00:10, 00:15)</code> 的窗口，返回 <code>00:10</code> ，即为此会话窗口内第一条记录的时间。</td>
</tr>
<tr>
<td><code>SESSION_END（&lt;time-attr&gt;, &lt;gap-interval&gt;）</code></td>
<td>Timestamp</td>
<td>返回窗口的结束时间（包含边界）。如<code>[00:00, 00:15)</code> 的窗口，返回 <code>00:15</code>，即为此会话窗口内最后一条记录的时间+<code>&lt;gap-interval&gt;</code>。</td>
</tr>
<tr>
<td><code>SESSION_ROWTIME（&lt;time-attr&gt;, &lt;gap-interval&gt;）</code></td>
<td>Timestamp（rowtime-attr）</td>
<td>返回窗口的结束时间（不包含边界）。如 <code>[00:00, 00:15)</code> 的窗口，返回<code>00:14:59.999</code> 。返回值是一个rowtime attribute，也就是可以基于该字段进行时间类型的操作。该参数只能用于基于event time的window 。</td>
</tr>
<tr>
<td><code>SESSION_PROCTIME（&lt;time-attr&gt;, &lt;gap-interval&gt;）</code></td>
<td>Timestamp（rowtime-attr）</td>
<td>返回窗口的结束时间（不包含边界）。如 <code>[00:00, 00:15)</code> 的窗口，返回 <code>00:14:59.999</code> 。返回值是一个 proctime attribute，也就是可以基于该字段进行时间类型的操作。该参数只能用于基于processing time的window 。</td>
</tr>
</tbody></table>
<p>会话窗口实例：<br>java代码同上<br>sql语句如下：<br>每隔5秒聚合</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> SESSION_START(t, <span class="type">INTERVAL</span> <span class="string">&#x27;5&#x27;</span> <span class="keyword">SECOND</span>) <span class="keyword">AS</span> window_start,</span><br><span class="line">SESSION_END(t, <span class="type">INTERVAL</span> <span class="string">&#x27;5&#x27;</span> <span class="keyword">SECOND</span>) <span class="keyword">AS</span> window_end, <span class="built_in">SUM</span>(v) <span class="keyword">FROM</span>  logT  <span class="keyword">GROUP</span> <span class="keyword">BY</span> SESSION(t, <span class="type">INTERVAL</span> <span class="string">&#x27;5&#x27;</span> <span class="keyword">SECOND</span>)</span><br></pre></td></tr></table></figure>
<p>sql结果：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">(2019-11-01 06:53:21.0,2019-11-01 06:53:26.0,208)  </span><br><span class="line">(2019-11-01 06:53:00.0,2019-11-01 06:53:05.0,300)  </span><br><span class="line">(2019-11-01 06:53:09.0,2019-11-01 06:53:17.0,507)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h3 id="OVER窗口"><a href="#OVER窗口" class="headerlink" title="OVER窗口"></a>OVER窗口</h3><p>OVER窗口（OVER Window）是传统数据库的标准开窗，不同于Group By Window，OVER窗口中每1个元素都对应1个窗口。窗口内的元素是当前元素往前多少个或往前多长时间的元素集合，因此流数据元素分布在多个窗口中。</p>
<p>在应用OVER窗口的流式数据中，每1个元素都对应1个OVER窗口。每1个元素都触发1次数据计算，每个触发计算的元素所确定的行，都是该元素所在窗口的最后1行。在实时计算的底层实现中，OVER窗口的数据进行全局统一管理（数据只存储1份），逻辑上为每1个元素维护1个OVER窗口，为每1个元素进行窗口计算，完成计算后会清除过期的数据。</p>
<p>Flink SQL中对OVER窗口的定义遵循标准SQL的定义语法，传统OVER窗口没有对其进行更细粒度的窗口类型命名划分。按照计算行的定义方式，OVER Window可以分为以下两类：</p>
<ul>
<li>ROWS OVER Window：每一行元素都被视为新的计算行，即每一行都是一个新的窗口。</li>
<li>RANGE OVER Window：具有相同时间值的所有元素行视为同一计算行，即具有相同时间值的所有行都是同一个窗口。</li>
</ul>
<p>Rows OVER Window语义</p>
<p>窗口数据</p>
<p>ROWS OVER Window的每个元素都确定一个窗口。ROWS OVER Window分为Unbounded（无界流）和Bounded（有界流）两种情况。<br>Unbounded ROWS OVER Window数据示例如下图所示。<br><img src="_v_images/20210412125049063_1513990833"></p>
<p>虽然上图所示窗口user1的w7、w8及user2的窗口w3、w4都是同一时刻到达，但它们仍然在不同的窗口，这一点与RANGE OVER Window不同。</p>
<p>Bounded ROWS OVER Window数据以3个元素（往前2个元素）的窗口为例，如下图所示。</p>
<p><img src="_v_images/20210412125048555_824932679"></p>
<p>虽然上图所示窗口user1的w5、w6及user2的窗口w1、w2都是同一时刻到达，但它们仍然在不同的窗口，这一点与RANGE OVER Window不同。</p>
<p>RANGE OVER Window语义</p>
<p>窗口数据</p>
<p>RANGE OVER Window所有具有共同元素值（元素时间戳）的元素行确定一个窗口，RANGE OVER Window分为Unbounded和Bounded的两种情况。<br>Unbounded RANGE OVER Window数据示例如下图所示。</p>
<p><img src="_v_images/20210412125048048_1968314666"><br>上图所示窗口user1的w7、user2的窗口w3，两个元素同一时刻到达，属于相同的window，这一点与ROWS OVER Window不同。</p>
<p>Bounded RANGE OVER Window数据，以3秒中数据<code>(INTERVAL &#39;2&#39; SECOND)</code>的窗口为例，如下图所示。</p>
<p><img src="_v_images/20210412125047258_2027952546"></p>
<p>上图所示窗口user1的w6、user2的窗口w3，元素都是同一时刻到达，属于相同的window，这一点与ROWS OVER Window不同。</p>
<p>OVER窗口实例：<br>java代码同上<br>初始数据如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 初始数据</span></span><br><span class="line">DataStream&lt;Tuple3&lt;Long, String,Integer&gt;&gt; log = env.fromCollection(Arrays.asList(</span><br><span class="line">        <span class="comment">//时间 14:53:00</span></span><br><span class="line">        <span class="keyword">new</span> Tuple3&lt;&gt;(<span class="number">1572591180_000L</span>,<span class="string">&quot;xiao_ming&quot;</span>,<span class="number">999</span>),</span><br><span class="line">        <span class="comment">//时间 14:53:09</span></span><br><span class="line">        <span class="keyword">new</span> Tuple3&lt;&gt;(<span class="number">1572591189_000L</span>,<span class="string">&quot;zhang_san&quot;</span>,<span class="number">303</span>),</span><br><span class="line">        <span class="comment">//时间 14:53:12</span></span><br><span class="line">        <span class="keyword">new</span> Tuple3&lt;&gt;(<span class="number">1572591192_000L</span>, <span class="string">&quot;xiao_li&quot;</span>,<span class="number">888</span>),</span><br><span class="line">        <span class="comment">//时间 14:53:21</span></span><br><span class="line">        <span class="keyword">new</span> Tuple3&lt;&gt;(<span class="number">1572591201_000L</span>,<span class="string">&quot;li_si&quot;</span>, <span class="number">908</span>),</span><br><span class="line">        <span class="comment">//2019-11-01 14:53:31</span></span><br><span class="line">        <span class="keyword">new</span> Tuple3&lt;&gt;(<span class="number">1572591211_000L</span>,<span class="string">&quot;li_si&quot;</span>, <span class="number">555</span>),</span><br><span class="line">        <span class="comment">//2019-11-01 14:53:41</span></span><br><span class="line">        <span class="keyword">new</span> Tuple3&lt;&gt;(<span class="number">1572591221_000L</span>,<span class="string">&quot;zhang_san&quot;</span>, <span class="number">666</span>),</span><br><span class="line">        <span class="comment">//2019-11-01 14:53:51</span></span><br><span class="line">        <span class="keyword">new</span> Tuple3&lt;&gt;(<span class="number">1572591231_000L</span>,<span class="string">&quot;xiao_ming&quot;</span>, <span class="number">777</span>),</span><br><span class="line">        <span class="comment">//2019-11-01 14:54:01</span></span><br><span class="line">        <span class="keyword">new</span> Tuple3&lt;&gt;(<span class="number">1572591241_000L</span>,<span class="string">&quot;xiao_ming&quot;</span>, <span class="number">213</span>),</span><br><span class="line">        <span class="comment">//2019-11-01 14:54:11</span></span><br><span class="line">        <span class="keyword">new</span> Tuple3&lt;&gt;(<span class="number">1572591251_000L</span>,<span class="string">&quot;zhang_san&quot;</span>, <span class="number">300</span>),</span><br><span class="line">        <span class="comment">//2019-11-01 14:54:21</span></span><br><span class="line">        <span class="keyword">new</span> Tuple3&lt;&gt;(<span class="number">1572591261_000L</span>,<span class="string">&quot;li_si&quot;</span>, <span class="number">112</span>)</span><br><span class="line">));</span><br></pre></td></tr></table></figure>
<p>ROWS over Windown sql语句如下：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> name,v,<span class="built_in">MAX</span>(v) <span class="keyword">OVER</span>(</span><br><span class="line"><span class="keyword">PARTITION</span> <span class="keyword">BY</span> name </span><br><span class="line"><span class="keyword">ORDER</span> <span class="keyword">BY</span> t </span><br><span class="line"><span class="keyword">ROWS</span> <span class="keyword">BETWEEN</span> <span class="number">2</span> PRECEDING <span class="keyword">AND</span> <span class="keyword">CURRENT</span> <span class="type">ROW</span></span><br><span class="line">) <span class="keyword">FROM</span> logT</span><br></pre></td></tr></table></figure>
<p>sql结果如下：  </p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">(zhang_san,303,303)  </span><br><span class="line">(xiao_li,888,888)  </span><br><span class="line">(li_si,908,908)  </span><br><span class="line">(xiao_ming,999,999)  </span><br><span class="line">(zhang_san,666,666)  </span><br><span class="line">(li_si,555,908)  </span><br><span class="line">(xiao_ming,777,999)  </span><br><span class="line">(li_si,112,908)  </span><br><span class="line">(zhang_san,300,666)  </span><br><span class="line">(xiao_ming,213,999)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>RANGE OVER Window sql 语句如下：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> name,v,<span class="built_in">MAX</span>(v) <span class="keyword">OVER</span>(</span><br><span class="line"><span class="keyword">PARTITION</span> <span class="keyword">BY</span> name </span><br><span class="line"><span class="keyword">ORDER</span> <span class="keyword">BY</span> t </span><br><span class="line"><span class="keyword">RANGE</span> <span class="keyword">BETWEEN</span> <span class="type">INTERVAL</span> <span class="string">&#x27;15&#x27;</span> <span class="keyword">SECOND</span> PRECEDING <span class="keyword">AND</span> <span class="keyword">CURRENT</span> <span class="type">ROW</span></span><br><span class="line">) <span class="keyword">FROM</span>  logT</span><br></pre></td></tr></table></figure>
<p>sql结果如下：  </p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">(xiao_ming,999,999)  </span><br><span class="line">(xiao_li,888,888)  </span><br><span class="line">(zhang_san,303,303)  </span><br><span class="line">(li_si,908,908)  </span><br><span class="line">(li_si,555,908)  </span><br><span class="line">(xiao_ming,777,777)  </span><br><span class="line">(zhang_san,666,666)  </span><br><span class="line">(li_si,112,112)  </span><br><span class="line">(xiao_ming,213,777)  </span><br><span class="line">(zhang_san,300,300)</span><br></pre></td></tr></table></figure>
<p>本文的java代码来自：<br><a target="_blank" rel="noopener" href="https://github.com/CheckChe0803/flink-simple-tutorial/tree/master/table/src/main/java/sql/window">https://github.com/CheckChe08…</a></p>
<h2 id="底层实现"><a href="#底层实现" class="headerlink" title="底层实现"></a>底层实现</h2><p><a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s/UkpkS_JiRGR0ibZKYechbg">https://mp.weixin.qq.com/s/UkpkS_JiRGR0ibZKYechbg</a></p>
<h3 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h3><p>窗口是无限流上一种核心机制，可以流分割为有限大小的“窗口”，同时，在窗口内进行聚合，从而把源源不断产生的数据根据不同的条件划分成一段一段有边界的数据区间，使用户能够利用窗口功能实现很多复杂的统计分析需求。</p>
<h3 id="Window分类"><a href="#Window分类" class="headerlink" title="Window分类"></a>Window分类</h3><p>1、TimeWindow与CountWindow Flink Window可以是时间驱动的（<code>TimeWindow</code>），也可以是数据驱动的（CountWindow）。由于flink-planner-blink SQL中目前只支持TimeWindow相应的表达语句（<code>TUMBLE</code>、<code>HOP</code>、<code>SESSION</code>），因此，本文主要介绍TimeWindow SQL示例和逻辑，CountWindow感兴趣的读者可自行分析。</p>
<p>2、TimeWindow子类型 Flink TimeWindow有滑动窗口(<code>HOP</code>)、滚动窗口(<code>TUMBLE</code>)以及会话窗口(<code>SESSION</code>)三种，所选取的字段时间，可以是系统时间(<code>PROCTIME</code>)或事件时间(<code>EVENT TIME</code>)两种，接来下依次介绍。</p>
<h4 id="Tumble-Window（滚动窗口）"><a href="#Tumble-Window（滚动窗口）" class="headerlink" title="Tumble Window（滚动窗口）"></a>Tumble Window（滚动窗口）</h4><p>翻转窗口Assigner将每个元素分配给具有指定大小的窗口。翻转窗口的大小是固定的，且不会重叠。例如，指定一个大小为5分钟的翻滚窗口，并每5分钟启动一个新窗口，如下图所示：</p>
<p><img src="_v_images/20210412125430504_1844331455" alt="图片"></p>
<p>TUMBLE ROWTIME语法示例：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> sessionOrderTableRowtime (</span><br><span class="line">    ctime <span class="type">TIMESTAMP</span>,</span><br><span class="line">    categoryName <span class="type">VARCHAR</span>,</span><br><span class="line">    shopName <span class="type">VARCHAR</span>,</span><br><span class="line">    itemName <span class="type">VARCHAR</span>,</span><br><span class="line">    userId <span class="type">VARCHAR</span>,</span><br><span class="line">    price <span class="type">FLOAT</span>,</span><br><span class="line">    action <span class="type">BIGINT</span>,</span><br><span class="line">    WATERMARK <span class="keyword">FOR</span> ctime <span class="keyword">AS</span> withOffset(ctime, <span class="number">1000</span>),</span><br><span class="line">    proc <span class="keyword">AS</span> PROCTIME()</span><br><span class="line">) <span class="keyword">with</span> (</span><br><span class="line">    `type` <span class="operator">=</span> <span class="string">&#x27;kafka&#x27;</span>,</span><br><span class="line">    format <span class="operator">=</span> <span class="string">&#x27;json&#x27;</span>,</span><br><span class="line">    updateMode <span class="operator">=</span> <span class="string">&#x27;append&#x27;</span>,</span><br><span class="line">    `group.id` <span class="operator">=</span> <span class="string">&#x27;groupId&#x27;</span>,</span><br><span class="line">    bootstrap.servers <span class="operator">=</span> <span class="string">&#x27;xxxxx:9092&#x27;</span>,</span><br><span class="line">    version <span class="operator">=</span> <span class="string">&#x27;0.10&#x27;</span>,</span><br><span class="line">    `zookeeper.connect` <span class="operator">=</span> <span class="string">&#x27;xxxxx:2181&#x27;</span>,</span><br><span class="line">    startingOffsets <span class="operator">=</span> <span class="string">&#x27;latest&#x27;</span>,</span><br><span class="line">    topic <span class="operator">=</span> <span class="string">&#x27;sessionsourceproctime&#x27;</span></span><br><span class="line">);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> popwindowsink (</span><br><span class="line">    countA <span class="type">BIGINT</span>,</span><br><span class="line">    ctime_start <span class="type">TIMESTAMP</span>,</span><br><span class="line">    ctime_end <span class="type">VARCHAR</span>,</span><br><span class="line">    ctime_rowtime <span class="type">VARCHAR</span>,</span><br><span class="line">    categoryName <span class="type">VARCHAR</span>,</span><br><span class="line">    price_sum <span class="type">FLOAT</span></span><br><span class="line">) <span class="keyword">with</span> (</span><br><span class="line">    format <span class="operator">=</span> <span class="string">&#x27;json&#x27;</span>,</span><br><span class="line">    updateMode <span class="operator">=</span> <span class="string">&#x27;append&#x27;</span>,</span><br><span class="line">    bootstrap.servers <span class="operator">=</span> <span class="string">&#x27;xxxxx:9092&#x27;</span>,</span><br><span class="line">    version <span class="operator">=</span> <span class="string">&#x27;0.10&#x27;</span>,</span><br><span class="line">    topic <span class="operator">=</span> <span class="string">&#x27;sessionsinkproctime&#x27;</span>,</span><br><span class="line">    `type` <span class="operator">=</span> <span class="string">&#x27;kafka&#x27;</span></span><br><span class="line">);</span><br><span class="line"></span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> popwindowsink</span><br><span class="line">(<span class="keyword">SELECT</span></span><br><span class="line"><span class="built_in">COUNT</span>(<span class="operator">*</span>),</span><br><span class="line">TUMBLE_START(ctime, <span class="type">INTERVAL</span> <span class="string">&#x27;5&#x27;</span> <span class="keyword">MINUTE</span>),</span><br><span class="line">DATE_FORMAT(TUMBLE_END(ctime, <span class="type">INTERVAL</span> <span class="string">&#x27;5&#x27;</span> <span class="keyword">MINUTE</span>), <span class="string">&#x27;yyyy-MM-dd-HH-mm-ss:SSS&#x27;</span>), <span class="comment">--将TUMBLE_END转为可视化的日期</span></span><br><span class="line">DATE_FORMAT(TUMBLE_ROWTIME(ctime, <span class="type">INTERVAL</span> <span class="string">&#x27;5&#x27;</span> <span class="keyword">MINUTE</span>), <span class="string">&#x27;yyyy-MM-dd-HH-mm-ss:SSS&#x27;</span>), <span class="comment">--这里TUMBLE_ROWTIME为TUMBLE_END-1ms，一般用于后续窗口级联聚合</span></span><br><span class="line">categoryName,</span><br><span class="line"><span class="built_in">SUM</span>(price)</span><br><span class="line"><span class="keyword">FROM</span> sessionOrderTableRowtime</span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> TUMBLE(ctime, <span class="type">INTERVAL</span> <span class="string">&#x27;5&#x27;</span> <span class="keyword">MINUTE</span>), categoryName)</span><br></pre></td></tr></table></figure>
<p>TUMBLEP ROCTIME语法示例：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> popwindowsink</span><br><span class="line">(<span class="keyword">SELECT</span></span><br><span class="line"><span class="built_in">COUNT</span>(<span class="operator">*</span>),</span><br><span class="line">TUMBLE_START(proc, <span class="type">INTERVAL</span> <span class="string">&#x27;5&#x27;</span> <span class="keyword">MINUTE</span>),</span><br><span class="line">DATE_FORMAT(TUMBLE_END(proc, <span class="type">INTERVAL</span> <span class="string">&#x27;5&#x27;</span> <span class="keyword">MINUTE</span>), <span class="string">&#x27;yyyy-MM-dd-HH-mm-ss:SSS&#x27;</span>),</span><br><span class="line">DATE_FORMAT(TUMBLE_PROCTIME(proc, <span class="type">INTERVAL</span> <span class="string">&#x27;5&#x27;</span> <span class="keyword">MINUTE</span>), <span class="string">&#x27;yyyy-MM-dd-HH-mm-ss:SSS&#x27;</span>), <span class="comment">--注意这里proc字段即Source DDL中指定的PROCTIME</span></span><br><span class="line">categoryName,</span><br><span class="line"><span class="built_in">SUM</span>(price)</span><br><span class="line"><span class="keyword">FROM</span> sessionOrderTableRowtime</span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> TUMBLE(proc, <span class="type">INTERVAL</span> <span class="string">&#x27;5&#x27;</span> <span class="keyword">MINUTE</span>), categoryName)</span><br></pre></td></tr></table></figure>
<p>ROWTIME与PROCTIME区别：</p>
<ul>
<li>在使用上：主要是填入的ctime、proc关键字的区别，这两个字段在Source DDL中指定方式不一样.</li>
<li>在实现原理上：ROWTIME模式，根据ctime对应的值，去确定窗口的start、end；PROCTIME模式，在WindowOperator处理数据时，获取本地系统时间，去确定窗口的start、end.</li>
</ul>
<p>由于生产系统中，主要使用ROWTIME来计算、聚合、统计，PROCTIME一般用于测试或对统计精度要求不高的场景，本文后续都主要以ROWTIME进行分析。</p>
<h4 id="Hop-Window（滑动窗口）"><a href="#Hop-Window（滑动窗口）" class="headerlink" title="Hop Window（滑动窗口）"></a>Hop Window（滑动窗口）</h4><p>滑动窗口Assigner将元素分配给多个固定长度的窗口。类似于滚动窗口分配程序，窗口的大小由窗口大小参数配置。因此，如果滑动窗口小于窗口大小，则滑动窗口可以重叠。在这种情况下，元素被分配到多个窗口。其实，滚动窗口TUMBLE是滑动窗口的一个特例。例子，设置一个10分钟长度的窗口，以5分钟间隔滑动。这样，每5分钟就会出现一个窗口，其中包含最近10分钟内到达的事件，如下图：</p>
<p><img src="_v_images/20210412125430097_940076266" alt="图片"></p>
<p>HOP ROWTIME语法示例：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> popwindowsink</span><br><span class="line">(<span class="keyword">SELECT</span></span><br><span class="line"><span class="built_in">COUNT</span>(<span class="operator">*</span>),</span><br><span class="line">HOP_START(ctime, <span class="type">INTERVAL</span> <span class="string">&#x27;5&#x27;</span> <span class="keyword">MINUTE</span>,  <span class="type">INTERVAL</span> <span class="string">&#x27;10&#x27;</span> <span class="keyword">MINUTE</span>),</span><br><span class="line">DATE_FORMAT(HOP_END(ctime, <span class="type">INTERVAL</span> <span class="string">&#x27;5&#x27;</span> <span class="keyword">MINUTE</span>,  <span class="type">INTERVAL</span> <span class="string">&#x27;10&#x27;</span> <span class="keyword">MINUTE</span>), <span class="string">&#x27;yyyy-MM-dd-HH-mm-ss:SSS&#x27;</span>),</span><br><span class="line">DATE_FORMAT(HOP_ROWTIME(ctime, <span class="type">INTERVAL</span> <span class="string">&#x27;5&#x27;</span> <span class="keyword">MINUTE</span>,  <span class="type">INTERVAL</span> <span class="string">&#x27;10&#x27;</span> <span class="keyword">MINUTE</span>), <span class="string">&#x27;yyyy-MM-dd-HH-mm-ss:SSS&#x27;</span>), <span class="comment">--注意这里ctime字段即Source DDL中指定的ROWTIME</span></span><br><span class="line">categoryName,</span><br><span class="line"><span class="built_in">SUM</span>(price)</span><br><span class="line"><span class="keyword">FROM</span> sessionOrderTableRowtime</span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> HOP(ctime, <span class="type">INTERVAL</span> <span class="string">&#x27;5&#x27;</span> <span class="keyword">MINUTE</span>,  <span class="type">INTERVAL</span> <span class="string">&#x27;10&#x27;</span> <span class="keyword">MINUTE</span>), categoryName)</span><br></pre></td></tr></table></figure>
<h4 id="Session-Window（会话窗口）"><a href="#Session-Window（会话窗口）" class="headerlink" title="Session Window（会话窗口）"></a>Session Window（会话窗口）</h4><p>会话窗口Assigner根据活动会话对元素进行分组。与翻滚窗口和滑动窗口相比，会话窗口不会重叠，也没有固定的开始和结束时间。相反，会话窗口在一段时间内不接收元素时关闭，即，当一段不活跃的间隙发生时，当前会话关闭，随后的元素被分配给新的会话。</p>
<p><img src="_v_images/20210412125429689_1075804874" alt="图片"></p>
<p>SESSION ROWTIME语法示例：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> popwindowsink</span><br><span class="line">(<span class="keyword">SELECT</span></span><br><span class="line"><span class="built_in">COUNT</span>(<span class="operator">*</span>),</span><br><span class="line">SESSION_START(ctime, <span class="type">INTERVAL</span> <span class="string">&#x27;5&#x27;</span> <span class="keyword">MINUTE</span>),</span><br><span class="line">DATE_FORMAT(SESSION_END(ctime, <span class="type">INTERVAL</span> <span class="string">&#x27;5&#x27;</span> <span class="keyword">MINUTE</span>, <span class="string">&#x27;yyyy-MM-dd-HH-mm-ss:SSS&#x27;</span>),</span><br><span class="line">DATE_FORMAT(SESSION_ROWTIME(ctime, <span class="type">INTERVAL</span> <span class="string">&#x27;5&#x27;</span> <span class="keyword">MINUTE</span>), <span class="string">&#x27;yyyy-MM-dd-HH-mm-ss:SSS&#x27;</span>), <span class="comment">--注意这里ctime字段即Source DDL中指定的ROWTIME</span></span><br><span class="line">categoryName,</span><br><span class="line"><span class="built_in">SUM</span>(price)</span><br><span class="line"><span class="keyword">FROM</span> sessionOrderTableRowtime</span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> SESSION(ctime, <span class="type">INTERVAL</span> <span class="string">&#x27;5&#x27;</span> <span class="keyword">MINUTE</span>), categoryName)</span><br></pre></td></tr></table></figure>
<h3 id="Window分类及整体流程"><a href="#Window分类及整体流程" class="headerlink" title="Window分类及整体流程"></a>Window分类及整体流程</h3><p><img src="_v_images/20210412125429282_375710651" alt="图片"></p>
<p>上图内部流程分析：</p>
<p>应用层SQL:<br>1.1 window分类及配置，包括滑动、翻转、会话类型窗口<br>1.2 window时间类型配置，默认待字段名的EventTime，也可以通过PROCTIME()配置为ProcessingTime<br>Calcite解析引擎:<br>2.1 Calcite SQL解析，包括逻辑、优化、物理计划和算子绑定(#translateToPlanInternal)，在本文特指StreamExecGroupWindowAggregateRule和StreamExecGroupWindowAggregate物理计划<br>WindowOperator算子创建相关:<br>3.1 StreamExecGroupWindowAggregate#createWindowOperator创建算子<br>3.2 WindowAssigner的创建，根据输入的数据，和窗口类型，生成多个窗口<br>3.3 processElement()真实处理数据，包括聚合运算，生成窗口，更新缓存，提交数据等功能<br>3.4 Trigger根据数据或时间，来决定窗口触发</p>
<h3 id="创建WindowOperator算子"><a href="#创建WindowOperator算子" class="headerlink" title="创建WindowOperator算子"></a>创建WindowOperator算子</h3><p>由于window语法主要是在group by语句中使用，calcite创建WindowOperator算子伴随着聚合策略的实现，包括聚合规则匹配(StreamExecGroupWindowAggregateRule)，以及生成聚合physical算子StreamExecGroupWindowAggregate两个子流程：</p>
<p><img src="_v_images/20210412125428973_1212879820" alt="图片"></p>
<p>上图内部流程分析：</p>
<p>a. StreamExecGroupWindowAggregateRule会对window进行提前匹配，<br>生成的WindowEmitStrategy内部具有：是否为EventTime表标识、是否为SessionWindow、early fire和late fire配置、延迟毫秒数（窗口结束时间加上这个毫秒数即数据清理时间）<br>b. StreamExecGroupWindowAggregateRule会获取聚合逻辑计划中，window配置的时间字段，记录时间字段index信息，window的触发和清理都会用到这个时间<br>c. StreamExecGroupWindowAggregate入口即为translateToPlanInternal，它的实现方式与spark比较类似，会先循环调用child子节点translateToPlan方法，生成inputtranform信息作为输入<br>d.创建aggregateHandler是一个代码生成的过程，其生成的创建的class实现了accumulate、retract、merge、update方法，这个handler最后也传递给了WindowOperater，处理数据时，可以进行聚合、回撤并输出最新数据给下游<br>e. StreamExecGroupWindowAggregate与window相关的最后一步就是调用#createWindowOperator创建算子，其内部先创建了一个WindowOperatorBuilder，设置window类型、retract标识、trigger(window触发条件)、聚合函数句柄等，最后创建WindowOperator</p>
<h3 id="WindowOperator处理数据图解"><a href="#WindowOperator处理数据图解" class="headerlink" title="WindowOperator处理数据图解"></a>WindowOperator处理数据图解</h3><p>在上一小节，已经完成了WindowOperator参数的设定，并创建实例，接下来我们主要分析WindowOperator真实处理数据的流程(起点在WindowOperator#processElement方法)：</p>
<p><img src="_v_images/20210412125428666_1517359800" alt="图片"></p>
<p>processElement处理数据流程：</p>
<p>a、 获取当前record具有的事件时间，如果是Processing Time模式，从时间服务Service里面获取时间即可<br>b、使用上一步获取的时间，接着调用windowFunction.assignWindow生成窗口，其内部实际上是调用各类型的WindowAssigner生成窗口，windowFunction有三大类，分别是Paned（滑动）、Merge（会话）、General（前两种以外的），WindowAssigner类型大致有5类，分别是Tumbling（翻转）、Sliding（滑动）、Session（会话）、CountTumbling 、CountSlide这几类,根据输入的一条数据和时间，可以生成1到多个窗口<br>c、接下来是遍历涉及的窗口进行聚合，包括从windowState获取聚合前值、使用句柄进行聚合、更新状态至windowState，将当前转态<br>d、上一步聚合完成后，就可以遍历窗口，使用TriggerContext（其实就是不同类型窗口Trigger触发器的代理），综合early fire、late fire、水印时间与窗口结束时间，综合判断是否触发窗口写出<br>e、如果TriggerContext判断出触发条件为true，则调用emitWindowResult写出，其内部有retract判断，更新当前state及previous state，写出数据等操作<br>f、如果TriggerContext判断出触发条件为false，则触发需要注册cleanupTimer,到达指定时间后，触发onEventTime或onProcessingTime<br>g、onEventTime或onProcessingTime功能十分类似，首先会触发emitWindowResult提交结果，另外会判断窗口结束时间+Lateness和当前时间是否相等，相等则表示可以清除窗口数据、当前state及previous state、窗口对应trigger。</p>
<h3 id="WindowOperator源码调试"><a href="#WindowOperator源码调试" class="headerlink" title="WindowOperator源码调试"></a>WindowOperator源码调试</h3><p>为了更直观的理解Window内部运行原理，这里我们引入一个Flink源码中已有的SQL Window测试用例，并进行了简单的修改（即修改为使用HOP滑动窗口）</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">classWindowJoinITCase&#123;</span><br><span class="line">  <span class="meta">@Test</span></span><br><span class="line">  <span class="function">def <span class="title">testRowTimeInnerJoinWithWindowAggregateOnFirstTime</span><span class="params">()</span>: Unit </span>= &#123;</span><br><span class="line">    val sqlQuery =</span><br><span class="line">      <span class="string">&quot;&quot;</span><span class="string">&quot;</span></span><br><span class="line"><span class="string">        |SELECT t1.key, HOP_END(t1.rowtime, INTERVAL &#x27;4&#x27; SECOND, INTERVAL &#x27;20&#x27; SECOND), COUNT(t1.key)</span></span><br><span class="line"><span class="string">        |FROM T1 AS t1</span></span><br><span class="line"><span class="string">        |GROUP BY HOP(t1.rowtime, INTERVAL &#x27;4&#x27; SECOND, INTERVAL &#x27;20&#x27; SECOND), t1.key</span></span><br><span class="line"><span class="string">        |&quot;</span><span class="string">&quot;&quot;</span>.stripMargin</span><br><span class="line"></span><br><span class="line">    val data1 = <span class="keyword">new</span> mutable.MutableList[(String, String, Long)]</span><br><span class="line">    data1.+=((<span class="string">&quot;A&quot;</span>, <span class="string">&quot;L-1&quot;</span>, <span class="number">1000L</span>))</span><br><span class="line">    data1.+=((<span class="string">&quot;A&quot;</span>, <span class="string">&quot;L-2&quot;</span>, <span class="number">2000L</span>))</span><br><span class="line">    data1.+=((<span class="string">&quot;A&quot;</span>, <span class="string">&quot;L-3&quot;</span>, <span class="number">3000L</span>))</span><br><span class="line">    <span class="comment">//data1.+=((&quot;B&quot;, &quot;L-8&quot;, 2000L))</span></span><br><span class="line">    data1.+=((<span class="string">&quot;B&quot;</span>, <span class="string">&quot;L-4&quot;</span>, <span class="number">4000L</span>))</span><br><span class="line">    data1.+=((<span class="string">&quot;C&quot;</span>, <span class="string">&quot;L-5&quot;</span>, <span class="number">2100L</span>))</span><br><span class="line">    data1.+=((<span class="string">&quot;A&quot;</span>, <span class="string">&quot;L-6&quot;</span>, <span class="number">10000L</span>))</span><br><span class="line">    data1.+=((<span class="string">&quot;A&quot;</span>, <span class="string">&quot;L-7&quot;</span>, <span class="number">13000L</span>))</span><br><span class="line"></span><br><span class="line">    val t1 = env.fromCollection(data1)</span><br><span class="line">      .assignTimestampsAndWatermarks(<span class="keyword">new</span> Row3WatermarkExtractor2)</span><br><span class="line">      .toTable(tEnv, <span class="string">&#x27;key, &#x27;</span>id, <span class="string">&#x27;rowtime)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    tEnv.registerTable(&quot;T1&quot;, t1)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    val sink = new TestingAppendSink</span></span><br><span class="line"><span class="string">    val t_r = tEnv.sqlQuery(sqlQuery)</span></span><br><span class="line"><span class="string">    val result = t_r.toAppendStream[Row]</span></span><br><span class="line"><span class="string">    result.addSink(sink)</span></span><br><span class="line"><span class="string">    env.execute()</span></span><br><span class="line"><span class="string">  &#125;</span></span><br><span class="line"><span class="string">&#125;</span></span><br></pre></td></tr></table></figure>
<p>1、StreamExecGroupWindowAggregate#createWindowOperator()创建算子</p>
<p>StreamExecGroupWindowAggregate#createWindowOperator()是创建WindowOperator算子的地方，对应的代码和注释：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">StreamExecGroupWindowAggregate</span></span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">private</span> def <span class="title">createWindowOperator</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">      config: TableConfig,</span></span></span><br><span class="line"><span class="function"><span class="params">      aggsHandler: GeneratedNamespaceAggsHandleFunction[_],</span></span></span><br><span class="line"><span class="function"><span class="params">      recordEqualiser: GeneratedRecordEqualiser,</span></span></span><br><span class="line"><span class="function"><span class="params">      accTypes: Array[LogicalType],</span></span></span><br><span class="line"><span class="function"><span class="params">      windowPropertyTypes: Array[LogicalType],</span></span></span><br><span class="line"><span class="function"><span class="params">      aggValueTypes: Array[LogicalType],</span></span></span><br><span class="line"><span class="function"><span class="params">      inputFields: Seq[LogicalType],</span></span></span><br><span class="line"><span class="function"><span class="params">      timeIdx: Int)</span>: WindowOperator[_, _] </span>= &#123;</span><br><span class="line"></span><br><span class="line">    val builder = WindowOperatorBuilder</span><br><span class="line">      .builder()</span><br><span class="line">      .withInputFields(inputFields.toArray)</span><br><span class="line">    val timeZoneOffset = -config.getTimeZone.getOffset(Calendar.ZONE_OFFSET)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 设置WindowOperatorBuilder，最后通过Builder创建WindowOperator</span></span><br><span class="line">    val newBuilder = window match &#123;</span><br><span class="line">      <span class="function"><span class="keyword">case</span> <span class="title">TumblingGroupWindow</span><span class="params">(_, timeField, size)</span> <span class="comment">//Tumble PROCTIME模式，内部设置Assiger</span></span></span><br><span class="line"><span class="function">          <span class="keyword">if</span> <span class="title">isProctimeAttribute</span><span class="params">(timeField)</span> &amp;&amp; <span class="title">hasTimeIntervalType</span><span class="params">(size)</span> </span>=&gt;</span><br><span class="line">        builder.tumble(toDuration(size), timeZoneOffset).withProcessingTime()</span><br><span class="line"></span><br><span class="line">      <span class="function"><span class="keyword">case</span> <span class="title">TumblingGroupWindow</span><span class="params">(_, timeField, size)</span> <span class="comment">//Tumble ROWTIME模式，内部设置Assiger</span></span></span><br><span class="line"><span class="function">          <span class="keyword">if</span> <span class="title">isRowtimeAttribute</span><span class="params">(timeField)</span> &amp;&amp; <span class="title">hasTimeIntervalType</span><span class="params">(size)</span> </span>=&gt;</span><br><span class="line">        builder.tumble(toDuration(size), timeZoneOffset).withEventTime(timeIdx)</span><br><span class="line"></span><br><span class="line">      <span class="function"><span class="keyword">case</span> <span class="title">SlidingGroupWindow</span><span class="params">(_, timeField, size, slide)</span> <span class="comment">//HOP PROCTIME模式，内部设置Assiger</span></span></span><br><span class="line"><span class="function">          <span class="keyword">if</span> <span class="title">isProctimeAttribute</span><span class="params">(timeField)</span> &amp;&amp; <span class="title">hasTimeIntervalType</span><span class="params">(size)</span> </span>=&gt;</span><br><span class="line">        builder.sliding(toDuration(size), toDuration(slide), timeZoneOffset)</span><br><span class="line">          .withProcessingTime()</span><br><span class="line">       .....</span><br><span class="line">      <span class="function"><span class="keyword">case</span> <span class="title">SessionGroupWindow</span><span class="params">(_, timeField, gap)</span></span></span><br><span class="line"><span class="function">          <span class="keyword">if</span> <span class="title">isRowtimeAttribute</span><span class="params">(timeField)</span> </span>=&gt;</span><br><span class="line">        builder.session(toDuration(gap)).withEventTime(timeIdx)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Retraction和Trigger设置</span></span><br><span class="line">    <span class="comment">//默认是no retract和EventTime.afterEndOfWindow</span></span><br><span class="line">    <span class="keyword">if</span> (emitStrategy.produceUpdates) &#123;</span><br><span class="line">      <span class="comment">// mark this operator will send retraction and set new trigger</span></span><br><span class="line">      newBuilder</span><br><span class="line">        .withSendRetraction()</span><br><span class="line">        .triggering(emitStrategy.getTrigger)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    newBuilder</span><br><span class="line">      .aggregate(aggsHandler, recordEqualiser, accTypes, aggValueTypes, windowPropertyTypes)</span><br><span class="line">      .withAllowedLateness(Duration.ofMillis(emitStrategy.getAllowLateness))</span><br><span class="line">      .build()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>2、WindowOperator#processElement()处理数据，注册Timer</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">StreamExecGroupWindowAggregate</span></span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">private</span> def <span class="title">createWindowOperator</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">      config: TableConfig,</span></span></span><br><span class="line"><span class="function"><span class="params">      aggsHandler: GeneratedNamespaceAggsHandleFunction[_],</span></span></span><br><span class="line"><span class="function"><span class="params">      recordEqualiser: GeneratedRecordEqualiser,</span></span></span><br><span class="line"><span class="function"><span class="params">      accTypes: Array[LogicalType],</span></span></span><br><span class="line"><span class="function"><span class="params">      windowPropertyTypes: Array[LogicalType],</span></span></span><br><span class="line"><span class="function"><span class="params">      aggValueTypes: Array[LogicalType],</span></span></span><br><span class="line"><span class="function"><span class="params">      inputFields: Seq[LogicalType],</span></span></span><br><span class="line"><span class="function"><span class="params">      timeIdx: Int)</span>: WindowOperator[_, _] </span>= &#123;</span><br><span class="line"></span><br><span class="line">    val builder = WindowOperatorBuilder</span><br><span class="line">      .builder()</span><br><span class="line">      .withInputFields(inputFields.toArray)</span><br><span class="line">    val timeZoneOffset = -config.getTimeZone.getOffset(Calendar.ZONE_OFFSET)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 设置WindowOperatorBuilder，最后通过Builder创建WindowOperator</span></span><br><span class="line">    val newBuilder = window match &#123;</span><br><span class="line">      <span class="function"><span class="keyword">case</span> <span class="title">TumblingGroupWindow</span><span class="params">(_, timeField, size)</span> <span class="comment">//Tumble PROCTIME模式，内部设置Assiger</span></span></span><br><span class="line"><span class="function">          <span class="keyword">if</span> <span class="title">isProctimeAttribute</span><span class="params">(timeField)</span> &amp;&amp; <span class="title">hasTimeIntervalType</span><span class="params">(size)</span> </span>=&gt;</span><br><span class="line">        builder.tumble(toDuration(size), timeZoneOffset).withProcessingTime()</span><br><span class="line"></span><br><span class="line">      <span class="function"><span class="keyword">case</span> <span class="title">TumblingGroupWindow</span><span class="params">(_, timeField, size)</span> <span class="comment">//Tumble ROWTIME模式，内部设置Assiger</span></span></span><br><span class="line"><span class="function">          <span class="keyword">if</span> <span class="title">isRowtimeAttribute</span><span class="params">(timeField)</span> &amp;&amp; <span class="title">hasTimeIntervalType</span><span class="params">(size)</span> </span>=&gt;</span><br><span class="line">        builder.tumble(toDuration(size), timeZoneOffset).withEventTime(timeIdx)</span><br><span class="line"></span><br><span class="line">      <span class="function"><span class="keyword">case</span> <span class="title">SlidingGroupWindow</span><span class="params">(_, timeField, size, slide)</span> <span class="comment">//HOP PROCTIME模式，内部设置Assiger</span></span></span><br><span class="line"><span class="function">          <span class="keyword">if</span> <span class="title">isProctimeAttribute</span><span class="params">(timeField)</span> &amp;&amp; <span class="title">hasTimeIntervalType</span><span class="params">(size)</span> </span>=&gt;</span><br><span class="line">        builder.sliding(toDuration(size), toDuration(slide), timeZoneOffset)</span><br><span class="line">          .withProcessingTime()</span><br><span class="line">       .....</span><br><span class="line">      <span class="function"><span class="keyword">case</span> <span class="title">SessionGroupWindow</span><span class="params">(_, timeField, gap)</span></span></span><br><span class="line"><span class="function">          <span class="keyword">if</span> <span class="title">isRowtimeAttribute</span><span class="params">(timeField)</span> </span>=&gt;</span><br><span class="line">        builder.session(toDuration(gap)).withEventTime(timeIdx)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Retraction和Trigger设置</span></span><br><span class="line">    <span class="comment">//默认是no retract和EventTime.afterEndOfWindow</span></span><br><span class="line">    <span class="keyword">if</span> (emitStrategy.produceUpdates) &#123;</span><br><span class="line">      <span class="comment">// mark this operator will send retraction and set new trigger</span></span><br><span class="line">      newBuilder</span><br><span class="line">        .withSendRetraction()</span><br><span class="line">        .triggering(emitStrategy.getTrigger)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    newBuilder</span><br><span class="line">      .aggregate(aggsHandler, recordEqualiser, accTypes, aggValueTypes, windowPropertyTypes)</span><br><span class="line">      .withAllowedLateness(Duration.ofMillis(emitStrategy.getAllowLateness))</span><br><span class="line">      .build()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>运行数据：</p>
<p><img src="_v_images/20210412125428458_505070513" alt="图片"></p>
<p>3、Timer触发 I、InternalTimerServiceImpl#advanceWatermark()</p>
<p>WindowOperator#onEventTime()的调用前，可以先看其上层调用：InternalTimerServiceImpl#advanceWatermark()</p>
<p><img src="_v_images/20210412125428150_761077256" alt="图片"></p>
<p>当获取的watermark为9999L时，把eventTimeTimerQueue队列中所有小于这个值的timer poll出来，调用WindowOperator.onEnventTime(timer)</p>
<p>II、WindwOperator#onEventTime()</p>
<p>WindwOperator#onEventTime()方法比较清晰，主要是window的触发和window的清理两段逻辑：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WindowOperator</span></span>&#123;</span><br><span class="line">    publicvoidonEventTime(InternalTimer&lt;K, W&gt; timer) <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">        setCurrentKey(timer.getKey());</span><br><span class="line"></span><br><span class="line">        triggerContext.window = timer.getNamespace();</span><br><span class="line">        <span class="keyword">if</span> (triggerContext.onEventTime(timer.getTimestamp())) &#123;</span><br><span class="line">            <span class="comment">// fire</span></span><br><span class="line">            emitWindowResult(triggerContext.window);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (windowAssigner.isEventTime()) &#123;</span><br><span class="line">            windowFunction.cleanWindowIfNeeded(triggerContext.window, timer.getTimestamp());</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>III、emitWindowResult()提交结果</p>
<p>#emitWindowResult()重点关注下其第一行代码：BaseRow aggResult = windowFunction.getWindowAggregationResult(window); 这个表示根据具体的TimeWindow{start=4000, end=24000}，去获取聚合数据，如果是滑动窗口，需要将4000, 8000 ,12000，16000 , 20000, 24000这几段affect窗口里面的聚合值合并起来，内部逻辑：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> classPanedWindowProcessFunction&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> BaseRow <span class="title">getWindowAggregationResult</span><span class="params">(W window)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        Iterable&lt;W&gt; panes = windowAssigner.splitIntoPanes(window);</span><br><span class="line">        BaseRow acc = windowAggregator.createAccumulators();</span><br><span class="line">        <span class="comment">// null namespace means use heap data views</span></span><br><span class="line">        windowAggregator.setAccumulators(<span class="keyword">null</span>, acc);</span><br><span class="line">        <span class="keyword">for</span> (W pane : panes) &#123;</span><br><span class="line">            BaseRow paneAcc = ctx.getWindowAccumulators(pane);</span><br><span class="line">            <span class="keyword">if</span> (paneAcc != <span class="keyword">null</span>) &#123;</span><br><span class="line">                windowAggregator.merge(pane, paneAcc);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> windowAggregator.getValue(window);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><img src="_v_images/20210412125427840_804453196" alt="图片"></p>
<h3 id="Emit（Trigger）触发器"><a href="#Emit（Trigger）触发器" class="headerlink" title="Emit（Trigger）触发器"></a>Emit（Trigger）触发器</h3><ul>
<li>配置方式指定Trigger：Flink1.9.0目前支持通过TableConifg配置earlyFireInterval、lateFireInterval毫秒数，来指定窗口结束之前、窗口结束之后的触发策略（默认是watermark超过窗口结束后触发一次），策略的解析在WindowEmitStrategy，在StreamExecGroupWindowAggregateRule就会创建和解析这个策略</li>
<li>SQL方式指定Trigger：Flink1.9.0代码中calcite部分已有SqlEmit相关的实现，后续可以支持SQL 语句（INSERT INTO）中配置EMIT触发器</li>
</ul>
<p>本文Emit和Trigger都是触发器这一个概念，只是使用的方式不一样</p>
<p>1、Emit策略 Emit 策略是指在Flink SQL 中，query的输出策略（如能忍受的延迟）可能在不同的场景有不同的需求，而这部分需求，传统的 ANSI SQL 并没有对应的语法支持。比如用户需求：1小时的时间窗口，窗口触发之前希望每分钟都能看到最新的结果，窗口触发之后希望不丢失迟到一天内的数据。针对这类需求，抽象出了EMIT语法，并扩展到了SQL语法。</p>
<p>2、用途 EMIT语法的用途目前总结起来主要提供了：控制延迟、数据精确性，两方面的功能。</p>
<ul>
<li>控制延迟。针对大窗口，设置窗口触发之前的EMIT输出频率，减少用户看到结果的延迟(WITH| WITHOUT DELAY)。</li>
<li>数据精确性。不丢弃窗口触发之后的迟到的数据，修正输出结果(minIdleStateRetentionTime，在WindowEmitStrategy中生成allowLateness)。</li>
</ul>
<p>在选择EMIT策略时，还需要与处理开销进行权衡。因为越低的输出延迟、越高的数据精确性，都会带来越高的计算开销。</p>
<p>3、语法 EMIT 语法是用来定义输出的策略，即是定义在输出（INSERT INTO）上的动作。当未配置时，保持原有默认行为，即 window 只在 watermark 触发时 EMIT 一个结果。</p>
<p>语法：INSERT INTO tableName query EMIT strategy [, strategy]*</p>
<p>strategy ::= {WITH DELAY timeInterval | WITHOUT DELAY} [BEFORE WATERMARK |AFTER WATERMARK]</p>
<p>timeInterval ::=‘string’ timeUnit</p>
<p>WITH DELAY：声明能忍受的结果延迟，即按指定 interval 进行间隔输出。WITHOUT DELAY：声明不忍受延迟，即每来一条数据就进行输出。BEFORE WATERMARK：窗口结束之前的策略配置，即watermark 触发之前。AFTER WATERMARK：窗口结束之后的策略配置，即watermark 触发之后。注：</p>
<ul>
<li>其中 strategy可以定义多个，同时定义before和after的策略。但不能同时定义两个 before 或 两个after 的策略。</li>
<li>若配置了AFTER WATERMARK 策略，需要显式地在TableConfig中配置minIdleStateRetentionTime标识能忍受的最大迟到时间。</li>
<li>minIdleStateRetentionTime在window中只影响窗口何时清除，不直接影响窗口何时触发， 例如配置为3600000，最多容忍1小时的迟到数据，超过这个时间的数据会直接丢弃</li>
</ul>
<p>4、示例 如果我们已经有一个TUMBLE（ctime, INTERVAL ‘1’ HOUR）的窗口，tumble_window 的输出是需要等到一小时结束才能看到结果，我们希望能尽早能看到窗口的结果（即使是不完整的结果）。例如，我们希望每分钟看到最新的窗口结果：INSERT INTO result SELECT * FROM tumble_window EMIT WITH DELAY ‘1’ MINUTE BEFORE WATERMARK – 窗口结束之前，每隔1分钟输出一次更新结果</p>
<p>tumble_window 会忽略并丢弃窗口结束后到达的数据，而这部分数据对我们来说很重要，希望能统计进最终的结果里。而且我们知道我们的迟到数据不会太多，且迟到时间不会超过一天以上，并且希望收到迟到的数据立刻就更新结果：INSERT INTO result SELECT * FROM tumble_window EMIT WITH DELAY ‘1’ MINUTE BEFORE WATERMARK, WITHOUT DELAY AFTER WATERMARK –窗口结束之后，每条到达的数据都输出</p>
<p>tEnv.getConfig.setIdleStateRetentionTime(Time.days(1), Time.days(2))//min、max，只有Time.days(1)这个参数直接对window生效</p>
<p>补充一下WITH DELAY ‘1’这种配置的周期触发策略（即DELAY大于0），最后都是由ProcessingTime系统时间触发：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">WindowEmitStrategy</span></span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">private</span> def <span class="title">createTriggerFromInterval</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">      enableDelayEmit: Boolean,</span></span></span><br><span class="line"><span class="function"><span class="params">      interval: Long)</span>: Option[Trigger[TimeWindow]] </span>= &#123;</span><br><span class="line">    <span class="keyword">if</span> (!enableDelayEmit) &#123;</span><br><span class="line">      None</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      <span class="keyword">if</span> (interval &gt; <span class="number">0</span>) &#123;</span><br><span class="line">       <span class="comment">// 系统时间触发，小于wm的所有timer都执行onProcessingTime()</span></span><br><span class="line">        Some(ProcessingTimeTriggers.every(Duration.ofMillis(interval)))</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">       <span class="comment">// 为0则每条都触发</span></span><br><span class="line">        Some(ElementTriggers.every())</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>5、Trigger类和结构关系 在源码中，Window Trigger的实现子类有10个左右，需要结合上一个小节的EMIT SQL能更容易理清他们之间的关系，这里简单介绍下：</p>
<p><img src="_v_images/20210412125427432_1763085070" alt="图片"></p>
<ul>
<li><p>AfterEndOfWindow：这个就是没配置任何EMIT策略时，默认的EvenTime、ProcTime</p>
</li>
<li><p>Window触发策略（即窗口结束后触发一次）</p>
</li>
<li><p>EveryElement：即delay=0，在processElement()时直接触发，无论是在窗口结束之前或者窗口结束之后都触发，且不再注册timer</p>
</li>
<li><p>AfterEndOfWindowNoLate：对应EMIT WITHOUT DELAY AFTER WATERMARK，窗口结束之前不输出，窗口结束之后无延迟输出</p>
</li>
<li><p>AfterFirstElementPeriodic：对应WITH DELAY ‘1’ MINUTE BEFORE| AFTER WATERMARK，即按系统时间周期执行，由ProcessingTime系统时间周期触发</p>
</li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://example.com/bigdata/Flink/Flink-sink/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="aaronzhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Guadazi">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/bigdata/Flink/Flink-sink/" class="post-title-link" itemprop="url">Flink-sink</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-04-11 14:58:00" itemprop="dateCreated datePublished" datetime="2021-04-11T14:58:00+08:00">2021-04-11</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-04-24 12:58:51" itemprop="dateModified" datetime="2021-04-24T12:58:51+08:00">2021-04-24</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/bigdata/" itemprop="url" rel="index"><span itemprop="name">bigdata</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="Sink的三种模式"><a href="#Sink的三种模式" class="headerlink" title="Sink的三种模式"></a>Sink的三种模式</h2><p><a target="_blank" rel="noopener" href="http://www.whitewood.me/2020/02/26/Flink-Table-%E7%9A%84%E4%B8%89%E7%A7%8D-Sink-%E6%A8%A1%E5%BC%8F/">Flink table的三种sink模式</a></p>
<p>Sink有INSERT、UPDATE 和 DELETE 三类，Table的sink模式有append、upsert和retract三种</p>
<table>
<thead>
<tr>
<th>Sink模式</th>
<th>Insert</th>
<th>Update</th>
<th>Delete</th>
<th>支持的存储</th>
</tr>
</thead>
<tbody><tr>
<td>Append(追加)</td>
<td>支持</td>
<td>不支持</td>
<td>不支持</td>
<td></td>
</tr>
<tr>
<td>Upsert(重复时更新)</td>
<td>支持</td>
<td>支持</td>
<td>支持</td>
<td>KV: HBase JDBC</td>
</tr>
<tr>
<td>Retract(允许撤销)</td>
<td>支持</td>
<td>支持</td>
<td>支持</td>
<td></td>
</tr>
</tbody></table>
<h3 id="Upsert模式与Retract模式的区别"><a href="#Upsert模式与Retract模式的区别" class="headerlink" title="Upsert模式与Retract模式的区别"></a>Upsert模式与Retract模式的区别</h3><p>Upsert模式需要唯一的key来传递更新消息，外部连接器需要明确知道这个唯一key的属性</p>
<p>Upsert模式和Retract模式</p>
<p>消息: 都为(Boolean,Row)二元组, 第一个元素代表操作类型:</p>
<table>
<thead>
<tr>
<th>模式</th>
<th>操作类型</th>
<th>插入</th>
<th>更新</th>
<th>删除</th>
</tr>
</thead>
<tbody><tr>
<td>Upsert模式</td>
<td>true 为 UPSERT消息(不存在则INSERT, 存在则UPDATE)<br/> false 为 DELETE消息</td>
<td>upsert消息</td>
<td>upsert消息</td>
<td>delete消息</td>
</tr>
<tr>
<td>Retract模式</td>
<td>true为添加消息<br/>false为撤回消息</td>
<td>添加消息</td>
<td>已更新行(上一行)的撤回消息<br/>更新行(新行)的添加消息</td>
<td>撤回消息</td>
</tr>
</tbody></table>
<h3 id="Append模式-窗口聚合中的应用"><a href="#Append模式-窗口聚合中的应用" class="headerlink" title="Append模式-窗口聚合中的应用"></a>Append模式-窗口聚合中的应用</h3><p>在实时聚合统计中，聚合统计的结果输出是由 Trigger 决定的，而 Append-Only 则意味着对于每个窗口实例（Pane，窗格）Trigger 只能触发一次，则就导致无法在迟到数据到达时再刷新结果。</p>
<p>通常来说，我们可以给 Watermark 设置一个较大的延迟容忍阈值来避免这种刷新（再有迟到数据则丢弃），但代价是却会引入较大的延迟。</p>
<h3 id="Upsert模式"><a href="#Upsert模式" class="headerlink" title="Upsert模式"></a>Upsert模式</h3><p>支持 Append-Only 的操作和在有主键的前提下的 Update 和 Delete 操作.</p>
<p><strong>重复时更新</strong></p>
<p>Upsert 模式依赖业务主键来实现输出结果的更新和删除，因此非常适合 KV 数据库，比如 HBase、JDBC 的 TableSink 都使用了这种方式。</p>
<p>Upsert 模式是目前来说比较实用的模式，因为大部分业务都会提供原子或复合类型的主键，而在支持 KV 的存储系统也非常多，但要注意的是不要变更主键，具体原因会在下一节谈到。</p>
<h3 id="Retract模式"><a href="#Retract模式" class="headerlink" title="Retract模式"></a>Retract模式</h3><p><strong>允许撤销</strong></p>
<p>举个例子，假设我们将电商订单按照承运快递公司进行分类计数，有如下的结果表。</p>
<table>
<thead>
<tr>
<th align="left">公司</th>
<th align="left">订单数</th>
</tr>
</thead>
<tbody><tr>
<td align="left">中通</td>
<td align="left">2</td>
</tr>
<tr>
<td align="left">圆通</td>
<td align="left">1</td>
</tr>
<tr>
<td align="left">顺丰</td>
<td align="left">3</td>
</tr>
</tbody></table>
<p>那么如果原本一单为中通的快递，后续更新为用顺丰发货，对于 Upsert 模式会产生 <code>(true, (顺丰, 4))</code> 这样一条 changelog，但中通的订单数没有被修正。相比之下，Retract 模式产出 <code>(false, (中通, 1))</code> 和 <code>(true, (顺丰, 1))</code> 两条数据，则可以正确地更新数据。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://example.com/bigdata/hive/hive-sample/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="aaronzhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Guadazi">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/bigdata/hive/hive-sample/" class="post-title-link" itemprop="url">Hive SQL优化样例</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-03-12 00:00:00" itemprop="dateCreated datePublished" datetime="2021-03-12T00:00:00+08:00">2021-03-12</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-04-24 12:58:52" itemprop="dateModified" datetime="2021-04-24T12:58:52+08:00">2021-04-24</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/BigData/" itemprop="url" rel="index"><span itemprop="name">BigData</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="comment">/*+ MAPJOIN(hj3,hj1,hj0)*/</span> <span class="number">20210324</span> <span class="keyword">as</span> fdate, TO_CHAR(SYSTIMESTAMP(), <span class="string">&#x27;yyyymmddhh24miss&#x27;</span>) <span class="keyword">as</span> fetl_time,hj2.fuin <span class="keyword">as</span> fuin,<span class="built_in">COUNT</span>(<span class="keyword">DISTINCT</span>(SUBSTR(<span class="built_in">CAST</span>(if (((hj2.fevent_time <span class="keyword">between</span> <span class="number">20210318000000</span> <span class="keyword">and</span> <span class="number">20210324235959</span>) <span class="keyword">and</span> (hj2.fscn <span class="keyword">in</span> (<span class="string">&#x27;1&#x27;</span>))) , hj2.fevent_time, <span class="keyword">null</span>) <span class="keyword">AS</span> STRING),<span class="number">1</span>,<span class="number">8</span>))) <span class="keyword">as</span> flabel_25942,   <span class="built_in">COUNT</span>(<span class="keyword">DISTINCT</span>(SUBSTR(<span class="built_in">CAST</span>(if (((hj2.fevent_time <span class="keyword">between</span> <span class="number">20210318000000</span> <span class="keyword">and</span> <span class="number">20210324235959</span>)  <span class="keyword">and</span> (hj2.fscn <span class="keyword">in</span> (<span class="string">&#x27;1&#x27;</span>))) , hj2.fevent_time, <span class="keyword">null</span>) <span class="keyword">AS</span> STRING),<span class="number">1</span>,<span class="number">8</span>))) <span class="keyword">as</span> flabel_25970, <span class="built_in">COUNT</span>(<span class="keyword">DISTINCT</span>(SUBSTR(<span class="built_in">CAST</span>(if (((hj2.furl_orig <span class="operator">=</span> <span class="string">&#x27;/mb/v5/fund/list/steady.shtml&#x27;</span>)  <span class="keyword">or</span> (hj2.furl_orig <span class="operator">=</span> <span class="string">&#x27;/mb/v4/fundlist/fund_all.shtml&#x27;</span>)    <span class="keyword">or</span> (hj2.furl_orig <span class="operator">=</span> <span class="string">&#x27;/mb/v4/fundlist/fund_all_v5.shtml&#x27;</span>))  <span class="keyword">and</span> ((hj2.fevent_time <span class="keyword">between</span> <span class="number">20210318000000</span> <span class="keyword">and</span> <span class="number">20210324235959</span>)) , hj2.fevent_time, <span class="keyword">null</span>) <span class="keyword">AS</span> STRING),<span class="number">1</span>,<span class="number">8</span>))) <span class="keyword">as</span> flabel_25972</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> dml_base::dml_evt_lct_cft_label_factory_mta_access_dd hj2</span><br><span class="line"></span><br><span class="line"><span class="keyword">left</span> <span class="keyword">join</span> dim_base::dim_lct_hq_beacon_report_manage hj1 <span class="keyword">on</span> if(hj2.furl_orig <span class="keyword">is</span> <span class="keyword">null</span></span><br><span class="line">                                                              <span class="keyword">or</span> <span class="built_in">trim</span>(hj2.furl_orig) <span class="operator">=</span> <span class="string">&#x27;&#x27;</span>, concat(<span class="string">&#x27;null_&#x27;</span>, <span class="built_in">floor</span>(rand() <span class="operator">*</span> <span class="number">10000</span>)), hj2.furl_orig) <span class="operator">=</span> hj1.fevent_code</span><br><span class="line"></span><br><span class="line"><span class="keyword">left</span> <span class="keyword">join</span> dim_base::dim_lct_hq_virtual_event_info hj3 <span class="keyword">on</span> if(hj2.furl_orig <span class="keyword">is</span> <span class="keyword">null</span></span><br><span class="line">                                                      <span class="keyword">or</span> <span class="built_in">trim</span>(hj2.furl_orig) <span class="operator">=</span> <span class="string">&#x27;&#x27;</span>, concat(<span class="string">&#x27;null_&#x27;</span>, <span class="built_in">floor</span>(rand() <span class="operator">*</span> <span class="number">10000</span>)), hj2.furl_orig) <span class="operator">=</span> hj3.fevent_code</span><br><span class="line"><span class="keyword">left</span> <span class="keyword">join</span> dim_base::dim_prd_lct_cft_fund_type_conf hj0 <span class="keyword">on</span> if(hj2.fspid_fundcode <span class="keyword">is</span> <span class="keyword">null</span></span><br><span class="line">                                                             <span class="keyword">or</span> <span class="built_in">trim</span>(hj2.fspid_fundcode) <span class="operator">=</span> <span class="string">&#x27;&#x27;</span>, concat(<span class="string">&#x27;null_&#x27;</span>, <span class="built_in">floor</span>(rand() <span class="operator">*</span> <span class="number">10000</span>)), hj2.fspid_fundcode) <span class="operator">=</span> hj0.fspid_fundcode</span><br><span class="line"></span><br><span class="line"><span class="keyword">where</span> hj2.fuin <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">null</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">and</span> <span class="built_in">trim</span>(hj2.fuin) <span class="operator">&lt;&gt;</span> <span class="string">&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">and</span> hj2.fdate <span class="operator">&gt;=</span> <span class="number">20210318</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">and</span> hj2.fdate <span class="operator">&lt;=</span> <span class="number">20210324</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> hj2.fuin</span><br></pre></td></tr></table></figure>




<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br></pre></td><td class="code"><pre><span class="line">ABSTRACT SYNTAX TREE:</span><br><span class="line">  (TOK_QUERY (TOK_FROM (TOK_LEFTOUTERJOIN (TOK_LEFTOUTERJOIN (TOK_LEFTOUTERJOIN (TOK_TABREF (TOK_TAB dml_evt_lct_cft_label_factory_mta_access_dd dml_base) hj2) (TOK_TABREF (TOK_TAB dim_lct_hq_beacon_report_manage dim_base) hj1) (= (<span class="function">TOK_FUNCTION <span class="title">if</span> <span class="params">(or (TOK_FUNCTION TOK_ISNULL (. (TOK_TABLE_OR_COL hj2)</span> furl_orig)) <span class="params">(= (TOK_FUNCTION trim (. (TOK_TABLE_OR_COL hj2)</span> furl_orig)) &#x27;&#x27;)) <span class="params">(TOK_FUNCTION concat <span class="string">&#x27;null_&#x27;</span> (TOK_FUNCTION floor (* (TOK_FUNCTION rand)</span> 10000))) <span class="params">(. (TOK_TABLE_OR_COL hj2)</span> furl_orig)) <span class="params">(. (TOK_TABLE_OR_COL hj1)</span> fevent_code))) <span class="params">(TOK_TABREF (TOK_TAB dim_lct_hq_virtual_event_info dim_base)</span> hj3) <span class="params">(= (TOK_FUNCTION <span class="keyword">if</span> (or (TOK_FUNCTION TOK_ISNULL (. (TOK_TABLE_OR_COL hj2)</span> furl_orig)) <span class="params">(= (TOK_FUNCTION trim (. (TOK_TABLE_OR_COL hj2)</span> furl_orig)) &#x27;&#x27;)) <span class="params">(TOK_FUNCTION concat <span class="string">&#x27;null_&#x27;</span> (TOK_FUNCTION floor (* (TOK_FUNCTION rand)</span> 10000))) <span class="params">(. (TOK_TABLE_OR_COL hj2)</span> furl_orig)) <span class="params">(. (TOK_TABLE_OR_COL hj3)</span> fevent_code))) <span class="params">(TOK_TABREF (TOK_TAB dim_prd_lct_cft_fund_type_conf dim_base)</span> hj0) <span class="params">(= (TOK_FUNCTION <span class="keyword">if</span> (or (TOK_FUNCTION TOK_ISNULL (. (TOK_TABLE_OR_COL hj2)</span> fspid_fundcode)) <span class="params">(= (TOK_FUNCTION trim (. (TOK_TABLE_OR_COL hj2)</span> fspid_fundcode)) &#x27;&#x27;)) <span class="params">(TOK_FUNCTION concat <span class="string">&#x27;null_&#x27;</span> (TOK_FUNCTION floor (* (TOK_FUNCTION rand)</span> 10000))) <span class="params">(. (TOK_TABLE_OR_COL hj2)</span> fspid_fundcode)) <span class="params">(. (TOK_TABLE_OR_COL hj0)</span> fspid_fundcode)))) <span class="params">(TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)</span>) <span class="params">(TOK_SELECT (TOK_HINTLIST (TOK_HINT TOK_MAPJOIN (TOK_HINTARGLIST (TOK_TABLE_OR_COL hj3)</span> <span class="params">(TOK_TABLE_OR_COL hj1)</span> <span class="params">(TOK_TABLE_OR_COL hj0)</span>))) <span class="params">(TOK_SELEXPR <span class="number">20210324</span> fdate)</span> <span class="params">(TOK_SELEXPR (TOK_FUNCTION TO_CHAR (TOK_FUNCTION SYSTIMESTAMP)</span> &#x27;yyyymmddhh24miss&#x27;) fetl_time) <span class="params">(TOK_SELEXPR (. (TOK_TABLE_OR_COL hj2)</span> fuin) fuin) <span class="params">(TOK_SELEXPR (TOK_FUNCTIONDI COUNT (TOK_FUNCTION SUBSTR (TOK_FUNCTION TOK_STRING (TOK_FUNCTION <span class="keyword">if</span> (and (and (&gt;= (. (TOK_TABLE_OR_COL hj2)</span> fevent_time) 20210318000000) <span class="params">(&lt;= (. (TOK_TABLE_OR_COL hj2)</span> fevent_time) 20210324235959)) <span class="params">(in (. (TOK_TABLE_OR_COL hj2)</span> fscn) &#x27;1&#x27;)) <span class="params">(. (TOK_TABLE_OR_COL hj2)</span> fevent_time) TOK_NULL)) 1 8)) flabel_25942) <span class="params">(TOK_SELEXPR (TOK_FUNCTIONDI COUNT (TOK_FUNCTION SUBSTR (TOK_FUNCTION TOK_STRING (TOK_FUNCTION <span class="keyword">if</span> (and (and (&gt;= (. (TOK_TABLE_OR_COL hj2)</span> fevent_time) 20210318000000) <span class="params">(&lt;= (. (TOK_TABLE_OR_COL hj2)</span> fevent_time) 20210324235959)) <span class="params">(in (. (TOK_TABLE_OR_COL hj2)</span> fscn) &#x27;1&#x27;)) <span class="params">(. (TOK_TABLE_OR_COL hj2)</span> fevent_time) TOK_NULL)) 1 8)) flabel_25970) <span class="params">(TOK_SELEXPR (TOK_FUNCTIONDI COUNT (TOK_FUNCTION SUBSTR (TOK_FUNCTION TOK_STRING (TOK_FUNCTION <span class="keyword">if</span> (and (or (or (= (. (TOK_TABLE_OR_COL hj2)</span> furl_orig) &#x27;/mb/v5/fund/list/steady.shtml&#x27;) <span class="params">(= (. (TOK_TABLE_OR_COL hj2)</span> furl_orig) &#x27;/mb/v4/fundlist/fund_all.shtml&#x27;)) <span class="params">(= (. (TOK_TABLE_OR_COL hj2)</span> furl_orig) &#x27;/mb/v4/fundlist/fund_all_v5.shtml&#x27;)) <span class="params">(and (&gt;= (. (TOK_TABLE_OR_COL hj2)</span> fevent_time) 20210318000000) <span class="params">(&lt;= (. (TOK_TABLE_OR_COL hj2)</span> fevent_time) 20210324235959))) <span class="params">(. (TOK_TABLE_OR_COL hj2)</span> fevent_time) TOK_NULL)) 1 8)) flabel_25972)) <span class="params">(TOK_WHERE (and (and (and (TOK_FUNCTION TOK_ISNOTNULL (. (TOK_TABLE_OR_COL hj2)</span> fuin)) <span class="params">(&lt;&gt; (TOK_FUNCTION trim (. (TOK_TABLE_OR_COL hj2)</span> fuin)) &#x27;&#x27;)) <span class="params">(&gt;= (. (TOK_TABLE_OR_COL hj2)</span> fdate) 20210318)) <span class="params">(&lt;= (. (TOK_TABLE_OR_COL hj2)</span> fdate) 20210324))) <span class="params">(TOK_GROUPBY (. (TOK_TABLE_OR_COL hj2)</span> fuin))))</span></span><br><span class="line"><span class="function"></span></span><br><span class="line"><span class="function">STAGE DEPENDENCIES:</span></span><br><span class="line"><span class="function">  Stage-1</span></span><br><span class="line"><span class="function">    type:root stage</span>;</span><br><span class="line">  Stage-<span class="number">2</span></span><br><span class="line">    type:;depends on:Stage-<span class="number">1</span>;</span><br><span class="line">  Stage-<span class="number">3</span></span><br><span class="line">    type:;depends on:Stage-<span class="number">2</span>;</span><br><span class="line">  Stage-<span class="number">0</span></span><br><span class="line">    type:root stage;</span><br><span class="line"></span><br><span class="line">STAGE PLANS:</span><br><span class="line">  Stage: Stage-<span class="number">1</span></span><br><span class="line">    Map Reduce</span><br><span class="line">      Alias -&gt; Map Operator Tree:</span><br><span class="line">        dml_base/dml_evt_lct_cft_label_factory_mta_access_dd#hj2 </span><br><span class="line">          Operator:          TableScan</span><br><span class="line">            alias: dml_base/dml_evt_lct_cft_label_factory_mta_access_dd#hj2</span><br><span class="line">            Operator:            Filter Operator</span><br><span class="line">              predicate:</span><br><span class="line">                  expr: (((<span class="function">fuin is not <span class="keyword">null</span> <span class="title">and</span> <span class="params">(trim(fuin)</span> &lt;&gt; &#x27;&#x27;)) <span class="title">and</span> <span class="params">(fdate &gt;= <span class="number">20210318</span>)</span>) <span class="title">and</span> <span class="params">(fdate &lt;= <span class="number">20210324</span>)</span>)</span></span><br><span class="line"><span class="function">                  type: <span class="keyword">boolean</span></span></span><br><span class="line"><span class="function">              Operator:              Common Join Operator</span></span><br><span class="line"><span class="function">                condition map:</span></span><br><span class="line"><span class="function">                     Left Outer Join0 to 1</span></span><br><span class="line"><span class="function">                     Left Outer Join0 to 2</span></span><br><span class="line"><span class="function">                condition expressions:</span></span><br><span class="line"><span class="function">                  0 </span>&#123;fdate&#125; &#123;fuin&#125; &#123;furl_orig&#125; &#123;fevent_time&#125; &#123;fspid_fundcode&#125; &#123;fscn&#125;</span><br><span class="line">                  <span class="number">1</span> </span><br><span class="line">                  <span class="number">2</span> </span><br><span class="line">                handleSkewJoin: <span class="keyword">false</span></span><br><span class="line">                keys:</span><br><span class="line">                  0 [class com.tencent.tdw_udf_cloud.hive.udf.generic.GenericUDFIf(class org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPOr(class org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPNull(Column[furl_orig](), class org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPEqual(class org.apache.hadoop.hive.ql.udf.generic.GenericUDFBridge(Column[furl_orig](), Const string ()(), class org.apache.hadoop.hive.ql.udf.generic.GenericUDFBridge(Const string null_, class org.apache.hadoop.hive.ql.udf.generic.GenericUDFBridge(class org.apache.hadoop.hive.ql.udf.generic.GenericUDFBridge(class org.apache.hadoop.hive.ql.udf.generic.GenericUDFBridge((), Const int 10000()()(), Column[furl_orig]()]</span><br><span class="line">                  <span class="number">1</span> [Column[fevent_code]]</span><br><span class="line">                  <span class="number">2</span> [Column[fevent_code]]</span><br><span class="line">                outputColumnNames: _col0, _col42, _col85, _col88, _col103, _col104</span><br><span class="line">                Position of Big Table: <span class="number">0</span></span><br><span class="line">                Operator:                File Output Operator</span><br><span class="line">                  compressed: <span class="keyword">false</span></span><br><span class="line">                  GlobalTableId: <span class="number">0</span></span><br><span class="line">                  table:</span><br><span class="line">                    table descs</span><br><span class="line">                      input format: org.apache.hadoop.mapred.SequenceFileInputFormat</span><br><span class="line">                      output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat</span><br><span class="line">      Local Work:</span><br><span class="line">        Map Reduce Local Work</span><br><span class="line">          Alias -&gt; Map Local Tables:</span><br><span class="line">            dim_base/dim_lct_hq_beacon_report_manage#hj1 </span><br><span class="line">              Fetch Operator</span><br><span class="line">                limit: -<span class="number">1</span></span><br><span class="line">            dim_base/dim_lct_hq_virtual_event_info#hj3 </span><br><span class="line">              Fetch Operator</span><br><span class="line">                limit: -<span class="number">1</span></span><br><span class="line">          Alias -&gt; Map Local Operator Tree:</span><br><span class="line">            dim_base/dim_lct_hq_beacon_report_manage#hj1 </span><br><span class="line">              Operator:              TableScan</span><br><span class="line">                alias: dim_base/dim_lct_hq_beacon_report_manage#hj1</span><br><span class="line">                Operator:                Common Join Operator</span><br><span class="line">                  condition map:</span><br><span class="line">                       Left Outer Join0 to <span class="number">1</span></span><br><span class="line">                       Left Outer Join0 to <span class="number">2</span></span><br><span class="line">                  condition expressions:</span><br><span class="line">                    <span class="number">0</span> &#123;fdate&#125; &#123;fuin&#125; &#123;furl_orig&#125; &#123;fevent_time&#125; &#123;fspid_fundcode&#125; &#123;fscn&#125;</span><br><span class="line">                    <span class="number">1</span> </span><br><span class="line">                    <span class="number">2</span> </span><br><span class="line">                  handleSkewJoin: <span class="keyword">false</span></span><br><span class="line">                  keys:</span><br><span class="line">                    0 [class com.tencent.tdw_udf_cloud.hive.udf.generic.GenericUDFIf(class org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPOr(class org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPNull(Column[furl_orig](), class org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPEqual(class org.apache.hadoop.hive.ql.udf.generic.GenericUDFBridge(Column[furl_orig](), Const string ()(), class org.apache.hadoop.hive.ql.udf.generic.GenericUDFBridge(Const string null_, class org.apache.hadoop.hive.ql.udf.generic.GenericUDFBridge(class org.apache.hadoop.hive.ql.udf.generic.GenericUDFBridge(class org.apache.hadoop.hive.ql.udf.generic.GenericUDFBridge((), Const int 10000()()(), Column[furl_orig]()]</span><br><span class="line">                    <span class="number">1</span> [Column[fevent_code]]</span><br><span class="line">                    <span class="number">2</span> [Column[fevent_code]]</span><br><span class="line">                  outputColumnNames: _col0, _col42, _col85, _col88, _col103, _col104</span><br><span class="line">                  Position of Big Table: <span class="number">0</span></span><br><span class="line">                  Operator:                  File Output Operator</span><br><span class="line">                    compressed: <span class="keyword">false</span></span><br><span class="line">                    GlobalTableId: <span class="number">0</span></span><br><span class="line">                    table:</span><br><span class="line">                      table descs</span><br><span class="line">                        input format: org.apache.hadoop.mapred.SequenceFileInputFormat</span><br><span class="line">                        output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat</span><br><span class="line">            dim_base/dim_lct_hq_virtual_event_info#hj3 </span><br><span class="line">              Operator:              TableScan</span><br><span class="line">                alias: dim_base/dim_lct_hq_virtual_event_info#hj3</span><br><span class="line">                Operator:                Common Join Operator</span><br><span class="line">                  condition map:</span><br><span class="line">                       Left Outer Join0 to <span class="number">1</span></span><br><span class="line">                       Left Outer Join0 to <span class="number">2</span></span><br><span class="line">                  condition expressions:</span><br><span class="line">                    <span class="number">0</span> &#123;fdate&#125; &#123;fuin&#125; &#123;furl_orig&#125; &#123;fevent_time&#125; &#123;fspid_fundcode&#125; &#123;fscn&#125;</span><br><span class="line">                    <span class="number">1</span> </span><br><span class="line">                    <span class="number">2</span> </span><br><span class="line">                  handleSkewJoin: <span class="keyword">false</span></span><br><span class="line">                  keys:</span><br><span class="line">                    0 [class com.tencent.tdw_udf_cloud.hive.udf.generic.GenericUDFIf(class org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPOr(class org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPNull(Column[furl_orig](), class org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPEqual(class org.apache.hadoop.hive.ql.udf.generic.GenericUDFBridge(Column[furl_orig](), Const string ()(), class org.apache.hadoop.hive.ql.udf.generic.GenericUDFBridge(Const string null_, class org.apache.hadoop.hive.ql.udf.generic.GenericUDFBridge(class org.apache.hadoop.hive.ql.udf.generic.GenericUDFBridge(class org.apache.hadoop.hive.ql.udf.generic.GenericUDFBridge((), Const int 10000()()(), Column[furl_orig]()]</span><br><span class="line">                    <span class="number">1</span> [Column[fevent_code]]</span><br><span class="line">                    <span class="number">2</span> [Column[fevent_code]]</span><br><span class="line">                  outputColumnNames: _col0, _col42, _col85, _col88, _col103, _col104</span><br><span class="line">                  Position of Big Table: <span class="number">0</span></span><br><span class="line">                  Operator:                  File Output Operator</span><br><span class="line">                    compressed: <span class="keyword">false</span></span><br><span class="line">                    GlobalTableId: <span class="number">0</span></span><br><span class="line">                    table:</span><br><span class="line">                      table descs</span><br><span class="line">                        input format: org.apache.hadoop.mapred.SequenceFileInputFormat</span><br><span class="line">                        output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat</span><br><span class="line">      Path -&gt; Alias:</span><br><span class="line">        hdfs:<span class="comment">//&lt;hive_hdfs_base_path&gt;/warehouse/dml_base.db/dml_evt_lct_cft_label_factory_mta_access_dd/par_20210318 [dml_base/dml_evt_lct_cft_label_factory_mta_access_dd#hj2]</span></span><br><span class="line">        hdfs:<span class="comment">//&lt;hive_hdfs_base_path&gt;/warehouse/dml_base.db/dml_evt_lct_cft_label_factory_mta_access_dd/par_20210319 [dml_base/dml_evt_lct_cft_label_factory_mta_access_dd#hj2]</span></span><br><span class="line">        hdfs:<span class="comment">//&lt;hive_hdfs_base_path&gt;/warehouse/dml_base.db/dml_evt_lct_cft_label_factory_mta_access_dd/par_20210320 [dml_base/dml_evt_lct_cft_label_factory_mta_access_dd#hj2]</span></span><br><span class="line">        hdfs:<span class="comment">//&lt;hive_hdfs_base_path&gt;/warehouse/dml_base.db/dml_evt_lct_cft_label_factory_mta_access_dd/par_20210321 [dml_base/dml_evt_lct_cft_label_factory_mta_access_dd#hj2]</span></span><br><span class="line">        hdfs:<span class="comment">//&lt;hive_hdfs_base_path&gt;/warehouse/dml_base.db/dml_evt_lct_cft_label_factory_mta_access_dd/par_20210322 [dml_base/dml_evt_lct_cft_label_factory_mta_access_dd#hj2]</span></span><br><span class="line">        hdfs:<span class="comment">//&lt;hive_hdfs_base_path&gt;/warehouse/dml_base.db/dml_evt_lct_cft_label_factory_mta_access_dd/par_20210323 [dml_base/dml_evt_lct_cft_label_factory_mta_access_dd#hj2]</span></span><br><span class="line">        hdfs:<span class="comment">//&lt;hive_hdfs_base_path&gt;/warehouse/dml_base.db/dml_evt_lct_cft_label_factory_mta_access_dd/par_20210324 [dml_base/dml_evt_lct_cft_label_factory_mta_access_dd#hj2]</span></span><br><span class="line"></span><br><span class="line">  Stage: Stage-<span class="number">2</span></span><br><span class="line">    Map Reduce</span><br><span class="line">      Alias -&gt; Map Operator Tree:</span><br><span class="line">        hdfs:<span class="comment">//&lt;tmp_hive_hdfs_base_path&gt;/20210325/&lt;hdfs_user&gt;_tdwadmin_20210325195415346_29895878_7_699597773/10002 </span></span><br><span class="line">          Operator:          Select Operator</span><br><span class="line">            expressions:</span><br><span class="line">                  expr: _col0</span><br><span class="line">                  type: bigint</span><br><span class="line">                  expr: _col42</span><br><span class="line">                  type: string</span><br><span class="line">                  expr: _col85</span><br><span class="line">                  type: string</span><br><span class="line">                  expr: _col88</span><br><span class="line">                  type: bigint</span><br><span class="line">                  expr: _col103</span><br><span class="line">                  type: string</span><br><span class="line">                  expr: _col104</span><br><span class="line">                  type: string</span><br><span class="line">            outputColumnNames: _col0, _col42, _col85, _col88, _col103, _col104</span><br><span class="line">            Operator:            Common Join Operator</span><br><span class="line">              condition map:</span><br><span class="line">                   Left Outer Join0 to <span class="number">1</span></span><br><span class="line">              condition expressions:</span><br><span class="line">                <span class="number">0</span> &#123;_col0&#125; &#123;_col42&#125; &#123;_col85&#125; &#123;_col88&#125; &#123;_col104&#125;</span><br><span class="line">                <span class="number">1</span> </span><br><span class="line">              handleSkewJoin: <span class="keyword">false</span></span><br><span class="line">              keys:</span><br><span class="line">                0 [class com.tencent.tdw_udf_cloud.hive.udf.generic.GenericUDFIf(class org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPOr(class org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPNull(Column[_col103](), class org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPEqual(class org.apache.hadoop.hive.ql.udf.generic.GenericUDFBridge(Column[_col103](), Const string ()(), class org.apache.hadoop.hive.ql.udf.generic.GenericUDFBridge(Const string null_, class org.apache.hadoop.hive.ql.udf.generic.GenericUDFBridge(class org.apache.hadoop.hive.ql.udf.generic.GenericUDFBridge(class org.apache.hadoop.hive.ql.udf.generic.GenericUDFBridge((), Const int 10000()()(), Column[_col103]()]</span><br><span class="line">                <span class="number">1</span> [Column[fspid_fundcode]]</span><br><span class="line">              outputColumnNames: _col0, _col42, _col85, _col88, _col104</span><br><span class="line">              Position of Big Table: <span class="number">0</span></span><br><span class="line">              Operator:              File Output Operator</span><br><span class="line">                compressed: <span class="keyword">false</span></span><br><span class="line">                GlobalTableId: <span class="number">0</span></span><br><span class="line">                table:</span><br><span class="line">                  table descs</span><br><span class="line">                    input format: org.apache.hadoop.mapred.SequenceFileInputFormat</span><br><span class="line">                    output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat</span><br><span class="line">      Local Work:</span><br><span class="line">        Map Reduce Local Work</span><br><span class="line">          Alias -&gt; Map Local Tables:</span><br><span class="line">            dim_base/dim_prd_lct_cft_fund_type_conf#hj0 </span><br><span class="line">              Fetch Operator</span><br><span class="line">                limit: -<span class="number">1</span></span><br><span class="line">          Alias -&gt; Map Local Operator Tree:</span><br><span class="line">            dim_base/dim_prd_lct_cft_fund_type_conf#hj0 </span><br><span class="line">              Operator:              TableScan</span><br><span class="line">                alias: dim_base/dim_prd_lct_cft_fund_type_conf#hj0</span><br><span class="line">                Operator:                Common Join Operator</span><br><span class="line">                  condition map:</span><br><span class="line">                       Left Outer Join0 to <span class="number">1</span></span><br><span class="line">                  condition expressions:</span><br><span class="line">                    <span class="number">0</span> &#123;_col0&#125; &#123;_col42&#125; &#123;_col85&#125; &#123;_col88&#125; &#123;_col104&#125;</span><br><span class="line">                    <span class="number">1</span> </span><br><span class="line">                  handleSkewJoin: <span class="keyword">false</span></span><br><span class="line">                  keys:</span><br><span class="line">                    0 [class com.tencent.tdw_udf_cloud.hive.udf.generic.GenericUDFIf(class org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPOr(class org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPNull(Column[_col103](), class org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPEqual(class org.apache.hadoop.hive.ql.udf.generic.GenericUDFBridge(Column[_col103](), Const string ()(), class org.apache.hadoop.hive.ql.udf.generic.GenericUDFBridge(Const string null_, class org.apache.hadoop.hive.ql.udf.generic.GenericUDFBridge(class org.apache.hadoop.hive.ql.udf.generic.GenericUDFBridge(class org.apache.hadoop.hive.ql.udf.generic.GenericUDFBridge((), Const int 10000()()(), Column[_col103]()]</span><br><span class="line">                    <span class="number">1</span> [Column[fspid_fundcode]]</span><br><span class="line">                  outputColumnNames: _col0, _col42, _col85, _col88, _col104</span><br><span class="line">                  Position of Big Table: <span class="number">0</span></span><br><span class="line">                  Operator:                  File Output Operator</span><br><span class="line">                    compressed: <span class="keyword">false</span></span><br><span class="line">                    GlobalTableId: <span class="number">0</span></span><br><span class="line">                    table:</span><br><span class="line">                      table descs</span><br><span class="line">                        input format: org.apache.hadoop.mapred.SequenceFileInputFormat</span><br><span class="line">                        output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat</span><br><span class="line">      Path -&gt; Alias:</span><br><span class="line">        hdfs:<span class="comment">//&lt;tmp_hive_hdfs_base_path&gt;/20210325/&lt;hdfs_user&gt;_tdwadmin_20210325195415346_29895878_7_699597773/10002 [hdfs://&lt;tmp_hive_hdfs_base_path&gt;/20210325/&lt;hdfs_user&gt;_tdwadmin_20210325195415346_29895878_7_699597773/10002]</span></span><br><span class="line"></span><br><span class="line">  Stage: Stage-<span class="number">3</span></span><br><span class="line">    Map Reduce</span><br><span class="line">      Alias -&gt; Map Operator Tree:</span><br><span class="line">        hdfs:<span class="comment">//&lt;tmp_hive_hdfs_base_path&gt;/20210325/&lt;hdfs_user&gt;_tdwadmin_20210325195415346_29895878_7_699597773/10002 </span></span><br><span class="line">          Operator:          Select Operator</span><br><span class="line">            expressions:</span><br><span class="line">                  expr: _col0</span><br><span class="line">                  type: bigint</span><br><span class="line">                  expr: _col42</span><br><span class="line">                  type: string</span><br><span class="line">                  expr: _col85</span><br><span class="line">                  type: string</span><br><span class="line">                  expr: _col88</span><br><span class="line">                  type: bigint</span><br><span class="line">                  expr: _col103</span><br><span class="line">                  type: string</span><br><span class="line">                  expr: _col104</span><br><span class="line">                  type: string</span><br><span class="line">            outputColumnNames: _col0, _col42, _col85, _col88, _col103, _col104</span><br><span class="line">            Operator:            Common Join Operator</span><br><span class="line">              condition map:</span><br><span class="line">                   Left Outer Join0 to <span class="number">1</span></span><br><span class="line">              condition expressions:</span><br><span class="line">                <span class="number">0</span> &#123;_col0&#125; &#123;_col42&#125; &#123;_col85&#125; &#123;_col88&#125; &#123;_col104&#125;</span><br><span class="line">                <span class="number">1</span> </span><br><span class="line">              handleSkewJoin: <span class="keyword">false</span></span><br><span class="line">              keys:</span><br><span class="line">                0 [class com.tencent.tdw_udf_cloud.hive.udf.generic.GenericUDFIf(class org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPOr(class org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPNull(Column[_col103](), class org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPEqual(class org.apache.hadoop.hive.ql.udf.generic.GenericUDFBridge(Column[_col103](), Const string ()(), class org.apache.hadoop.hive.ql.udf.generic.GenericUDFBridge(Const string null_, class org.apache.hadoop.hive.ql.udf.generic.GenericUDFBridge(class org.apache.hadoop.hive.ql.udf.generic.GenericUDFBridge(class org.apache.hadoop.hive.ql.udf.generic.GenericUDFBridge((), Const int 10000()()(), Column[_col103]()]</span><br><span class="line">                <span class="number">1</span> [Column[fspid_fundcode]]</span><br><span class="line">              outputColumnNames: _col0, _col42, _col85, _col88, _col104</span><br><span class="line">              Position of Big Table: <span class="number">0</span></span><br><span class="line">              Operator:              File Output Operator</span><br><span class="line">                compressed: <span class="keyword">false</span></span><br><span class="line">                GlobalTableId: <span class="number">0</span></span><br><span class="line">                table:</span><br><span class="line">                  table descs</span><br><span class="line">                    input format: org.apache.hadoop.mapred.SequenceFileInputFormat</span><br><span class="line">                    output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat</span><br><span class="line">      Path -&gt; Alias:</span><br><span class="line">        hdfs:<span class="comment">//&lt;tmp_hive_hdfs_base_path&gt;/20210325/&lt;hdfs_user&gt;_tdwadmin_20210325195415346_29895878_7_699597773/10002 [hdfs://&lt;tmp_hive_hdfs_base_path&gt;/20210325/&lt;hdfs_user&gt;_tdwadmin_20210325195415346_29895878_7_699597773/10002]</span></span><br><span class="line">      Reduce Operator Tree:</span><br><span class="line">        Operator:        Group By Operator</span><br><span class="line">          aggregations:</span><br><span class="line">                expr: count(DISTINCT KEY._col1:<span class="number">1.</span>_col0)</span><br><span class="line">                expr: count(DISTINCT KEY._col1:<span class="number">2.</span>_col0)</span><br><span class="line">          keys:</span><br><span class="line">                expr: KEY._col0</span><br><span class="line">                type: string</span><br><span class="line">          mode: mergepartial</span><br><span class="line">          outputColumnNames: _col0, _col1, _col2</span><br><span class="line">          UseNewGroupBy: <span class="keyword">true</span></span><br><span class="line">          Operator:          Select Operator</span><br><span class="line">            expressions:</span><br><span class="line">                  expr: <span class="number">20210324</span></span><br><span class="line">                  type: <span class="keyword">int</span></span><br><span class="line">                  expr: (systimestamp to_char <span class="string">&#x27;yyyymmddhh24miss&#x27;</span>)</span><br><span class="line">                  type: string</span><br><span class="line">                  expr: _col0</span><br><span class="line">                  type: string</span><br><span class="line">                  expr: _col1</span><br><span class="line">                  type: bigint</span><br><span class="line">                  expr: _col1</span><br><span class="line">                  type: bigint</span><br><span class="line">                  expr: _col2</span><br><span class="line">                  type: bigint</span><br><span class="line">            outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5</span><br><span class="line">            Operator:            File Output Operator</span><br><span class="line">              compressed: <span class="keyword">false</span></span><br><span class="line">              GlobalTableId: <span class="number">0</span></span><br><span class="line">              table:</span><br><span class="line">                table descs</span><br><span class="line">                  input format: org.apache.hadoop.mapred.TextInputFormat</span><br><span class="line">                  output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat</span><br><span class="line"></span><br><span class="line">  Stage: Stage-<span class="number">0</span></span><br><span class="line">    Fetch Operator</span><br><span class="line">      limit: <span class="number">100000000</span></span><br></pre></td></tr></table></figure>




<p>error</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">submitSql has error: </span><br><span class="line">org.apache.spark.sql.AnalysisException: nondeterministic expressions are only allowed in</span><br><span class="line">Project, Filter, Aggregate or Window, found:</span><br><span class="line"> ((IF(((hj2.`furl_orig` IS NULL) OR (com.tencent.tdw_udf_cloud.hive.udf.UDFTrim(hj2.`furl_orig`) = <span class="string">&#x27;&#x27;</span>)), com.tencent.tdw_udf_cloud.hive.udf.UDFConcat(<span class="string">&#x27;null_&#x27;</span>, com.tencent.tdw_udf_cloud.hive.udf.UDFFloor((org.apache.hadoop.hive.ql.udf.UDFRand() * CAST(<span class="number">10000</span> AS DOUBLE)))), hj2.`furl_orig`)) = hj1.`fevent_code`)</span><br><span class="line">in operator Join LeftOuter, (if ((isnull(furl_orig#92) || (HiveSimpleUDF#com.tencent.tdw_udf_cloud.hive.udf.UDFTrim(furl_orig#92) = ))) HiveSimpleUDF#com.tencent.tdw_udf_cloud.hive.udf.UDFConcat(null_,HiveSimpleUDF#com.tencent.tdw_udf_cloud.hive.udf.UDFFloor((HiveSimpleUDF#org.apache.hadoop.hive.ql.udf.UDFRand() * cast(10000 as double)))) else furl_orig#92 = fevent_code#132)</span><br><span class="line">               ;;</span><br><span class="line">Aggregate [fuin#49], [20210324 AS fdate#0, HiveSimpleUDF#com.tencent.tdw_udf_cloud.hive.udf.UDFToChar(HiveSimpleUDF#org.apache.hadoop.hive.ql.udf.UDFSysTimestamp(),yyyymmddhh24miss) AS fetl_time#1, fuin#49 AS fuin#2, count(distinct HiveSimpleUDF#com.tencent.tdw_udf_cloud.hive.udf.UDFSubstr(cast(if ((((fevent_time#95L &gt;= 20210318000000) &amp;&amp; (fevent_time#95L &lt;= 20210324235959)) &amp;&amp; fscn#111 IN (1))) fevent_time#95L else cast(null as bigint) as string),1,8)) AS flabel_25942#3L, count(distinct HiveSimpleUDF#com.tencent.tdw_udf_cloud.hive.udf.UDFSubstr(cast(if ((((fevent_time#95L &gt;= 20210318000000) &amp;&amp; (fevent_time#95L &lt;= 20210324235959)) &amp;&amp; fscn#111 IN (1))) fevent_time#95L else cast(null as bigint) as string),1,8)) AS flabel_25970#4L, count(distinct HiveSimpleUDF#com.tencent.tdw_udf_cloud.hive.udf.UDFSubstr(cast(if (((((furl_orig#92 = /mb/v5/fund/list/steady.shtml) || (furl_orig#92 = /mb/v4/fundlist/fund_all.shtml)) || (furl_orig#92 = /mb/v4/fundlist/fund_all_v5.shtml)) &amp;&amp; ((fevent_time#95L &gt;= 20210318000000) &amp;&amp; (fevent_time#95L &lt;= 20210324235959)))) fevent_time#95L else cast(null as bigint) as string),1,8)) AS flabel_25972#5L]</span><br><span class="line">+- Filter ((isnotnull(fuin#49) &amp;&amp; NOT (HiveSimpleUDF#com.tencent.tdw_udf_cloud.hive.udf.UDFTrim(fuin#49) = )) &amp;&amp; ((fdate#7L &gt;= cast(20210318 as bigint)) &amp;&amp; (fdate#7L &lt;= cast(20210324 as bigint))))</span><br><span class="line">   +- Join LeftOuter, (if ((isnull(fspid_fundcode#110) || (HiveSimpleUDF#com.tencent.tdw_udf_cloud.hive.udf.UDFTrim(fspid_fundcode#110) = ))) HiveSimpleUDF#com.tencent.tdw_udf_cloud.hive.udf.UDFConcat(null_,HiveSimpleUDF#com.tencent.tdw_udf_cloud.hive.udf.UDFFloor((HiveSimpleUDF#org.apache.hadoop.hive.ql.udf.UDFRand() * cast(10000 as double)))) else fspid_fundcode#110 = fspid_fundcode#209)</span><br><span class="line">      :- Join LeftOuter, (if ((isnull(furl_orig#92) || (HiveSimpleUDF#com.tencent.tdw_udf_cloud.hive.udf.UDFTrim(furl_orig#92) = ))) HiveSimpleUDF#com.tencent.tdw_udf_cloud.hive.udf.UDFConcat(null_,HiveSimpleUDF#com.tencent.tdw_udf_cloud.hive.udf.UDFFloor((HiveSimpleUDF#org.apache.hadoop.hive.ql.udf.UDFRand() * cast(10000 as double)))) else furl_orig#92 = fevent_code#201)</span><br><span class="line">      :  :- Join LeftOuter, (if ((isnull(furl_orig#92) || (HiveSimpleUDF#com.tencent.tdw_udf_cloud.hive.udf.UDFTrim(furl_orig#92) = ))) HiveSimpleUDF#com.tencent.tdw_udf_cloud.hive.udf.UDFConcat(null_,HiveSimpleUDF#com.tencent.tdw_udf_cloud.hive.udf.UDFFloor((HiveSimpleUDF#org.apache.hadoop.hive.ql.udf.UDFRand() * cast(10000 as double)))) else furl_orig#92 = fevent_code#132)</span><br><span class="line">      :  :  :- SubqueryAlias hj2</span><br><span class="line">      :  :  :  +- MetastoreRelation dml_base, dml_evt_lct_cft_label_factory_mta_access_dd</span><br><span class="line">      :  :  +- BroadcastHint</span><br><span class="line">      :  :     +- SubqueryAlias hj1</span><br><span class="line">      :  :        +- MetastoreRelation dim_base, dim_lct_hq_beacon_report_manage</span><br><span class="line">      :  +- BroadcastHint</span><br><span class="line">      :     +- SubqueryAlias hj3</span><br><span class="line">      :        +- MetastoreRelation dim_base, dim_lct_hq_virtual_event_info</span><br><span class="line">      +- BroadcastHint</span><br><span class="line">         +- SubqueryAlias hj0</span><br><span class="line">            +- MetastoreRelation dim_base, dim_prd_lct_cft_fund_type_conf</span><br></pre></td></tr></table></figure>




<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">java.lang.RuntimeException: Map local work failed at org.apache.hadoop.hive.ql.exec.ExecMapper.processOldMapLocalWork(ExecMapper.java:<span class="number">317</span>) at org.apache.hadoop.hive.ql.exec.ExecMapper.map(ExecMapper.java:<span class="number">151</span>) at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:<span class="number">54</span>) at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:<span class="number">453</span>) at org.apache.hadoop.mapred.MapTask.run(MapTask.java:<span class="number">343</span>) at org.apache.hadoop.mapred.YarnChild$<span class="number">2.</span>run(YarnChild.java:<span class="number">175</span>) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:<span class="number">422</span>) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:<span class="number">2286</span>) at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:<span class="number">169</span>) Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: java.io.IOException: 没有那个文件或目录 at org.apache.hadoop.hive.ql.exec.persistence.HashMapWrapper.getPersistentHash(HashMapWrapper.java:<span class="number">189</span>) at org.apache.hadoop.hive.ql.exec.persistence.HashMapWrapper.put(HashMapWrapper.java:<span class="number">155</span>) at org.apache.hadoop.hive.ql.exec.MapJoinOperator.process(MapJoinOperator.java:<span class="number">474</span>) at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:<span class="number">471</span>) at org.apache.hadoop.hive.ql.exec.TableScanOperator.process(TableScanOperator.java:<span class="number">37</span>) at org.apache.hadoop.hive.ql.exec.ExecMapper.processOldMapLocalWork(ExecMapper.java:<span class="number">302</span>) ... <span class="number">9</span> more Caused by: java.io.IOException: 没有那个文件或目录 at java.io.UnixFileSystem.createFileExclusively(Native Method) at java.io.File.createTempFile(File.java:<span class="number">2024</span>) at org.apache.hadoop.hive.ql.exec.persistence.HashMapWrapper.getPersistentHash(HashMapWrapper.java:<span class="number">176</span>) ... <span class="number">14</span> more	N/A</span><br></pre></td></tr></table></figure>


<p>定位出错误原因是：强制指定了mapjoin，内存溢出了</p>
<p>Hive的自动join策略选择：</p>
<p>由于开启了hive.auto.convert.join，但是实际小表大小是hive.mapjoin.smalltable.filesize（默认25M，小表不会超过25M）。由于使用的是orc压缩，解压缩后可能大小到了250M，存放到内存大小可能就会超过1G。mapjoin的时候，hive orcfile 放到内存中会放大40倍<br> 可以看到JVM Max Heap Size大小为：1013645312 （大约1G）</p>
<h2 id="一句话总结"><a href="#一句话总结" class="headerlink" title="一句话总结"></a>一句话总结</h2><p>由于使用了hive.auto.convert.join,对小表进行广播，但是原表是orc的，存放到内存可能膨胀到大于localtask的堆内存大小，导致sql执行失败。</p>
<h2 id="解决措施"><a href="#解决措施" class="headerlink" title="解决措施"></a>解决措施</h2><h6 id="方案一"><a href="#方案一" class="headerlink" title="方案一"></a>方案一</h6><p>调大localtask的内存，set hive.mapred.local.mem=XX ，默认1G，调大到4G</p>
<h6 id="方案二"><a href="#方案二" class="headerlink" title="方案二"></a>方案二</h6><p>直接关表autojoin，将hive.auto.convert.join设置成false</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://example.com/bigdata/Spark/Spark-SQL/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="aaronzhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Guadazi">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/bigdata/Spark/Spark-SQL/" class="post-title-link" itemprop="url">Spark SQL</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-01-15 19:47:00" itemprop="dateCreated datePublished" datetime="2021-01-15T19:47:00+08:00">2021-01-15</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-04-24 12:58:52" itemprop="dateModified" datetime="2021-04-24T12:58:52+08:00">2021-04-24</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/bigdata/" itemprop="url" rel="index"><span itemprop="name">bigdata</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="Spark-SQL"><a href="#Spark-SQL" class="headerlink" title="Spark-SQL"></a>Spark-SQL</h1><h2 id="join"><a href="#join" class="headerlink" title="join"></a>join</h2><p>Spark 中支持多种连接类型：</p>
<ul>
<li>Inner Join : 内连接；</li>
<li>Full Outer Join : 全外连接；</li>
<li>Left Outer Join : 左外连接；</li>
<li>Right Outer Join : 右外连接；</li>
<li>Left Semi Join : 左半连接；</li>
<li>Left Anti Join : 左反连接；</li>
<li>Natural Join : 自然连接；</li>
<li>Cross (or Cartesian) Join : 交叉 (或笛卡尔) 连接</li>
</ul>
<p><img src="vx_images/289801248595.png" alt="SQL JOINS"></p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">emp 员工表</span><br><span class="line"> <span class="operator">|</span><span class="comment">-- ENAME: 员工姓名</span></span><br><span class="line"> <span class="operator">|</span><span class="comment">-- DEPTNO: 部门编号</span></span><br><span class="line"> <span class="operator">|</span><span class="comment">-- EMPNO: 员工编号</span></span><br><span class="line"> <span class="operator">|</span><span class="comment">-- HIREDATE: 入职时间</span></span><br><span class="line"> <span class="operator">|</span><span class="comment">-- JOB: 职务</span></span><br><span class="line"> <span class="operator">|</span><span class="comment">-- MGR: 上级编号</span></span><br><span class="line"> <span class="operator">|</span><span class="comment">-- SAL: 薪资</span></span><br><span class="line"> <span class="operator">|</span><span class="comment">-- COMM: 奖金  </span></span><br><span class="line"></span><br><span class="line">dept 部门表</span><br><span class="line"> <span class="operator">|</span><span class="comment">-- DEPTNO: 部门编号</span></span><br><span class="line"> <span class="operator">|</span><span class="comment">-- DNAME:  部门名称</span></span><br><span class="line"> <span class="operator">|</span><span class="comment">-- LOC:    部门所在城市</span></span><br><span class="line"></span><br><span class="line"><span class="comment">-- LEFT SEMI JOIN</span></span><br><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> emp <span class="keyword">LEFT</span> SEMI <span class="keyword">JOIN</span> dept <span class="keyword">ON</span> emp.deptno <span class="operator">=</span> dept.deptno</span><br><span class="line"><span class="comment">-- 等价于如下的 IN 语句</span></span><br><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> emp <span class="keyword">WHERE</span> deptno <span class="keyword">IN</span> (<span class="keyword">SELECT</span> deptno <span class="keyword">FROM</span> dept)</span><br><span class="line"></span><br><span class="line"><span class="comment">-- LEFT ANTI JOIN</span></span><br><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> emp <span class="keyword">LEFT</span> ANTI <span class="keyword">JOIN</span> dept <span class="keyword">ON</span> emp.deptno <span class="operator">=</span> dept.deptno</span><br><span class="line"><span class="comment">-- 等价于如下的 IN 语句</span></span><br><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> emp <span class="keyword">WHERE</span> deptno <span class="keyword">NOT</span> <span class="keyword">IN</span> (<span class="keyword">SELECT</span> deptno <span class="keyword">FROM</span> dept)</span><br><span class="line"></span><br><span class="line"><span class="comment">--CROSS JOIN</span></span><br><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> emp <span class="keyword">CROSS</span> <span class="keyword">JOIN</span> dept <span class="keyword">ON</span> emp.deptno <span class="operator">=</span> dept.deptno</span><br><span class="line"></span><br><span class="line"><span class="comment">--自然连接是在两张表中寻找那些数据类型和列名都相同的字段，然后自动地将他们连接起来，并返回所有符合条件的结果。</span></span><br><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> emp <span class="keyword">NATURAL</span> <span class="keyword">JOIN</span> dept</span><br><span class="line"></span><br><span class="line"><span class="comment">--程序自动推断出使用两张表都存在的 dept 列进行连接</span></span><br><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> emp <span class="keyword">JOIN</span> dept <span class="keyword">ON</span> emp.deptno <span class="operator">=</span> dept.deptno</span><br></pre></td></tr></table></figure>
<h3 id="内部实现"><a href="#内部实现" class="headerlink" title="内部实现"></a>内部实现</h3><p>broadcast join –&gt; hash join  –&gt; sort-merge join</p>
<p>在对大表与大表之间进行连接操作时，通常都会触发 <code>Shuffle Join</code>，两表的所有分区节点会进行 <code>All-to-All</code> 的通讯，这种查询通常比较昂贵，会对网络 IO 会造成比较大的负担。</p>
<p><img src="vx_images/4246497595210.png" alt="https://github.com/heibaiying"></p>
<p>而对于大表和小表的连接操作，Spark 会在一定程度上进行优化，如果小表的数据量小于 Worker Node 的内存空间，Spark 会考虑将小表的数据广播到每一个 Worker Node，在每个工作节点内部执行连接计算，这可以降低网络的 IO，但会加大每个 Worker Node 的 CPU 负担。</p>
<p><img src="vx_images/4195188806118"></p>
<p>是否采用广播方式进行 <code>Join</code> 取决于程序内部对小表的判断，如果想明确使用广播方式进行 <code>Join</code>，则可以在 DataFrame API 中使用 <code>broadcast</code> 方法指定需要广播的小表：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">empDF.join(broadcast(deptDF), joinExpression).show()</span><br></pre></td></tr></table></figure>

<h2 id="优化"><a href="#优化" class="headerlink" title="优化"></a>优化</h2><table>
<thead>
<tr>
<th align="left"><strong>优化规则</strong></th>
<th align="left"><strong>规则名称</strong></th>
<th align="left"><strong>简介</strong></th>
</tr>
</thead>
<tbody><tr>
<td align="left">列裁剪</td>
<td align="left">column_prune</td>
<td align="left">对于上层算子不需要的列，不在下层算子输出该列，减少计算</td>
</tr>
<tr>
<td align="left">子查询去关联</td>
<td align="left">decorrelate</td>
<td align="left">尝试对相关子查询进行改写，将其转换为普通 join 或 aggregation 计算</td>
</tr>
<tr>
<td align="left">聚合消除</td>
<td align="left">aggregation_eliminate</td>
<td align="left">尝试消除执行计划中的某些不必要的聚合算子</td>
</tr>
<tr>
<td align="left">投影消除</td>
<td align="left">projection_eliminate</td>
<td align="left">消除执行计划中不必要的投影算子</td>
</tr>
<tr>
<td align="left">最大最小消除</td>
<td align="left">max_min_eliminate</td>
<td align="left">改写聚合中的 max/min 计算，转化为 <code>order by</code> + <code>limit 1</code></td>
</tr>
<tr>
<td align="left">谓词下推</td>
<td align="left">predicate_push_down</td>
<td align="left">尝试将执行计划中过滤条件下推到离数据源更近的算子上</td>
</tr>
<tr>
<td align="left">外连接消除</td>
<td align="left">outer_join_eliminate</td>
<td align="left">尝试消除执行计划中不必要的 left join 或者 right join</td>
</tr>
<tr>
<td align="left">分区裁剪</td>
<td align="left">partition_processor</td>
<td align="left">将分区表查询改成为用 union all，并裁剪掉不满足过滤条件的分区</td>
</tr>
<tr>
<td align="left">聚合下推</td>
<td align="left">aggregation_push_down</td>
<td align="left">尝试将执行计划中的聚合算子下推到更底层的计算节点</td>
</tr>
<tr>
<td align="left">TopN 下推</td>
<td align="left">topn_push_down</td>
<td align="left">尝试将执行计划中的 TopN 算子下推到离数据源更近的算子上</td>
</tr>
<tr>
<td align="left">Join 重排序</td>
<td align="left">join_reorder</td>
<td align="left">对多表 join 确定连接顺序</td>
</tr>
</tbody></table>
<h2 id="逻辑优化"><a href="#逻辑优化" class="headerlink" title="逻辑优化"></a>逻辑优化</h2><h3 id="子查询相关的优化"><a href="#子查询相关的优化" class="headerlink" title="子查询相关的优化"></a>子查询相关的优化</h3><p>关联子查询去关联</p>
<h3 id="列裁剪"><a href="#列裁剪" class="headerlink" title="列裁剪"></a>列裁剪</h3><h3 id="关联子查询去关联"><a href="#关联子查询去关联" class="headerlink" title="关联子查询去关联"></a>关联子查询去关联</h3><h3 id="Max-Min-消除"><a href="#Max-Min-消除" class="headerlink" title="Max/Min 消除"></a>Max/Min 消除</h3><h3 id="谓词下推"><a href="#谓词下推" class="headerlink" title="谓词下推"></a>谓词下推</h3><h3 id="分区裁剪"><a href="#分区裁剪" class="headerlink" title="分区裁剪"></a>分区裁剪</h3><h3 id="TopN-和-Limit-下推"><a href="#TopN-和-Limit-下推" class="headerlink" title="TopN 和 Limit 下推"></a>TopN 和 Limit 下推</h3><h3 id="Join-Reorder"><a href="#Join-Reorder" class="headerlink" title="Join Reorder"></a>Join Reorder</h3><h2 id="物理优化"><a href="#物理优化" class="headerlink" title="物理优化"></a>物理优化</h2><h3 id="选择最优的索引进行表的访问"><a href="#选择最优的索引进行表的访问" class="headerlink" title="选择最优的索引进行表的访问"></a>选择最优的索引进行表的访问</h3><h3 id="收集统计信息来获得表的数据分布情况"><a href="#收集统计信息来获得表的数据分布情况" class="headerlink" title="收集统计信息来获得表的数据分布情况"></a>收集统计信息来获得表的数据分布情况</h3><h3 id="在错误索引的解决方案中会介绍当发现-TiDB-索引选错时，你应该使用那些手段来让它使用正确的索引"><a href="#在错误索引的解决方案中会介绍当发现-TiDB-索引选错时，你应该使用那些手段来让它使用正确的索引" class="headerlink" title="在错误索引的解决方案中会介绍当发现 TiDB 索引选错时，你应该使用那些手段来让它使用正确的索引"></a>在错误索引的解决方案中会介绍当发现 TiDB 索引选错时，你应该使用那些手段来让它使用正确的索引</h3><h3 id="在-Distinct-优化中会介绍在物理优化中会做的一个有关-DISTINCT-关键字的优化，在这一小节中会介绍它的优缺点以及如何使用它。"><a href="#在-Distinct-优化中会介绍在物理优化中会做的一个有关-DISTINCT-关键字的优化，在这一小节中会介绍它的优缺点以及如何使用它。" class="headerlink" title="在 Distinct 优化中会介绍在物理优化中会做的一个有关 DISTINCT 关键字的优化，在这一小节中会介绍它的优缺点以及如何使用它。"></a>在 Distinct 优化中会介绍在物理优化中会做的一个有关 DISTINCT 关键字的优化，在这一小节中会介绍它的优缺点以及如何使用它。</h3><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="[参考文献]"></a>[参考文献]</h2><ol>
<li><a target="_blank" rel="noopener" href="https://www.infoq.cn/article/an-article-mastering-sql-on-hadoop-core-technology">The Business Intelligence for Hadoop Benchmark</a></li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://example.com/bigdata/Flink/RealTimeSystem-backpress/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="aaronzhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Guadazi">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/bigdata/Flink/RealTimeSystem-backpress/" class="post-title-link" itemprop="url">【转】实时流处理系统反压机制</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-01-15 19:47:00" itemprop="dateCreated datePublished" datetime="2021-01-15T19:47:00+08:00">2021-01-15</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-04-24 12:58:51" itemprop="dateModified" datetime="2021-04-24T12:58:51+08:00">2021-04-24</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/bigdata/" itemprop="url" rel="index"><span itemprop="name">bigdata</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="实时流处理系统反压机制（BackPressure）综述-转"><a href="#实时流处理系统反压机制（BackPressure）综述-转" class="headerlink" title="实时流处理系统反压机制（BackPressure）综述[转]"></a>实时流处理系统反压机制（BackPressure）综述[转]</h1><p> 发表于 2018-11-15 |  更新于 2018-12-03 |  分类于 <a target="_blank" rel="noopener" href="http://ileaf.tech/category/#/BigData">BigData </a>|  阅读次数 333</p>
<p>本文主要关于实时流处理系统反压机制，最近看到反压问题看到此文章很好，在此分享并mark一下。</p>
<blockquote>
<p>本文主要关于实时流处理系统反压机制，最近看到反压问题看到此文章很好，在此分享并mark一下。<br>(￢_￢)ﾉ最近菜叶子没自己写见谅。<br>本文转自 <a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_21125183/article/details/80708142">实时流处理系统反压机制（BackPressure）综述</a><br><a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_21125183/article/details/80708142">https://blog.csdn.net/qq_21125183/article/details/80708142</a><br><a target="_blank" rel="noopener" href="https://www.jianshu.com/p/28fcd51d4edd">开启Back Pressure使生产环境的Spark Streaming应用更稳定、有效</a></p>
</blockquote>
<p>反压机制（BackPressure）被广泛应用到实时流处理系统中，流处理系统需要能优雅地处理反压（backpressure）问题。<br>反压通常产生于这样的场景：短时负载高峰导致系统接收数据的速率远高于它处理数据的速率。<br>许多日常问题都会导致反压，例如，垃圾回收停顿可能会导致流入的数据快速堆积，或者遇到大促或秒杀活动导致流量陡增。<br>反压如果不能得到正确的处理，可能会导致资源耗尽甚至系统崩溃。反压机制就是指系统能够自己检测到被阻塞的Operator，然后系统自适应地降低源头或者上游的发送速率。</p>
<p>目前主流的流处理系统 Apache Storm、JStorm、Spark Streaming、S4、Apache Flink、Twitter Heron都采用反压机制解决这个问题，不过他们的实现各自不同。</p>
<p><img src="_v_images/20201030212807758_57452691.png" alt="实时流处理系统反压机制01"></p>
<p>不同的组件可以不同的速度执行（并且每个组件中的处理速度随时间改变）。 例如，考虑一个工作流程，或由于数据倾斜或任务调度而导致数据被处理十分缓慢。<br>在这种情况下，如果上游阶段不减速，将导致缓冲区建立长队列(队列占用内存、硬盘空间，节点负载加重)，或导致系统丢弃元组。<br>如果元组在中途丢弃，那么效率可能会有损失，因为已经为这些元组产生的计算被浪费了。<br>并且在一些流处理系统中比如Strom，会将这些丢失的元组重新发送，这样会导致数据的一致性问题(at least once语义)，并且还会导致某些Operator状态叠加。<br>进而整个程序输出结果不准确。第二由于系统接收数据的速率是随着时间改变的，短时负载高峰导致系统接收数据的速率远高于它处理数据的速率的情况，也会导致Tuple在中途丢失。<br>所以实时流处理系统必须能够解决发送速率远大于系统能处理速率这个问题，大多数实时流处理系统采用反压（BackPressure）机制解决这个问题。</p>
<p>下面我们就来介绍一下不同的实时流处理系统采用的反压机制：</p>
<h1 id="Strom-反压机制"><a href="#Strom-反压机制" class="headerlink" title="Strom 反压机制"></a>Strom 反压机制</h1><h2 id="Storm-1-0-以前的反压机制"><a href="#Storm-1-0-以前的反压机制" class="headerlink" title="Storm 1.0 以前的反压机制"></a>Storm 1.0 以前的反压机制</h2><p>对于开启了acker机制的storm程序，可以通过设置conf.setMaxSpoutPending参数来实现反压效果，<strong>如果下游组件(bolt)处理速度跟不上导致spout发送的tuple没有及时确认的数超过了参数设定的值，spout会停止发送数据</strong>，这种方式的缺点是很难调优conf.setMaxSpoutPending参数的设置以达到最好的反压效果，设小了会导致吞吐上不去，设大了会导致worker OOM；有震荡，数据流会处于一个颠簸状态，效果不如逐级反压；另外对于关闭acker机制的程序无效；</p>
<h2 id="Storm-Automatic-Backpressure"><a href="#Storm-Automatic-Backpressure" class="headerlink" title="Storm Automatic Backpressure"></a>Storm Automatic Backpressure</h2><p>新的storm自动反压机制(Automatic Back Pressure)通过监控bolt中的接收队列的情况，当超过高水位值时专门的线程会将反压信息写到 Zookeeper ，Zookeeper上的watch会通知该拓扑的所有Worker都进入反压状态，最后Spout降低tuple发送的速度。</p>
<p><img src="_v_images/20201030212806451_178512803.png" alt="实时流处理系统反压机制02"></p>
<p>每个Executor都有一个接受队列和发送队列用来接收Tuple和发送Spout或者Bolt生成的Tuple元组。每个Worker进程都有一个单的的接收线程监听接收端口。<br>它从每个网络上进来的消息发送到Executor的接收队列中。Executor接收队列存放Worker或者Worker内部其他Executor发过来的消息。<br>Executor工作线程从接收队列中拿出数据，然后调用execute方法，发送Tuple到Executor的发送队列。<br>Executor的发送线程从发送队列中获取消息，按照消息目的地址选择发送到Worker的传输队列中或者其他Executor的接收队列中。<br>最后Worker的发送线程从传输队列中读取消息，然后将Tuple元组发送到网络中。</p>
<ol>
<li>当Worker进程中的Executor线程发现自己的接收队列满了时，也就是接收队列达到<code>high watermark</code>的阈值后，因此它会发送通知消息到背压线程。</li>
<li>背压线程将当前worker进程的信息注册到Zookeeper的Znode节点中。具体路径就是 <code>/Backpressure/topo1/wk1</code>下</li>
<li>Zookeepre的Znode Watcher监视/Backpreesure/topo1下的节点目录变化情况，如果发现目录增加了znode节点说明或者其他变化。这就说明该Topo1需要反压控制，然后它会通知Topo1所有的Worker进入反压状态。</li>
<li>最终Spout降低tuple发送的速度。</li>
</ol>
<h1 id="JStorm-反压机制"><a href="#JStorm-反压机制" class="headerlink" title="JStorm 反压机制"></a>JStorm 反压机制</h1><p>JStorm做了两级的反压，第一级和Jstorm类似，通过执行队列来监测，但是不会通过ZK来协调，而是通过Topology Master来协调。<br>在队列中会标记high water mark和low water mark，当执行队列超过high water mark时，就认为bolt来不及处理，则向TM发一条控制消息，上游开始减慢发送速率，直到下游低于low water mark时解除反压。</p>
<p>此外，在Netty层也做了一级反压，由于每个Worker Task都有自己的发送和接收的缓冲区，可以对缓冲区设定限额、控制大小，如果spout数据量特别大，缓冲区填满会导致下游bolt的接收缓冲区填满，造成了反压。</p>
<p><img src="_v_images/20201030212805000_1695102842.png" alt="实时流处理系统反压机制03"></p>
<p>限流机制：jstorm的限流机制， 当下游bolt发生阻塞时， 并且阻塞task的比例超过某个比例时（现在默认设置为0.1），触发反压</p>
<p>限流方式：计算阻塞Task的地方执行线程执行时间，Spout每发送一个tuple等待相应时间，然后讲这个时间发送给Spout， 于是， spout每发送一个tuple，就会等待这个执行时间。</p>
<p>Task阻塞判断方式：在jstorm 连续4次采样周期中采样，队列情况，当队列超过80%（可以设置）时，即可认为该task处在阻塞状态。</p>
<h1 id="SparkStreaming-反压机制"><a href="#SparkStreaming-反压机制" class="headerlink" title="SparkStreaming 反压机制"></a>SparkStreaming 反压机制</h1><h2 id="为什么引入反压机制Backpressure"><a href="#为什么引入反压机制Backpressure" class="headerlink" title="为什么引入反压机制Backpressure"></a>为什么引入反压机制Backpressure</h2><p>默认情况下，Spark Streaming通过Receiver以生产者生产数据的速率接收数据，计算过程中会出现<code>batch processing time &gt; batch interval</code>的情况，其中<code>batch processing time</code> 为实际计算一个批次花费时间， <code>batch interval</code>为Streaming应用设置的批处理间隔。<br>这意味着Spark Streaming的数据接收速率高于Spark从队列中移除数据的速率，也就是数据处理能力低，在设置间隔内不能完全处理当前接收速率接收的数据。如果这种情况持续过长的时间，会造成数据在内存中堆积，导致Receiver所在Executor内存溢出等问题（如果设置StorageLevel包含disk, 则内存存放不下的数据会溢写至disk, 加大延迟）。<br>Spark 1.5以前版本，用户如果要限制Receiver的数据接收速率，可以通过设置静态配制参数“<code>spark.streaming.receiver.maxRate</code>”的值来实现，此举虽然可以通过限制接收速率，来适配当前的处理能力，防止内存溢出，但也会引入其它问题。比如：producer数据生产高于maxRate，当前集群处理能力也高于maxRate，这就会造成资源利用率下降等问题。为了更好的协调数据接收速率与资源处理能力，Spark Streaming 从v1.5开始引入反压机制（back-pressure）,通过动态控制数据接收速率来适配集群数据处理能力。</p>
<h2 id="反压机制Backpressure"><a href="#反压机制Backpressure" class="headerlink" title="反压机制Backpressure"></a>反压机制Backpressure</h2><p><code>Spark Streaming Backpressure</code>: 根据JobScheduler反馈作业的执行信息来动态调整Receiver数据接收率。通过属性“spark.streaming.backpressure.enabled”来控制是否启用backpressure机制，默认值false，即不启用。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sparkConf.set(<span class="string">&quot;spark.streaming.backpressure.enabled&quot;</span>,”<span class="keyword">true</span>”)</span><br></pre></td></tr></table></figure>
<p>SparkStreaming 架构图如下所示:</p>
<p><img src="_v_images/20201030212803393_1135733829.png" alt="实时流处理系统反压机制04"></p>
<p>SparkStreaming 反压过程执行如下图所示：</p>
<p>在原架构的基础上加上一个新的组件RateController,这个组件负责监听“OnBatchCompleted”事件，然后从中抽取<code>processingDelay</code> 及<code>schedulingDelay</code>信息. <code>Estimator</code>依据这些信息估算出最大处理速度（rate），最后由基于<code>Receiver</code>的Input Stream将rate通过ReceiverTracker与ReceiverSupervisorImpl转发给BlockGenerator（继承自RateLimiter）.</p>
<p><img src="_v_images/20201030212801986_428923565.png" alt="实时流处理系统反压机制05"></p>
<h2 id="direct模式-BackPressure-此部分详细说明了direct模式接收：转自-开启Back-Pressure使生产环境的Spark-Streaming应用更稳定、有效"><a href="#direct模式-BackPressure-此部分详细说明了direct模式接收：转自-开启Back-Pressure使生产环境的Spark-Streaming应用更稳定、有效" class="headerlink" title="direct模式-BackPressure(此部分详细说明了direct模式接收：转自-开启Back Pressure使生产环境的Spark Streaming应用更稳定、有效)"></a>direct模式-BackPressure(此部分详细说明了direct模式接收：转自-<a target="_blank" rel="noopener" href="https://www.jianshu.com/p/28fcd51d4edd">开启Back Pressure使生产环境的Spark Streaming应用更稳定、有效</a>)</h2><p>当Spark Streaming与Kafka使用Direct API集群时，我们可以很方便的去控制最大数据摄入量–通过一个被称作spark.streaming.kafka.maxRatePerPartition的参数。根据文档描述，他的含义是：Direct API读取每一个Kafka partition数据的最大速率（每秒读取的消息量）。<br>配置项spark.streaming.kafka.maxRatePerPartition，对防止流式应用在下边两种情况下出现流量过载时尤其重要：<br>1.Kafka Topic中有大量未处理的消息，并且我们设置是Kafka auto.offset.reset参数值为smallest，他可以防止第一个批次出现数据流量过载情况。<br>2.当Kafka 生产者突然飙升流量的时候，他可以防止批次处理出现数据流量过载情况。</p>
<p>但是，配置Kafka每个partition每批次最大的摄入量是个静态值，也算是个缺点。随着时间的变化，在生产环境运行了一段时间的Spark Streaming应用，每批次每个Kafka partition摄入数据最大量的最优值也是变化的。有时候，是因为消息的大小会变，导致数据处理时间变化。有时候，是因为流计算所使用的多租户集群会变得非常繁忙，比如在白天时候，一些其他的数据应用（例如Impala/Hive/MR作业）竞争共享的系统资源时（CPU/内存/网络/磁盘IO）。<br>背压机制可以解决该问题。背压机制是呼声比较高的功能，他允许根据前一批次数据的处理情况，动态、自动的调整后续数据的摄入量，这样的反馈回路使得我们可以应对流式应用流量波动的问题。<br>Spark Streaming的背压机制是在Spark1.5版本引进的，我们可以添加如下代码启用改功能：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sparkConf.set(&quot;spark.streaming.backpressure.enabled&quot;,”true”)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>那应用启动后的第一个批次流量怎么控制呢？因为他没有前面批次的数据处理时间，所以没有参考的数据去评估这一批次最优的摄入量。在Spark官方文档中有个被称作spark.streaming.backpressure.initialRate的配置，看起来是控制开启背压机制时初始化的摄入量。其实不然，该参数只对receiver模式起作用，并不适用于direct模式。推荐的方法是使用spark.streaming.kafka.maxRatePerPartition控制背压机制起作用前的第一批次数据的最大摄入量。我通常建议设置spark.streaming.kafka.maxRatePerPartition的值为最优估计值的1.5到2倍，让背压机制的算法去调整后续的值。请注意，spark.streaming.kafka.maxRatePerPartition的值会一直控制最大的摄入量，所以背压机制的算法值不会超过他。<br>另一个需要注意的是，在第一个批次处理完成前，紧接着的批次都将使用spark.streaming.kafka.maxRatePerPartition的值作为摄入量。通过Spark UI可以看到，批次间隔为5s，当批次调度延迟31秒时候，前7个批次的摄入量是20条记录。直到第八个批次，背压机制起作用时，摄入量变为5条记录。</p>
<h1 id="Heron-反压机制"><a href="#Heron-反压机制" class="headerlink" title="Heron 反压机制"></a>Heron 反压机制</h1><p><img src="_v_images/20201030212800445_1650579597.png" alt="实时流处理系统反压机制06"></p>
<p>当下游处理速度跟不上上游发送速度时，一旦StreamManager 发现一个或多个Heron Instance 速度变慢，立刻对本地spout进行降级，降低本地Spout发送速度, 停止从这些spout读取数据。并且受影响的StreamManager 会发送一个特殊的start backpressure message 给其他的StreamManager ，要求他们对spout进行本地降级。 当其他StreamManager 接收到这个特殊消息时，他们通过不读取当地Spout中的Tuple来进行降级。一旦出问题的Heron Instance 恢复速度后，本地的SM 会发送stop backpressure message 解除降级。</p>
<p>很多Socket Channel与应用程序级别的Buffer相关联，该缓冲区由high watermark 和low watermark组成。 当缓冲区大小达到high watermark时触发反压，并保持有效，直到缓冲区大小低于low watermark。 此设计的基本原理是防止拓扑在进入和退出背压缓解模式之间快速振荡。</p>
<h1 id="Flink-反压机制"><a href="#Flink-反压机制" class="headerlink" title="Flink 反压机制"></a>Flink 反压机制</h1><p>Flink 没有使用任何复杂的机制来解决反压问题，因为根本不需要那样的方案！它利用自身作为纯数据流引擎的优势来优雅地响应反压问题。下面我们会深入分析 Flink 是如何在 Task 之间传输数据的，以及数据流如何实现自然降速的。 Flink 在运行时主要由 operators 和 streams 两大组件构成。每个 operator 会消费中间态的流，并在流上进行转换，然后生成新的流。对于 Flink 的网络机制一种形象的类比是，Flink 使用了高效有界的分布式阻塞队列，就像 Java 通用的阻塞队列（BlockingQueue）一样。还记得经典的线程间通信案例：生产者消费者模型吗？使用 BlockingQueue 的话，一个较慢的接受者会降低发送者的发送速率，因为一旦队列满了（有界队列）发送者会被阻塞。Flink 解决反压的方案就是这种感觉。 在 Flink 中，这些分布式阻塞队列就是这些逻辑流，而队列容量是通过缓冲池来（LocalBufferPool）实现的。每个被生产和被消费的流都会被分配一个缓冲池。缓冲池管理着一组缓冲(Buffer)，缓冲在被消费后可以被回收循环利用。这很好理解：你从池子中拿走一个缓冲，填上数据，在数据消费完之后，又把缓冲还给池子，之后你可以再次使用它。</p>
<h2 id="Flink-网络传输中的内存管理"><a href="#Flink-网络传输中的内存管理" class="headerlink" title="Flink 网络传输中的内存管理"></a>Flink 网络传输中的内存管理</h2><p>如下图所示展示了 Flink 在网络传输场景下的内存管理。网络上传输的数据会写到 Task 的 InputGate（IG） 中，经过 Task 的处理后，再由 Task 写到 ResultPartition（RS） 中。每个 Task 都包括了输入和输入，输入和输出的数据存在 Buffer 中（都是字节数据）。Buffer 是 MemorySegment 的包装类。</p>
<p><img src="_v_images/20201030212758832_1369878662.png" alt="实时流处理系统反压机制07"></p>
<ol>
<li>TaskManager（TM）在启动时，会先初始化NetworkEnvironment对象，TM 中所有与网络相关的东西都由该类来管理（如 Netty 连接），其中就包括NetworkBufferPool。根据配置，Flink 会在 NetworkBufferPool 中生成一定数量（默认2048个）的内存块 MemorySegment（关于 Flink 的内存管理，后续文章会详细谈到），内存块的总数量就代表了网络传输中所有可用的内存。NetworkEnvironment 和 NetworkBufferPool 是 Task 之间共享的，每个 TM 只会实例化一个。</li>
<li>Task 线程启动时，会向 NetworkEnvironment 注册，NetworkEnvironment 会为 Task 的 InputGate（IG）和 ResultPartition（RP） 分别创建一个 LocalBufferPool（缓冲池）并设置可申请的 MemorySegment（内存块）数量。IG 对应的缓冲池初始的内存块数量与 IG 中 InputChannel 数量一致，RP 对应的缓冲池初始的内存块数量与 RP 中的 ResultSubpartition 数量一致。不过，每当创建或销毁缓冲池时，NetworkBufferPool 会计算剩余空闲的内存块数量，并平均分配给已创建的缓冲池。注意，这个过程只是指定了缓冲池所能使用的内存块数量，并没有真正分配内存块，只有当需要时才分配。为什么要动态地为缓冲池扩容呢？因为内存越多，意味着系统可以更轻松地应对瞬时压力（如GC），不会频繁地进入反压状态，所以我们要利用起那部分闲置的内存块。</li>
<li>在 Task 线程执行过程中，当 Netty 接收端收到数据时，为了将 Netty 中的数据拷贝到 Task 中，InputChannel（实际是 RemoteInputChannel）会向其对应的缓冲池申请内存块（上图中的①）。如果缓冲池中也没有可用的内存块且已申请的数量还没到池子上限，则会向 NetworkBufferPool 申请内存块（上图中的②）并交给 InputChannel 填上数据（上图中的③和④）。如果缓冲池已申请的数量达到上限了呢？或者 NetworkBufferPool 也没有可用内存块了呢？这时候，Task 的 Netty Channel 会暂停读取，上游的发送端会立即响应停止发送，拓扑会进入反压状态。当 Task 线程写数据到 ResultPartition 时，也会向缓冲池请求内存块，如果没有可用内存块时，会阻塞在请求内存块的地方，达到暂停写入的目的。</li>
<li>当一个内存块被消费完成之后（在输入端是指内存块中的字节被反序列化成对象了，在输出端是指内存块中的字节写入到 Netty Channel 了），会调用 Buffer.recycle() 方法，会将内存块还给 LocalBufferPool （上图中的⑤）。如果LocalBufferPool中当前申请的数量超过了池子容量（由于上文提到的动态容量，由于新注册的 Task 导致该池子容量变小），则LocalBufferPool会将该内存块回收给 NetworkBufferPool（上图中的⑥）。如果没超过池子容量，则会继续留在池子中，减少反复申请的开销。</li>
</ol>
<h2 id="Flink-反压机制-1"><a href="#Flink-反压机制-1" class="headerlink" title="Flink 反压机制"></a>Flink 反压机制</h2><p>下面这张图简单展示了两个 Task 之间的数据传输以及 Flink 如何感知到反压的：</p>
<p><img src="_v_images/20201030212757424_78076714.png" alt="实时流处理系统反压机制08"></p>
<ol>
<li>记录“A”进入了 Flink 并且被 Task 1 处理。（这里省略了 Netty 接收、反序列化等过程）</li>
<li>记录被序列化到 buffer 中。</li>
<li>该 buffer 被发送到 Task 2，然后 Task 2 从这个 buffer 中读出记录。</li>
</ol>
<p><strong>不要忘了：记录能被 Flink 处理的前提是，必须有空闲可用的 Buffer。</strong></p>
<p>结合上面两张图看：Task 1 在输出端有一个相关联的 LocalBufferPool（称缓冲池1），Task 2 在输入端也有一个相关联的 LocalBufferPool（称缓冲池2）。如果缓冲池1中有空闲可用的 buffer 来序列化记录 “A”，我们就序列化并发送该 buffer。</p>
<p>这里我们需要注意两个场景：</p>
<ul>
<li>本地传输：如果 Task 1 和 Task 2 运行在同一个 worker 节点（TaskManager），该 buffer 可以直接交给下一个 Task。一旦 Task 2 消费了该 buffer，则该 buffer 会被缓冲池1回收。如果 Task 2 的速度比 1 慢，那么 buffer 回收的速度就会赶不上 Task 1 取 buffer 的速度，导致缓冲池1无可用的 buffer，Task 1 等待在可用的 buffer 上。最终形成 Task 1 的降速。</li>
<li>远程传输：如果 Task 1 和 Task 2 运行在不同的 worker 节点上，那么 buffer 会在发送到网络（TCP Channel）后被回收。在接收端，会从 LocalBufferPool 中申请 buffer，然后拷贝网络中的数据到 buffer 中。如果没有可用的 buffer，会停止从 TCP 连接中读取数据。在输出端，通过 Netty 的水位值机制来保证不往网络中写入太多数据（后面会说）。如果网络中的数据（Netty输出缓冲中的字节数）超过了高水位值，我们会等到其降到低水位值以下才继续写入数据。这保证了网络中不会有太多的数据。如果接收端停止消费网络中的数据（由于接收端缓冲池没有可用 buffer），网络中的缓冲数据就会堆积，那么发送端也会暂停发送。另外，这会使得发送端的缓冲池得不到回收，writer 阻塞在向 LocalBufferPool 请求 buffer，阻塞了 writer 往 ResultSubPartition 写数据。</li>
</ul>
<p>这种固定大小缓冲池就像阻塞队列一样，保证了 Flink 有一套健壮的反压机制，使得 Task 生产数据的速度不会快于消费的速度。我们上面描述的这个方案可以从两个 Task 之间的数据传输自然地扩展到更复杂的 pipeline 中，保证反压机制可以扩散到整个 pipeline。</p>
<h2 id="反压实验"><a href="#反压实验" class="headerlink" title="反压实验"></a>反压实验</h2><p>另外，官方博客中为了展示反压的效果，给出了一个简单的实验。下面这张图显示了：随着时间的改变，生产者（黄色线）和消费者（绿色线）每5秒的平均吞吐与最大吞吐（在单一JVM中每秒达到8百万条记录）的百分比。我们通过衡量task每5秒钟处理的记录数来衡量平均吞吐。该实验运行在单 JVM 中，不过使用了完整的 Flink 功能栈。</p>
<p><img src="_v_images/20201030212755917_1236027751.png" alt="实时流处理系统反压机制09"></p>
<p>首先，我们运行生产task到它最大生产速度的60%（我们通过Thread.sleep()来模拟降速）。消费者以同样的速度处理数据。然后，我们将消费task的速度降至其最高速度的30%。你就会看到背压问题产生了，正如我们所见，生产者的速度也自然降至其最高速度的30%。接着，停止消费task的人为降速，之后生产者和消费者task都达到了其最大的吞吐。接下来，我们再次将消费者的速度降至30%，pipeline给出了立即响应：生产者的速度也被自动降至30%。最后，我们再次停止限速，两个task也再次恢复100%的速度。总而言之，我们可以看到：生产者和消费者在 pipeline 中的处理都在跟随彼此的吞吐而进行适当的调整，这就是我们希望看到的反压的效果。</p>
<h2 id="Flink-反压监控"><a href="#Flink-反压监控" class="headerlink" title="Flink 反压监控"></a>Flink 反压监控</h2><p>在 Storm/JStorm 中，只要监控到队列满了，就可以记录下拓扑进入反压了。但是 Flink 的反压太过于天然了，导致我们无法简单地通过监控队列来监控反压状态。Flink 在这里使用了一个 trick 来实现对反压的监控。如果一个 Task 因为反压而降速了，那么它会卡在向 LocalBufferPool 申请内存块上。那么这时候，该 Task 的 stack trace 就会长下面这样：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">java.lang.Object.wait(Native Method)</span><br><span class="line">o.a.f.[...].LocalBufferPool.requestBuffer(LocalBufferPool.java:<span class="number">163</span>)</span><br><span class="line">o.a.f.[...].LocalBufferPool.requestBufferBlocking(LocalBufferPool.java:<span class="number">133</span>) &lt;--- BLOCKING request</span><br><span class="line">[...]</span><br></pre></td></tr></table></figure>
<p>那么事情就简单了。通过不断地采样每个 task 的 stack trace 就可以实现反压监控。</p>
<p><img src="_v_images/20201030212753993_1098219801.png" alt="实时流处理系统反压机制10"></p>
<p>Flink 的实现中，只有当 Web 页面切换到某个 Job 的 Backpressure 页面，才会对这个 Job 触发反压检测，因为反压检测还是挺昂贵的。JobManager 会通过 Akka 给每个 TaskManager 发送TriggerStackTraceSample消息。默认情况下，TaskManager 会触发100次 stack trace 采样，每次间隔 50ms（也就是说一次反压检测至少要等待5秒钟）。并将这 100 次采样的结果返回给 JobManager，由 JobManager 来计算反压比率（反压出现的次数/采样的次数），最终展现在 UI 上。UI 刷新的默认周期是一分钟，目的是不对 TaskManager 造成太大的负担。</p>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>Flink不需要一种特殊的机制来处理反压，因为Flink 中的数据传输相当于已经提供了应对反压的机制。因此，Flink 所能获得的最大吞吐量由其 pipeline 中最慢的组件决定。相对于 Storm/JStorm 的实现，Flink 的实现更为简洁优雅，源码中也看不见与反压相关的代码，无需 Zookeeper/TopologyMaster 的参与也降低了系统的负载，也利于对反压更迅速的响应。</p>
<blockquote>
<p>本文转自 <a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_21125183/article/details/80708142">实时流处理系统反压机制（BackPressure）综述</a><br><a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_21125183/article/details/80708142">https://blog.csdn.net/qq_21125183/article/details/80708142</a><br><a target="_blank" rel="noopener" href="https://www.jianshu.com/p/28fcd51d4edd">开启Back Pressure使生产环境的Spark Streaming应用更稳定、有效</a></p>
</blockquote>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://example.com/bigdata/storm/Storm-runtime/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="aaronzhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Guadazi">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/bigdata/storm/Storm-runtime/" class="post-title-link" itemprop="url">Storm runtime</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-01-15 19:47:00" itemprop="dateCreated datePublished" datetime="2021-01-15T19:47:00+08:00">2021-01-15</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-04-24 12:58:52" itemprop="dateModified" datetime="2021-04-24T12:58:52+08:00">2021-04-24</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/bigdata/" itemprop="url" rel="index"><span itemprop="name">bigdata</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="Storm-runtime"><a href="#Storm-runtime" class="headerlink" title="Storm-runtime"></a>Storm-runtime</h1><p><img src="_v_images/20210116161223695_155417172.jpg"></p>
<p>master-slave结构:</p>
<ul>
<li>Nimbus是主节点，负责分发用户代码，指派Supervisor上的worker进程，运行topology的(Spout/Bolt)Task</li>
<li>Supervisor是从节点，守护进程. 负责启动和终止worker进程. 通过Storm的配置文件中的 supervisor.slots.ports配置项，可以指定在一个Supervisor上最大允许多少个Slot，每个Slot通过端口号来唯一标识，一个端口号 对应一个Worker进程（如果该Worker进程被启动）。</li>
</ul>
<p><img src="_v_images/20210116162304428_695142382.jpg"></p>
<p>运行流程</p>
<p>1）户端提交拓扑到nimbus。</p>
<p>2） Nimbus针对该拓扑建立本地的目录根据topology的配置计算task，分配task，在zookeeper上建立assignments节点存储 task和supervisor机器节点中woker的对应关系；</p>
<p>在zookeeper上创建taskbeats节点来监控task的心跳；启动topology。</p>
<p>3） Supervisor去zookeeper上获取分配的tasks，启动多个woker进行，每个woker生成task，一个task一个线程；根据topology 信息初始化建立task之间的连接;Task和Task之间是通过zeroMQ管理的；后整个拓扑运行起来。</p>
<h2 id="内核-队列"><a href="#内核-队列" class="headerlink" title="内核-队列"></a>内核-队列</h2><p>在Storm中大量使用Disrupt Queue来解耦Storm内部的消息处理过程。分析这些Queue的分布，是分析Storm运行时态的基础。</p>
<p><img src="_v_images/20210116174558832_184704273.png"></p>
<p>在Storm的worker中，最小的执行单元是executor,一个executor只会有一个component。目前一个component只会有一个task。而一个executor会有两个Disruptor Queue, 一个用于接受数据的receive Disrupor Queue和一个用于发送send Disrupor Queue。然后在worker中有一个全局的send Disrupor Queue。分析完队列的分布，在分析topology的运行时状况。</p>
<p>当一个Topology提交到Storm集群后，task被分配到各个wrker中开始执行后，task是怎么执行的。Storm的Task分为两类，一类是消息的源头Spout,它负责在源头产生消息；然后就是Bolt，它是执行单元。但是这两者各自的工作逻辑如下。</p>
<p>我们来分析Spout在运行时的工作状况。Spout的nextTuple用于发送数据，接口注释上说明它不能阻塞，因为它与active， deactive，ack, fail在一个处理线程里面被处理。但是nextTuple被阻塞会有什么副作用列？当nextTuple被阻塞，应用代码中另起线程调用SpoutCollector会有什么后果？下图是SpoutExecutor的执行逻辑。</p>
<p><img src="_v_images/20210116174558730_1417384061.png"></p>
<p>在SpoutExecutor处理循环里面，第一步做的事情是从receive Disrupor Queue里面消费里面的消息。这里面的就是SpoutExecutor所收到的消息。处理逻辑如下图所示</p>
<p><img src="_v_images/20210116174558627_554147077.png"></p>
<p>然后SpoutExecutor会将overflow中的数据再次发送。overflow是用于接受SpoutCollector.emit()无发及时发送的数据的，具体SpoutCollector发送数据的逻辑见下文分析。但是这里在发送overflow的数据时，与SpoutCollector.emit()有个区别，就是数据还是无法被正常发送时，数据会丢弃，也就是不会被再次写入overflow中。当overflow中没有数据，以及pending中的数据量小于TOPOLOGY_MAX_SPOUT_PENDING时，判断topology的状态。当topology不是deactive状态时，如果topology有触发active命令，会调用spout的active接口，然后调用spout的nextTuple接口。否则调用deactive接口。所以这里当Spout的nextTuple被阻塞时，spout没办法处理acker回报的消息，回报的消息会阻塞在executor的receive DisruptorQueue中，当receive DisruptorQueue塞满后，是会阻塞在对应的网络处理模块中，storm中经典的是zeroMQ,老版本的zeroMQ是没有设置水位，这样会大量堆积到内存中，因为zeroMQ是C++的，占用的是堆外内存，JVM无法管理，最坏就是把机器的内存耗光。如果这个是另起线程调用SpoutCollector的emit，当emit数据速度过快，会导致overflow中堆积数据，导致worker内存消耗。</p>
<p>上面讲到SpoutCollector在emit数据会写入overflow,那什么情况下会写入overflow。SpoutCollector.emit是否真的就把数据发送到了网络。下面是SpoutCollector的处理逻辑。</p>
<p><img src="_v_images/20210116174558522_1112615304.png"></p>
<p>Spout当调用SpoutCollect.emit()发送时，首先的逻辑是根据根据消息的STREAM ID,和对应的values,得到目的端task id。当topology开启acker机制时，会生成一个随机的rootId，否则使用一个默认值作为rootId。然后为消息生成一个随机的messageId，由rootId和messageId组成对应的tuple Id。这里tuple Id是有rootId为key,messageId为value的一个HashMap。这里采用HashMap的作用会在下面Spout的ack机制是做说明。在将生成的tuple Id和用会的values组成一个tuple。并判断overflower是否有数据，让overflow中有数据时，数据会直接放入overflow中，而不会放入executor  的send Disraptor Queue中。当overflow为空时，会将tuple放入send Disraptor Queue中。当捕获Disraptor Queue的InsufficientCapacityException时，数据就放入了overflow中。然后处理ack。当开启了ack机制，会在pending中加入对应的tuple数据，然后项acker发送ack init消息。</p>
<p>根据上面的发送过程，数据emit只是被写入了executor的send Disraptor Queue。而数据在Spout端的丢失多是数据在overflow中被SpoutExecutor在次处理时，send Disraptor Queue满导致。</p>
<p>关于pending，首先它是一个RotatingMap。它通过定时旋转，以达到定时器的目的。在SpoutExecutor的RotatingMap中有两个桶，然后executor有个定时线程，会按照用户设定的message timeout second，定期向executor的receive DisruptorQueue写入SYSTEM_TICK_STREAM_ID消息，然后SpoutExecutor处理SYSTEM_TICK_STREAM_ID消息时就旋转RotatingMap，当被清除的桶中有数据时，被清除的桶中的数据会调用fail接口，通知业务逻辑fail。这就是当超时间设置比业务逻辑短时，导致数据重复的原因。还有一种是acker消息丢失，导致数据重复。由于RotatingMap的底层是HashMap，中间没有锁，overflow是个LinkedList，所以SpoutCollector和SpoutExecutor不能在两个线程中并发 处理。</p>
<p>BoltExecutor的处理逻辑非常简单，就是消费receive Disrupor Queue中数据，然后调用bolt的excue。但是这里也是单线程处理的，阻塞或者处理速度不匹配，就会导致数据在Disrupor Queue或者网络模块中堆积，其中使用zeroMQ的副作用最大。</p>
<p>BoltCollector的emit与SpoutCollector的emit处理相比，首先是少了overflow承接无法发送的数据，会直接丢弃。其次是没有pending和acker消息的发送。</p>
<p>接下来分析一下Storm的Acker机制。Storm的Ack机制在Storm刚刚开源时被大书特书，处理原理也确实非常的精彩。由于这里ID都是随机数，所以这里不会在讨论随机数的唯一问题。</p>
<p><img src="_v_images/20210116174558419_714266215.png"></p>
<p>如上图所示，spout发送t1给bolt a, bolt a 在t1的基础上生成t2,t3,t4给bolt b，bolt b ack所收到的数据。下面来追踪整个id变化的过程。</p>
<p>Spout 发送t1， message id为&lt;r_1, m_1&gt;,发送ack</p>
<p>Acker收到ack init, map中缓存&lt;r_1,m_1&gt;</p>
<p>Bolt a 收到t1, messageId为&lt;r_1,m_1&gt;,生成t2，t3, t4</p>
<p>Bolt a 发送t2, anchor t1, t2,messageId为&lt;r_1,m_2&gt;, 更新t1的ackVal为m_2</p>
<p>Bolt a 发送t3, anchor t1, t3,messageId为&lt;r_1,m_3&gt;, 更新t1的ackVal为m_2^m_3</p>
<p>Bolt a 发送t4, anchor t1, t4,messageId为&lt;r_1,m_4&gt;, 更新t1的ackVal为m_2^m_3^m_4</p>
<p>Bolt a ack t1, 向acker发送ack消息&lt;r_1, m_1^ m_2^m_3^m_4&gt;</p>
<p>Acker 收到bolt a的ack消息，更新缓存为&lt;r_1, m_1^ m_1^ m_2^m_3^m_4&gt;即&lt;r_1,  m_2^m_3^m_4&gt;</p>
<p>Bolt b ack t2, 向acker发送ack消息&lt;r_1, m_2&gt;</p>
<p>Acker 收到bolt b的ack消息，更新缓存为&lt;r_1, m_2^ m_2^m_3^m_4&gt;即&lt;r_1, m_3^m_4&gt;</p>
<p>Bolt b ack t3, 向acker发送ack消息&lt;r_1, m_3&gt;</p>
<p>Acker 收到bolt b的ack消息，更新缓存为&lt;r_1, m_3^m_3^m_4&gt;即&lt;r_1, m_4&gt;</p>
<p>Bolt b ack t4, 向acker发送ack消息&lt;r_1, m_4&gt;</p>
<p>Acker 收到bolt b的ack消息，更新缓存为&lt;r_1, m_4^m_4&gt;即&lt;r_1, 0&gt;</p>
<p>messageId确认完毕，向Spout发送ack消息。当消息没有被ack,会一直在spout的pending队列中，知道被ack或者超时。</p>
<p>它基本上使用两个long值就跟踪了一个消息在整个流中的处理过程。</p>
<p>【参考文献】</p>
<hr>
<ol>
<li><a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s/PRDSu-qOxb17qjdfpO1IYA">Apache storm内核原理</a></li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://example.com/bigdata/Flink/Flink-window/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="aaronzhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Guadazi">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/bigdata/Flink/Flink-window/" class="post-title-link" itemprop="url">Flink-window</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-01-15 19:47:00" itemprop="dateCreated datePublished" datetime="2021-01-15T19:47:00+08:00">2021-01-15</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-04-24 12:58:52" itemprop="dateModified" datetime="2021-04-24T12:58:52+08:00">2021-04-24</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/bigdata/" itemprop="url" rel="index"><span itemprop="name">bigdata</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="Flink-window"><a href="#Flink-window" class="headerlink" title="Flink-window"></a>Flink-window</h1><p>窗口会自动管理状态和触发计算，Flink 提供了丰富的窗口函数来进行计算。主要包括以下两种：</p>
<ul>
<li>ProcessWindowFunction，全量计算会把所有数据缓存到状态里，一直到窗口结束时统一计算。相对来说，状态会比较大，计算效率也会低一些；</li>
<li>AggregateFunction，增量计算就是来一条数据就算一条，可能我们的状态就会特别的小，计算效率也会比 ProcessWindowFunction 高很多，但是如果状态存储在磁盘频繁访问状态可能会影响性能。</li>
</ul>
<p><img src="vx_images/5498125178676"></p>
<h2 id="窗口的触发"><a href="#窗口的触发" class="headerlink" title="窗口的触发"></a>窗口的触发</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime);</span><br><span class="line">SingleOutputStreamOperator&lt;ItemEntity&gt; streamOperator = env</span><br><span class="line">        .socketTextStream(<span class="string">&quot;127.0.0.1&quot;</span>, <span class="number">9091</span>)</span><br><span class="line">        .map(<span class="keyword">new</span> MapFunction&lt;String, ItemEntity&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> ItemEntity <span class="title">map</span><span class="params">(String s)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                String[] split = <span class="keyword">null</span>;</span><br><span class="line">                <span class="keyword">if</span> (s.isEmpty() || (split = s.split(<span class="string">&quot;,&quot;</span>)).length != <span class="number">2</span>) &#123;</span><br><span class="line">                    <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">                &#125;</span><br><span class="line">                ItemEntity itemEntity = ItemEntity.builder().timestamp(split[<span class="number">0</span>])</span><br><span class="line">                        .eventId(split[<span class="number">1</span>])</span><br><span class="line">                        .build();</span><br><span class="line">                <span class="keyword">return</span> itemEntity;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;)</span><br><span class="line">        .filter(Objects::nonNull)</span><br><span class="line">        .assignTimestampsAndWatermarks(<span class="keyword">new</span> AscendingTimestampExtractor&lt;ItemEntity&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">long</span> <span class="title">extractAscendingTimestamp</span><span class="params">(ItemEntity itemEntity)</span> </span>&#123;</span><br><span class="line">                <span class="keyword">long</span> timestamp = itemEntity.getTimestamp();</span><br><span class="line">                <span class="keyword">return</span> timestamp;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line">streamOperator</span><br><span class="line">        .keyBy(<span class="keyword">new</span> KeySelector&lt;ItemEntity, String&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> String <span class="title">getKey</span><span class="params">(ItemEntity itemEntity)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                String eventId = itemEntity.getEventId();</span><br><span class="line">                <span class="keyword">return</span> eventId;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;)</span><br><span class="line">        .window(TumblingEventTimeWindows.of(Time.seconds(<span class="number">5</span>)))</span><br><span class="line">        .process(</span><br><span class="line">                <span class="keyword">new</span> ProcessWindowFunction&lt;ItemEntity, Tuple2&lt;String, String&gt;, String, TimeWindow&gt;() &#123;</span><br><span class="line">                    <span class="meta">@Override</span></span><br><span class="line">                    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">process</span><span class="params">(String s, Context context,</span></span></span><br><span class="line"><span class="function"><span class="params">                            Iterable&lt;ItemEntity&gt; iterable,</span></span></span><br><span class="line"><span class="function"><span class="params">                            Collector&lt;Tuple2&lt;String, String&gt;&gt; collector)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                        <span class="keyword">for</span> (ItemEntity itemEntity : iterable) &#123;</span><br><span class="line">                            <span class="keyword">long</span> timestamp = itemEntity.getTimestamp();</span><br><span class="line">                            Date date = <span class="keyword">new</span> Date(timestamp);</span><br><span class="line">                            SimpleDateFormat simpleDateFormat = <span class="keyword">new</span> SimpleDateFormat(</span><br><span class="line">                                    <span class="string">&quot;yyyy-MM-dd HH:mm:ss&quot;</span>);</span><br><span class="line">                            collector.collect(Tuple2.of(itemEntity.getEventId(),</span><br><span class="line">                                    timestamp + <span class="string">&quot;-&gt;&quot;</span> + simpleDateFormat.format(date)));</span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;)</span><br><span class="line">        .print();</span><br><span class="line">env.execute(<span class="string">&quot;test-window&quot;</span>);</span><br></pre></td></tr></table></figure>
<p>5秒的窗口</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">2020-04-01 00:01:00,1</span><br><span class="line">2020-04-01 00:01:00,1</span><br><span class="line">2020-04-01 00:01:06,1 # 触发计算</span><br><span class="line">(1,1585670460000-&gt;2020-04-01 00:01:00)</span><br><span class="line">(1,1585670460000-&gt;2020-04-01 00:01:00)</span><br><span class="line">2020-04-01 00:01:00,1 # 窗口已经关闭，旧数据</span><br><span class="line"><span class="meta">#</span><span class="bash"> WARN  org.apache.flink.streaming.api.functions.timestamps.AscendingTimestampExtractor [] - Timestamp monotony violated: 1585670460000 &lt; 1585670466000</span></span><br><span class="line">2020-04-01 00:01:06,1</span><br><span class="line">2020-04-01 00:01:07,1</span><br><span class="line">2020-04-01 00:01:11,1 # 触发计算</span><br><span class="line">(1,1585670466000-&gt;2020-04-01 00:01:06)</span><br><span class="line">(1,1585670466000-&gt;2020-04-01 00:01:06)</span><br><span class="line">(1,1585670467000-&gt;2020-04-01 00:01:07)</span><br></pre></td></tr></table></figure>


<h2 id="window的抽象概念"><a href="#window的抽象概念" class="headerlink" title="window的抽象概念"></a>window的抽象概念</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">Keyed Windows</span><br><span class="line"></span><br><span class="line">stream</span><br><span class="line">       .keyBy(...)               &lt;-  keyed versus non-keyed windows</span><br><span class="line">       .window(...)              &lt;-  required: <span class="string">&quot;assigner&quot;</span></span><br><span class="line">      [.trigger(...)]            &lt;-  optional: <span class="string">&quot;trigger&quot;</span> (<span class="keyword">else</span> <span class="keyword">default</span> trigger)</span><br><span class="line">      [.evictor(...)]            &lt;-  optional: <span class="string">&quot;evictor&quot;</span> (<span class="keyword">else</span> no evictor)</span><br><span class="line">      [.allowedLateness(...)]    &lt;-  optional: <span class="string">&quot;lateness&quot;</span> (<span class="keyword">else</span> zero)</span><br><span class="line">      [.sideOutputLateData(...)] &lt;-  optional: <span class="string">&quot;output tag&quot;</span> (<span class="keyword">else</span> no side output <span class="keyword">for</span> late data)</span><br><span class="line">       .reduce/aggregate/fold/apply()      &lt;-  required: <span class="string">&quot;function&quot;</span></span><br><span class="line">      [.getSideOutput(...)]      &lt;-  optional: <span class="string">&quot;output tag&quot;</span></span><br><span class="line">Non-Keyed Windows</span><br><span class="line"></span><br><span class="line">stream</span><br><span class="line">       .windowAll(...)           &lt;-  required: <span class="string">&quot;assigner&quot;</span></span><br><span class="line">      [.trigger(...)]            &lt;-  optional: <span class="string">&quot;trigger&quot;</span> (<span class="keyword">else</span> <span class="keyword">default</span> trigger)</span><br><span class="line">      [.evictor(...)]            &lt;-  optional: <span class="string">&quot;evictor&quot;</span> (<span class="keyword">else</span> no evictor)</span><br><span class="line">      [.allowedLateness(...)]    &lt;-  optional: <span class="string">&quot;lateness&quot;</span> (<span class="keyword">else</span> zero)</span><br><span class="line">      [.sideOutputLateData(...)] &lt;-  optional: <span class="string">&quot;output tag&quot;</span> (<span class="keyword">else</span> no side output <span class="keyword">for</span> late data)</span><br><span class="line">       .reduce/aggregate/fold/apply()      &lt;-  required: <span class="string">&quot;function&quot;</span></span><br><span class="line">      [.getSideOutput(...)]      &lt;-  optional: <span class="string">&quot;output tag&quot;</span></span><br></pre></td></tr></table></figure>
<h3 id="window-assigner"><a href="#window-assigner" class="headerlink" title="window assigner"></a>window assigner</h3><h3 id="window-trigger"><a href="#window-trigger" class="headerlink" title="window trigger"></a>window trigger</h3><h3 id="window-evictor"><a href="#window-evictor" class="headerlink" title="window evictor"></a>window evictor</h3><h2 id="windowOperator工作流程"><a href="#windowOperator工作流程" class="headerlink" title="windowOperator工作流程"></a>windowOperator工作流程</h2><p><img src="_v_images/20201206173044901_44534749.png"></p>
<h3 id="window-state"><a href="#window-state" class="headerlink" title="window state"></a>window state</h3><p><img src="_v_images/20201206173354630_1171217287.png"></p>
<h2 id="Session-window"><a href="#Session-window" class="headerlink" title="Session window"></a>Session window</h2><p><a target="_blank" rel="noopener" href="http://wuchong.me/blog/2016/06/06/flink-internals-session-window/">Flink 原理与实现：Session Window</a></p>
<p><code>SESSION(time_attr, interval)</code>定义一个会话时间窗口。<br>会话时间窗口没有一个固定的持续时间，但是它们的边界会根据 <code>interval</code> 所定义的不活跃时间所确定；即一个会话时间窗口在定义的间隔时间内没有时间出现，该窗口会被关闭。例如时间窗口的间隔时间是 30 分钟，当其不活跃的时间达到30分钟后，若观测到新的记录，则会启动一个新的会话时间窗口（否则该行数据会被添加到当前的窗口），且若在 30 分钟内没有观测到新纪录，这个窗口将会被关闭。会话时间窗口可以使用事件时间（批处理、流处理）或处理时间（流处理）。</p>
<p>流式数据处理中，很多操作要依赖于时间属性进行，因此时间属性也是流式引擎能够保证准确处理数据的基石。在这篇文章中，我们将对 Flink 中时间属性和窗口的实现逻辑进行分析。</p>
<p>【参考文献】</p>
<ol>
<li><a target="_blank" rel="noopener" href="http://wuchong.me/blog/2016/05/25/flink-internals-window-mechanism/">flink原理与实现: window机制</a></li>
<li><a target="_blank" rel="noopener" href="https://xie.infoq.cn/article/0d5038bc42862b3db79c571bd">flink窗口应用与实现</a></li>
<li><a target="_blank" rel="noopener" href="https://blog.csdn.net/a1240466196/article/details/108334436">Flink原理: 窗口原理详解</a></li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://example.com/bigdata/iceberg/%E6%95%B0%E6%8D%AE%E6%B9%96/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="aaronzhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Guadazi">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/bigdata/iceberg/%E6%95%B0%E6%8D%AE%E6%B9%96/" class="post-title-link" itemprop="url">数据湖</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-01-15 19:47:00" itemprop="dateCreated datePublished" datetime="2021-01-15T19:47:00+08:00">2021-01-15</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-04-24 12:58:52" itemprop="dateModified" datetime="2021-04-24T12:58:52+08:00">2021-04-24</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/bigdata/" itemprop="url" rel="index"><span itemprop="name">bigdata</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="数据湖"><a href="#数据湖" class="headerlink" title="数据湖"></a>数据湖</h1><p><img src="vx_images/4964231239501" alt="图片"></p>
<h2 id="数据仓库-VS-数据湖"><a href="#数据仓库-VS-数据湖" class="headerlink" title="数据仓库 VS 数据湖"></a>数据仓库 VS 数据湖</h2><p>相较而言，数据湖是较新的技术，拥有不断演变的架构。数据湖存储任何形式（包括结构化和非结构化）和任何格式（包括文本、音频、视频和图像）的原始数据。根据定义，<code>数据湖不会接受数据治理</code>，但专家们一致认为<code>良好的数据管理对预防数据湖转变为数据沼泽不可或缺</code>。数据湖在数据读取期间创建模式。与数据仓库相比，数据湖缺乏结构性，而且更灵活，并且提供了更高的敏捷性。值得一提的是，数据湖非常适合使用机器学习和深度学习来执行各种任务，比如数据挖掘和数据分析，以及提取非结构化数据等。<br><img src="vx_images/3593617797024" alt="图片"></p>
<p><img src="vx_images/5612546586116.png"></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/3/"><i class="fa fa-angle-left" aria-label="Previous page"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/3/">3</a><span class="page-number current">4</span><a class="page-number" href="/page/5/">5</a><span class="space">&hellip;</span><a class="page-number" href="/page/24/">24</a><a class="extend next" rel="next" href="/page/5/"><i class="fa fa-angle-right" aria-label="Next page"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">aaronzhang</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">239</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">16</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">128</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">aaronzhang</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  

</body>
</html>
