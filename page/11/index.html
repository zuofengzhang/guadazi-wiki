<!DOCTYPE html>
<html lang="zh-Hans">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta property="og:type" content="website">
<meta property="og:title" content="Guadazi">
<meta property="og:url" content="http://example.com/page/11/index.html">
<meta property="og:site_name" content="Guadazi">
<meta property="og:locale">
<meta property="article:author" content="aaronzhang">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://example.com/page/11/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'zh-Hans'
  };
</script>

  <title>Guadazi</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Guadazi</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://example.com/Java/metric/03.%E5%A4%A7%E5%A0%86%E9%97%AE%E9%A2%98%E5%AE%9A%E4%BD%8D/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="aaronzhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Guadazi">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/Java/metric/03.%E5%A4%A7%E5%A0%86%E9%97%AE%E9%A2%98%E5%AE%9A%E4%BD%8D/" class="post-title-link" itemprop="url">大堆问题定位</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2019-08-10 14:58:00" itemprop="dateCreated datePublished" datetime="2019-08-10T14:58:00+08:00">2019-08-10</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-06-01 20:52:39" itemprop="dateModified" datetime="2021-06-01T20:52:39+08:00">2021-06-01</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/java/" itemprop="url" rel="index"><span itemprop="name">java</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="03-大堆问题定位"><a href="#03-大堆问题定位" class="headerlink" title="03.大堆问题定位"></a>03.大堆问题定位</h1><ol>
<li>首先确定是内存问题，还是内存泄露。如果是内存问题，则可以通过JVM监控等手段，判断内存持续增长的区域；假设确定了是内存泄露，也可能是堆内或者堆外</li>
<li>堆内内存泄露的情况，最有效的手段莫过于Heap Dump，使用<code>jmap -heap $PID</code>、<code>jcmd</code>或者<code>HeapDumpOnOutOfMemoryError</code>等等手段都可以获取，使用MAT找超级powerful的机器分析；MAT 其实提供了分析脚本可以在不用 IDE 加载整个 HEAP 获取到需要的信息, 也就是通过脚本解析 HEAP 逐步分析, 具体可参考 <a target="_blank" rel="noopener" href="https://www.techpaste.com/2015/07/how-to-analyse-large-heap-dumps">How to Analyse Large Heap Dumps</a>. PS: 曾经分析过 108G 的堆</li>
<li>实际生产中，这么大的堆，不管是dump对生产系统的影响，还是dump本身的难度，都往往不切实际，相对低成本的手段：</li>
<li><code>jmap -histo $PID</code>或<code>jmap -histo:live $PID</code>获得当前堆内对象的个数统计，并采样多次，查看一下里面object的分布，看哪类对象比较多，一般来说200G的堆，做一次<code>jmap -histo</code>可能要几十秒到一分钟，可能会触发full gc，如果使用 <code>CMS</code> 或 <code>G1</code> 的话, 可加入 <code>-XX:+ExplicitGCInvokesConcurrent</code> 使用并发收集器显式；如果不能探明的话, 只能使用 jmap 将整个堆 dump 下来。<br>所以如果有类似timeout killer的守护线程，要注意不要让它把进程kill掉</li>
<li>详细的GC日志等，比如判断引用堆积情况等，看看没有没什么异常， 比如有没有因为metaspace 满了而导致的GC。这种情况，可以看看是不是打开XX:+TraceClassLoading -XX:+TraceClassUnloading 分析下类加载情况。</li>
<li>使用Tencent JDK，可以利用old object sampling技术，不做Heap dump定位相当一部分memory leak</li>
<li>堆外内存： 确认下是否是 Java 的 direct bytebuffer 泄露, 由于采用 reference 机制回收, 如果一直没有触发 JVM GC 或回收线程偏少也会导致堆外内存回收缓慢导致泄露<br>如果是 Native 或者使用 Unsafe 方式直接向 OS 申请内存, 可通过 NMT以及 pmap 等查看, JDK 团队分享的 <a target="_blank" rel="noopener" href="http://km.oa.com/group/42239/articles/show/404478?ts=1574932416">http://km.oa.com/group/42239/articles/show/404478?ts=1574932416</a> 可谓是面面俱到.</li>
</ol>
<h2 id="使用MAT命令行分析"><a href="#使用MAT命令行分析" class="headerlink" title="使用MAT命令行分析"></a>使用MAT命令行分析</h2><p><a target="_blank" rel="noopener" href="https://www.techpaste.com/2015/07/how-to-analyse-large-heap-dumps">How to Analyse Large Heap Dumps</a></p>
<ol>
<li><p>下载MAT</p>
</li>
<li><p>到MAT的安装目录下，打开<code>MemoryAnalyzer.ini</code>，调整MAT的启动jvm参数</p>
 <figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">-Xms6144m</span></span><br><span class="line"><span class="attr">-Xmx8192m</span></span><br><span class="line"><span class="meta">-XX</span>:<span class="string">+UseConcMarkSweepGC</span></span><br><span class="line"><span class="meta">-XX</span>:<span class="string">+UseParNewGC</span></span><br><span class="line"><span class="meta">-XX</span>:<span class="string">+CMSParallelRemarkEnabled</span></span><br><span class="line"><span class="meta">-XX</span>:<span class="string">+CMSClassUnloadingEnabled</span></span><br><span class="line"><span class="meta">-XX</span>:<span class="string">+UseCMSInitiatingOccupancyOnly</span></span><br></pre></td></tr></table></figure>
<p> 堆最大大小调整为机器内存大小</p>
</li>
<li><p>dump文件所在文件夹，确保有dump文件两倍的空间</p>
</li>
<li><p>到MAT的安装目录下，使用root账户运行命令<br> For UNIX:</p>
 <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">./ParseHeapDump.sh /opt/heap_dump/jvm.hprof org.eclipse.mat.api:suspects</span><br><span class="line">./ParseHeapDump.sh /opt/heap_dump/jvm.hprof org.eclipse.mat.api:overview</span><br><span class="line">./ParseHeapDump.sh /opt/heap_dump/jvm.hprof org.eclipse.mat.api:top_components</span><br></pre></td></tr></table></figure>
<p> For Windows:</p>
 <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ParseHeapDump.bat D:\heap_dump\jvm.hprof org.eclipse.mat.api:suspects</span><br><span class="line">ParseHeapDump.bat D:\heap_dump\jvm.hprof org.eclipse.mat.api:overview</span><br><span class="line">ParseHeapDump.bat D:\heap_dump\jvm.hprof org.eclipse.mat.api:top_components</span><br></pre></td></tr></table></figure></li>
<li><p>使用MAT打开生成的分析结果文件</p>
</li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://example.com/Java/metric/JMeter/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="aaronzhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Guadazi">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/Java/metric/JMeter/" class="post-title-link" itemprop="url">JMeter与性能压测</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2019-08-10 14:58:00" itemprop="dateCreated datePublished" datetime="2019-08-10T14:58:00+08:00">2019-08-10</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-06-01 20:52:39" itemprop="dateModified" datetime="2021-06-01T20:52:39+08:00">2021-06-01</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/java/" itemprop="url" rel="index"><span itemprop="name">java</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="JMeter与性能压测"><a href="#JMeter与性能压测" class="headerlink" title="JMeter与性能压测"></a>JMeter与性能压测</h1><p> jmeter是一款纯java的性能测试工具，跨平台运行方便、提供图形化界面设置、简单易用。</p>
<p>在性能测试方法论中，很典型的方法就是二八原则，量化业务需求。</p>
<p>二八原则：指80%的业务量在20%的时间里完成。</p>
<p>如何理解，下面我们来个例子吧</p>
<p>用户登录场景：早高峰时段，8：50—9：10，5000坐席上线登陆。</p>
<pre><code>  业务量：5000个 

  时间：20x60=1200秒

吞吐量=80%x业务量/(20%*时间)=4000/240=16.7/秒</code></pre>
<p>而并非5000/1200=4.1/秒</p>
<p>实际上，登录请求数分布是一个正态分布，最高峰时肯定比4.1/秒更高，高峰段实际上完成了80%的业务量，却只花了20%的时间。</p>
<p>温馨提示：</p>
<p>1.二八原则计算的结果并非在线并发用户数，是系统要达到的处理能力（吞吐量），初学者容易被误导，那这这个数据就去设置并发数，这是错误滴。</p>
<p>2.如果你的系统性能要求更高，也可以选择一九原则或更严格的算法，二八原则比较通用，一般系统性能比较接近这个算法而已，大家应该活用。</p>
<p>3.tps、响应时间、在线并发数三者关系详解：<a target="_blank" rel="noopener" href="http://blog.csdn.net/musen518/article/details/43795047">点击打开链接</a></p>
<p>  三者关系图</p>
<p><img src="_v_images/20200121162958712_1262110967"></p>
<ol start="2">
<li> 结论</li>
</ol>
<ul>
<li>小并发数区间测试，找拐点（如：100-300并发持续5分钟，可以发现上图中200并发时出现拐点）</li>
<li>大并发数区间测试，找符合需求的最大并发数（如：1800-2200并发持续5分钟，可以找到满足响应时间在3秒内的最大并发数2000）</li>
<li>利用最大并发数，压测环境在极限时的资源消耗（压测时间1小时以内）</li>
<li>80%最大并发数，进行稳定性测试（压测时间1小时以上）</li>
</ul>
<p>注：执行机资源消耗必须监控上，保证能提供稳定的并发负载。</p>
<p>注：这里的响应时间是90%响应时间</p>
<p>tps:</p>
<p>每秒事务处理量 - <a target="_blank" rel="noopener" href="https://baike.baidu.com/item/%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95">性能测试</a>的术语介绍</p>
<p>TPS(Transaction Per Second)</p>
<p>每秒钟系统能够处理的交易或事务的数量。它是衡量系统处理能力的重要指标。TPS是<a target="_blank" rel="noopener" href="https://baike.baidu.com/item/LoadRunner">LoadRunner</a>中重要的性能参数指标。</p>
<p> 1.下载安装</p>
<p>仅仅需要从apache的网站找到下载包，解压到本地文件目录即可。</p>
<p><a target="_blank" rel="noopener" href="http://jmeter.apache.org/download_jmeter.cgi">http://jmeter.apache.org/download_jmeter.cgi</a></p>
<p>2.启动</p>
<p>解压目录中存在一个bin的目录，里面有很多批处理文件和脚本文件，window系统运行jmeter.bat即可。需要关注的是bin目录中的jmeter.properties文件，这是运行相关的配置文件. 特别是TCP Sampler configuration部分几个配置会和后面内容相关</p>
<p>3.建立一种类型测试</p>
<p>这里只描述简单的tcp测试建立步骤，因为目前支持的测试类型很多，无法一一陈述，功能细节部分可以参考JMeter文档</p>
<p>1）创建测试线程组</p>
<p><strong>1. 启动测试用接口</strong><br>首先我们写一段 php 代码，通过 PHP 内置的 Server 启动它。</p>
<figure class="highlight php"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="variable">$user_id</span> = <span class="variable">$_GET</span>[<span class="string">&#x27;user_id&#x27;</span>];</span><br><span class="line">file_put_contents(<span class="string">&#x27;/tmp/1.log&#x27;</span>, <span class="variable">$user_id</span>.PHP_EOL,  FILE_APPEND);</span><br><span class="line"><span class="keyword">echo</span> <span class="variable">$user_id</span>;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>以上代码保存为 <code>index.php</code></p>
<p>命令中执行 <code>php -S 127.0.0.1:8080</code></p>
<p>在浏览器访问 <code>http://127.0.0.1:8080/index.php?user_id=1</code> , 输出 <code>1</code> 说明服务接口正常</p>
<p><strong>2. 创建线程组</strong><br>使用 JMeter 测试应用性能首先要创建一个线程组<br>右键 “Text Plan”, 在弹出的菜单栏选择 “Add-&gt;Threads(Users)-&gt;Thread Group”</p>
<p>就创建了一个线程组：</p>
<p><img src="_v_images/20200121162958609_236107942.png"></p>
<p>“Number of Threads (users): ” 即并发用户数，相当于 ab 命令的 -c 参数<br>“Loop Count:” 循环请求次数， 即每个线程请求多少次， 这个数据乘以线程数相当于 ab 命令的 -n 参数</p>
<p>我们设置了 “Number of Threads (users)” 为 5 ， “Loop Count” 为 60 ， 相当于ab 命令</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ab -c 5 -n 300 http:&#x2F;&#x2F;xxx.com</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p><strong>2. 创建测试请求</strong><br>右键我们刚刚创建的线程组“Thread Group”, 选择 “Add-&gt; Sampler-&gt; HTTP Request”</p>
<p><img src="_v_images/20200121162958391_1716814489.png"></p>
<p>这一步相当于通过多个参数拼出要测试的接口地址。</p>
<p>注意<code>Path</code>中， <code>$&#123;__counter(false)&#125;</code> 为 JMeter 内置的函数， 它的返回值为当前请求次数<br>**这样保证了我们每次向服务器请求的 <code>user_id</code> 的值都不一样 **</p>
<p>此时我们将要进行的测试等同于 ab 测试命令：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ab -c 5 -n 300 http:&#x2F;&#x2F;127.0.0.1&#x2F;index.php?user_id&#x3D;1</span><br></pre></td></tr></table></figure>
<h2 id="、-Counter-函数"><a href="#、-Counter-函数" class="headerlink" title="、_Counter 函数"></a>、_Counter 函数</h2><p>每次调用计数器函数都会产生一个新值，从1开始每次加1。计数器既可以被配置成针对每个虚拟用户是独立的，也可以被配置成所有虚拟用户公用的。如果每个虚拟用户的计数器是独立增长的，那么通常被用于记录测试计划运行了多少遍。全局计数器通常被用于记录发送了多少次请求。</p>
<p>计数器使用一个整数值来记录，允许的最大值为2,147,483,647。</p>
<p>功能：这个函数是一个计数器，用于统计函数的使用次数，它从1开始，每调用这个函数一次它就会自动加1，它有两个参数，第一个参数是布尔型的，只能设置成“TRUE”或者“FALSE”，如果是TRUE，那么每个用户有自己的计数器，可以用于统计每个线程歌执行了多少次。如果是FALSE，那就使用全局计数器，可以统计出这次测试共运行了多少次。第二个参数是“函数名称”</p>
<p><strong>格式：</strong>${__counter(FALSE,test)}</p>
<p><strong>使用：</strong>我们将“_counter”函数生成的参数复制到某个参数下面，如果为TRUE格式，则每个线程各自统计，最大数为循环数，如果为FALSE，则所有线程一起统计，最大数为线程数乘以循环数</p>
<p><strong>参数：</strong></p>
<p>第一个参数：True，如果测试人员希望每个虚拟用户的计数器保持独立，与其他用户的计数器相区别。False，全局计数器</p>
<p>第二个参数：重用计数器函数创建值的引用名。测试人员可以这样引用计数器的值：${test}。这样一来，测试人员就可以创建一个计数器后，在多个地方引用它的值。</p>
<p>以上，摘自网络（不知道怎么用，只好摘抄，记录下来等灵感~~~~(&gt;_&lt;)~~~~ ）。</p>
<p>目前，我测试_Counter函数，就是在参数列表加一个参数，值填写为${__counter(FALSE,test)}</p>
<p>）  </p>
<p><strong>3.开始测试</strong><br>右键线程组 “Thread Group”， 选择 “Add-&gt; Listener-&gt;Summary Report “, 创建一个结果报表</p>
<p>然后点击， 菜单栏中的绿色按钮, 开始测试：</p>
<p><img src="_v_images/20200121162958170_1920589116.png"></p>
<p>结果如图:</p>
<p> <img src="_v_images/20200121162957964_563024753.png"></p>
<p>打开 ‘/tmp/1.log’ 可以看到，每次请求的 user_id的值都是不同的。</p>
<h1 id="Thread-Group-线程组"><a href="#Thread-Group-线程组" class="headerlink" title="Thread Group(线程组)"></a>Thread Group(线程组)</h1><blockquote>
<p>1.线程组，或者可以叫用户组，进行性能测试时的用户资源池。</p>
<p>2.是任何一个测试计划执行的开始点。</p>
<p>3.上一篇提到的“控制器”和“HTTP请求”(采集器)必须在线程组内；监听器等其他组件，可以直接放在测试计划下。</p>
<p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/linglingyuese/archive/2013/03/06/linglingyuese-three.html">https://www.cnblogs.com/linglingyuese/archive/2013/03/06/linglingyuese-three.html</a></p>
<p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/hait1234/p/6767212.html">https://www.cnblogs.com/hait1234/p/6767212.html</a></p>
</blockquote>
<p>二、Thread Group线程组功能分区</p>
<p>总的来说，一个线程组有三个功能分区，这里分别标注为区域1、区域2、区域3。</p>
<p><img src="_v_images/20200121162957741_1278470356.png"></p>
<p>1.区域1：在取样器错误后要执行的动作，这个区域的主要作用很明显，在线程内的采样器失败后，接下来做什么。</p>
<pre><code> （1）继续：选择此项，将继续执行接下来的操作。

 （2）Start Next Loop：忽略错误，执行下一个循环。

 （3）停止线程：退出该线程（不再进行此线程的任何操作）。

 （4）停止测试：等待当前执行的采样器结束后，结束整个测试。

 （5）Stop Test Now：直接停止整个测试。（注意与4的“停止测试”进行区分）。</code></pre>
<p>2.区域2：线程属性，这里可以设置线程数（模拟的用户数）和循环次数。含义如下图所示：</p>
<p><img src="_v_images/20200121162957533_2123434422.png"></p>
<p>ramp up:斜坡上升; [动词短语] 加强，加大;</p>
<p> 相当于warm up的一个词,包含准备,热身,加速的意思,可用在生产中小批量的试制中, 也可以指人初入公司的锻炼. 在项目初始阶段要做许多准备工作。</p>
<p>3.区域3：调度器配置（全部都在调度器复选框被选中的前提下，下面的选项才会生效。）</p>
<p><img src="_v_images/20200121162957230_2144403715.png"></p>
<p>最重要的Tcp Sampler:tcp取样器</p>
<h4 id="TCPClient-classname"><a href="#TCPClient-classname" class="headerlink" title="TCPClient classname"></a>TCPClient classname</h4><p>TCP Sampler提供了3个Sampler的实现，分别是</p>
<p>org.apache.jmeter.protocol.tcp.sampler.TCPClientImpl </p>
<p>org.apache.jmeter.protocol.tcp.sampler.BinaryTCPClientImpl和<br>org.apache.jmeter.protocol.tcp.sampler.LengthPrefixedBinaryTCPClientImpl。</p>
<p>其中TCPClientImpl实现了以文本编辑器中所编辑的纯文本为内容进行发送，BinaryTCPClientImpl则以文本编辑器中所编辑的16进制字符（hex）内容为基础转换为二进制的字节内容进行发送，LengthPrefixedBinaryTCPClientImpl则会在BinaryTCPClientImpl基础上默认以发送内容的长度以字节前缀进行填充。</p>
<p>我们可以通过配置jmeter.properties文件中tcp.handler属性来设置默认的TCPClient。</p>
<h2 id="测试基于文本套接字应用"><a href="#测试基于文本套接字应用" class="headerlink" title="测试基于文本套接字应用"></a>测试基于文本套接字应用</h2><p>被测应用的源码请参见<a target="_blank" rel="noopener" href="https://link.jianshu.com/?t=https://github.com/XMeterSaaSService/Blog_sample_project/blob/master/socket_echo/src/main/java/net/xmeter/echo/TextServer.java">这里</a>. 如果想运行该程序，请点击该链接下载socket_echo-0.0.1-SNAPSHOT.jar，并且在命令行下执行:</p>
<p><a target="_blank" rel="noopener" href="https://github.com/XMeterSaaSService/Blog/_sample/_project/tree/master/socket_echo">https://github.com/XMeterSaaSService/Blog\_sample\_project/tree/master/socket_echo</a> </p>
<p>（javac 和java可以去掉包名后再在命令行执行）</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">java -cp socket_echo-0.0.1-SNAPSHOT.jar net.xmeter.echo.TextServer这个程序源码：</span><br></pre></td></tr></table></figure>
<p><img src="_v_images/20200121162957026_1366283370.gif"></p>
<p><a href="javascript:void(0);" title="复制代码"><img src="_v_images/20200121162956724_1452766871.gif" alt="复制代码"></a></p>
<p>import java.io.BufferedReader; import java.io.IOException; import java.io.InputStreamReader; import java.io.PrintWriter; import java.net.ServerSocket; import java.net.Socket; import java.util.concurrent.ExecutorService; import java.util.concurrent.Executors; import java.util.concurrent.atomic.AtomicInteger; public class TextServer { public static AtomicInteger sessions = new AtomicInteger(0); public void handleRequest(final Socket socket) {<br>        ExecutorService executor = Executors.newSingleThreadExecutor();</p>
<pre><code>    executor.submit(new Runnable() &#123;
        @Override public void run() &#123; try &#123;
                BufferedReader is = new BufferedReader(new InputStreamReader(socket.getInputStream()));
                PrintWriter os = new PrintWriter(socket.getOutputStream()); while(true) &#123;
                    String line = is.readLine(); if(line == null) &#123;
                        System.out.println(&quot;Probably the client side closed the connection, now close me as well.&quot;);
                        socket.close(); break;
                    &#125;
                    System.out.println(&quot;Received message: &quot; + line);
                    os.println(&quot;Echo: &quot; + line);
                    os.flush(); if(&quot;bye&quot;.equals(line)) &#123; break;
                    &#125;
                &#125;
            &#125; catch(Exception ex) &#123;
                ex.printStackTrace();
            &#125; finally &#123; try &#123;
                    socket.close(); int num = sessions.decrementAndGet();
                    System.out.println(&quot;Now totally has &quot; + num + &quot; of conn.&quot;);
                &#125; catch (IOException e) &#123;
                    e.printStackTrace();
                &#125;
            &#125;
        &#125;

    &#125;);

&#125; public static void main(String\[\] args) &#123; try &#123;
        ServerSocket server = new ServerSocket(4700); while(true) &#123;
            Socket socket = server.accept();
            TextServer srv = new TextServer();
            srv.handleRequest(socket); int num = sessions.incrementAndGet();
            System.out.println(&quot;Received new conn, now totally has &quot; + num + &quot; of conn.&quot;);
        &#125;
    &#125; catch (Exception e) &#123;
        e.printStackTrace();
    &#125;
&#125;</code></pre>
<p>}</p>
<p><a href="javascript:void(0);" title="复制代码"><img src="_v_images/20200121162956522_679224056.gif" alt="复制代码"></a></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(这个程序测试：</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">注意几图：hello后面有个换行， ENDof line Byte value 填写的是10.LF (NL line feed, new line) 换行键 ，ascill是10.os.println(&quot;Echo: &quot; + line); 用的是println，服务端返回的最后是一个换行符。如果不填写EOF byte value,那么客户端将会一直阻塞没有返回。</span><br></pre></td></tr></table></figure>
<p>我们发<strong>现EOL原来是与读数据相关的，就是设定来自于服务器数据流的一个结束标识字节。没有设置EOL将会一直读到输入流结束为</strong>止。</p>
<p>这里值得注意的是，这是个十进制的值（千万不要写成hex），比如你可以查询ASCII表，来确认一个表示结束字符的十进制值，我们以$作为案例，改造一下Mock TCP Server，输出结尾为$，如下面代码：</p>
<p>)</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"></span><br></pre></td></tr></table></figure>
<p>（请确保您的机器上已经安装了Java）。 该程序会在4700端口建立一个ServerSocket，等待来自客户端的请求，客户端如果发送了一个字符串，服务器端返回“Echo: “ + 客户端发送的字符串。如下图所示，如果我们使用telnet连接到服务器端的套接字应用，双方就可以直接进行通信了。</p>
<h5 id="TCPClientImpl"><a href="#TCPClientImpl" class="headerlink" title="TCPClientImpl"></a>TCPClientImpl</h5><p>我们使用TCPClientImpl对Mock TCP Server进行测试，配置参考下图：</p>
<p><img src="_v_images/20200121162956318_161208051.png"></p>
<p>点击运行测试，你会发现测试发生了阻塞，原因是服务器使用了readLine获取客户端的发送数据，需要根据发送数据中的CRLF（\r或\n）判断一行的<strong>结束。而我们制作的发送内容并不包括CRLF标识内容，因此，服务器阻塞在了读数据，测试客户端得不到服务器响应，同样</strong>也阻塞在了读数据，正确的配置需要添加一个“回车”（不能是”\r”或”\n”，因为TCPClientImpl会自动将其转换为对应的两个字符而不是CRLF标识）参考下图</p>
<p>TCP 取样器通过TCP/IP来连接特定服务器，连上服务器之后发送消息，然后等待服务器回复。</p>
<p>如果“Re-use connection”(重复使用连接) 复选框被选中了，在同一个线程中Samplers(取样器)共享连接，包含相同主机名和端口，不同主机/端口合并将会使用不同线程。如果“Re-use connection” 和 “Close connection”(关闭连接)同时被选中，这个套接字在运行完当前Samplers将会关闭。再下一个Sampler将会另外创建一个新套接字。你可能想要在每次线程循环结束之后关闭套接字。</p>
<p>如果一个错误被检测到或者“Re-use connection” 没有被选中，这个套接字将会关闭，另外套接字将会在接下Samplers被再一次打开。</p>
<p>详细看这篇文章：</p>
<h1 id="Apache-JMeter-TCPSampler的使用及自定义"><a href="#Apache-JMeter-TCPSampler的使用及自定义" class="headerlink" title="Apache JMeter TCPSampler的使用及自定义"></a><a target="_blank" rel="noopener" href="https://blog.csdn.net/xreztento/article/details/73741697">Apache JMeter TCPSampler的使用及自定义</a></h1><p> 还有这篇文章：<a target="_blank" rel="noopener" href="https://www.jianshu.com/p/63e08071075e">https://www.jianshu.com/p/63e08071075e</a></p>
<h1 id="JMeter—–TCP-Sampler（TCP-取样器）"><a href="#JMeter—–TCP-Sampler（TCP-取样器）" class="headerlink" title="JMeter—–TCP Sampler（TCP 取样器）"></a><a target="_blank" rel="noopener" href="https://blog.csdn.net/m0_37355951/article/details/74779977">JMeter—–TCP Sampler（TCP 取样器）</a></h1><p>jmeter报告结果中会出现三个时间</p>
<ol>
<li><p>Elapsed time    经过的时间(= Sample time = Load time = Response time ) </p>
<p>   这个时间是我们测试常用的时间，也是整个请求的消耗时间，从发送到接收完成全程消耗的时间</p>
</li>
<li><p>Latency time  延迟时间</p>
<p>  不常用，表示请求发送到刚开始接收响应时，这个时间&lt;Elapsed time</p>
</li>
</ol>
<p>3. Connection time  建立连接时间 （2.13新增参数）</p>
<pre><code>   不常用，请求连接建立的时间，这个时间 &lt; Latency time &lt; Elapsed time</code></pre>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://example.com/bigdata/Flink/Flink-backpress-oppo/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="aaronzhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Guadazi">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/bigdata/Flink/Flink-backpress-oppo/" class="post-title-link" itemprop="url">【转发】咱们从头到尾讲一次 Flink 网络流控和反压剖析</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2019-08-10 14:58:00" itemprop="dateCreated datePublished" datetime="2019-08-10T14:58:00+08:00">2019-08-10</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-06-01 20:52:39" itemprop="dateModified" datetime="2021-06-01T20:52:39+08:00">2021-06-01</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/bigdata/" itemprop="url" rel="index"><span itemprop="name">bigdata</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>本文根据 Apache Flink 系列直播整理而成，由 Apache Flink Contributor、OPPO 大数据平台研发负责人张俊老师分享。主要内容如下：</p>
<ul>
<li><p>网络流控的概念与背景</p>
</li>
<li><p>TCP的流控机制</p>
</li>
<li><p>Flink TCP-based 反压机制（before V1.5）</p>
</li>
<li><p>Flink Credit-based 反压机制 （since V1.5）</p>
</li>
<li><p>总结与思考</p>
</li>
</ul>
<h2 id="网络流控的概念与背景"><a href="#网络流控的概念与背景" class="headerlink" title="网络流控的概念与背景"></a>网络流控的概念与背景</h2><h3 id="为什么需要网络流控"><a href="#为什么需要网络流控" class="headerlink" title="为什么需要网络流控"></a>为什么需要网络流控</h3><p><img src="_v_images/20200714171454820_132283117"></p>
<p>首先我们可以看下这张最精简的网络流控的图，Producer 的吞吐率是 2MB/s，Consumer 是 1MB/s，这个时候我们就会发现在网络通信的时候我们的 Producer 的速度是比 Consumer 要快的，有 1MB/s 的这样的速度差，假定我们两端都有一个 Buffer，Producer 端有一个发送用的 Send Buffer，Consumer 端有一个接收用的 Receive Buffer，在网络端的吞吐率是 2MB/s，过了 5s 后我们的 Receive Buffer 可能就撑不住了，这时候会面临两种情况：</p>
<ul>
<li><p>1.如果 Receive Buffer 是有界的，这时候新到达的数据就只能被丢弃掉了。</p>
</li>
<li><p>2.如果 Receive Buffer 是无界的，Receive Buffer 会持续的扩张，最终会导致 Consumer 的内存耗尽。</p>
</li>
</ul>
<h3 id="网络流控的实现：静态限速"><a href="#网络流控的实现：静态限速" class="headerlink" title="网络流控的实现：静态限速"></a>网络流控的实现：静态限速</h3><p><img src="_v_images/20200714171454414_685240728"></p>
<p>为了解决这个问题，我们就需要网络流控来解决上下游速度差的问题，传统的做法可以在 Producer 端实现一个类似 Rate Limiter 这样的静态限流，Producer 的发送速率是 2MB/s，但是经过限流这一层后，往 Send Buffer 去传数据的时候就会降到 1MB/s 了，这样的话 Producer 端的发送速率跟 Consumer 端的处理速率就可以匹配起来了，就不会导致上述问题。但是这个解决方案有两点限制：</p>
<ul>
<li><p>1、事先无法预估 Consumer 到底能承受多大的速率</p>
</li>
<li><p>2、 Consumer 的承受能力通常会动态地波动</p>
</li>
</ul>
<h3 id="网络流控的实现：动态反馈-自动反压"><a href="#网络流控的实现：动态反馈-自动反压" class="headerlink" title="网络流控的实现：动态反馈/自动反压"></a>网络流控的实现：动态反馈/自动反压</h3><p><img src="_v_images/20200714171454209_1020595047"></p>
<p>针对静态限速的问题我们就演进到了动态反馈（自动反压）的机制，我们需要 Consumer 能够及时的给 Producer 做一个 feedback，即告知 Producer 能够承受的速率是多少。动态反馈分为两种：</p>
<ul>
<li><p>1、负反馈：接受速率小于发送速率时发生，告知 Producer 降低发送速率</p>
</li>
<li><p>2、正反馈：发送速率小于接收速率时发生，告知 Producer 可以把发送速率提上来</p>
</li>
</ul>
<p>让我们来看几个经典案例</p>
<h3 id="案例一：Storm-反压实现"><a href="#案例一：Storm-反压实现" class="headerlink" title="案例一：Storm 反压实现"></a>案例一：Storm 反压实现</h3><p><img src="_v_images/20200714171454004_1684651238"></p>
<p>上图就是 Storm 里实现的反压机制，可以看到 Storm 在每一个 Bolt 都会有一个监测反压的线程（Backpressure Thread），这个线程一但检测到 Bolt 里的接收队列（recv queue）出现了严重阻塞就会把这个情况写到 ZooKeeper 里，ZooKeeper 会一直被 Spout 监听，监听到有反压的情况就会停止发送，通过这样的方式匹配上下游的发送接收速率。</p>
<h3 id="案例二：Spark-Streaming-反压实现"><a href="#案例二：Spark-Streaming-反压实现" class="headerlink" title="案例二：Spark Streaming 反压实现"></a>案例二：Spark Streaming 反压实现</h3><p><img src="_v_images/20200714171453699_1486221085"></p>
<p>Spark Streaming 里也有做类似这样的 feedback 机制，上图 Fecher 会实时的从 Buffer、Processing 这样的节点收集一些指标然后通过 Controller 把速度接收的情况再反馈到 Receiver，实现速率的匹配。</p>
<h3 id="疑问：为什么-Flink（before-V1-5）里没有用类似的方式实现-feedback-机制？"><a href="#疑问：为什么-Flink（before-V1-5）里没有用类似的方式实现-feedback-机制？" class="headerlink" title="疑问：为什么 Flink（before V1.5）里没有用类似的方式实现 feedback 机制？"></a>疑问：为什么 Flink（before V1.5）里没有用类似的方式实现 feedback 机制？</h3><p>首先在解决这个疑问之前我们需要先了解一下 Flink 的网络传输是一个什么样的架构。</p>
<p><img src="_v_images/20200714171453393_758272414"></p>
<p>这张图就体现了 Flink 在做网络传输的时候基本的数据的流向，发送端在发送网络数据前要经历自己内部的一个流程，会有一个自己的 Network Buffer，在底层用 Netty 去做通信，Netty 这一层又有属于自己的 ChannelOutbound Buffer，因为最终是要通过 Socket 做网络请求的发送，所以在 Socket 也有自己的 Send Buffer，同样在接收端也有对应的三级 Buffer。学过计算机网络的时候我们应该了解到，TCP 是自带流量控制的。实际上 Flink （before V1.5）就是通过 TCP 的流控机制来实现 feedback 的。</p>
<h2 id="TCP-流控机制"><a href="#TCP-流控机制" class="headerlink" title="TCP 流控机制"></a>TCP 流控机制</h2><p>根据下图我们来简单的回顾一下 TCP 包的格式结构。首先，他有 Sequence number 这样一个机制给每个数据包做一个编号，还有 ACK number 这样一个机制来确保 TCP 的数据传输是可靠的，除此之外还有一个很重要的部分就是 Window Size，接收端在回复消息的时候会通过 Window Size 告诉发送端还可以发送多少数据。</p>
<p><img src="_v_images/20200714171452987_650184722"></p>
<p>接下来我们来简单看一下这个过程。</p>
<h3 id="TCP-流控：滑动窗口"><a href="#TCP-流控：滑动窗口" class="headerlink" title="TCP 流控：滑动窗口"></a>TCP 流控：滑动窗口</h3><p><img src="_v_images/20200714171452683_1953611527"></p>
<p>TCP 的流控就是基于滑动窗口的机制，现在我们有一个 Socket 的发送端和一个 Socket 的接收端，目前我们的发送端的速率是我们接收端的 3 倍，这样会发生什么样的一个情况呢？假定初始的时候我们发送的 window 大小是 3，然后我们接收端的 window 大小是固定的，就是接收端的 Buffer 大小为 5。</p>
<p><img src="_v_images/20200714171452278_1991948370"></p>
<p>首先，发送端会一次性发 3 个 packets，将 1，2，3 发送给接收端，接收端接收到后会将这 3 个 packets 放到 Buffer 里去。</p>
<p><img src="_v_images/20200714171452072_41416741"></p>
<p>接收端一次消费 1 个 packet，这时候 1 就已经被消费了，然后我们看到接收端的滑动窗口会往前滑动一格，这时候 2，3 还在 Buffer 当中 而 4，5，6 是空出来的，所以接收端会给发送端发送 ACK = 4 ，代表发送端可以从 4 开始发送，同时会将 window 设置为 3 （Buffer 的大小 5 减去已经存下的 2 和 3），发送端接收到回应后也会将他的滑动窗口向前移动到 4，5，6。</p>
<p><img src="_v_images/20200714171451767_719493078"></p>
<p>这时候发送端将 4，5，6 发送，接收端也能成功的接收到 Buffer 中去。</p>
<p><img src="_v_images/20200714171451562_313469730"></p>
<p>到这一阶段后，接收端就消费到 2 了，同样他的窗口也会向前滑动一个，这时候他的 Buffer 就只剩一个了，于是向发送端发送 ACK = 7、window = 1。发送端收到之后滑动窗口也向前移，但是这个时候就不能移动 3 格了，虽然发送端的速度允许发 3 个 packets 但是 window 传值已经告知只能接收一个，所以他的滑动窗口就只能往前移一格到 7 ，这样就达到了限流的效果，发送端的发送速度从 3 降到 1。</p>
<p><img src="_v_images/20200714171451057_482639053"></p>
<p><img src="_v_images/20200714171450752_1192910727"></p>
<p>我们再看一下这种情况，这时候发送端将 7 发送后，接收端接收到，但是由于接收端的消费出现问题，一直没有从 Buffer 中去取，这时候接收端向发送端发送 ACK = 8、window = 0 ，由于这个时候 window = 0，发送端是不能发送任何数据，也就会使发送端的发送速度降为 0。这个时候发送端不发送任何数据了，接收端也不进行任何的反馈了，那么如何知道消费端又开始消费了呢？</p>
<p><img src="_v_images/20200714171450147_1390602232"></p>
<p><img src="_v_images/20200714171449642_832142513"></p>
<p><img src="_v_images/20200714171449037_394744327"></p>
<p>TCP 当中有一个 ZeroWindowProbe 的机制，发送端会定期的发送 1 个字节的探测消息，这时候接收端就会把 window 的大小进行反馈。当接收端的消费恢复了之后，接收到探测消息就可以将 window 反馈给发送端端了从而恢复整个流程。TCP 就是通过这样一个滑动窗口的机制实现 feedback。</p>
<h2 id="Flink-TCP-based-反压机制（before-V1-5）"><a href="#Flink-TCP-based-反压机制（before-V1-5）" class="headerlink" title="Flink TCP-based 反压机制（before V1.5）"></a>Flink TCP-based 反压机制（before V1.5）</h2><h3 id="示例：WindowWordCount"><a href="#示例：WindowWordCount" class="headerlink" title="示例：WindowWordCount"></a>示例：WindowWordCount</h3><p><img src="_v_images/20200714171448732_1736435542"></p>
<p>大体的逻辑就是从 Socket 里去接收数据，每 5s 去进行一次 WordCount，将这个代码提交后就进入到了编译阶段。</p>
<h3 id="编译阶段：生成-JobGraph"><a href="#编译阶段：生成-JobGraph" class="headerlink" title="编译阶段：生成 JobGraph"></a>编译阶段：生成 JobGraph</h3><p><img src="_v_images/20200714171448227_116055159"></p>
<p>这时候还没有向集群去提交任务，在 Client 端会将 StreamGraph 生成 JobGraph，JobGraph 就是做为向集群提交的最基本的单元。在生成 JobGrap 的时候会做一些优化，将一些没有 Shuffle 机制的节点进行合并。有了 JobGraph 后就会向集群进行提交，进入运行阶段。</p>
<h3 id="运行阶段：调度-ExecutionGraph"><a href="#运行阶段：调度-ExecutionGraph" class="headerlink" title="运行阶段：调度 ExecutionGraph"></a>运行阶段：调度 ExecutionGraph</h3><p><img src="_v_images/20200714171447922_1674684296"></p>
<p>JobGraph 提交到集群后会生成 ExecutionGraph ，这时候就已经具备基本的执行任务的雏形了，把每个任务拆解成了不同的 SubTask，上图 ExecutionGraph 中的 Intermediate Result Partition 就是用于发送数据的模块，最终会将 ExecutionGraph 交给 JobManager 的调度器，将整个 ExecutionGraph 调度起来。然后我们概念化这样一张物理执行图，可以看到每个 Task 在接收数据时都会通过这样一个 InputGate 可以认为是负责接收数据的，再往前有这样一个 ResultPartition 负责发送数据，在 ResultPartition 又会去做分区跟下游的 Task 保持一致，就形成了 ResultSubPartition 和 InputChannel 的对应关系。这就是从逻辑层上来看的网络传输的通道，基于这么一个概念我们可以将反压的问题进行拆解。</p>
<h3 id="问题拆解：反压传播两个阶段"><a href="#问题拆解：反压传播两个阶段" class="headerlink" title="问题拆解：反压传播两个阶段"></a>问题拆解：反压传播两个阶段</h3><p><img src="_v_images/20200714171447717_722983245"></p>
<p>反压的传播实际上是分为两个阶段的，对应着上面的执行图，我们一共涉及 3 个 TaskManager，在每个 TaskManager 里面都有相应的 Task 在执行，还有负责接收数据的 InputGate，发送数据的 ResultPartition，这就是一个最基本的数据传输的通道。在这时候假设最下游的 Task （Sink）出现了问题，处理速度降了下来这时候是如何将这个压力反向传播回去呢？这时候就分为两种情况：</p>
<ul>
<li><p>跨 TaskManager ，反压如何从 InputGate 传播到 ResultPartition</p>
</li>
<li><p>TaskManager 内，反压如何从 ResultPartition 传播到 InputGate</p>
</li>
</ul>
<h3 id="跨-TaskManager-数据传输"><a href="#跨-TaskManager-数据传输" class="headerlink" title="跨 TaskManager 数据传输"></a>跨 TaskManager 数据传输</h3><p><img src="_v_images/20200714171447411_1651413338"></p>
<p>前面提到，发送数据需要 ResultPartition，在每个 ResultPartition 里面会有分区 ResultSubPartition，中间还会有一些关于内存管理的 Buffer。 对于一个 TaskManager 来说会有一个统一的 Network BufferPool 被所有的 Task 共享，在初始化时会从 Off-heap Memory 中申请内存，申请到内存的后续内存管理就是同步 Network BufferPool 来进行的，不需要依赖 JVM GC 的机制去释放。有了 Network BufferPool 之后可以为每一个 ResultSubPartition 创建 Local BufferPool 。 如上图左边的 TaskManager 的 Record Writer 写了 &lt;1，2&gt; 这个两个数据进来，因为 ResultSubPartition 初始化的时候为空，没有 Buffer 用来接收，就会向 Local BufferPool 申请内存，这时 Local BufferPool 也没有足够的内存于是将请求转到 Network BufferPool，最终将申请到的 Buffer 按原链路返还给 ResultSubPartition，&lt;1，2&gt; 这个两个数据就可以被写入了。之后会将 ResultSubPartition 的 Buffer 拷贝到 Netty 的 Buffer 当中最终拷贝到 Socket 的 Buffer 将消息发送出去。然后接收端按照类似的机制去处理将消息消费掉。 接下来我们来模拟上下游处理速度不匹配的场景，发送端的速率为 2，接收端的速率为 1，看一下反压的过程是怎样的。</p>
<h3 id="跨-TaskManager-反压过程"><a href="#跨-TaskManager-反压过程" class="headerlink" title="跨 TaskManager 反压过程"></a>跨 TaskManager 反压过程</h3><p><img src="_v_images/20200714171447006_680735181"></p>
<p>因为速度不匹配就会导致一段时间后 InputChannel 的 Buffer 被用尽，于是他会向 Local BufferPool 申请新的 Buffer ，这时候可以看到 Local BufferPool 中的一个 Buffer 就会被标记为 Used。</p>
<p><img src="_v_images/20200714171446799_1194236091"></p>
<p>发送端还在持续以不匹配的速度发送数据，然后就会导致 InputChannel 向 Local BufferPool 申请 Buffer 的时候发现没有可用的 Buffer 了，这时候就只能向 Network BufferPool 去申请，当然每个 Local BufferPool 都有最大的可用的 Buffer，防止一个 Local BufferPool 把 Network BufferPool 耗尽。这时候看到 Network BufferPool 还是有可用的 Buffer 可以向其申请。</p>
<p><img src="_v_images/20200714171445992_941635712"></p>
<p>一段时间后，发现 Network BufferPool 没有可用的 Buffer，或是 Local BufferPool 的最大可用 Buffer 到了上限无法向 Network BufferPool 申请，没有办法去读取新的数据，这时 Netty AutoRead 就会被禁掉，Netty 就不会从 Socket 的 Buffer 中读取数据了。</p>
<p><img src="_v_images/20200714171444186_160162504"></p>
<p>显然，再过不久 Socket 的 Buffer 也被用尽，这时就会将 Window = 0 发送给发送端（前文提到的 TCP 滑动窗口的机制）。这时发送端的 Socket 就会停止发送。</p>
<p><img src="_v_images/20200714171443780_1304730499"></p>
<p>很快发送端的 Socket 的 Buffer 也被用尽，Netty 检测到 Socket 无法写了之后就会停止向 Socket 写数据。</p>
<p><img src="_v_images/20200714171443573_720011262"></p>
<p>Netty 停止写了之后，所有的数据就会阻塞在 Netty 的 Buffer 当中了，但是 Netty 的 Buffer 是无界的，可以通过 Netty 的水位机制中的 high watermark 控制他的上界。当超过了 high watermark，Netty 就会将其 channel 置为不可写，ResultSubPartition 在写之前都会检测 Netty 是否可写，发现不可写就会停止向 Netty 写数据。</p>
<p><img src="_v_images/20200714171443267_566801867"></p>
<p>这时候所有的压力都来到了 ResultSubPartition，和接收端一样他会不断的向 Local BufferPool 和 Network BufferPool 申请内存。</p>
<p><img src="_v_images/20200714171442761_1041985779"></p>
<p>Local BufferPool 和 Network BufferPool 都用尽后整个 Operator 就会停止写数据，达到跨 TaskManager 的反压。</p>
<h3 id="TaskManager-内反压过程"><a href="#TaskManager-内反压过程" class="headerlink" title="TaskManager 内反压过程"></a>TaskManager 内反压过程</h3><p>了解了跨 TaskManager 反压过程后再来看 TaskManager 内反压过程就更好理解了，下游的 TaskManager 反压导致本 TaskManager 的 ResultSubPartition 无法继续写入数据，于是 Record Writer 的写也被阻塞住了，因为 Operator 需要有输入才能有计算后的输出，输入跟输出都是在同一线程执行， Record Writer 阻塞了，Record Reader 也停止从 InputChannel 读数据，这时上游的 TaskManager 还在不断地发送数据，最终将这个 TaskManager 的 Buffer 耗尽。具体流程可以参考下图，这就是 TaskManager 内的反压过程。</p>
<p><img src="_v_images/20200714171442455_926070537"></p>
<p><img src="_v_images/20200714171442149_493428448"></p>
<p><img src="_v_images/20200714171441744_1291490922"></p>
<p><img src="_v_images/20200714171441439_2105575637"></p>
<h2 id="Flink-Credit-based-反压机制（since-V1-5）"><a href="#Flink-Credit-based-反压机制（since-V1-5）" class="headerlink" title="Flink Credit-based 反压机制（since V1.5）"></a>Flink Credit-based 反压机制（since V1.5）</h2><h3 id="TCP-based-反压的弊端"><a href="#TCP-based-反压的弊端" class="headerlink" title="TCP-based 反压的弊端"></a>TCP-based 反压的弊端</h3><p><img src="_v_images/20200714171441134_1354461165"></p>
<p>在介绍 Credit-based 反压机制之前，先分析下 TCP 反压有哪些弊端。</p>
<ul>
<li><p>在一个 TaskManager 中可能要执行多个 Task，如果多个 Task 的数据最终都要传输到下游的同一个 TaskManager 就会复用同一个 Socket 进行传输，这个时候如果单个 Task 产生反压，就会导致复用的 Socket 阻塞，其余的 Task 也无法使用传输，checkpoint barrier 也无法发出导致下游执行 checkpoint 的延迟增大。</p>
</li>
<li><p>依赖最底层的 TCP 去做流控，会导致反压传播路径太长，导致生效的延迟比较大。</p>
</li>
</ul>
<h3 id="引入-Credit-based-反压"><a href="#引入-Credit-based-反压" class="headerlink" title="引入 Credit-based 反压"></a>引入 Credit-based 反压</h3><p>这个机制简单的理解起来就是在 Flink 层面实现类似 TCP 流控的反压机制来解决上述的弊端，Credit 可以类比为 TCP 的 Window 机制。</p>
<h3 id="Credit-based-反压过程"><a href="#Credit-based-反压过程" class="headerlink" title="Credit-based 反压过程"></a>Credit-based 反压过程</h3><p><img src="_v_images/20200714171440828_629774125"></p>
<p>如图所示在 Flink 层面实现反压机制，就是每一次 ResultSubPartition 向 InputChannel 发送消息的时候都会发送一个 backlog size 告诉下游准备发送多少消息，下游就会去计算有多少的 Buffer 去接收消息，算完之后如果有充足的 Buffer 就会返还给上游一个 Credit 告知他可以发送消息（图上两个 ResultSubPartition 和 InputChannel 之间是虚线是因为最终还是要通过 Netty 和 Socket 去通信），下面我们看一个具体示例。</p>
<p><img src="_v_images/20200714171440422_2073806088"></p>
<p>假设我们上下游的速度不匹配，上游发送速率为 2，下游接收速率为 1，可以看到图上在 ResultSubPartition 中累积了两条消息，10 和 11， backlog 就为 2，这时就会将发送的数据 &lt;8,9&gt; 和 backlog = 2 一同发送给下游。下游收到了之后就会去计算是否有 2 个 Buffer 去接收，可以看到 InputChannel 中已经不足了这时就会从 Local BufferPool 和 Network BufferPool 申请，好在这个时候 Buffer 还是可以申请到的。</p>
<p><img src="_v_images/20200714171440216_190878629"></p>
<p>过了一段时间后由于上游的发送速率要大于下游的接受速率，下游的 TaskManager 的 Buffer 已经到达了申请上限，这时候下游就会向上游返回 Credit = 0，ResultSubPartition 接收到之后就不会向 Netty 去传输数据，上游 TaskManager 的 Buffer 也很快耗尽，达到反压的效果，这样在 ResultSubPartition 层就能感知到反压，不用通过 Socket 和 Netty 一层层地向上反馈，降低了反压生效的延迟。同时也不会将 Socket 去阻塞，解决了由于一个 Task 反压导致 TaskManager 和 TaskManager 之间的 Socket 阻塞的问题。</p>
<h2 id="总结与思考"><a href="#总结与思考" class="headerlink" title="总结与思考"></a>总结与思考</h2><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><ul>
<li><p>网络流控是为了在上下游速度不匹配的情况下，防止下游出现过载</p>
</li>
<li><p>网络流控有静态限速和动态反压两种手段</p>
</li>
<li><p>Flink 1.5 之前是基于 TCP 流控 + bounded buffer 实现反压</p>
</li>
<li><p>Flink 1.5 之后实现了自己托管的 credit - based 流控机制，在应用层模拟 TCP 的流控机制</p>
</li>
</ul>
<h3 id="思考"><a href="#思考" class="headerlink" title="思考"></a>思考</h3><p>有了动态反压，静态限速是不是完全没有作用了？</p>
<p><img src="_v_images/20200714171439509_423419298"></p>
<p>实际上动态反压不是万能的，我们流计算的结果最终是要输出到一个外部的存储（Storage），外部数据存储到 Sink 端的反压是不一定会触发的，这要取决于外部存储的实现，像 Kafka 这样是实现了限流限速的消息中间件可以通过协议将反压反馈给 Sink 端，但是像 ES 无法将反压进行传播反馈给 Sink 端，这种情况下为了防止外部存储在大的数据量下被打爆，我们就可以通过静态限速的方式在 Source 端去做限流。所以说动态反压并不能完全替代静态限速的，需要根据合适的场景去选择处理方案。</p>
<p>作者：阿里云云栖号<br>链接：<a target="_blank" rel="noopener" href="https://juejin.im/post/5dce4b265188254a2b1faddf">https://juejin.im/post/5dce4b265188254a2b1faddf</a><br>来源：掘金<br>著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://example.com/bigdata/Flink/DataTypesAndSerialization/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="aaronzhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Guadazi">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/bigdata/Flink/DataTypesAndSerialization/" class="post-title-link" itemprop="url">Flink:数据类型与序列化</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2019-08-10 14:58:00" itemprop="dateCreated datePublished" datetime="2019-08-10T14:58:00+08:00">2019-08-10</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-06-01 20:52:39" itemprop="dateModified" datetime="2021-06-01T20:52:39+08:00">2021-06-01</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/bigdata/" itemprop="url" rel="index"><span itemprop="name">bigdata</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="Flink-Data-Types-amp-Serialization"><a href="#Flink-Data-Types-amp-Serialization" class="headerlink" title="Flink Data Types &amp; Serialization"></a>Flink Data Types &amp; Serialization</h1><h2 id="使用case-class的坑"><a href="#使用case-class的坑" class="headerlink" title="使用case class的坑"></a>使用case class的坑</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Event</span>(<span class="params">id: <span class="type">Int</span></span>) </span>&#123;</span><br><span class="line">  <span class="keyword">val</span> lb = <span class="keyword">new</span> <span class="type">ListBuffer</span>[<span class="type">Int</span>]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">13:00:43,342 INFO  org.apache.flink.api.java.typeutils.TypeExtractor             - class org.myorg.quickstart.Event does not contain a setter for field id</span><br><span class="line">13:00:43,343 INFO  org.apache.flink.api.java.typeutils.TypeExtractor             - Class class org.myorg.quickstart.Event cannot be used as a POJO type because not all fields are valid POJO fields, and must be processed as GenericType. Please read the Flink documentation on &quot;Data Types &amp; Serialization&quot; for details of the effect on performance.</span><br></pre></td></tr></table></figure>
<p><strong>提示信息：找不到setter，对于POJO类型必须所有的字段必须要有setter和getter</strong><br>命名是case class啊</p>
<p>再看生产环境的例子： 折腾了一下午</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Event</span>(<span class="params">uin: <span class="type">String</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">                 sPid: <span class="type">String</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">                 applyId: <span class="type">String</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">                 bankType: <span class="type">Long</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">                 transactionId: <span class="type">String</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">                 amount: <span class="type">Long</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">                 createTime: <span class="type">Long</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">                 bizType: <span class="type">Long</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">                 modifyTime: <span class="type">String</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">                 equ: <span class="type">ListBuffer</span>[<span class="type">Int</span>]</span></span></span><br><span class="line"><span class="class"><span class="params">                </span>) </span>&#123;</span><br><span class="line"></span><br><span class="line">   <span class="function"><span class="keyword">def</span> <span class="title">this</span></span>(uin: <span class="type">String</span>,</span><br><span class="line">           sPid: <span class="type">String</span>,</span><br><span class="line">           applyId: <span class="type">String</span>,</span><br><span class="line">           bankType: <span class="type">Long</span>,</span><br><span class="line">           transactionId: <span class="type">String</span>,</span><br><span class="line">           amount: <span class="type">Long</span>,</span><br><span class="line">           createTime: <span class="type">Long</span>,</span><br><span class="line">           bizType: <span class="type">Long</span>,</span><br><span class="line">           modifyTime: <span class="type">String</span>,</span><br><span class="line">           equ: <span class="type">List</span>[<span class="type">Int</span>]</span><br><span class="line">          ) = &#123;</span><br><span class="line">    <span class="keyword">this</span>(uin, sPid, applyId, bankType, transactionId, amount, createTime, bizType, modifyTime)</span><br><span class="line">    <span class="keyword">if</span> (equ != <span class="literal">null</span>) &#123;</span><br><span class="line">      equities.appendAll(equ)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">var</span> _active = <span class="literal">false</span></span><br><span class="line">  <span class="keyword">val</span> equities: <span class="type">ListBuffer</span>[<span class="type">Int</span>] = <span class="keyword">new</span> <span class="type">ListBuffer</span>[<span class="type">Int</span>]</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">addEquity</span></span>(id: <span class="type">Int</span>): <span class="type">Event</span> = &#123;</span><br><span class="line">    equities.append(id)</span><br><span class="line">    <span class="keyword">this</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">equityString</span></span>(separator: <span class="type">String</span>): <span class="type">String</span> = &#123;</span><br><span class="line">    equities.mkString(separator)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">setActive</span></span>(): <span class="type">Event</span> = &#123;</span><br><span class="line">    _active = <span class="literal">true</span></span><br><span class="line">    <span class="keyword">this</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">setActive</span></span>(active: <span class="type">String</span>): <span class="type">Event</span> = &#123;</span><br><span class="line">    _active = <span class="string">&quot;1&quot;</span>.equals(active)</span><br><span class="line">    <span class="keyword">this</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">isActive</span></span>() = &#123;</span><br><span class="line">    _active</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>使用的是flink 1.6版本的，case class识别出来了，但是equities没有传递到下一个算子中，始终没有值</p>
<p>老老实实的修改成普通类</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Event</span>(<span class="params">_uin: <span class="type">String</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">            _sPid: <span class="type">String</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">            _applyId: <span class="type">String</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">            _bankType: <span class="type">Long</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">            _transactionId: <span class="type">String</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">            _amount: <span class="type">Long</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">            _createTime: <span class="type">Long</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">            _bizType: <span class="type">Long</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">            _modifyTime: <span class="type">String</span></span></span></span><br><span class="line"><span class="class"><span class="params">           </span>) <span class="keyword">extends</span> <span class="title">Serializable</span> </span>&#123;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">var</span> equities: <span class="type">ListBuffer</span>[<span class="type">Int</span>] = <span class="keyword">new</span> <span class="type">ListBuffer</span>[<span class="type">Int</span>]</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">var</span> uin: <span class="type">String</span> = _uin</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">var</span> sPid: <span class="type">String</span> = _sPid</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">var</span> applyId: <span class="type">String</span> = _applyId</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">var</span> bankType: <span class="type">Long</span> = _bankType</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">var</span> transactionId: <span class="type">String</span> = _transactionId</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">var</span> amount: <span class="type">Long</span> = _amount</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">var</span> createTime: <span class="type">Long</span> = _createTime</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">var</span> bizType: <span class="type">Long</span> = _bizType</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">var</span> modifyTime: <span class="type">String</span> = _modifyTime</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">var</span> _active = <span class="literal">false</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">this</span></span>(uin: <span class="type">String</span>,</span><br><span class="line">           sPid: <span class="type">String</span>,</span><br><span class="line">           applyId: <span class="type">String</span>,</span><br><span class="line">           bankType: <span class="type">Long</span>,</span><br><span class="line">           transactionId: <span class="type">String</span>,</span><br><span class="line">           amount: <span class="type">Long</span>,</span><br><span class="line">           createTime: <span class="type">Long</span>,</span><br><span class="line">           bizType: <span class="type">Long</span>,</span><br><span class="line">           modifyTime: <span class="type">String</span>,</span><br><span class="line">           equ: <span class="type">List</span>[<span class="type">Int</span>]</span><br><span class="line">          ) = &#123;</span><br><span class="line">    <span class="keyword">this</span>(uin, sPid, applyId, bankType, transactionId, amount, createTime, bizType, modifyTime)</span><br><span class="line">    <span class="keyword">if</span> (equ != <span class="literal">null</span>) &#123;</span><br><span class="line">      equities.appendAll(equ)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">getUin</span></span>: <span class="type">String</span> = uin</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">setUin</span></span>(<span class="type">Uin</span>: <span class="type">String</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">this</span>.uin = <span class="type">Uin</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">getSPid</span></span>: <span class="type">String</span> = sPid</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">setSPid</span></span>(<span class="type">SPid</span>: <span class="type">String</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">this</span>.sPid = <span class="type">SPid</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">getApplyId</span></span>: <span class="type">String</span> = applyId</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">setApplyId</span></span>(<span class="type">ApplyId</span>: <span class="type">String</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">this</span>.applyId = <span class="type">ApplyId</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">getBankType</span></span>: <span class="type">Long</span> = bankType</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">setBankType</span></span>(<span class="type">BankType</span>: <span class="type">Long</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">this</span>.bankType = <span class="type">BankType</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">getTransactionId</span></span>: <span class="type">String</span> = transactionId</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">setTransactionId</span></span>(<span class="type">TransactionId</span>: <span class="type">String</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">this</span>.transactionId = <span class="type">TransactionId</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">getAmount</span></span>: <span class="type">Long</span> = amount</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">setAmount</span></span>(<span class="type">Amount</span>: <span class="type">Long</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">this</span>.amount = <span class="type">Amount</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">getCreateTime</span></span>: <span class="type">Long</span> = createTime</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">setCreateTime</span></span>(<span class="type">CreateTime</span>: <span class="type">Long</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">this</span>.createTime = <span class="type">CreateTime</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">getBizType</span></span>: <span class="type">Long</span> = bizType</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">setBizType</span></span>(<span class="type">BizType</span>: <span class="type">Long</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">this</span>.bizType = <span class="type">BizType</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">getModifyTime</span></span>: <span class="type">String</span> = modifyTime</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">setModifyTime</span></span>(<span class="type">ModifyTime</span>: <span class="type">String</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">this</span>.modifyTime = <span class="type">ModifyTime</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">addEquity</span></span>(id: <span class="type">Int</span>): <span class="type">Event</span> = &#123;</span><br><span class="line">    equities.append(id)</span><br><span class="line">    <span class="keyword">this</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">equityString</span></span>(separator: <span class="type">String</span>): <span class="type">String</span> = &#123;</span><br><span class="line">    equities.mkString(separator)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">setActive</span></span>(): <span class="type">Event</span> = &#123;</span><br><span class="line">    _active = <span class="literal">true</span></span><br><span class="line">    <span class="keyword">this</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">setActive</span></span>(active: <span class="type">String</span>): <span class="type">Event</span> = &#123;</span><br><span class="line">    _active = <span class="string">&quot;1&quot;</span>.equals(active)</span><br><span class="line">    <span class="keyword">this</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">isActive</span></span>() = &#123;</span><br><span class="line">    _active</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">rights</span></span>(split: <span class="type">String</span>) = &#123;</span><br><span class="line">    <span class="type">RightEvent</span>(<span class="keyword">this</span>, equities.mkString(split))</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">getEquities</span> </span>= equities</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">setEquities</span></span>(equities: <span class="type">ListBuffer</span>[<span class="type">Int</span>]) = &#123;</span><br><span class="line">    <span class="keyword">this</span>.equities = equities</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>可以了</p>
<p>初步估计，序列化除了问题</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://example.com/bigdata/Flink/%E8%BF%BD%E6%BA%90%E7%B4%A2%E9%AA%A5/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="aaronzhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Guadazi">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/bigdata/Flink/%E8%BF%BD%E6%BA%90%E7%B4%A2%E9%AA%A5/" class="post-title-link" itemprop="url">【转载】Flink追源索骥</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2019-08-10 14:58:00" itemprop="dateCreated datePublished" datetime="2019-08-10T14:58:00+08:00">2019-08-10</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-06-01 20:52:39" itemprop="dateModified" datetime="2021-06-01T20:52:39+08:00">2021-06-01</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/bigdata/" itemprop="url" rel="index"><span itemprop="name">bigdata</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="追源索骥：透过源码看懂Flink核心框架的执行流程"><a href="#追源索骥：透过源码看懂Flink核心框架的执行流程" class="headerlink" title="追源索骥：透过源码看懂Flink核心框架的执行流程"></a>追源索骥：透过源码看懂Flink核心框架的执行流程</h1><p>标签（空格分隔）： flink</p>
<hr>
<p>[TOC]</p>
<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>Flink是大数据处理领域最近很火的一个开源的分布式、高性能的流式处理框架，其对数据的处理可以达到毫秒级别。本文以一个来自官网的WordCount例子为引，全面阐述flink的核心架构及执行流程，希望读者可以借此更加深入的理解Flink逻辑。</p>
<blockquote>
<p>本文跳过了一些基本概念，如果对相关概念感到迷惑，请参考官网文档。另外在本文写作过程中，Flink正式发布了其1.5 RELEASE版本，在其发布之后完成的内容将按照1.5的实现来组织。</p>
</blockquote>
<h2 id="1-从-Hello-World-WordCount开始"><a href="#1-从-Hello-World-WordCount开始" class="headerlink" title="1.从 Hello,World WordCount开始"></a>1.从 <del>Hello,World</del> WordCount开始</h2><p>首先，我们把WordCount的例子再放一遍：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">SocketTextStreamWordCount</span> </span>&#123;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">	<span class="keyword">if</span> (args.length != <span class="number">2</span>)&#123;</span><br><span class="line">		System.err.println(<span class="string">&quot;USAGE:\nSocketTextStreamWordCount &lt;hostname&gt; &lt;port&gt;&quot;</span>);</span><br><span class="line">		<span class="keyword">return</span>;</span><br><span class="line">	&#125;</span><br><span class="line">	String hostName = args[<span class="number">0</span>];</span><br><span class="line">	Integer port = Integer.parseInt(args[<span class="number">1</span>]);</span><br><span class="line">	<span class="comment">// set up the execution environment</span></span><br><span class="line">	<span class="keyword">final</span> StreamExecutionEnvironment env = StreamExecutionEnvironment</span><br><span class="line">			.getExecutionEnvironment();</span><br><span class="line"></span><br><span class="line">	<span class="comment">// get input data</span></span><br><span class="line">	DataStream&lt;String&gt; text = env.socketTextStream(hostName, port);</span><br><span class="line">	</span><br><span class="line">	text.flatMap(<span class="keyword">new</span> LineSplitter()).setParallelism(<span class="number">1</span>)</span><br><span class="line">	<span class="comment">// group by the tuple field &quot;0&quot; and sum up tuple field &quot;1&quot;</span></span><br><span class="line">			.keyBy(<span class="number">0</span>)</span><br><span class="line">			.sum(<span class="number">1</span>).setParallelism(<span class="number">1</span>)</span><br><span class="line">			.print();</span><br><span class="line"></span><br><span class="line">	<span class="comment">// execute program</span></span><br><span class="line">	env.execute(<span class="string">&quot;Java WordCount from SocketTextStream Example&quot;</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">	<span class="comment">/**</span></span><br><span class="line"><span class="comment">	 * Implements the string tokenizer that splits sentences into words as a user-defined</span></span><br><span class="line"><span class="comment">	 * FlatMapFunction. The function takes a line (String) and splits it into</span></span><br><span class="line"><span class="comment">	 * multiple pairs in the form of &quot;(word,1)&quot; (Tuple2&amp;lt;String, Integer&amp;gt;).</span></span><br><span class="line"><span class="comment">	 */</span></span><br><span class="line">	<span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="class"><span class="keyword">class</span> <span class="title">LineSplitter</span> <span class="keyword">implements</span> <span class="title">FlatMapFunction</span>&lt;<span class="title">String</span>, <span class="title">Tuple2</span>&lt;<span class="title">String</span>, <span class="title">Integer</span>&gt;&gt; </span>&#123;</span><br><span class="line">		<span class="meta">@Override</span></span><br><span class="line">		<span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">flatMap</span><span class="params">(String value, Collector&lt;Tuple2&lt;String, Integer&gt;&gt; out)</span> </span>&#123;</span><br><span class="line">			<span class="comment">// normalize and split the line</span></span><br><span class="line">			String[] tokens = value.toLowerCase().split(<span class="string">&quot;\\W+&quot;</span>);</span><br><span class="line">			<span class="comment">// emit the pairs</span></span><br><span class="line">			<span class="keyword">for</span> (String token : tokens) &#123;</span><br><span class="line">				<span class="keyword">if</span> (token.length() &gt; <span class="number">0</span>) &#123;</span><br><span class="line">					out.collect(<span class="keyword">new</span> Tuple2&lt;String, Integer&gt;(token, <span class="number">1</span>));</span><br><span class="line">				&#125;</span><br><span class="line">			&#125;</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<p>首先从命令行中获取socket对端的ip和端口，然后启动一个执行环境，从socket中读取数据，split成单个单词的流，并按单词进行总和的计数，最后打印出来。这个例子相信接触过大数据计算或者函数式编程的人都能看懂，就不过多解释了。</p>
<h3 id="1-1-flink执行环境"><a href="#1-1-flink执行环境" class="headerlink" title="1.1 flink执行环境"></a>1.1 flink执行环境</h3><p>程序的启动，从这句开始：<code> final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()</code>。<br>这行代码会返回一个可用的执行环境。执行环境是整个flink程序执行的上下文，记录了相关配置（如并行度等），并提供了一系列方法，如读取输入流的方法，以及真正开始运行整个代码的execute方法等。对于分布式流处理程序来说，我们在代码中定义的flatMap,keyBy等等操作，事实上可以理解为一种声明，告诉整个程序我们采用了什么样的算子，而真正开启计算的代码不在此处。由于我们是在本地运行flink程序，因此这行代码会返回一个LocalStreamEnvironment，最后我们要调用它的execute方法来开启真正的任务。我们先接着往下看。</p>
<h3 id="1-2-算子（Operator）的注册（声明）"><a href="#1-2-算子（Operator）的注册（声明）" class="headerlink" title="1.2 算子（Operator）的注册（声明）"></a>1.2 算子（Operator）的注册（声明）</h3><p>我们以flatMap为例,<code>text.flatMap(new LineSplitter())</code>这一句话跟踪进去是这样的：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> &lt;R&gt; <span class="function">SingleOutputStreamOperator&lt;R&gt; <span class="title">flatMap</span><span class="params">(FlatMapFunction&lt;T, R&gt; flatMapper)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">		TypeInformation&lt;R&gt; outType = TypeExtractor.getFlatMapReturnTypes(clean(flatMapper),</span><br><span class="line">				getType(), Utils.getCallLocationName(), <span class="keyword">true</span>);</span><br><span class="line"></span><br><span class="line">		<span class="keyword">return</span> transform(<span class="string">&quot;Flat Map&quot;</span>, outType, <span class="keyword">new</span> StreamFlatMap&lt;&gt;(clean(flatMapper)));</span><br><span class="line"></span><br><span class="line">	&#125;</span><br></pre></td></tr></table></figure>
<p>里面完成了两件事，一是用反射拿到了flatMap算子的输出类型，二是生成了一个Operator。flink流式计算的核心概念，就是将数据从输入流一个个传递给Operator进行链式处理，最后交给输出流的过程。对数据的每一次处理在逻辑上成为一个operator，并且为了本地化处理的效率起见，operator之间也可以串成一个chain一起处理（可以参考责任链模式帮助理解）。下面这张图表明了flink是如何看待用户的处理流程的：抽象化为一系列operator，以source开始，以sink结尾，中间的operator做的操作叫做transform，并且可以把几个操作串在一起执行。<br><img src="http://static.zybuluo.com/bethunebtj/jal2x1y6zqs4jug4ryqnvu3l/image_1cae39t06eoo3ml1be8o0412c69.png" alt="image_1cae39t06eoo3ml1be8o0412c69.png-43.5kB"><br>我们也可以更改flink的设置，要求它不要对某个操作进行chain处理，或者从某个操作开启一个新chain等。<br>上面代码中的最后一行transform方法的作用是返回一个SingleOutputStreamOperator，它继承了Datastream类并且定义了一些辅助方法，方便对流的操作。在返回之前，transform方法还把它注册到了执行环境中（后面生成执行图的时候还会用到它）。其他的操作，包括keyBy，sum和print，都只是不同的算子，在这里出现都是一样的效果，即生成一个operator并注册给执行环境用于生成DAG。</p>
<h3 id="1-3-程序的执行"><a href="#1-3-程序的执行" class="headerlink" title="1.3 程序的执行"></a>1.3 程序的执行</h3><p>程序执行即<code>env.execute(&quot;Java WordCount from SocketTextStream Example&quot;)</code>这行代码。</p>
<h4 id="1-3-1-本地模式下的execute方法"><a href="#1-3-1-本地模式下的execute方法" class="headerlink" title="1.3.1 本地模式下的execute方法"></a>1.3.1 本地模式下的execute方法</h4><p>这行代码主要做了以下事情：</p>
<ul>
<li>生成StreamGraph。代表程序的拓扑结构，是从用户代码直接生成的图。</li>
<li>生成JobGraph。这个图是要交给flink去生成task的图。</li>
<li>生成一系列配置</li>
<li>将JobGraph和配置交给flink集群去运行。如果不是本地运行的话，还会把jar文件通过网络发给其他节点。</li>
<li>以本地模式运行的话，可以看到启动过程，如启动性能度量、web模块、JobManager、ResourceManager、taskManager等等</li>
<li>启动任务。值得一提的是在启动任务之前，先启动了一个用户类加载器，这个类加载器可以用来做一些在运行时动态加载类的工作。</li>
</ul>
<h4 id="1-3-2-远程模式（RemoteEnvironment）的execute方法"><a href="#1-3-2-远程模式（RemoteEnvironment）的execute方法" class="headerlink" title="1.3.2 远程模式（RemoteEnvironment）的execute方法"></a>1.3.2 远程模式（RemoteEnvironment）的execute方法</h4><p>远程模式的程序执行更加有趣一点。第一步仍然是获取StreamGraph，然后调用executeRemotely方法进行远程执行。<br>该方法首先创建一个用户代码加载器</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ClassLoader usercodeClassLoader = JobWithJars.buildUserCodeClassLoader(jarFiles, globalClasspaths,   getClass().getClassLoader());</span><br></pre></td></tr></table></figure>
<p>然后创建一系列配置，交给Client对象。Client这个词有意思，看见它就知道这里绝对是跟远程集群打交道的客户端。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">ClusterClient client;</span><br><span class="line"></span><br><span class="line"><span class="keyword">try</span> &#123;</span><br><span class="line">	client = <span class="keyword">new</span> StandaloneClusterClient(configuration);</span><br><span class="line">	client.setPrintStatusDuringExecution(getConfig().isSysoutLoggingEnabled());</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">try</span> &#123;</span><br><span class="line">	<span class="keyword">return</span> client.run(streamGraph, jarFiles, globalClasspaths, usercodeClassLoader).getJobExecutionResult();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>client的run方法首先生成一个JobGraph，然后将其传递给JobClient。关于Client、JobClient、JobManager到底谁管谁，可以看这张图：<br><img src="http://static.zybuluo.com/bethunebtj/6hhl3e1fumlr0aq78d2m35nt/image_1cae7g15p6k94no1ves121c5pd9.png" alt="image_1cae7g15p6k94no1ves121c5pd9.png-19.7kB"><br>确切的说，JobClient负责以异步的方式和JobManager通信（Actor是scala的异步模块），具体的通信任务由JobClientActor完成。相对应的，JobManager的通信任务也由一个Actor完成。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">JobListeningContext jobListeningContext = submitJob(actorSystem, config, highAvailabilityServices, jobGraph, timeout, sysoutLogUpdates,	classLoader);</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> awaitJobResult(jobListeningContext);</span><br></pre></td></tr></table></figure>
<p>可以看到，该方法阻塞在awaitJobResult方法上，并最终返回了一个JobListeningContext，透过这个Context可以得到程序运行的状态和结果。</p>
<h4 id="1-3-3-程序启动过程"><a href="#1-3-3-程序启动过程" class="headerlink" title="1.3.3 程序启动过程"></a>1.3.3 程序启动过程</h4><p>上面提到，整个程序真正意义上开始执行，是这里：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">env.execute(<span class="string">&quot;Java WordCount from SocketTextStream Example&quot;</span>);</span><br></pre></td></tr></table></figure>
<p>远程模式和本地模式有一点不同，我们先按本地模式来调试。<br>我们跟进源码，（在本地调试模式下）会启动一个miniCluster，然后开始执行代码：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// LocalStreamEnvironment.java</span></span><br><span class="line"></span><br><span class="line">	<span class="meta">@Override</span></span><br><span class="line">	<span class="function"><span class="keyword">public</span> JobExecutionResult <span class="title">execute</span><span class="params">(String jobName)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        </span><br><span class="line">        <span class="comment">//生成各种图结构</span></span><br><span class="line">        ......</span><br><span class="line"></span><br><span class="line">		<span class="keyword">try</span> &#123;</span><br><span class="line">		    <span class="comment">//启动集群，包括启动JobMaster，进行leader选举等等</span></span><br><span class="line">			miniCluster.start();</span><br><span class="line">			configuration.setInteger(RestOptions.PORT, miniCluster.getRestAddress().getPort());</span><br><span class="line">            </span><br><span class="line">            <span class="comment">//提交任务到JobMaster</span></span><br><span class="line">			<span class="keyword">return</span> miniCluster.executeJobBlocking(jobGraph);</span><br><span class="line">		&#125;</span><br><span class="line">		<span class="keyword">finally</span> &#123;</span><br><span class="line">			transformations.clear();</span><br><span class="line">			miniCluster.close();</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br></pre></td></tr></table></figure>
<p>这个方法里有一部分逻辑是与生成图结构相关的，我们放在第二章里讲；现在我们先接着往里跟：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//MiniCluster.java</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> JobExecutionResult <span class="title">executeJobBlocking</span><span class="params">(JobGraph job)</span> <span class="keyword">throws</span> JobExecutionException, InterruptedException </span>&#123;</span><br><span class="line">		checkNotNull(job, <span class="string">&quot;job is null&quot;</span>);</span><br><span class="line">        </span><br><span class="line">        <span class="comment">//在这里，最终把job提交给了jobMaster</span></span><br><span class="line">		<span class="keyword">final</span> CompletableFuture&lt;JobSubmissionResult&gt; submissionFuture = submitJob(job);</span><br><span class="line"></span><br><span class="line">		<span class="keyword">final</span> CompletableFuture&lt;JobResult&gt; jobResultFuture = submissionFuture.thenCompose(</span><br><span class="line">			(JobSubmissionResult ignored) -&gt; requestJobResult(job.getJobID()));</span><br><span class="line"></span><br><span class="line">    ......</span><br><span class="line">    </span><br><span class="line">	&#125;</span><br></pre></td></tr></table></figure>
<p>正如我在注释里写的，这一段代码核心逻辑就是调用那个<code>submitJob</code>方法。那么我们再接着看这个方法：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> CompletableFuture&lt;JobSubmissionResult&gt; <span class="title">submitJob</span><span class="params">(JobGraph jobGraph)</span> </span>&#123;</span><br><span class="line">	<span class="keyword">final</span> DispatcherGateway dispatcherGateway;</span><br><span class="line">	<span class="keyword">try</span> &#123;</span><br><span class="line">		dispatcherGateway = getDispatcherGateway();</span><br><span class="line">	&#125; <span class="keyword">catch</span> (LeaderRetrievalException | InterruptedException e) &#123;</span><br><span class="line">		ExceptionUtils.checkInterrupted(e);</span><br><span class="line">		<span class="keyword">return</span> FutureUtils.completedExceptionally(e);</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="comment">// we have to allow queued scheduling in Flip-6 mode because we need to request slots</span></span><br><span class="line">	<span class="comment">// from the ResourceManager</span></span><br><span class="line">	jobGraph.setAllowQueuedScheduling(<span class="keyword">true</span>);</span><br><span class="line"></span><br><span class="line">	<span class="keyword">final</span> CompletableFuture&lt;Void&gt; jarUploadFuture = uploadAndSetJarFiles(dispatcherGateway, jobGraph);</span><br><span class="line"></span><br><span class="line">	<span class="keyword">final</span> CompletableFuture&lt;Acknowledge&gt; acknowledgeCompletableFuture = jarUploadFuture.thenCompose(</span><br><span class="line">	</span><br><span class="line">	<span class="comment">//在这里执行了真正的submit操作</span></span><br><span class="line">		(Void ack) -&gt; dispatcherGateway.submitJob(jobGraph, rpcTimeout));</span><br><span class="line"></span><br><span class="line">	<span class="keyword">return</span> acknowledgeCompletableFuture.thenApply(</span><br><span class="line">		(Acknowledge ignored) -&gt; <span class="keyword">new</span> JobSubmissionResult(jobGraph.getJobID()));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这里的<code>Dispatcher</code>是一个接收job，然后指派JobMaster去启动任务的类,我们可以看看它的类结构，有两个实现。在本地环境下启动的是<code>MiniDispatcher</code>，在集群上提交任务时，集群上启动的是<code>StandaloneDispatcher</code>。</p>
<p><img src="http://static.zybuluo.com/bethunebtj/y9hjeinc58dqc7wiepv2iim4/image_1cenfj3p9fp110p0a8unn1mrh9.png" alt="image_1cenfj3p9fp110p0a8unn1mrh9.png-27.4kB"></p>
<p>那么这个Dispatcher又做了什么呢？它启动了一个<code>JobManagerRunner</code>（这里我要吐槽Flink的命名，这个东西应该叫做JobMasterRunner才对，flink里的JobMaster和JobManager不是一个东西），委托JobManagerRunner去启动该Job的<code>JobMaster</code>。我们看一下对应的代码：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//jobManagerRunner.java</span></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">verifyJobSchedulingStatusAndStartJobManager</span><span class="params">(UUID leaderSessionId)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">        ......</span><br><span class="line"></span><br><span class="line">	    <span class="keyword">final</span> CompletableFuture&lt;Acknowledge&gt; startFuture = jobMaster.start(<span class="keyword">new</span> JobMasterId(leaderSessionId), rpcTimeout);</span><br><span class="line"></span><br><span class="line">        ......</span><br><span class="line">	&#125;</span><br></pre></td></tr></table></figure>
<p>然后，JobMaster经过了一堆方法嵌套之后，执行到了这里：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">scheduleExecutionGraph</span><span class="params">()</span> </span>&#123;</span><br><span class="line">	checkState(jobStatusListener == <span class="keyword">null</span>);</span><br><span class="line">	<span class="comment">// register self as job status change listener</span></span><br><span class="line">	jobStatusListener = <span class="keyword">new</span> JobManagerJobStatusListener();</span><br><span class="line">	executionGraph.registerJobStatusListener(jobStatusListener);</span><br><span class="line"></span><br><span class="line">	<span class="keyword">try</span> &#123;</span><br><span class="line">	    <span class="comment">//这里调用了ExecutionGraph的启动方法</span></span><br><span class="line">		executionGraph.scheduleForExecution();</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="keyword">catch</span> (Throwable t) &#123;</span><br><span class="line">		executionGraph.failGlobal(t);</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>我们知道，flink的框架里有三层图结构，其中ExecutionGraph就是真正被执行的那一层，所以到这里为止，一个任务从提交到真正执行的流程就走完了，我们再回顾一下（顺便提一下远程提交时的流程区别）：</p>
<ul>
<li>客户端代码的execute方法执行；</li>
<li>本地环境下，MiniCluster完成了大部分任务，直接把任务委派给了MiniDispatcher；</li>
<li>远程环境下，启动了一个<code>RestClusterClient</code>，这个类会以HTTP Rest的方式把用户代码提交到集群上；</li>
<li>远程环境下，请求发到集群上之后，必然有个handler去处理，在这里是<code>JobSubmitHandler</code>。这个类接手了请求后，委派StandaloneDispatcher启动job，到这里之后，本地提交和远程提交的逻辑往后又统一了；</li>
<li>Dispatcher接手job之后，会实例化一个<code>JobManagerRunner</code>，然后用这个runner启动job；</li>
<li>JobManagerRunner接下来把job交给了<code>JobMaster</code>去处理；</li>
<li>JobMaster使用<code>ExecutionGraph</code>的方法启动了整个执行图；整个任务就启动起来了。</li>
</ul>
<p>至此，第一部分就讲完了。</p>
<h2 id="2-理解flink的图结构"><a href="#2-理解flink的图结构" class="headerlink" title="2.理解flink的图结构"></a>2.理解flink的图结构</h2><p>第一部分讲到，我们的主函数最后一项任务就是生成StreamGraph，然后生成JobGraph，然后以此开始调度任务运行，所以接下来我们从这里入手，继续探索flink。</p>
<h3 id="2-1-flink的三层图结构"><a href="#2-1-flink的三层图结构" class="headerlink" title="2.1 flink的三层图结构"></a>2.1 flink的三层图结构</h3><p>事实上，flink总共提供了三种图的抽象，我们前面已经提到了StreamGraph和JobGraph，还有一种是ExecutionGraph，是用于调度的基本数据结构。<br><img src="http://static.zybuluo.com/bethunebtj/nseitc0kyuq0n44s7qcp6ij9/image_1caf1oll019fp1odv1bh9idosr79.png" alt="image_1caf1oll019fp1odv1bh9idosr79.png-486.3kB"><br>上面这张图清晰的给出了flink各个图的工作原理和转换过程。其中最后一个物理执行图并非flink的数据结构，而是程序开始执行后，各个task分布在不同的节点上，所形成的物理上的关系表示。</p>
<ul>
<li>从JobGraph的图里可以看到，数据从上一个operator流到下一个operator的过程中，上游作为生产者提供了IntermediateDataSet，而下游作为消费者需要JobEdge。事实上，JobEdge是一个通信管道，连接了上游生产的dataset和下游的JobVertex节点。</li>
<li>在JobGraph转换到ExecutionGraph的过程中，主要发生了以下转变：</li>
<li> 加入了并行度的概念，成为真正可调度的图结构</li>
<li> 生成了与JobVertex对应的ExecutionJobVertex，ExecutionVertex，与IntermediateDataSet对应的IntermediateResult和IntermediateResultPartition等，并行将通过这些类实现</li>
<li>ExecutionGraph已经可以用于调度任务。我们可以看到，flink根据该图生成了一一对应的Task，每个task对应一个ExecutionGraph的一个Execution。Task用InputGate、InputChannel和ResultPartition对应了上面图中的IntermediateResult和ExecutionEdge。</li>
</ul>
<p>那么，flink抽象出这三层图结构，四层执行逻辑的意义是什么呢？<br>StreamGraph是对用户逻辑的映射。JobGraph在此基础上进行了一些优化，比如把一部分操作串成chain以提高效率。ExecutionGraph是为了调度存在的，加入了并行处理的概念。而在此基础上真正执行的是Task及其相关结构。</p>
<h3 id="2-2-StreamGraph的生成"><a href="#2-2-StreamGraph的生成" class="headerlink" title="2.2 StreamGraph的生成"></a>2.2 StreamGraph的生成</h3><p>在第一节的算子注册部分，我们可以看到，flink把每一个算子transform成一个对流的转换（比如上文中返回的SingleOutputStreamOperator是一个DataStream的子类），并且注册到执行环境中，用于生成StreamGraph。实际生成StreamGraph的入口是<code>StreamGraphGenerator.generate(env, transformations)</code> 其中的transformations是一个list，里面记录的就是我们在transform方法中放进来的算子。</p>
<h4 id="2-2-1-StreamTransformation类代表了流的转换"><a href="#2-2-1-StreamTransformation类代表了流的转换" class="headerlink" title="2.2.1 StreamTransformation类代表了流的转换"></a>2.2.1 StreamTransformation类代表了流的转换</h4><p>StreamTransformation代表了从一个或多个DataStream生成新DataStream的操作。顺便，DataStream类在内部组合了一个StreamTransformation类，实际的转换操作均通过该类完成。<br><img src="http://static.zybuluo.com/bethunebtj/69v9syr2p5k5om3c4jox9wh0/image_1caf64b7c1gjnv2eebi1v9e1cvum.png" alt="image_1caf64b7c1gjnv2eebi1v9e1cvum.png-129.4kB"><br>我们可以看到，从source到各种map,union再到sink操作全部被映射成了StreamTransformation。<br>其映射过程如下所示：<br><img src="http://static.zybuluo.com/bethunebtj/a8sjspg8agzl3utnncntds9q/image_1caf6ak4rkqsc1u1hci93fe0d13.png" alt="image_1caf6ak4rkqsc1u1hci93fe0d13.png-36.6kB"></p>
<p>以MapFunction为例：</p>
<ul>
<li><p>首先，用户代码里定义的UDF会被当作其基类对待，然后交给StreamMap这个operator做进一步包装。事实上，每一个Transformation都对应了一个StreamOperator。</p>
</li>
<li><p>由于map这个操作只接受一个输入，所以再被进一步包装为OneInputTransformation。</p>
</li>
<li><p>最后，将该transformation注册到执行环境中，当执行上文提到的generate方法时，生成StreamGraph图结构。</p>
<blockquote>
<p>另外，并不是每一个 StreamTransformation 都会转换成runtime层中的物理操作。有一些只是逻辑概念，比如union、split/select、partition等。如下图所示的转换树，在运行时会优化成下方的操作图。<br><img src="http://static.zybuluo.com/bethunebtj/6zmlsivd9cjdm5nhsacuk3o1/image_1caf71h79s0s3fodem1aeb1j3m1g.png" alt="image_1caf71h79s0s3fodem1aeb1j3m1g.png-83.8kB"></p>
</blockquote>
</li>
</ul>
<h4 id="2-2-2-StreamGraph生成函数分析"><a href="#2-2-2-StreamGraph生成函数分析" class="headerlink" title="2.2.2 StreamGraph生成函数分析"></a>2.2.2 StreamGraph生成函数分析</h4><p>我们从StreamGraphGenerator.generate()方法往下看：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br></pre></td><td class="code"><pre><span class="line">public static StreamGraph generate(StreamExecutionEnvironment env, List&lt;StreamTransformation&lt;?&gt;&gt; transformations) &#123;</span><br><span class="line">	return new StreamGraphGenerator(env).generateInternal(transformations);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">   &#x2F;&#x2F;注意，StreamGraph的生成是从sink开始的</span><br><span class="line">private StreamGraph generateInternal(List&lt;StreamTransformation&lt;?&gt;&gt; transformations) &#123;</span><br><span class="line">	for (StreamTransformation&lt;?&gt; transformation: transformations) &#123;</span><br><span class="line">		transform(transformation);</span><br><span class="line">	&#125;</span><br><span class="line">	return streamGraph;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F;这个方法的核心逻辑就是判断传入的steamOperator是哪种类型，并执行相应的操作，详情见下面那一大堆if-else</span><br><span class="line">private Collection&lt;Integer&gt; transform(StreamTransformation&lt;?&gt; transform) &#123;</span><br><span class="line"></span><br><span class="line">	if (alreadyTransformed.containsKey(transform)) &#123;</span><br><span class="line">		return alreadyTransformed.get(transform);</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	LOG.debug(&quot;Transforming &quot; + transform);</span><br><span class="line"></span><br><span class="line">	if (transform.getMaxParallelism() &lt;&#x3D; 0) &#123;</span><br><span class="line"></span><br><span class="line">		&#x2F;&#x2F; if the max parallelism hasn&#39;t been set, then first use the job wide max parallelism</span><br><span class="line">		&#x2F;&#x2F; from theExecutionConfig.</span><br><span class="line">		int globalMaxParallelismFromConfig &#x3D; env.getConfig().getMaxParallelism();</span><br><span class="line">		if (globalMaxParallelismFromConfig &gt; 0) &#123;</span><br><span class="line">			transform.setMaxParallelism(globalMaxParallelismFromConfig);</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	&#x2F;&#x2F; call at least once to trigger exceptions about MissingTypeInfo</span><br><span class="line">	transform.getOutputType();</span><br><span class="line"></span><br><span class="line">	Collection&lt;Integer&gt; transformedIds;</span><br><span class="line">	&#x2F;&#x2F;这里对操作符的类型进行判断，并以此调用相应的处理逻辑.简而言之，处理的核心无非是递归的将该节点和节点的上游节点加入图</span><br><span class="line">	if (transform instanceof OneInputTransformation&lt;?, ?&gt;) &#123;</span><br><span class="line">		transformedIds &#x3D; transformOneInputTransform((OneInputTransformation&lt;?, ?&gt;) transform);</span><br><span class="line">	&#125; else if (transform instanceof TwoInputTransformation&lt;?, ?, ?&gt;) &#123;</span><br><span class="line">		transformedIds &#x3D; transformTwoInputTransform((TwoInputTransformation&lt;?, ?, ?&gt;) transform);</span><br><span class="line">	&#125; else if (transform instanceof SourceTransformation&lt;?&gt;) &#123;</span><br><span class="line">		transformedIds &#x3D; transformSource((SourceTransformation&lt;?&gt;) transform);</span><br><span class="line">	&#125; else if (transform instanceof SinkTransformation&lt;?&gt;) &#123;</span><br><span class="line">		transformedIds &#x3D; transformSink((SinkTransformation&lt;?&gt;) transform);</span><br><span class="line">	&#125; else if (transform instanceof UnionTransformation&lt;?&gt;) &#123;</span><br><span class="line">		transformedIds &#x3D; transformUnion((UnionTransformation&lt;?&gt;) transform);</span><br><span class="line">	&#125; else if (transform instanceof SplitTransformation&lt;?&gt;) &#123;</span><br><span class="line">		transformedIds &#x3D; transformSplit((SplitTransformation&lt;?&gt;) transform);</span><br><span class="line">	&#125; else if (transform instanceof SelectTransformation&lt;?&gt;) &#123;</span><br><span class="line">		transformedIds &#x3D; transformSelect((SelectTransformation&lt;?&gt;) transform);</span><br><span class="line">	&#125; else if (transform instanceof FeedbackTransformation&lt;?&gt;) &#123;</span><br><span class="line">		transformedIds &#x3D; transformFeedback((FeedbackTransformation&lt;?&gt;) transform);</span><br><span class="line">	&#125; else if (transform instanceof CoFeedbackTransformation&lt;?&gt;) &#123;</span><br><span class="line">		transformedIds &#x3D; transformCoFeedback((CoFeedbackTransformation&lt;?&gt;) transform);</span><br><span class="line">	&#125; else if (transform instanceof PartitionTransformation&lt;?&gt;) &#123;</span><br><span class="line">		transformedIds &#x3D; transformPartition((PartitionTransformation&lt;?&gt;) transform);</span><br><span class="line">	&#125; else if (transform instanceof SideOutputTransformation&lt;?&gt;) &#123;</span><br><span class="line">		transformedIds &#x3D; transformSideOutput((SideOutputTransformation&lt;?&gt;) transform);</span><br><span class="line">	&#125; else &#123;</span><br><span class="line">		throw new IllegalStateException(&quot;Unknown transformation: &quot; + transform);</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">       &#x2F;&#x2F;注意这里和函数开始时的方法相对应，在有向图中要注意避免循环的产生</span><br><span class="line">	&#x2F;&#x2F; need this check because the iterate transformation adds itself before</span><br><span class="line">	&#x2F;&#x2F; transforming the feedback edges</span><br><span class="line">	if (!alreadyTransformed.containsKey(transform)) &#123;</span><br><span class="line">		alreadyTransformed.put(transform, transformedIds);</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	if (transform.getBufferTimeout() &gt; 0) &#123;</span><br><span class="line">		streamGraph.setBufferTimeout(transform.getId(), transform.getBufferTimeout());</span><br><span class="line">	&#125;</span><br><span class="line">	if (transform.getUid() !&#x3D; null) &#123;</span><br><span class="line">		streamGraph.setTransformationUID(transform.getId(), transform.getUid());</span><br><span class="line">	&#125;</span><br><span class="line">	if (transform.getUserProvidedNodeHash() !&#x3D; null) &#123;</span><br><span class="line">		streamGraph.setTransformationUserHash(transform.getId(), transform.getUserProvidedNodeHash());</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	if (transform.getMinResources() !&#x3D; null &amp;&amp; transform.getPreferredResources() !&#x3D; null) &#123;</span><br><span class="line">		streamGraph.setResources(transform.getId(), transform.getMinResources(), transform.getPreferredResources());</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	return transformedIds;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>因为map，filter等常用操作都是OneInputStreamOperator,我们就来看看<code>transformOneInputTransform((OneInputTransformation&lt;?, ?&gt;) transform)</code>方法。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br></pre></td><td class="code"><pre><span class="line">private &lt;IN, OUT&gt; Collection&lt;Integer&gt; transformOneInputTransform(OneInputTransformation&lt;IN, OUT&gt; transform) &#123;</span><br><span class="line"></span><br><span class="line">		Collection&lt;Integer&gt; inputIds &#x3D; transform(transform.getInput());</span><br><span class="line"></span><br><span class="line">		&#x2F;&#x2F; 在递归处理节点过程中，某个节点可能已经被其他子节点先处理过了，需要跳过</span><br><span class="line">		if (alreadyTransformed.containsKey(transform)) &#123;</span><br><span class="line">			return alreadyTransformed.get(transform);</span><br><span class="line">		&#125;</span><br><span class="line"></span><br><span class="line">        &#x2F;&#x2F;这里是获取slotSharingGroup。这个group用来定义当前我们在处理的这个操作符可以跟什么操作符chain到一个slot里进行操作</span><br><span class="line">        &#x2F;&#x2F;因为有时候我们可能不满意flink替我们做的chain聚合</span><br><span class="line">        &#x2F;&#x2F;一个slot就是一个执行task的基本容器</span><br><span class="line">		String slotSharingGroup &#x3D; determineSlotSharingGroup(transform.getSlotSharingGroup(), inputIds);</span><br><span class="line"></span><br><span class="line">        &#x2F;&#x2F;把该operator加入图</span><br><span class="line">		streamGraph.addOperator(transform.getId(),</span><br><span class="line">				slotSharingGroup,</span><br><span class="line">				transform.getOperator(),</span><br><span class="line">				transform.getInputType(),</span><br><span class="line">				transform.getOutputType(),</span><br><span class="line">				transform.getName());</span><br><span class="line">        </span><br><span class="line">        &#x2F;&#x2F;对于keyedStream，我们还要记录它的keySelector方法</span><br><span class="line">        &#x2F;&#x2F;flink并不真正为每个keyedStream保存一个key，而是每次需要用到key的时候都使用keySelector方法进行计算</span><br><span class="line">        &#x2F;&#x2F;因此，我们自定义的keySelector方法需要保证幂等性</span><br><span class="line">        &#x2F;&#x2F;到后面介绍keyGroup的时候我们还会再次提到这一点</span><br><span class="line">		if (transform.getStateKeySelector() !&#x3D; null) &#123;</span><br><span class="line">			TypeSerializer&lt;?&gt; keySerializer &#x3D; transform.getStateKeyType().createSerializer(env.getConfig());</span><br><span class="line">			streamGraph.setOneInputStateKey(transform.getId(), transform.getStateKeySelector(), keySerializer);</span><br><span class="line">		&#125;</span><br><span class="line"></span><br><span class="line">		streamGraph.setParallelism(transform.getId(), transform.getParallelism());</span><br><span class="line">		streamGraph.setMaxParallelism(transform.getId(), transform.getMaxParallelism());</span><br><span class="line">        </span><br><span class="line">        &#x2F;&#x2F;为当前节点和它的依赖节点建立边</span><br><span class="line">        &#x2F;&#x2F;这里可以看到之前提到的select union partition等逻辑节点被合并入edge的过程</span><br><span class="line">		for (Integer inputId: inputIds) &#123;</span><br><span class="line">			streamGraph.addEdge(inputId, transform.getId(), 0);</span><br><span class="line">		&#125;</span><br><span class="line"></span><br><span class="line">		return Collections.singleton(transform.getId());</span><br><span class="line">	&#125;</span><br><span class="line">	</span><br><span class="line">	public void addEdge(Integer upStreamVertexID, Integer downStreamVertexID, int typeNumber) &#123;</span><br><span class="line">		addEdgeInternal(upStreamVertexID,</span><br><span class="line">				downStreamVertexID,</span><br><span class="line">				typeNumber,</span><br><span class="line">				null,</span><br><span class="line">				new ArrayList&lt;String&gt;(),</span><br><span class="line">				null);</span><br><span class="line"></span><br><span class="line">	&#125;</span><br><span class="line">    &#x2F;&#x2F;addEdge的实现，会合并一些逻辑节点</span><br><span class="line">	private void addEdgeInternal(Integer upStreamVertexID,</span><br><span class="line">			Integer downStreamVertexID,</span><br><span class="line">			int typeNumber,</span><br><span class="line">			StreamPartitioner&lt;?&gt; partitioner,</span><br><span class="line">			List&lt;String&gt; outputNames,</span><br><span class="line">			OutputTag outputTag) &#123;</span><br><span class="line">        &#x2F;&#x2F;如果输入边是侧输出节点，则把side的输入边作为本节点的输入边，并递归调用</span><br><span class="line">		if (virtualSideOutputNodes.containsKey(upStreamVertexID)) &#123;</span><br><span class="line">			int virtualId &#x3D; upStreamVertexID;</span><br><span class="line">			upStreamVertexID &#x3D; virtualSideOutputNodes.get(virtualId).f0;</span><br><span class="line">			if (outputTag &#x3D;&#x3D; null) &#123;</span><br><span class="line">				outputTag &#x3D; virtualSideOutputNodes.get(virtualId).f1;</span><br><span class="line">			&#125;</span><br><span class="line">			addEdgeInternal(upStreamVertexID, downStreamVertexID, typeNumber, partitioner, null, outputTag);</span><br><span class="line">			&#x2F;&#x2F;如果输入边是select，则把select的输入边作为本节点的输入边</span><br><span class="line">		&#125; else if (virtualSelectNodes.containsKey(upStreamVertexID)) &#123;</span><br><span class="line">			int virtualId &#x3D; upStreamVertexID;</span><br><span class="line">			upStreamVertexID &#x3D; virtualSelectNodes.get(virtualId).f0;</span><br><span class="line">			if (outputNames.isEmpty()) &#123;</span><br><span class="line">				&#x2F;&#x2F; selections that happen downstream override earlier selections</span><br><span class="line">				outputNames &#x3D; virtualSelectNodes.get(virtualId).f1;</span><br><span class="line">			&#125;</span><br><span class="line">			addEdgeInternal(upStreamVertexID, downStreamVertexID, typeNumber, partitioner, outputNames, outputTag);</span><br><span class="line">			&#x2F;&#x2F;如果是partition节点</span><br><span class="line">		&#125; else if (virtualPartitionNodes.containsKey(upStreamVertexID)) &#123;</span><br><span class="line">			int virtualId &#x3D; upStreamVertexID;</span><br><span class="line">			upStreamVertexID &#x3D; virtualPartitionNodes.get(virtualId).f0;</span><br><span class="line">			if (partitioner &#x3D;&#x3D; null) &#123;</span><br><span class="line">				partitioner &#x3D; virtualPartitionNodes.get(virtualId).f1;</span><br><span class="line">			&#125;</span><br><span class="line">			addEdgeInternal(upStreamVertexID, downStreamVertexID, typeNumber, partitioner, outputNames, outputTag);</span><br><span class="line">		&#125; else &#123;</span><br><span class="line">		&#x2F;&#x2F;正常的edge处理逻辑</span><br><span class="line">			StreamNode upstreamNode &#x3D; getStreamNode(upStreamVertexID);</span><br><span class="line">			StreamNode downstreamNode &#x3D; getStreamNode(downStreamVertexID);</span><br><span class="line"></span><br><span class="line">			&#x2F;&#x2F; If no partitioner was specified and the parallelism of upstream and downstream</span><br><span class="line">			&#x2F;&#x2F; operator matches use forward partitioning, use rebalance otherwise.</span><br><span class="line">			if (partitioner &#x3D;&#x3D; null &amp;&amp; upstreamNode.getParallelism() &#x3D;&#x3D; downstreamNode.getParallelism()) &#123;</span><br><span class="line">				partitioner &#x3D; new ForwardPartitioner&lt;Object&gt;();</span><br><span class="line">			&#125; else if (partitioner &#x3D;&#x3D; null) &#123;</span><br><span class="line">				partitioner &#x3D; new RebalancePartitioner&lt;Object&gt;();</span><br><span class="line">			&#125;</span><br><span class="line"></span><br><span class="line">			if (partitioner instanceof ForwardPartitioner) &#123;</span><br><span class="line">				if (upstreamNode.getParallelism() !&#x3D; downstreamNode.getParallelism()) &#123;</span><br><span class="line">					throw new UnsupportedOperationException(&quot;Forward partitioning does not allow &quot; +</span><br><span class="line">							&quot;change of parallelism. Upstream operation: &quot; + upstreamNode + &quot; parallelism: &quot; + upstreamNode.getParallelism() +</span><br><span class="line">							&quot;, downstream operation: &quot; + downstreamNode + &quot; parallelism: &quot; + downstreamNode.getParallelism() +</span><br><span class="line">							&quot; You must use another partitioning strategy, such as broadcast, rebalance, shuffle or global.&quot;);</span><br><span class="line">				&#125;</span><br><span class="line">			&#125;</span><br><span class="line"></span><br><span class="line">			StreamEdge edge &#x3D; new StreamEdge(upstreamNode, downstreamNode, typeNumber, outputNames, partitioner, outputTag);</span><br><span class="line"></span><br><span class="line">			getStreamNode(edge.getSourceId()).addOutEdge(edge);</span><br><span class="line">			getStreamNode(edge.getTargetId()).addInEdge(edge);</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br></pre></td></tr></table></figure>
<h4 id="2-2-3-WordCount函数的StreamGraph"><a href="#2-2-3-WordCount函数的StreamGraph" class="headerlink" title="2.2.3 WordCount函数的StreamGraph"></a>2.2.3 WordCount函数的StreamGraph</h4><p>flink提供了一个StreamGraph可视化显示工具，<a target="_blank" rel="noopener" href="http://flink.apache.org/visualizer/">在这里</a><br>我们可以把我们的程序的执行计划打印出来<code>System.out.println(env.getExecutionPlan());</code> 复制到这个网站上，点击生成，如图所示：<br><img src="http://static.zybuluo.com/bethunebtj/sfckex3xgu33m3srk2bc5hgk/image_1cafgsliu1n2n1uj21p971b0h6m71t.png" alt="image_1cafgsliu1n2n1uj21p971b0h6m71t.png-25.7kB"><br>可以看到，我们源程序被转化成了4个operator。<br>另外，在operator之间的连线上也显示出了flink添加的一些逻辑流程。由于我设定了每个操作符的并行度都是1，所以在每个操作符之间都是直接FORWARD，不存在shuffle的过程。</p>
<h3 id="2-3-JobGraph的生成"><a href="#2-3-JobGraph的生成" class="headerlink" title="2.3 JobGraph的生成"></a>2.3 JobGraph的生成</h3><p>flink会根据上一步生成的StreamGraph生成JobGraph，然后将JobGraph发送到server端进行ExecutionGraph的解析。</p>
<h4 id="2-3-1-JobGraph生成源码"><a href="#2-3-1-JobGraph生成源码" class="headerlink" title="2.3.1 JobGraph生成源码"></a>2.3.1 JobGraph生成源码</h4><p>与StreamGraph类似，JobGraph的入口方法是<code>StreamingJobGraphGenerator.createJobGraph()</code>。我们直接来看源码</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">private JobGraph createJobGraph() &#123;</span><br><span class="line"></span><br><span class="line">		&#x2F;&#x2F; 设置启动模式为所有节点均在一开始就启动</span><br><span class="line">		jobGraph.setScheduleMode(ScheduleMode.EAGER);</span><br><span class="line"></span><br><span class="line">		&#x2F;&#x2F; 为每个节点生成hash id</span><br><span class="line">		Map&lt;Integer, byte[]&gt; hashes &#x3D; defaultStreamGraphHasher.traverseStreamGraphAndGenerateHashes(streamGraph);</span><br><span class="line"></span><br><span class="line">		&#x2F;&#x2F; 为了保持兼容性创建的hash</span><br><span class="line">		List&lt;Map&lt;Integer, byte[]&gt;&gt; legacyHashes &#x3D; new ArrayList&lt;&gt;(legacyStreamGraphHashers.size());</span><br><span class="line">		for (StreamGraphHasher hasher : legacyStreamGraphHashers) &#123;</span><br><span class="line">			legacyHashes.add(hasher.traverseStreamGraphAndGenerateHashes(streamGraph));</span><br><span class="line">		&#125;</span><br><span class="line"></span><br><span class="line">		Map&lt;Integer, List&lt;Tuple2&lt;byte[], byte[]&gt;&gt;&gt; chainedOperatorHashes &#x3D; new HashMap&lt;&gt;();</span><br><span class="line">        &#x2F;&#x2F;生成jobvertex，串成chain等</span><br><span class="line">        &#x2F;&#x2F;这里的逻辑大致可以理解为，挨个遍历节点，如果该节点是一个chain的头节点，就生成一个JobVertex，如果不是头节点，就要把自身配置并入头节点，然后把头节点和自己的出边相连；对于不能chain的节点，当作只有头节点处理即可</span><br><span class="line">		setChaining(hashes, legacyHashes, chainedOperatorHashes);</span><br><span class="line">        &#x2F;&#x2F;设置输入边edge</span><br><span class="line">		setPhysicalEdges();</span><br><span class="line">        &#x2F;&#x2F;设置slot共享group</span><br><span class="line">		setSlotSharing();</span><br><span class="line">        &#x2F;&#x2F;配置检查点</span><br><span class="line">		configureCheckpointing();</span><br><span class="line"></span><br><span class="line">		&#x2F;&#x2F; 如果有之前的缓存文件的配置的话，重新读入</span><br><span class="line">		for (Tuple2&lt;String, DistributedCache.DistributedCacheEntry&gt; e : streamGraph.getEnvironment().getCachedFiles()) &#123;</span><br><span class="line">			DistributedCache.writeFileInfoToConfig(e.f0, e.f1, jobGraph.getJobConfiguration());</span><br><span class="line">		&#125;</span><br><span class="line"></span><br><span class="line">		&#x2F;&#x2F; 传递执行环境配置</span><br><span class="line">		try &#123;</span><br><span class="line">			jobGraph.setExecutionConfig(streamGraph.getExecutionConfig());</span><br><span class="line">		&#125;</span><br><span class="line">		catch (IOException e) &#123;</span><br><span class="line">			throw new IllegalConfigurationException(&quot;Could not serialize the ExecutionConfig.&quot; +</span><br><span class="line">					&quot;This indicates that non-serializable types (like custom serializers) were registered&quot;);</span><br><span class="line">		&#125;</span><br><span class="line"></span><br><span class="line">		return jobGraph;</span><br><span class="line">	&#125;</span><br></pre></td></tr></table></figure>
<h4 id="2-3-2-operator-chain的逻辑"><a href="#2-3-2-operator-chain的逻辑" class="headerlink" title="2.3.2 operator chain的逻辑"></a>2.3.2 operator chain的逻辑</h4><blockquote>
<p>为了更高效地分布式执行，Flink会尽可能地将operator的subtask链接（chain）在一起形成task。每个task在一个线程中执行。将operators链接成task是非常有效的优化：它能减少线程之间的切换，减少消息的序列化/反序列化，减少数据在缓冲区的交换，减少了延迟的同时提高整体的吞吐量。</p>
</blockquote>
<p><img src="http://static.zybuluo.com/bethunebtj/jcjalvv130ex52vkglkt56r2/image_1cafj7s6bittk5tt0bequlig2a.png" alt="image_1cafj7s6bittk5tt0bequlig2a.png-158.7kB"><br>上图中将KeyAggregation和Sink两个operator进行了合并，因为这两个合并后并不会改变整体的拓扑结构。但是，并不是任意两个 operator 就能 chain 一起的,其条件还是很苛刻的：</p>
<blockquote>
<ul>
<li>上下游的并行度一致</li>
<li>下游节点的入度为1 （也就是说下游节点没有来自其他节点的输入）</li>
<li>上下游节点都在同一个 slot group 中（下面会解释 slot group）</li>
<li>下游节点的 chain 策略为 ALWAYS（可以与上下游链接，map、flatmap、filter等默认是ALWAYS）</li>
<li>上游节点的 chain 策略为 ALWAYS 或 HEAD（只能与下游链接，不能与上游链接，Source默认是HEAD）</li>
<li>两个节点间数据分区方式是 forward（参考理解数据流的分区）</li>
<li>用户没有禁用 chain</li>
</ul>
</blockquote>
<p>flink的chain逻辑是一种很常见的设计，比如spring的interceptor也是类似的实现方式。通过把操作符串成一个大操作符，flink避免了把数据序列化后通过网络发送给其他节点的开销，能够大大增强效率。</p>
<h4 id="2-3-3-JobGraph的提交"><a href="#2-3-3-JobGraph的提交" class="headerlink" title="2.3.3 JobGraph的提交"></a>2.3.3 JobGraph的提交</h4><p>前面已经提到，JobGraph的提交依赖于JobClient和JobManager之间的异步通信，如图所示：<br><img src="http://static.zybuluo.com/bethunebtj/dj015uuqpnb4ct7810qfilhe/image_1cafn516r1p68kt31g7r196rcsv2n.png" alt="image_1cafn516r1p68kt31g7r196rcsv2n.png-40.1kB"><br>在submitJobAndWait方法中，其首先会创建一个JobClientActor的ActorRef,然后向其发起一个SubmitJobAndWait消息，该消息将JobGraph的实例提交给JobClientActor。发起模式是ask，它表示需要一个应答消息。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Future&lt;Object&gt; future &#x3D; Patterns.ask(jobClientActor, new JobClientMessages.SubmitJobAndWait(jobGraph), new Timeout(AkkaUtils.INF_TIMEOUT()));</span><br><span class="line">answer &#x3D; Await.result(future, AkkaUtils.INF_TIMEOUT());</span><br></pre></td></tr></table></figure>
<p>该SubmitJobAndWait消息被JobClientActor接收后，最终通过调用tryToSubmitJob方法触发真正的提交动作。当JobManager的actor接收到来自client端的请求后，会执行一个submitJob方法，主要做以下事情：</p>
<blockquote>
<ul>
<li>向BlobLibraryCacheManager注册该Job；</li>
<li>构建ExecutionGraph对象；</li>
<li>对JobGraph中的每个顶点进行初始化；</li>
<li>将DAG拓扑中从source开始排序，排序后的顶点集合附加到Exec&gt; - utionGraph对象；</li>
<li>获取检查点相关的配置，并将其设置到ExecutionGraph对象；</li>
<li>向ExecutionGraph注册相关的listener；</li>
<li>执行恢复操作或者将JobGraph信息写入SubmittedJobGraphStore以在后续用于恢复目的；</li>
<li>响应给客户端JobSubmitSuccess消息；</li>
<li>对ExecutionGraph对象进行调度执行；</li>
</ul>
</blockquote>
<p>最后，JobManger会返回消息给JobClient，通知该任务是否提交成功。</p>
<h3 id="2-4-ExecutionGraph的生成"><a href="#2-4-ExecutionGraph的生成" class="headerlink" title="2.4 ExecutionGraph的生成"></a>2.4 ExecutionGraph的生成</h3><p>与StreamGraph和JobGraph不同，ExecutionGraph并不是在我们的客户端程序生成，而是在服务端（JobManager处）生成的，顺便flink只维护一个JobManager。其入口代码是<code>ExecutionGraphBuilder.buildGraph（...）</code><br>该方法长200多行，其中一大半是checkpoiont的相关逻辑，我们暂且略过，直接看核心方法<code>executionGraph.attachJobGraph(sortedTopology)</code><br>因为ExecutionGraph事实上只是改动了JobGraph的每个节点，而没有对整个拓扑结构进行变动，所以代码里只是挨个遍历jobVertex并进行处理：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line">for (JobVertex jobVertex : topologiallySorted) &#123;</span><br><span class="line"></span><br><span class="line">			if (jobVertex.isInputVertex() &amp;&amp; !jobVertex.isStoppable()) &#123;</span><br><span class="line">				this.isStoppable &#x3D; false;</span><br><span class="line">			&#125;</span><br><span class="line"></span><br><span class="line">			&#x2F;&#x2F;在这里生成ExecutionGraph的每个节点</span><br><span class="line">			&#x2F;&#x2F;首先是进行了一堆赋值，将任务信息交给要生成的图节点，以及设定并行度等等</span><br><span class="line">			&#x2F;&#x2F;然后是创建本节点的IntermediateResult，根据本节点的下游节点的个数确定创建几份</span><br><span class="line">			&#x2F;&#x2F;最后是根据设定好的并行度创建用于执行task的ExecutionVertex</span><br><span class="line">			&#x2F;&#x2F;如果job有设定inputsplit的话，这里还要指定inputsplits</span><br><span class="line">			ExecutionJobVertex ejv &#x3D; new ExecutionJobVertex(</span><br><span class="line">				this,</span><br><span class="line">				jobVertex,</span><br><span class="line">				1,</span><br><span class="line">				rpcCallTimeout,</span><br><span class="line">				globalModVersion,</span><br><span class="line">				createTimestamp);</span><br><span class="line">            </span><br><span class="line">            &#x2F;&#x2F;这里要处理所有的JobEdge</span><br><span class="line">            &#x2F;&#x2F;对每个edge，获取对应的intermediateResult，并记录到本节点的输入上</span><br><span class="line">            &#x2F;&#x2F;最后，把每个ExecutorVertex和对应的IntermediateResult关联起来</span><br><span class="line">			ejv.connectToPredecessors(this.intermediateResults);</span><br><span class="line"></span><br><span class="line">			ExecutionJobVertex previousTask &#x3D; this.tasks.putIfAbsent(jobVertex.getID(), ejv);</span><br><span class="line">			if (previousTask !&#x3D; null) &#123;</span><br><span class="line">				throw new JobException(String.format(&quot;Encountered two job vertices with ID %s : previous&#x3D;[%s] &#x2F; new&#x3D;[%s]&quot;,</span><br><span class="line">						jobVertex.getID(), ejv, previousTask));</span><br><span class="line">			&#125;</span><br><span class="line"></span><br><span class="line">			for (IntermediateResult res : ejv.getProducedDataSets()) &#123;</span><br><span class="line">				IntermediateResult previousDataSet &#x3D; this.intermediateResults.putIfAbsent(res.getId(), res);</span><br><span class="line">				if (previousDataSet !&#x3D; null) &#123;</span><br><span class="line">					throw new JobException(String.format(&quot;Encountered two intermediate data set with ID %s : previous&#x3D;[%s] &#x2F; new&#x3D;[%s]&quot;,</span><br><span class="line">							res.getId(), res, previousDataSet));</span><br><span class="line">				&#125;</span><br><span class="line">			&#125;</span><br><span class="line"></span><br><span class="line">			this.verticesInCreationOrder.add(ejv);</span><br><span class="line">			this.numVerticesTotal +&#x3D; ejv.getParallelism();</span><br><span class="line">			newExecJobVertices.add(ejv);</span><br><span class="line">		&#125;</span><br></pre></td></tr></table></figure>
<p>至此，ExecutorGraph就创建完成了。</p>
<h2 id="3-任务的调度与执行"><a href="#3-任务的调度与执行" class="headerlink" title="3. 任务的调度与执行"></a>3. 任务的调度与执行</h2><p>关于flink的任务执行架构，官网的这两张图就是最好的说明：<br><img src="http://static.zybuluo.com/bethunebtj/qiv2wip1rok62ljo0tef3qf0/image_1cafnu1pl1d8c15m219b8vkb2334.png" alt="image_1cafnu1pl1d8c15m219b8vkb2334.png-112.9kB"><br>Flink 集群启动后，首先会启动一个 JobManger 和多个的 TaskManager。用户的代码会由JobClient 提交给 JobManager，JobManager 再把来自不同用户的任务发给 不同的TaskManager 去执行，每个TaskManager管理着多个task，task是执行计算的最小结构， TaskManager 将心跳和统计信息汇报给 JobManager。TaskManager 之间以流的形式进行数据的传输。上述除了task外的三者均为独立的 JVM 进程。<br>要注意的是，TaskManager和job并非一一对应的关系。flink调度的最小单元是task而非TaskManager，也就是说，来自不同job的不同task可能运行于同一个TaskManager的不同线程上。<br><img src="http://static.zybuluo.com/bethunebtj/b7cmjn41b1zp5sco34kgvusn/image_1cclle7ui2j41nf611gs1is18m19.png" alt="image_1cclle7ui2j41nf611gs1is18m19.png-127.5kB"><br>一个flink任务所有可能的状态如上图所示。图上画的很明白，就不再赘述了。</p>
<h3 id="3-1-计算资源的调度"><a href="#3-1-计算资源的调度" class="headerlink" title="3.1 计算资源的调度"></a>3.1 计算资源的调度</h3><p>Task slot是一个TaskManager内资源分配的最小载体，代表了一个固定大小的资源子集，每个TaskManager会将其所占有的资源平分给它的slot。<br>通过调整 task slot 的数量，用户可以定义task之间是如何相互隔离的。每个 TaskManager 有一个slot，也就意味着每个task运行在独立的 JVM 中。每个 TaskManager 有多个slot的话，也就是说多个task运行在同一个JVM中。<br>而在同一个JVM进程中的task，可以共享TCP连接（基于多路复用）和心跳消息，可以减少数据的网络传输，也能共享一些数据结构，一定程度上减少了每个task的消耗。<br>每个slot可以接受单个task，也可以接受多个连续task组成的pipeline，如下图所示，FlatMap函数占用一个taskslot，而key Agg函数和sink函数共用一个taskslot：<br><img src="http://static.zybuluo.com/bethunebtj/6ypu9v09z0mit936uk0mcddi/image_1cafpf21c1jh3s5ap1fisu4v23h.png" alt="image_1cafpf21c1jh3s5ap1fisu4v23h.png-44.7kB"><br>为了达到共用slot的目的，除了可以以chain的方式pipeline算子，我们还可以允许SlotSharingGroup，如下图所示：<br><img src="http://static.zybuluo.com/bethunebtj/tgamd7vw9qcdttvihlmvhie9/image_1cafpko68b3r1lk0dpsnmbj3c3u.png" alt="image_1cafpko68b3r1lk0dpsnmbj3c3u.png-61.2kB"><br>我们可以把不能被chain成一条的两个操作如flatmap和key&amp;sink放在一个TaskSlot里执行，这样做可以获得以下好处：</p>
<ul>
<li>共用slot使得我们不再需要计算每个任务需要的总task数目，直接取最高算子的并行度即可</li>
<li>对计算资源的利用率更高。例如，通常的轻量级操作map和重量级操作Aggregate不再分别需要一个线程，而是可以在同一个线程内执行，而且对于slot有限的场景，我们可以增大每个task的并行度了。<br>接下来我们还是用官网的图来说明flink是如何重用slot的：<br><img src="http://static.zybuluo.com/bethunebtj/l0n9ny2y198x0daucmyo0zb4/image_1cafqroarkjkuje1hfi18gor654b.png" alt="image_1cafqroarkjkuje1hfi18gor654b.png-137kB"></li>
</ul>
<ol>
<li>TaskManager1分配一个SharedSlot0</li>
<li>把source task放入一个SimpleSlot0，再把该slot放入SharedSlot0</li>
<li>把flatmap task放入一个SimpleSlot1，再把该slot放入SharedSlot0</li>
<li>因为我们的flatmap task并行度是2，因此不能再放入SharedSlot0，所以向TaskMange21申请了一个新的SharedSlot0</li>
<li>把第二个flatmap task放进一个新的SimpleSlot，并放进TaskManager2的SharedSlot0</li>
<li>开始处理key&amp;sink task，因为其并行度也是2，所以先把第一个task放进TaskManager1的SharedSlot</li>
<li>把第二个key&amp;sink放进TaskManager2的SharedSlot</li>
</ol>
<h3 id="3-2-JobManager执行job"><a href="#3-2-JobManager执行job" class="headerlink" title="3.2 JobManager执行job"></a>3.2 JobManager执行job</h3><p>JobManager负责接收 flink 的作业，调度 task，收集 job 的状态、管理 TaskManagers。被实现为一个 akka actor。</p>
<h4 id="3-2-1-JobManager的组件"><a href="#3-2-1-JobManager的组件" class="headerlink" title="3.2.1 JobManager的组件"></a>3.2.1 JobManager的组件</h4><ul>
<li>BlobServer 是一个用来管理二进制大文件的服务，比如保存用户上传的jar文件，该服务会将其写到磁盘上。还有一些相关的类，如BlobCache，用于TaskManager向JobManager下载用户的jar文件</li>
<li>InstanceManager 用来管理当前存活的TaskManager的组件，记录了TaskManager的心跳信息等</li>
<li>CompletedCheckpointStore 用于保存已完成的checkpoint相关信息，持久化到内存中或者zookeeper上</li>
<li>MemoryArchivist 保存了已经提交到flink的作业的相关信息，如JobGraph等</li>
</ul>
<h4 id="3-2-2-JobManager的启动过程"><a href="#3-2-2-JobManager的启动过程" class="headerlink" title="3.2.2 JobManager的启动过程"></a>3.2.2 JobManager的启动过程</h4><p>先列出JobManager启动的核心代码</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line">def runJobManager(</span><br><span class="line">      configuration: Configuration,</span><br><span class="line">      executionMode: JobManagerMode,</span><br><span class="line">      listeningAddress: String,</span><br><span class="line">      listeningPort: Int)</span><br><span class="line">    : Unit &#x3D; &#123;</span><br><span class="line"></span><br><span class="line">    val numberProcessors &#x3D; Hardware.getNumberCPUCores()</span><br><span class="line"></span><br><span class="line">    val futureExecutor &#x3D; Executors.newScheduledThreadPool(</span><br><span class="line">      numberProcessors,</span><br><span class="line">      new ExecutorThreadFactory(&quot;jobmanager-future&quot;))</span><br><span class="line"></span><br><span class="line">    val ioExecutor &#x3D; Executors.newFixedThreadPool(</span><br><span class="line">      numberProcessors,</span><br><span class="line">      new ExecutorThreadFactory(&quot;jobmanager-io&quot;))</span><br><span class="line"></span><br><span class="line">    val timeout &#x3D; AkkaUtils.getTimeout(configuration)</span><br><span class="line"></span><br><span class="line">    &#x2F;&#x2F; we have to first start the JobManager ActorSystem because this determines the port if 0</span><br><span class="line">    &#x2F;&#x2F; was chosen before. The method startActorSystem will update the configuration correspondingly.</span><br><span class="line">    val jobManagerSystem &#x3D; startActorSystem(</span><br><span class="line">      configuration,</span><br><span class="line">      listeningAddress,</span><br><span class="line">      listeningPort)</span><br><span class="line"></span><br><span class="line">    val highAvailabilityServices &#x3D; HighAvailabilityServicesUtils.createHighAvailabilityServices(</span><br><span class="line">      configuration,</span><br><span class="line">      ioExecutor,</span><br><span class="line">      AddressResolution.NO_ADDRESS_RESOLUTION)</span><br><span class="line"></span><br><span class="line">    val metricRegistry &#x3D; new MetricRegistryImpl(</span><br><span class="line">      MetricRegistryConfiguration.fromConfiguration(configuration))</span><br><span class="line"></span><br><span class="line">    metricRegistry.startQueryService(jobManagerSystem, null)</span><br><span class="line"></span><br><span class="line">    val (_, _, webMonitorOption, _) &#x3D; try &#123;</span><br><span class="line">      startJobManagerActors(</span><br><span class="line">        jobManagerSystem,</span><br><span class="line">        configuration,</span><br><span class="line">        executionMode,</span><br><span class="line">        listeningAddress,</span><br><span class="line">        futureExecutor,</span><br><span class="line">        ioExecutor,</span><br><span class="line">        highAvailabilityServices,</span><br><span class="line">        metricRegistry,</span><br><span class="line">        classOf[JobManager],</span><br><span class="line">        classOf[MemoryArchivist],</span><br><span class="line">        Option(classOf[StandaloneResourceManager])</span><br><span class="line">      )</span><br><span class="line">    &#125; catch &#123;</span><br><span class="line">      case t: Throwable &#x3D;&gt;</span><br><span class="line">        futureExecutor.shutdownNow()</span><br><span class="line">        ioExecutor.shutdownNow()</span><br><span class="line"></span><br><span class="line">        throw t</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    &#x2F;&#x2F; block until everything is shut down</span><br><span class="line">    jobManagerSystem.awaitTermination()</span><br><span class="line">    </span><br><span class="line">    .......</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ul>
<li>配置Akka并生成ActorSystem，启动JobManager</li>
<li>启动HA和metric相关服务</li>
<li>在<code>startJobManagerActors()</code>方法中启动JobManagerActors，以及webserver，TaskManagerActor，ResourceManager等等</li>
<li>阻塞等待终止</li>
<li>集群通过LeaderService等选出JobManager的leader</li>
</ul>
<h4 id="3-2-3-JobManager启动Task"><a href="#3-2-3-JobManager启动Task" class="headerlink" title="3.2.3 JobManager启动Task"></a>3.2.3 JobManager启动Task</h4><p>JobManager 是一个Actor，通过各种消息来完成核心逻辑：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">override def handleMessage: Receive &#x3D; &#123;</span><br><span class="line">  case GrantLeadership(newLeaderSessionID) &#x3D;&gt;</span><br><span class="line">    log.info(s&quot;JobManager $getAddress was granted leadership with leader session ID &quot; +</span><br><span class="line">      s&quot;$newLeaderSessionID.&quot;)</span><br><span class="line">    leaderSessionID &#x3D; newLeaderSessionID</span><br><span class="line">    </span><br><span class="line">    .......</span><br></pre></td></tr></table></figure>
<p>有几个比较重要的消息：</p>
<ul>
<li>GrantLeadership 获得leader授权，将自身被分发到的 session id 写到 zookeeper，并恢复所有的 jobs</li>
<li>RevokeLeadership 剥夺leader授权，打断清空所有的 job 信息，但是保留作业缓存，注销所有的 TaskManagers</li>
<li>RegisterTaskManagers 注册 TaskManager，如果之前已经注册过，则只给对应的 Instance 发送消息，否则启动注册逻辑：在 InstanceManager 中注册该 Instance 的信息，并停止 Instance BlobLibraryCacheManager 的端口【供下载 lib 包用】，同时使用 watch 监听 task manager 的存活</li>
<li>SubmitJob 提交 jobGraph<br>最后一项SubmintJob就是我们要关注的，从客户端收到JobGraph，转换为ExecutionGraph并执行的过程。<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">private def submitJob(jobGraph: JobGraph, jobInfo: JobInfo, isRecovery: Boolean &#x3D; false): Unit &#x3D; &#123;</span><br><span class="line">    </span><br><span class="line">    ......</span><br><span class="line">    </span><br><span class="line">    executionGraph &#x3D; ExecutionGraphBuilder.buildGraph(</span><br><span class="line">          executionGraph,</span><br><span class="line">          jobGraph,</span><br><span class="line">          flinkConfiguration,</span><br><span class="line">          futureExecutor,</span><br><span class="line">          ioExecutor,</span><br><span class="line">          scheduler,</span><br><span class="line">          userCodeLoader,</span><br><span class="line">          checkpointRecoveryFactory,</span><br><span class="line">          Time.of(timeout.length, timeout.unit),</span><br><span class="line">          restartStrategy,</span><br><span class="line">          jobMetrics,</span><br><span class="line">          numSlots,</span><br><span class="line">          blobServer,</span><br><span class="line">          log.logger)</span><br><span class="line">          </span><br><span class="line">    ......</span><br><span class="line">    </span><br><span class="line">    if (leaderElectionService.hasLeadership) &#123;</span><br><span class="line">            log.info(s&quot;Scheduling job $jobId ($jobName).&quot;)</span><br><span class="line">            </span><br><span class="line">            executionGraph.scheduleForExecution()</span><br><span class="line">            </span><br><span class="line">          &#125; else &#123;</span><br><span class="line">            self ! decorateMessage(RemoveJob(jobId, removeJobFromStateBackend &#x3D; false))</span><br><span class="line"></span><br><span class="line">            log.warn(s&quot;Submitted job $jobId, but not leader. The other leader needs to recover &quot; +</span><br><span class="line">              &quot;this. I am not scheduling the job for execution.&quot;)</span><br><span class="line">    </span><br><span class="line">    ......</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
首先做一些准备工作，然后获取一个ExecutionGraph，判断是否是恢复的job，然后将job保存下来，并且通知客户端本地已经提交成功了，最后如果确认本JobManager是leader，则执行<code>executionGraph.scheduleForExecution()</code>方法，这个方法经过一系列调用，把每个ExecutionVertex传递给了Excution类的deploy方法：<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">public void deploy() throws JobException &#123;</span><br><span class="line"></span><br><span class="line">        ......</span><br><span class="line"></span><br><span class="line">		try &#123;</span><br><span class="line">			&#x2F;&#x2F; good, we are allowed to deploy</span><br><span class="line">			if (!slot.setExecutedVertex(this)) &#123;</span><br><span class="line">				throw new JobException(&quot;Could not assign the ExecutionVertex to the slot &quot; + slot);</span><br><span class="line">			&#125;</span><br><span class="line"></span><br><span class="line">			&#x2F;&#x2F; race double check, did we fail&#x2F;cancel and do we need to release the slot?</span><br><span class="line">			if (this.state !&#x3D; DEPLOYING) &#123;</span><br><span class="line">				slot.releaseSlot();</span><br><span class="line">				return;</span><br><span class="line">			&#125;</span><br><span class="line"></span><br><span class="line">			if (LOG.isInfoEnabled()) &#123;</span><br><span class="line">				LOG.info(String.format(&quot;Deploying %s (attempt #%d) to %s&quot;, vertex.getTaskNameWithSubtaskIndex(),</span><br><span class="line">						attemptNumber, getAssignedResourceLocation().getHostname()));</span><br><span class="line">			&#125;</span><br><span class="line"></span><br><span class="line">			final TaskDeploymentDescriptor deployment &#x3D; vertex.createDeploymentDescriptor(</span><br><span class="line">				attemptId,</span><br><span class="line">				slot,</span><br><span class="line">				taskState,</span><br><span class="line">				attemptNumber);</span><br><span class="line"></span><br><span class="line">			final TaskManagerGateway taskManagerGateway &#x3D; slot.getTaskManagerGateway();</span><br><span class="line"></span><br><span class="line">			final CompletableFuture&lt;Acknowledge&gt; submitResultFuture &#x3D; taskManagerGateway.submitTask(deployment, timeout);</span><br><span class="line"></span><br><span class="line">            ......</span><br><span class="line">		&#125;</span><br><span class="line">		catch (Throwable t) &#123;</span><br><span class="line">			markFailed(t);</span><br><span class="line">			ExceptionUtils.rethrow(t);</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br></pre></td></tr></table></figure>
我们首先生成了一个TaskDeploymentDescriptor，然后交给了<code>taskManagerGateway.submitTask()</code>方法执行。接下来的部分，就属于TaskManager的范畴了。<h3 id="3-3-TaskManager执行task"><a href="#3-3-TaskManager执行task" class="headerlink" title="3.3 TaskManager执行task"></a>3.3 TaskManager执行task</h3><h4 id="3-3-1-TaskManager的基本组件"><a href="#3-3-1-TaskManager的基本组件" class="headerlink" title="3.3.1 TaskManager的基本组件"></a>3.3.1 TaskManager的基本组件</h4>TaskManager是flink中资源管理的基本组件，是所有执行任务的基本容器，提供了内存管理、IO管理、通信管理等一系列功能，本节对各个模块进行简要介绍。</li>
</ul>
<ol>
<li>MemoryManager flink并没有把所有内存的管理都委托给JVM，因为JVM普遍存在着存储对象密度低、大内存时GC对系统影响大等问题。所以flink自己抽象了一套内存管理机制，将所有对象序列化后放在自己的MemorySegment上进行管理。MemoryManger涉及内容较多，将在后续章节进行继续剖析。</li>
<li>IOManager flink通过IOManager管理磁盘IO的过程，提供了同步和异步两种写模式，又进一步区分了block、buffer和bulk三种读写方式。<br>IOManager提供了两种方式枚举磁盘文件，一种是直接遍历文件夹下所有文件，另一种是计数器方式，对每个文件名以递增顺序访问。<br>在底层，flink将文件IO抽象为FileIOChannle，封装了底层实现。<br><img src="http://static.zybuluo.com/bethunebtj/d3j6qnbjywjzknu6pb3pou6i/image_1cag7idg4vfj1l871n0l1k0e1f7u4o.png" alt="image_1cag7idg4vfj1l871n0l1k0e1f7u4o.png-194.1kB"><br>可以看到，flink在底层实际上都是以异步的方式进行读写。</li>
<li>NetworkEnvironment 是TaskManager的网络 IO 组件，包含了追踪中间结果和数据交换的数据结构。它的构造器会统一将配置的内存先分配出来，抽象成 NetworkBufferPool 统一管理内存的申请和释放。意思是说，在输入和输出数据时，不管是保留在本地内存，等待chain在一起的下个操作符进行处理，还是通过网络把本操作符的计算结果发送出去，都被抽象成了NetworkBufferPool。后续我们还将对这个组件进行详细分析。</li>
</ol>
<h4 id="3-3-2-TaskManager执行Task"><a href="#3-3-2-TaskManager执行Task" class="headerlink" title="3.3.2 TaskManager执行Task"></a>3.3.2 TaskManager执行Task</h4><p>对于TM来说，执行task就是把收到的<code>TaskDeploymentDescriptor</code>对象转换成一个task并执行的过程。TaskDeploymentDescriptor这个类保存了task执行所必须的所有内容，例如序列化的算子，输入的InputGate和输出的ResultPartition的定义，该task要作为几个subtask执行等等。<br>按照正常逻辑思维，很容易想到TM的submitTask方法的行为：首先是确认资源，如寻找JobManager和Blob，而后建立连接，解序列化算子，收集task相关信息，接下来就是创建一个新的<code>Task</code>对象，这个task对象就是真正执行任务的关键所在。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">val task &#x3D; new Task(</span><br><span class="line">        jobInformation,</span><br><span class="line">        taskInformation,</span><br><span class="line">        tdd.getExecutionAttemptId,</span><br><span class="line">        tdd.getAllocationId,</span><br><span class="line">        tdd.getSubtaskIndex,</span><br><span class="line">        tdd.getAttemptNumber,</span><br><span class="line">        tdd.getProducedPartitions,</span><br><span class="line">        tdd.getInputGates,</span><br><span class="line">        tdd.getTargetSlotNumber,</span><br><span class="line">        tdd.getTaskStateHandles,</span><br><span class="line">        memoryManager,</span><br><span class="line">        ioManager,</span><br><span class="line">        network,</span><br><span class="line">        bcVarManager,</span><br><span class="line">        taskManagerConnection,</span><br><span class="line">        inputSplitProvider,</span><br><span class="line">        checkpointResponder,</span><br><span class="line">        blobCache,</span><br><span class="line">        libCache,</span><br><span class="line">        fileCache,</span><br><span class="line">        config,</span><br><span class="line">        taskMetricGroup,</span><br><span class="line">        resultPartitionConsumableNotifier,</span><br><span class="line">        partitionStateChecker,</span><br><span class="line">        context.dispatcher)</span><br></pre></td></tr></table></figure>
<p>如果读者是从头开始看这篇blog，里面有很多对象应该已经比较明确其作用了（除了那个brVarManager，这个是管理广播变量的，广播变量是一类会被分发到每个任务中的共享变量）。接下来的主要任务，就是把这个task启动起来,然后报告说已经启动task了：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F; all good, we kick off the task, which performs its own initialization</span><br><span class="line">task.startTaskThread()</span><br><span class="line"></span><br><span class="line">sender ! decorateMessage(Acknowledge.get())</span><br></pre></td></tr></table></figure>
<h4 id="3-3-2-1-生成Task对象"><a href="#3-3-2-1-生成Task对象" class="headerlink" title="3.3.2.1 生成Task对象"></a>3.3.2.1 生成Task对象</h4><p>在执行new Task()方法时，第一步是把构造函数里的这些变量赋值给当前task的fields。<br>接下来是初始化ResultPartition和InputGate。这两个类描述了task的输出数据和输入数据。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line">for (ResultPartitionDeploymentDescriptor desc: resultPartitionDeploymentDescriptors) &#123;</span><br><span class="line">	ResultPartitionID partitionId &#x3D; new ResultPartitionID(desc.getPartitionId(), executionId);</span><br><span class="line"></span><br><span class="line">	this.producedPartitions[counter] &#x3D; new ResultPartition(</span><br><span class="line">	    taskNameWithSubtaskAndId,</span><br><span class="line">		this,</span><br><span class="line">		jobId,</span><br><span class="line">		partitionId,</span><br><span class="line">		desc.getPartitionType(),</span><br><span class="line">		desc.getNumberOfSubpartitions(),</span><br><span class="line">		desc.getMaxParallelism(),</span><br><span class="line">		networkEnvironment.getResultPartitionManager(),</span><br><span class="line">		resultPartitionConsumableNotifier,</span><br><span class="line">		ioManager,</span><br><span class="line">		desc.sendScheduleOrUpdateConsumersMessage());		</span><br><span class="line">	&#x2F;&#x2F;为每个partition初始化对应的writer </span><br><span class="line">	writers[counter] &#x3D; new ResultPartitionWriter(producedPartitions[counter]);</span><br><span class="line"></span><br><span class="line">	++counter;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; Consumed intermediate result partitions</span><br><span class="line">this.inputGates &#x3D; new SingleInputGate[inputGateDeploymentDescriptors.size()];</span><br><span class="line">this.inputGatesById &#x3D; new HashMap&lt;&gt;();</span><br><span class="line"></span><br><span class="line">counter &#x3D; 0;</span><br><span class="line"></span><br><span class="line">for (InputGateDeploymentDescriptor inputGateDeploymentDescriptor: inputGateDeploymentDescriptors) &#123;</span><br><span class="line">	SingleInputGate gate &#x3D; SingleInputGate.create(</span><br><span class="line">		taskNameWithSubtaskAndId,</span><br><span class="line">		jobId,</span><br><span class="line">		executionId,</span><br><span class="line">		inputGateDeploymentDescriptor,</span><br><span class="line">		networkEnvironment,</span><br><span class="line">		this,</span><br><span class="line">		metricGroup.getIOMetricGroup());</span><br><span class="line"></span><br><span class="line">	inputGates[counter] &#x3D; gate;</span><br><span class="line">	inputGatesById.put(gate.getConsumedResultId(), gate);</span><br><span class="line"></span><br><span class="line">	++counter;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>最后，创建一个Thread对象，并把自己放进该对象，这样在执行时，自己就有了自身的线程的引用。</p>
<h4 id="3-3-2-2-运行Task对象"><a href="#3-3-2-2-运行Task对象" class="headerlink" title="3.3.2.2 运行Task对象"></a>3.3.2.2 运行Task对象</h4><p> Task对象本身就是一个Runable，因此在其run方法里定义了运行逻辑。<br> 第一步是切换Task的状态：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">      while (true) &#123;</span><br><span class="line">	ExecutionState current &#x3D; this.executionState;</span><br><span class="line">	&#x2F;&#x2F;&#x2F;&#x2F;如果当前的执行状态为CREATED，则将其设置为DEPLOYING状态</span><br><span class="line">	if (current &#x3D;&#x3D; ExecutionState.CREATED) &#123;</span><br><span class="line">		if (transitionState(ExecutionState.CREATED, ExecutionState.DEPLOYING)) &#123;</span><br><span class="line">			&#x2F;&#x2F; success, we can start our work</span><br><span class="line">			break;</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">	&#x2F;&#x2F;如果当前执行状态为FAILED，则发出通知并退出run方法</span><br><span class="line">	else if (current &#x3D;&#x3D; ExecutionState.FAILED) &#123;</span><br><span class="line">		&#x2F;&#x2F; we were immediately failed. tell the TaskManager that we reached our final state</span><br><span class="line">		notifyFinalState();</span><br><span class="line">		if (metrics !&#x3D; null) &#123;</span><br><span class="line">			metrics.close();</span><br><span class="line">		&#125;</span><br><span class="line">		return;</span><br><span class="line">	&#125;</span><br><span class="line">	&#x2F;&#x2F;如果当前执行状态为CANCELING，则将其修改为CANCELED状态，并退出run</span><br><span class="line">	else if (current &#x3D;&#x3D; ExecutionState.CANCELING) &#123;</span><br><span class="line">		if (transitionState(ExecutionState.CANCELING, ExecutionState.CANCELED)) &#123;</span><br><span class="line">			&#x2F;&#x2F; we were immediately canceled. tell the TaskManager that we reached our final state</span><br><span class="line">			notifyFinalState();</span><br><span class="line">			if (metrics !&#x3D; null) &#123;</span><br><span class="line">				metrics.close();</span><br><span class="line">			&#125;</span><br><span class="line">			return;</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">	&#x2F;&#x2F;否则说明发生了异常</span><br><span class="line">	else &#123;</span><br><span class="line">		if (metrics !&#x3D; null) &#123;</span><br><span class="line">			metrics.close();</span><br><span class="line">		&#125;</span><br><span class="line">		throw new IllegalStateException(&quot;Invalid state for beginning of operation of task &quot; + this + &#39;.&#39;);</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>其实这里有个值得关注的点，就是flink里大量使用了这种while(true)的写法来修改和检测状态，emmm…<br>接下来，就是导入用户类加载器并加载用户代码。<br>然后，是向网络管理器注册当前任务（flink的各个算子在运行时进行数据交换需要依赖网络管理器），分配一些缓存以保存数据<br>然后，读入指定的缓存文件。<br>然后，再把task创建时传入的那一大堆变量用于创建一个执行环境Envrionment。<br>再然后，对于那些并不是第一次执行的task（比如失败后重启的）要恢复其状态。<br>接下来最重要的是</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">invokable.invoke();</span><br></pre></td></tr></table></figure>
<p>方法。为什么这么说呢，因为这个方法就是用户代码所真正被执行的入口。比如我们写的什么new MapFunction()的逻辑，最终就是在这里被执行的。这里说一下这个invokable，这是一个抽象类，提供了可以被TaskManager执行的对象的基本抽象。<br>这个invokable是在解析JobGraph的时候生成相关信息的，并在此处形成真正可执行的对象</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F; now load the task&#39;s invokable code</span><br><span class="line">&#x2F;&#x2F;通过反射生成对象</span><br><span class="line">invokable &#x3D; loadAndInstantiateInvokable(userCodeClassLoader, nameOfInvokableClass);</span><br></pre></td></tr></table></figure>
<p><img src="http://static.zybuluo.com/bethunebtj/9bemw0us5cocnej8lq4x64rk/image_1cbkaa8r9182i18ct1kfu8g829m9.png" alt="image_1cbkaa8r9182i18ct1kfu8g829m9.png-29.9kB"><br>上图显示了flink提供的可被执行的Task类型。从名字上就可以看出各个task的作用，在此不再赘述。<br>接下来就是invoke方法了，因为我们的wordcount例子用了流式api，在此我们以StreamTask的invoke方法为例进行说明。</p>
<h4 id="3-3-2-3-StreamTask的执行逻辑"><a href="#3-3-2-3-StreamTask的执行逻辑" class="headerlink" title="3.3.2.3 StreamTask的执行逻辑"></a>3.3.2.3 StreamTask的执行逻辑</h4><p>先上部分核心代码：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line">public final void invoke() throws Exception &#123;</span><br><span class="line"></span><br><span class="line">	boolean disposed &#x3D; false;</span><br><span class="line">    try &#123;</span><br><span class="line">			&#x2F;&#x2F; -------- Initialize ---------</span><br><span class="line">			&#x2F;&#x2F;先做一些赋值操作</span><br><span class="line">            ......</span><br><span class="line"></span><br><span class="line">	&#x2F;&#x2F; if the clock is not already set, then assign a default TimeServiceProvider</span><br><span class="line">	&#x2F;&#x2F;处理timer</span><br><span class="line">	if (timerService &#x3D;&#x3D; null) &#123;</span><br><span class="line">		ThreadFactory timerThreadFactory &#x3D;</span><br><span class="line">			new DispatcherThreadFactory(TRIGGER_THREAD_GROUP, &quot;Time Trigger for &quot; + getName());</span><br><span class="line"></span><br><span class="line">		timerService &#x3D; new SystemProcessingTimeService(this, getCheckpointLock(), timerThreadFactory);</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">    &#x2F;&#x2F;把之前JobGraph串起来的chain的信息形成实现</span><br><span class="line">	operatorChain &#x3D; new OperatorChain&lt;&gt;(this);</span><br><span class="line">	headOperator &#x3D; operatorChain.getHeadOperator();</span><br><span class="line"></span><br><span class="line">	&#x2F;&#x2F; task specific initialization</span><br><span class="line">	&#x2F;&#x2F;这个init操作的起名非常诡异，因为这里主要是处理算子采用了自定义的checkpoint检查机制的情况，但是起了一个非常大众脸的名字</span><br><span class="line">	init();</span><br><span class="line"></span><br><span class="line">	&#x2F;&#x2F; save the work of reloading state, etc, if the task is already canceled</span><br><span class="line">	if (canceled) &#123;</span><br><span class="line">		throw new CancelTaskException();</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	&#x2F;&#x2F; -------- Invoke --------</span><br><span class="line">	LOG.debug(&quot;Invoking &#123;&#125;&quot;, getName());</span><br><span class="line">			</span><br><span class="line">	&#x2F;&#x2F; we need to make sure that any triggers scheduled in open() cannot be</span><br><span class="line">	&#x2F;&#x2F; executed before all operators are opened</span><br><span class="line">	synchronized (lock) &#123;</span><br><span class="line"></span><br><span class="line">		&#x2F;&#x2F; both the following operations are protected by the lock</span><br><span class="line">		&#x2F;&#x2F; so that we avoid race conditions in the case that initializeState()</span><br><span class="line">		&#x2F;&#x2F; registers a timer, that fires before the open() is called.</span><br><span class="line"></span><br><span class="line">        &#x2F;&#x2F;初始化操作符状态，主要是一些state啥的</span><br><span class="line">		initializeState();</span><br><span class="line">		&#x2F;&#x2F;对于富操作符，执行其open操作</span><br><span class="line">		openAllOperators();</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	&#x2F;&#x2F; final check to exit early before starting to run</span><br><span class="line">	f (canceled) &#123;</span><br><span class="line">	    throw new CancelTaskException();</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	&#x2F;&#x2F; let the task do its work</span><br><span class="line">	&#x2F;&#x2F;真正开始执行的代码</span><br><span class="line">	isRunning &#x3D; true;</span><br><span class="line">	run();</span><br></pre></td></tr></table></figure>
<p>StreamTask.invoke()方法里，第一个值得一说的是<code>TimerService</code>。Flink在2015年决定向StreamTask类加入timer service的时候解释到：</p>
<blockquote>
<p>This integrates the timer as a service in StreamTask that StreamOperators can use by calling a method on the StreamingRuntimeContext. This also ensures that the timer callbacks can not be called concurrently with other methods on the StreamOperator. This behaviour is ensured by an ITCase.</p>
</blockquote>
<p>第二个要注意的是chain操作。前面提到了，flink会出于优化的角度，把一些算子chain成一个整体的算子作为一个task来执行。比如wordcount例子中，Source和FlatMap算子就被chain在了一起。在进行chain操作的时候，会设定头节点，并且指定输出的RecordWriter。</p>
<p>接下来不出所料仍然是初始化，只不过初始化的对象变成了各个operator。如果是有checkpoint的，那就从state信息里恢复，不然就作为全新的算子处理。从源码中可以看到，flink针对keyed算子和普通算子做了不同的处理。keyed算子在初始化时需要计算出一个group区间，这个区间的值在整个生命周期里都不会再变化，后面key就会根据hash的不同结果，分配到特定的group中去计算。顺便提一句，flink的keyed算子保存的是对每个数据的key的计算方法，而非真实的key，用户需要自己保证对每一行数据提供的keySelector的幂等性。至于为什么要用KeyGroup的设计，这就牵扯到扩容的范畴了，将在后面的章节进行讲述。<br>对于<code>openAllOperators()</code>方法，就是对各种RichOperator执行其open方法，通常可用于在执行计算之前加载资源。<br>最后，run方法千呼万唤始出来，该方法经过一系列跳转，最终调用chain上的第一个算子的run方法。在wordcount的例子中，它最终调用了SocketTextStreamFunction的run，建立socket连接并读入文本。</p>
<h3 id="3-4-StreamTask与StreamOperator"><a href="#3-4-StreamTask与StreamOperator" class="headerlink" title="3.4 StreamTask与StreamOperator"></a>3.4 StreamTask与StreamOperator</h3><p>前面提到，Task对象在执行过程中，把执行的任务交给了StreamTask这个类去执行。在我们的wordcount例子中，实际初始化的是OneInputStreamTask的对象（参考上面的类图）。那么这个对象是如何执行用户的代码的呢？</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">protected void run() throws Exception &#123;</span><br><span class="line">	&#x2F;&#x2F; cache processor reference on the stack, to make the code more JIT friendly</span><br><span class="line">	final StreamInputProcessor&lt;IN&gt; inputProcessor &#x3D; this.inputProcessor;</span><br><span class="line"></span><br><span class="line">	while (running &amp;&amp; inputProcessor.processInput()) &#123;</span><br><span class="line">		&#x2F;&#x2F; all the work happens in the &quot;processInput&quot; method</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>它做的，就是把任务直接交给了InputProcessor去执行processInput方法。这是一个<code>StreamInputProcessor</code>的实例，该processor的任务就是处理输入的数据，包括用户数据、watermark和checkpoint数据等。我们先来看看这个processor是如何产生的：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">public void init() throws Exception &#123;</span><br><span class="line">	StreamConfig configuration &#x3D; getConfiguration();</span><br><span class="line"></span><br><span class="line">	TypeSerializer&lt;IN&gt; inSerializer &#x3D; configuration.getTypeSerializerIn1(getUserCodeClassLoader());</span><br><span class="line">	int numberOfInputs &#x3D; configuration.getNumberOfInputs();</span><br><span class="line"></span><br><span class="line">	if (numberOfInputs &gt; 0) &#123;</span><br><span class="line">		InputGate[] inputGates &#x3D; getEnvironment().getAllInputGates();</span><br><span class="line"></span><br><span class="line">		inputProcessor &#x3D; new StreamInputProcessor&lt;&gt;(</span><br><span class="line">				inputGates,</span><br><span class="line">				inSerializer,</span><br><span class="line">				this,</span><br><span class="line">				configuration.getCheckpointMode(),</span><br><span class="line">				getCheckpointLock(),</span><br><span class="line">				getEnvironment().getIOManager(),</span><br><span class="line">				getEnvironment().getTaskManagerInfo().getConfiguration(),</span><br><span class="line">				getStreamStatusMaintainer(),</span><br><span class="line">				this.headOperator);</span><br><span class="line"></span><br><span class="line">		&#x2F;&#x2F; make sure that stream tasks report their I&#x2F;O statistics</span><br><span class="line">		inputProcessor.setMetricGroup(getEnvironment().getMetricGroup().getIOMetricGroup());</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这是OneInputStreamTask的init方法，从configs里面获取StreamOperator信息，生成自己的inputProcessor。那么inputProcessor是如何处理数据的呢？我们接着跟进源码：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br></pre></td><td class="code"><pre><span class="line">public boolean processInput() throws Exception &#123;</span><br><span class="line">		if (isFinished) &#123;</span><br><span class="line">			return false;</span><br><span class="line">		&#125;</span><br><span class="line">		if (numRecordsIn &#x3D;&#x3D; null) &#123;</span><br><span class="line">			numRecordsIn &#x3D; ((OperatorMetricGroup) streamOperator.getMetricGroup()).getIOMetricGroup().getNumRecordsInCounter();</span><br><span class="line">		&#125;</span><br><span class="line"></span><br><span class="line">        &#x2F;&#x2F;这个while是用来处理单个元素的（不要想当然以为是循环处理元素的）</span><br><span class="line">		while (true) &#123;</span><br><span class="line">		    &#x2F;&#x2F;注意 1在下面</span><br><span class="line">		    &#x2F;&#x2F;2.接下来，会利用这个反序列化器得到下一个数据记录，并进行解析（是用户数据还是watermark等等），然后进行对应的操作</span><br><span class="line">			if (currentRecordDeserializer !&#x3D; null) &#123;</span><br><span class="line">				DeserializationResult result &#x3D; currentRecordDeserializer.getNextRecord(deserializationDelegate);</span><br><span class="line"></span><br><span class="line">				if (result.isBufferConsumed()) &#123;</span><br><span class="line">					currentRecordDeserializer.getCurrentBuffer().recycle();</span><br><span class="line">					currentRecordDeserializer &#x3D; null;</span><br><span class="line">				&#125;</span><br><span class="line"></span><br><span class="line">				if (result.isFullRecord()) &#123;</span><br><span class="line">					StreamElement recordOrMark &#x3D; deserializationDelegate.getInstance();</span><br><span class="line"></span><br><span class="line">                    &#x2F;&#x2F;如果元素是watermark，就准备更新当前channel的watermark值（并不是简单赋值，因为有乱序存在），</span><br><span class="line">					if (recordOrMark.isWatermark()) &#123;</span><br><span class="line">						&#x2F;&#x2F; handle watermark</span><br><span class="line">						statusWatermarkValve.inputWatermark(recordOrMark.asWatermark(), currentChannel);</span><br><span class="line">						continue;</span><br><span class="line">					&#125; else if (recordOrMark.isStreamStatus()) &#123;</span><br><span class="line">					&#x2F;&#x2F;如果元素是status，就进行相应处理。可以看作是一个flag，标志着当前stream接下来即将没有元素输入（idle），或者当前即将由空闲状态转为有元素状态（active）。同时，StreamStatus还对如何处理watermark有影响。通过发送status，上游的operator可以很方便的通知下游当前的数据流的状态。</span><br><span class="line">						&#x2F;&#x2F; handle stream status</span><br><span class="line">						statusWatermarkValve.inputStreamStatus(recordOrMark.asStreamStatus(), currentChannel);</span><br><span class="line">						continue;</span><br><span class="line">					&#125; else if (recordOrMark.isLatencyMarker()) &#123;</span><br><span class="line">					&#x2F;&#x2F;LatencyMarker是用来衡量代码执行时间的。在Source处创建，携带创建时的时间戳，流到Sink时就可以知道经过了多长时间</span><br><span class="line">						&#x2F;&#x2F; handle latency marker</span><br><span class="line">						synchronized (lock) &#123;</span><br><span class="line">							streamOperator.processLatencyMarker(recordOrMark.asLatencyMarker());</span><br><span class="line">						&#125;</span><br><span class="line">						continue;</span><br><span class="line">					&#125; else &#123;</span><br><span class="line">					&#x2F;&#x2F;这里就是真正的，用户的代码即将被执行的地方。从章节1到这里足足用了三万字，有点万里长征的感觉</span><br><span class="line">						&#x2F;&#x2F; now we can do the actual processing</span><br><span class="line">						StreamRecord&lt;IN&gt; record &#x3D; recordOrMark.asRecord();</span><br><span class="line">						synchronized (lock) &#123;</span><br><span class="line">							numRecordsIn.inc();</span><br><span class="line">							streamOperator.setKeyContextElement1(record);</span><br><span class="line">							streamOperator.processElement(record);</span><br><span class="line">						&#125;</span><br><span class="line">						return true;</span><br><span class="line">					&#125;</span><br><span class="line">				&#125;</span><br><span class="line">			&#125;</span><br><span class="line"></span><br><span class="line">            &#x2F;&#x2F;1.程序首先获取下一个buffer</span><br><span class="line">            &#x2F;&#x2F;这一段代码是服务于flink的FaultTorrent机制的，后面我会讲到，这里只需理解到它会尝试获取buffer，然后赋值给当前的反序列化器</span><br><span class="line">			final BufferOrEvent bufferOrEvent &#x3D; barrierHandler.getNextNonBlocked();</span><br><span class="line">			if (bufferOrEvent !&#x3D; null) &#123;</span><br><span class="line">				if (bufferOrEvent.isBuffer()) &#123;</span><br><span class="line">					currentChannel &#x3D; bufferOrEvent.getChannelIndex();</span><br><span class="line">					currentRecordDeserializer &#x3D; recordDeserializers[currentChannel];</span><br><span class="line">					currentRecordDeserializer.setNextBuffer(bufferOrEvent.getBuffer());</span><br><span class="line">				&#125;</span><br><span class="line">				else &#123;</span><br><span class="line">					&#x2F;&#x2F; Event received</span><br><span class="line">					final AbstractEvent event &#x3D; bufferOrEvent.getEvent();</span><br><span class="line">					if (event.getClass() !&#x3D; EndOfPartitionEvent.class) &#123;</span><br><span class="line">						throw new IOException(&quot;Unexpected event: &quot; + event);</span><br><span class="line">					&#125;</span><br><span class="line">				&#125;</span><br><span class="line">			&#125;</span><br><span class="line">			else &#123;</span><br><span class="line">				isFinished &#x3D; true;</span><br><span class="line">				if (!barrierHandler.isEmpty()) &#123;</span><br><span class="line">					throw new IllegalStateException(&quot;Trailing data in checkpoint barrier handler.&quot;);</span><br><span class="line">				&#125;</span><br><span class="line">				return false;</span><br><span class="line">			&#125;</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br></pre></td></tr></table></figure>
<p>到此为止，以上部分就是一个flink程序启动后，到执行用户代码之前，flink框架所做的准备工作。回顾一下：</p>
<ul>
<li>启动一个环境</li>
<li>生成StreamGraph</li>
<li>注册和选举JobManager</li>
<li>在各节点生成TaskManager，并根据JobGraph生成对应的Task</li>
<li>启动各个task，准备执行代码</li>
</ul>
<p>接下来，我们挑几个Operator看看flink是如何抽象这些算子的。</p>
<h2 id="4-StreamOperator的抽象与实现"><a href="#4-StreamOperator的抽象与实现" class="headerlink" title="4. StreamOperator的抽象与实现"></a>4. StreamOperator的抽象与实现</h2><h3 id="4-1-数据源的逻辑——StreamSource与时间模型"><a href="#4-1-数据源的逻辑——StreamSource与时间模型" class="headerlink" title="4.1 数据源的逻辑——StreamSource与时间模型"></a>4.1 数据源的逻辑——StreamSource与时间模型</h3><p>StreamSource抽象了一个数据源，并且指定了一些如何处理数据的模式。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br></pre></td><td class="code"><pre><span class="line">public class StreamSource&lt;OUT, SRC extends SourceFunction&lt;OUT&gt;&gt;</span><br><span class="line">		extends AbstractUdfStreamOperator&lt;OUT, SRC&gt; implements StreamOperator&lt;OUT&gt; &#123;</span><br><span class="line"></span><br><span class="line">    ......</span><br><span class="line"></span><br><span class="line">	public void run(final Object lockingObject, final StreamStatusMaintainer streamStatusMaintainer) throws Exception &#123;</span><br><span class="line">		run(lockingObject, streamStatusMaintainer, output);</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	public void run(final Object lockingObject,</span><br><span class="line">			final StreamStatusMaintainer streamStatusMaintainer,</span><br><span class="line">			final Output&lt;StreamRecord&lt;OUT&gt;&gt; collector) throws Exception &#123;</span><br><span class="line"></span><br><span class="line">		final TimeCharacteristic timeCharacteristic &#x3D; getOperatorConfig().getTimeCharacteristic();</span><br><span class="line"></span><br><span class="line">		LatencyMarksEmitter latencyEmitter &#x3D; null;</span><br><span class="line">		if (getExecutionConfig().isLatencyTrackingEnabled()) &#123;</span><br><span class="line">			latencyEmitter &#x3D; new LatencyMarksEmitter&lt;&gt;(</span><br><span class="line">				getProcessingTimeService(),</span><br><span class="line">				collector,</span><br><span class="line">				getExecutionConfig().getLatencyTrackingInterval(),</span><br><span class="line">				getOperatorConfig().getVertexID(),</span><br><span class="line">				getRuntimeContext().getIndexOfThisSubtask());</span><br><span class="line">		&#125;</span><br><span class="line"></span><br><span class="line">		final long watermarkInterval &#x3D; getRuntimeContext().getExecutionConfig().getAutoWatermarkInterval();</span><br><span class="line"></span><br><span class="line">		this.ctx &#x3D; StreamSourceContexts.getSourceContext(</span><br><span class="line">			timeCharacteristic,</span><br><span class="line">			getProcessingTimeService(),</span><br><span class="line">			lockingObject,</span><br><span class="line">			streamStatusMaintainer,</span><br><span class="line">			collector,</span><br><span class="line">			watermarkInterval,</span><br><span class="line">			-1);</span><br><span class="line"></span><br><span class="line">		try &#123;</span><br><span class="line">			userFunction.run(ctx);</span><br><span class="line"></span><br><span class="line">			&#x2F;&#x2F; if we get here, then the user function either exited after being done (finite source)</span><br><span class="line">			&#x2F;&#x2F; or the function was canceled or stopped. For the finite source case, we should emit</span><br><span class="line">			&#x2F;&#x2F; a final watermark that indicates that we reached the end of event-time</span><br><span class="line">			if (!isCanceledOrStopped()) &#123;</span><br><span class="line">				ctx.emitWatermark(Watermark.MAX_WATERMARK);</span><br><span class="line">			&#125;</span><br><span class="line">		&#125; finally &#123;</span><br><span class="line">			&#x2F;&#x2F; make sure that the context is closed in any case</span><br><span class="line">			ctx.close();</span><br><span class="line">			if (latencyEmitter !&#x3D; null) &#123;</span><br><span class="line">				latencyEmitter.close();</span><br><span class="line">			&#125;</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">    ......</span><br><span class="line"></span><br><span class="line">	private static class LatencyMarksEmitter&lt;OUT&gt; &#123;</span><br><span class="line">		private final ScheduledFuture&lt;?&gt; latencyMarkTimer;</span><br><span class="line"></span><br><span class="line">		public LatencyMarksEmitter(</span><br><span class="line">				final ProcessingTimeService processingTimeService,</span><br><span class="line">				final Output&lt;StreamRecord&lt;OUT&gt;&gt; output,</span><br><span class="line">				long latencyTrackingInterval,</span><br><span class="line">				final int vertexID,</span><br><span class="line">				final int subtaskIndex) &#123;</span><br><span class="line"></span><br><span class="line">			latencyMarkTimer &#x3D; processingTimeService.scheduleAtFixedRate(</span><br><span class="line">				new ProcessingTimeCallback() &#123;</span><br><span class="line">					@Override</span><br><span class="line">					public void onProcessingTime(long timestamp) throws Exception &#123;</span><br><span class="line">						try &#123;</span><br><span class="line">							&#x2F;&#x2F; ProcessingTimeService callbacks are executed under the checkpointing lock</span><br><span class="line">							output.emitLatencyMarker(new LatencyMarker(timestamp, vertexID, subtaskIndex));</span><br><span class="line">						&#125; catch (Throwable t) &#123;</span><br><span class="line">							&#x2F;&#x2F; we catch the Throwables here so that we don&#39;t trigger the processing</span><br><span class="line">							&#x2F;&#x2F; timer services async exception handler</span><br><span class="line">							LOG.warn(&quot;Error while emitting latency marker.&quot;, t);</span><br><span class="line">						&#125;</span><br><span class="line">					&#125;</span><br><span class="line">				&#125;,</span><br><span class="line">				0L,</span><br><span class="line">				latencyTrackingInterval);</span><br><span class="line">		&#125;</span><br><span class="line"></span><br><span class="line">		public void close() &#123;</span><br><span class="line">			latencyMarkTimer.cancel(true);</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>在StreamSource生成上下文之后，接下来就是把上下文交给SourceFunction去执行:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">userFunction.run(ctx);</span><br></pre></td></tr></table></figure>
<p>SourceFunction是对Function的一个抽象，就好像MapFunction，KeyByFunction一样，用户选择实现这些函数，然后flink框架就能利用这些函数进行计算，完成用户逻辑。<br>我们的wordcount程序使用了flink提供的一个<code>SocketTextStreamFunction</code>。我们可以看一下它的实现逻辑，对source如何运行有一个基本的认识：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line">public void run(SourceContext&lt;String&gt; ctx) throws Exception &#123;</span><br><span class="line">		final StringBuilder buffer &#x3D; new StringBuilder();</span><br><span class="line">		long attempt &#x3D; 0;</span><br><span class="line"></span><br><span class="line">		while (isRunning) &#123;</span><br><span class="line"></span><br><span class="line">			try (Socket socket &#x3D; new Socket()) &#123;</span><br><span class="line">				currentSocket &#x3D; socket;</span><br><span class="line"></span><br><span class="line">				LOG.info(&quot;Connecting to server socket &quot; + hostname + &#39;:&#39; + port);</span><br><span class="line">				socket.connect(new InetSocketAddress(hostname, port), CONNECTION_TIMEOUT_TIME);</span><br><span class="line">				BufferedReader reader &#x3D; new BufferedReader(new InputStreamReader(socket.getInputStream()));</span><br><span class="line"></span><br><span class="line">				char[] cbuf &#x3D; new char[8192];</span><br><span class="line">				int bytesRead;</span><br><span class="line">				&#x2F;&#x2F;核心逻辑就是一直读inputSocket,然后交给collect方法</span><br><span class="line">				while (isRunning &amp;&amp; (bytesRead &#x3D; reader.read(cbuf)) !&#x3D; -1) &#123;</span><br><span class="line">					buffer.append(cbuf, 0, bytesRead);</span><br><span class="line">					int delimPos;</span><br><span class="line">					while (buffer.length() &gt;&#x3D; delimiter.length() &amp;&amp; (delimPos &#x3D; buffer.indexOf(delimiter)) !&#x3D; -1) &#123;</span><br><span class="line">						String record &#x3D; buffer.substring(0, delimPos);</span><br><span class="line">						&#x2F;&#x2F; truncate trailing carriage return</span><br><span class="line">						if (delimiter.equals(&quot;\n&quot;) &amp;&amp; record.endsWith(&quot;\r&quot;)) &#123;</span><br><span class="line">							record &#x3D; record.substring(0, record.length() - 1);</span><br><span class="line">						&#125;</span><br><span class="line">						&#x2F;&#x2F;读到数据后，把数据交给collect方法，collect方法负责把数据交到合适的位置（如发布为br变量，或者交给下个operator，或者通过网络发出去）</span><br><span class="line">						ctx.collect(record);</span><br><span class="line">						buffer.delete(0, delimPos + delimiter.length());</span><br><span class="line">					&#125;</span><br><span class="line">				&#125;</span><br><span class="line">			&#125;</span><br><span class="line"></span><br><span class="line">			&#x2F;&#x2F; if we dropped out of this loop due to an EOF, sleep and retry</span><br><span class="line">			if (isRunning) &#123;</span><br><span class="line">				attempt++;</span><br><span class="line">				if (maxNumRetries &#x3D;&#x3D; -1 || attempt &lt; maxNumRetries) &#123;</span><br><span class="line">					LOG.warn(&quot;Lost connection to server socket. Retrying in &quot; + delayBetweenRetries + &quot; msecs...&quot;);</span><br><span class="line">					Thread.sleep(delayBetweenRetries);</span><br><span class="line">				&#125;</span><br><span class="line">				else &#123;</span><br><span class="line">					&#x2F;&#x2F; this should probably be here, but some examples expect simple exists of the stream source</span><br><span class="line">					&#x2F;&#x2F; throw new EOFException(&quot;Reached end of stream and reconnects are not enabled.&quot;);</span><br><span class="line">					break;</span><br><span class="line">				&#125;</span><br><span class="line">			&#125;</span><br><span class="line">		&#125;</span><br><span class="line"></span><br><span class="line">		&#x2F;&#x2F; collect trailing data</span><br><span class="line">		if (buffer.length() &gt; 0) &#123;</span><br><span class="line">			ctx.collect(buffer.toString());</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br></pre></td></tr></table></figure>
<p>整段代码里，只有collect方法有些复杂度，后面我们在讲到flink的对象机制时会结合来讲，此处知道collect方法会收集结果，然后发送给接收者即可。在我们的wordcount里，这个算子的接收者就是被chain在一起的flatmap算子，不记得这个示例程序的话，可以返回第一章去看一下。</p>
<h3 id="4-2-从数据输入到数据处理——OneInputStreamOperator-amp-AbstractUdfStreamOperator"><a href="#4-2-从数据输入到数据处理——OneInputStreamOperator-amp-AbstractUdfStreamOperator" class="headerlink" title="4.2 从数据输入到数据处理——OneInputStreamOperator &amp; AbstractUdfStreamOperator"></a>4.2 从数据输入到数据处理——OneInputStreamOperator &amp; AbstractUdfStreamOperator</h3><p>StreamSource是用来开启整个流的算子，而承接输入数据并进行处理的算子就是OneInputStreamOperator、TwoInputStreamOperator等。<br><img src="http://static.zybuluo.com/bethunebtj/9itne7dj58lkkb4mtrt9c8q5/image_1cdc1tbgs136k1ppf17at14fumjf2d.png" alt="image_1cdc1tbgs136k1ppf17at14fumjf2d.png-126.7kB"><br>整个StreamOperator的继承关系如上图所示（图很大，建议点开放大看）。<br>OneInputStreamOperator这个接口的逻辑很简单：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">public interface OneInputStreamOperator&lt;IN, OUT&gt; extends StreamOperator&lt;OUT&gt; &#123;</span><br><span class="line"></span><br><span class="line">	&#x2F;**</span><br><span class="line">	 * Processes one element that arrived at this operator.</span><br><span class="line">	 * This method is guaranteed to not be called concurrently with other methods of the operator.</span><br><span class="line">	 *&#x2F;</span><br><span class="line">	void processElement(StreamRecord&lt;IN&gt; element) throws Exception;</span><br><span class="line"></span><br><span class="line">	&#x2F;**</span><br><span class="line">	 * Processes a &#123;@link Watermark&#125;.</span><br><span class="line">	 * This method is guaranteed to not be called concurrently with other methods of the operator.</span><br><span class="line">	 *</span><br><span class="line">	 * @see org.apache.flink.streaming.api.watermark.Watermark</span><br><span class="line">	 *&#x2F;</span><br><span class="line">	void processWatermark(Watermark mark) throws Exception;</span><br><span class="line"></span><br><span class="line">	void processLatencyMarker(LatencyMarker latencyMarker) throws Exception;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>而实现了这个接口的StreamFlatMap算子也很简单，没什么可说的：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">public class StreamFlatMap&lt;IN, OUT&gt;</span><br><span class="line">		extends AbstractUdfStreamOperator&lt;OUT, FlatMapFunction&lt;IN, OUT&gt;&gt;</span><br><span class="line">		implements OneInputStreamOperator&lt;IN, OUT&gt; &#123;</span><br><span class="line"></span><br><span class="line">	private static final long serialVersionUID &#x3D; 1L;</span><br><span class="line"></span><br><span class="line">	private transient TimestampedCollector&lt;OUT&gt; collector;</span><br><span class="line"></span><br><span class="line">	public StreamFlatMap(FlatMapFunction&lt;IN, OUT&gt; flatMapper) &#123;</span><br><span class="line">		super(flatMapper);</span><br><span class="line">		chainingStrategy &#x3D; ChainingStrategy.ALWAYS;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	@Override</span><br><span class="line">	public void open() throws Exception &#123;</span><br><span class="line">		super.open();</span><br><span class="line">		collector &#x3D; new TimestampedCollector&lt;&gt;(output);</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	@Override</span><br><span class="line">	public void processElement(StreamRecord&lt;IN&gt; element) throws Exception &#123;</span><br><span class="line">		collector.setTimestamp(element);</span><br><span class="line">		userFunction.flatMap(element.getValue(), collector);</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>从类图里可以看到，flink为我们封装了一个算子的基类<code>AbstractUdfStreamOperator</code>，提供了一些通用功能，比如把context赋给算子，保存快照等等，其中最为大家了解的应该是这两个：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">@Override</span><br><span class="line">public void open() throws Exception &#123;</span><br><span class="line">	super.open();</span><br><span class="line">	FunctionUtils.openFunction(userFunction, new Configuration());</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">@Override</span><br><span class="line">public void close() throws Exception &#123;</span><br><span class="line">	super.close();</span><br><span class="line">	functionsClosed &#x3D; true;</span><br><span class="line">	FunctionUtils.closeFunction(userFunction);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这两个就是flink提供的<code>Rich***Function</code>系列算子的open和close方法被执行的地方。</p>
<h3 id="4-3-StreamSink"><a href="#4-3-StreamSink" class="headerlink" title="4.3 StreamSink"></a>4.3 StreamSink</h3><p>StreamSink着实没什么可说的，逻辑很简单，值得一提的只有两个方法：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">@Override</span><br><span class="line">public void processElement(StreamRecord&lt;IN&gt; element) throws Exception &#123;</span><br><span class="line">	sinkContext.element &#x3D; element;</span><br><span class="line">	userFunction.invoke(element.getValue(), sinkContext);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">@Override</span><br><span class="line">protected void reportOrForwardLatencyMarker(LatencyMarker maker) &#123;</span><br><span class="line">	&#x2F;&#x2F; all operators are tracking latencies</span><br><span class="line">	this.latencyGauge.reportLatency(maker, true);</span><br><span class="line"></span><br><span class="line">	&#x2F;&#x2F; sinks don&#39;t forward latency markers</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>其中，<code>processElement</code> 是继承自StreamOperator的方法。<code>reportOrForwardLatencyMarker</code>是用来计算延迟的，前面提到StreamSource会产生LateMarker，用于记录数据计算时间，就是在这里完成了计算。</p>
<p>算子这部分逻辑相对简单清晰，就讲这么多吧。</p>
<h2 id="5-为执行保驾护航——Fault-Tolerant与保证Exactly-Once语义"><a href="#5-为执行保驾护航——Fault-Tolerant与保证Exactly-Once语义" class="headerlink" title="5. 为执行保驾护航——Fault Tolerant与保证Exactly-Once语义"></a>5. 为执行保驾护航——Fault Tolerant与保证Exactly-Once语义</h2><h3 id="5-1-Fault-Tolerant演进之路"><a href="#5-1-Fault-Tolerant演进之路" class="headerlink" title="5.1 Fault Tolerant演进之路"></a>5.1 Fault Tolerant演进之路</h3><p>对于7×24小时不间断运行的流程序来说，要保证fault tolerant是很难的，这不像是离线任务，如果失败了只需要清空已有结果，重新跑一次就可以了。对于流任务，如果要保证能够重新处理已处理过的数据，就要把数据保存下来；而这就面临着几个问题：比如一是保存多久的数据？二是重复计算的数据应该怎么处理，怎么保证幂等性？<br>对于一个流系统，我们有以下希望：</p>
<ol>
<li>最好能做到exactly-once</li>
<li>处理延迟越低越好</li>
<li>吞吐量越高越好</li>
<li>计算模型应当足够简单易用，又具有足够的表达力</li>
<li>从错误恢复的开销越低越好</li>
<li>足够的流控制能力（背压能力）</li>
</ol>
<h4 id="5-1-1-Storm的Record-acknowledgement模式"><a href="#5-1-1-Storm的Record-acknowledgement模式" class="headerlink" title="5.1.1 Storm的Record acknowledgement模式"></a>5.1.1 Storm的Record acknowledgement模式</h4><p>storm的fault tolerant是这样工作的：每一个被storm的operator处理的数据都会向其上一个operator发送一份应答消息，通知其已被下游处理。storm的源operator保存了所有已发送的消息的每一个下游算子的应答消息，当它收到来自sink的应答时，它就知道该消息已经被完整处理，可以移除了。<br>如果没有收到应答，storm就会重发该消息。显而易见，这是一种at least once的逻辑。另外，这种方式面临着严重的幂等性问题，例如对一个count算子，如果count的下游算子出错，source重发该消息，那么防止该消息被count两遍的逻辑需要程序员自己去实现。最后，这样一种处理方式非常低效，吞吐量很低。</p>
<h4 id="5-1-2-Spark-streaming的micro-batch模式"><a href="#5-1-2-Spark-streaming的micro-batch模式" class="headerlink" title="5.1.2 Spark streaming的micro batch模式"></a>5.1.2 Spark streaming的micro batch模式</h4><p>前面提到，storm的实现方式就注定了与高吞吐量无缘。那么，为了提高吞吐量，把一批数据聚集在一起处理就是很自然的选择。Spark Streaming的实现就是基于这样的思路：<br>我们可以在完全的连续计算与完全的分批计算中间取折中，通过控制每批计算数据的大小来控制延迟与吞吐量的制约，如果想要低延迟，就用小一点的batch，如果想要大吞吐量，就不得不忍受更高的延迟（更久的等待数据到来的时间和更多的计算），如下图所示。<br><img src="http://static.zybuluo.com/bethunebtj/1uwp211uaxpb6nqbztfkh3u1/image_1ceop58ha180p1h3ren58jk15gb9.png" alt="image_1ceop58ha180p1h3ren58jk15gb9.png-105.7kB"><br>以这样的方式，可以在每个batch中做到exactly-once，但是这种方式也有其弊端：<br>首先，batch的方式使得一些需要跨batch的操作变得非常困难，例如session window；用户不得不自己想办法去实现相关逻辑。<br>其次，batch模式很难做好背压。当一个batch因为种种原因处理慢了，那么下一个batch要么不得不容纳更多的新来数据，要么不得不堆积更多的batch，整个任务可能会被拖垮，这是一个非常致命的问题。<br>最后，batch的方式基本意味着其延迟是有比较高的下限的，实时性上不好。</p>
<h4 id="5-1-3-Google-Cloud-Dataflow的事务式模型"><a href="#5-1-3-Google-Cloud-Dataflow的事务式模型" class="headerlink" title="5.1.3 Google Cloud Dataflow的事务式模型"></a>5.1.3 Google Cloud Dataflow的事务式模型</h4><p>我们在传统数据库，如mysql中使用binlog来完成事务，这样的思路也可以被用在实现exactly-once模型中。例如，我们可以log下每个数据元素每一次被处理时的结果和当时所处的操作符的状态。这样，当我们需要fault tolerant时，我们只需要读一下log就可以了。这种模式规避了storm和spark所面临的问题，并且能够很好的实现exactly-once，唯一的弊端是：如何尽可能的减少log的成本？Flink给了我们答案。</p>
<h4 id="5-1-4-Flink的分布式快照机制"><a href="#5-1-4-Flink的分布式快照机制" class="headerlink" title="5.1.4 Flink的分布式快照机制"></a>5.1.4 Flink的分布式快照机制</h4><p> 实现exactly-once的关键是什么？是能够准确的知道和快速记录下来当前的operator的状态、当前正在处理的元素（以及正处在不同算子之间传递的元素）。如果上面这些可以做到，那么fault tolerant无非就是从持久化存储中读取上次记录的这些元信息，并且恢复到程序中。那么Flink是如何实现的呢？</p>
<p>Flink的分布式快照的核心是其轻量级异步分布式快照机制。为了实现这一机制，flink引入了一个概念，叫做Barrier。Barrier是一种标记，它被source产生并且插入到流数据中，被发送到下游节点。当下游节点处理到该barrier标志时，这就意味着在该barrier插入到流数据时，已经进入系统的数据在当前节点已经被处理完毕。<br><img src="http://static.zybuluo.com/bethunebtj/r0h3z9im5o9ijqlvvl7vjgrt/image_1ceos05badva20hb5glen1voqm.png" alt="image_1ceos05badva20hb5glen1voqm.png-15.3kB"></p>
<p>如图所示，每当一个barrier流过一个算子节点时，就说明了在该算子上，可以触发一次检查点，用以保存当前节点的状态和已经处理过的数据，这就是一份快照。（在这里可以联想一下micro-batch，把barrier想象成分割每个batch的逻辑，会好理解一点）这样的方式下，记录快照就像和前面提到的micro-batch一样容易。</p>
<p>与此同时，该算子会向下游发送该barrier。因为数据在算子之间是按顺序发送的，所以当下游节点收到该barrier时，也就意味着同样的一批数据在下游节点上也处理完毕，可以进行一次checkpoint，保存基于该节点的一份快照，快照完成后，会通知JobMananger自己完成了这个快照。这就是分布式快照的基本含义。</p>
<p>再看这张图：<br><img src="http://static.zybuluo.com/bethunebtj/fp1rtm1pjv12lo6nld7bns5j/image_1ceot7q13apu1a04170af7j1jao34.png" alt="image_1ceot7q13apu1a04170af7j1jao34.png-66.6kB"><br>有时，有的算子的上游节点和下游节点都不止一个，应该怎么处理呢？如果有不止一个下游节点，就向每个下游发送barrier。同理，如果有不止一个上游节点，那么就要等到所有上游节点的同一批次的barrier到达之后，才能触发checkpoint。因为每个节点运算速度不同，所以有的上游节点可能已经在发下个barrier周期的数据了，有的上游节点还没发送本次的barrier，这时候，当前算子就要缓存一下提前到来的数据，等比较慢的上游节点发送barrier之后，才能处理下一批数据。</p>
<p>当整个程序的最后一个算子sink都收到了这个barrier，也就意味着这个barrier和上个barrier之间所夹杂的这批元素已经全部落袋为安。这时，最后一个算子通知JobManager整个流程已经完成，而JobManager随后发出通知，要求所有算子删除本次快照内容，以完成清理。这整个部分，就是Flink的<strong>两阶段提交的checkpoint过程</strong>，如下面四幅图所示：<br><img src="http://static.zybuluo.com/bethunebtj/achr7r6gcstodi7m9gc270r5/image_1ceot517e14g31u2u1mnt12o91dkb1g.png" alt="image_1ceot517e14g31u2u1mnt12o91dkb1g.png-175.5kB"></p>
<p><img src="http://static.zybuluo.com/bethunebtj/sibwkuskxs20xjcqkn872xg5/image_1ceot5kqbnik1f2i1dss1q5c1a1t.png" alt="image_1ceot5kqbnik1f2i1dss1q5c1a1t.png-221.3kB"></p>
<p><img src="http://static.zybuluo.com/bethunebtj/0ly9zl3w3twknw7ftalv722a/image_1ceot64dppjtojkq3n1jl5j0h2a.png" alt="image_1ceot64dppjtojkq3n1jl5j0h2a.png-297.8kB"></p>
<p><img src="http://static.zybuluo.com/bethunebtj/b5wrovrsrghkxuumgf6rgabc/image_1ceot6kes56sidn1f2u1voo19kf2n.png" alt="image_1ceot6kes56sidn1f2u1voo19kf2n.png-255.5kB"></p>
<p>总之，通过这种方式，flink实现了我们前面提到的六项对流处理框架的要求：exactly-once、低延迟、高吞吐、易用的模型、方便的恢复机制。</p>
<p>最后，贴一个美团做的flink与storm的性能对比：<a target="_blank" rel="noopener" href="https://tech.meituan.com/Flink_Benchmark.html">flink与storm的性能对比</a></p>
<h3 id="5-2-checkpoint的生命周期"><a href="#5-2-checkpoint的生命周期" class="headerlink" title="5.2 checkpoint的生命周期"></a>5.2 checkpoint的生命周期</h3><p>接下来，我们结合源码来看看flink的checkpoint到底是如何实现其生命周期的：</p>
<blockquote>
<p>由于flink提供的SocketSource并不支持checkpoint，所以这里我以<code>FlinkKafkaConsumer010</code>作为sourceFunction。</p>
</blockquote>
<h4 id="5-2-1-触发checkpoint"><a href="#5-2-1-触发checkpoint" class="headerlink" title="5.2.1 触发checkpoint"></a>5.2.1 触发checkpoint</h4><p>要完成一次checkpoint，第一步必然是发起checkpoint请求。那么，这个请求是哪里发出的，怎么发出的，又由谁控制呢？<br>还记得如果我们要设置checkpoint的话，需要指定checkpoint间隔吧？既然是一个指定间隔触发的功能，那应该会有类似于Scheduler的东西存在，flink里，这个负责触发checkpoint的类是<code>CheckpointCoordinator</code>。</p>
<p>flink在提交job时，会启动这个类的<code>startCheckpointScheduler</code>方法，如下所示</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">public void startCheckpointScheduler() &#123;</span><br><span class="line">	synchronized (lock) &#123;</span><br><span class="line">		if (shutdown) &#123;</span><br><span class="line">			throw new IllegalArgumentException(&quot;Checkpoint coordinator is shut down&quot;);</span><br><span class="line">		&#125;</span><br><span class="line"></span><br><span class="line">		&#x2F;&#x2F; make sure all prior timers are cancelled</span><br><span class="line">		stopCheckpointScheduler();</span><br><span class="line"></span><br><span class="line">		periodicScheduling &#x3D; true;</span><br><span class="line">		currentPeriodicTrigger &#x3D; timer.scheduleAtFixedRate(</span><br><span class="line">				new ScheduledTrigger(), </span><br><span class="line">				baseInterval, baseInterval, TimeUnit.MILLISECONDS);</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">private final class ScheduledTrigger implements Runnable &#123;</span><br><span class="line"></span><br><span class="line">	@Override</span><br><span class="line">	public void run() &#123;</span><br><span class="line">		try &#123;</span><br><span class="line">			triggerCheckpoint(System.currentTimeMillis(), true);</span><br><span class="line">		&#125;</span><br><span class="line">		catch (Exception e) &#123;</span><br><span class="line">			LOG.error(&quot;Exception while triggering checkpoint.&quot;, e);</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>启动之后，就会以设定好的频率调用<code>triggerCheckPoint()</code>方法。这个方法太长，我大概说一下都做了什么：</p>
<ul>
<li>检查符合触发checkpoint的条件，例如如果禁止了周期性的checkpoint，尚未达到触发checkpoint的最小间隔等等，就直接return</li>
<li>检查是否所有需要checkpoint和需要响应checkpoint的ACK（ack涉及到checkpoint的两阶段提交，后面会讲）的task都处于running状态，否则return</li>
<li>如果都符合，那么执行<code>checkpointID = checkpointIdCounter.getAndIncrement();</code>以生成一个新的id，然后生成一个<code>PendingCheckpoint</code>。PendingCheckpoint是一个启动了的checkpoint，但是还没有被确认。等到所有的task都确认了本次checkpoint，那么这个checkpoint对象将转化为一个<code>CompletedCheckpoint</code>。</li>
<li>定义一个超时callback，如果checkpoint执行了很久还没完成，就把它取消</li>
<li>触发MasterHooks，用户可以定义一些额外的操作，用以增强checkpoint的功能（如准备和清理外部资源）</li>
<li>接下来是核心逻辑：</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">  &#x2F;&#x2F; send the messages to the tasks that trigger their checkpoint</span><br><span class="line">for (Execution execution: executions) &#123;</span><br><span class="line">	execution.triggerCheckpoint(checkpointID, timestamp, checkpointOptions);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这里是调用了Execution的triggerCheckpoint方法，一个execution就是一个executionVertex的实际执行者。我们看一下这个方法：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">public void triggerCheckpoint(long checkpointId, long timestamp, CheckpointOptions checkpointOptions) &#123;</span><br><span class="line">	final LogicalSlot slot &#x3D; assignedResource;</span><br><span class="line"></span><br><span class="line">	if (slot !&#x3D; null) &#123;</span><br><span class="line">	&#x2F;&#x2F;TaskManagerGateway是用来跟taskManager进行通信的组件</span><br><span class="line">		final TaskManagerGateway taskManagerGateway &#x3D; slot.getTaskManagerGateway();</span><br><span class="line"></span><br><span class="line">		taskManagerGateway.triggerCheckpoint(attemptId, getVertex().getJobId(), checkpointId, timestamp, checkpointOptions);</span><br><span class="line">	&#125; else &#123;</span><br><span class="line">		LOG.debug(&quot;The execution has no slot assigned. This indicates that the execution is &quot; +</span><br><span class="line">			&quot;no longer running.&quot;);</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>再往下跟就进入了<code>Task</code>类的范畴，我们将在下一小节进行解读。本小节主要讲了<code>CheckpointCoordinator</code>类是如何触发一次checkpoint，从其名字也可以看出来其功能：检查点协调器。</p>
<h4 id="5-2-2-Task层面checkpoint的准备工作"><a href="#5-2-2-Task层面checkpoint的准备工作" class="headerlink" title="5.2.2 Task层面checkpoint的准备工作"></a>5.2.2 Task层面checkpoint的准备工作</h4><p>先说Task类中的部分，该类创建了一个<code>CheckpointMetaData</code>的对象，并且生成了一个Runable匿名类用于执行checkpoint，然后以异步的方式触发了该Runable：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">public void triggerCheckpointBarrier(</span><br><span class="line">		final long checkpointID,</span><br><span class="line">		long checkpointTimestamp,</span><br><span class="line">		final CheckpointOptions checkpointOptions) &#123;</span><br><span class="line"></span><br><span class="line">           ......</span><br><span class="line"></span><br><span class="line">		Runnable runnable &#x3D; new Runnable() &#123;</span><br><span class="line">			@Override</span><br><span class="line">			public void run() &#123;</span><br><span class="line">				&#x2F;&#x2F; set safety net from the task&#39;s context for checkpointing thread</span><br><span class="line">				LOG.debug(&quot;Creating FileSystem stream leak safety net for &#123;&#125;&quot;, Thread.currentThread().getName());</span><br><span class="line">				FileSystemSafetyNet.setSafetyNetCloseableRegistryForThread(safetyNetCloseableRegistry);</span><br><span class="line"></span><br><span class="line">				try &#123;</span><br><span class="line">					boolean success &#x3D; invokable.triggerCheckpoint(checkpointMetaData, checkpointOptions);</span><br><span class="line">					if (!success) &#123;</span><br><span class="line">						checkpointResponder.declineCheckpoint(</span><br><span class="line">								getJobID(), getExecutionId(), checkpointID,</span><br><span class="line">								new CheckpointDeclineTaskNotReadyException(taskName));</span><br><span class="line">					&#125;</span><br><span class="line">				&#125;</span><br><span class="line">				</span><br><span class="line">                   ......</span><br><span class="line">			&#125;</span><br><span class="line">		&#125;;</span><br><span class="line">		executeAsyncCallRunnable(runnable, String.format(&quot;Checkpoint Trigger for %s (%s).&quot;, taskNameWithSubtask, executionId));</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>上面代码里的invokable事实上就是我们的StreamTask了。Task类实际上是将checkpoint委托给了更具体的类去执行，而StreamTask也将委托给更具体的类，直到业务代码。<br>StreamTask是这样实现的：</p>
<ul>
<li>如果task还在运行，那就可以进行checkpoint。方法是先向下游所有出口广播一个Barrier，然后触发本task的State保存。</li>
<li>如果task结束了，那我们就要通知下游取消本次checkpoint，方法是发送一个CancelCheckpointMarker，这是类似于Barrier的另一种消息。</li>
<li>注意，从这里开始，整个执行链路上开始出现Barrier，可以和前面讲Fault Tolerant原理的地方结合看一下。 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">private boolean performCheckpoint(</span><br><span class="line">		CheckpointMetaData checkpointMetaData,</span><br><span class="line">		CheckpointOptions checkpointOptions,</span><br><span class="line">		CheckpointMetrics checkpointMetrics) throws Exception &#123;</span><br><span class="line"></span><br><span class="line">	synchronized (lock) &#123;</span><br><span class="line">		if (isRunning) &#123;</span><br><span class="line">		</span><br><span class="line">			operatorChain.broadcastCheckpointBarrier(</span><br><span class="line">					checkpointMetaData.getCheckpointId(),</span><br><span class="line">					checkpointMetaData.getTimestamp(),</span><br><span class="line">					checkpointOptions);</span><br><span class="line"></span><br><span class="line">			checkpointState(checkpointMetaData, checkpointOptions, checkpointMetrics);</span><br><span class="line">			return true;</span><br><span class="line">		&#125;</span><br><span class="line">		else &#123;</span><br><span class="line"></span><br><span class="line">               ......</span><br><span class="line">               </span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
完成<code>broadcastCheckpointBarrier</code>方法后，在<code>checkpointState()</code>方法中，StreamTask还做了很多别的工作：<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">public void executeCheckpointing() throws Exception &#123;</span><br><span class="line">	</span><br><span class="line">	......</span><br><span class="line"></span><br><span class="line">	try &#123;</span><br><span class="line">	    &#x2F;&#x2F;这里，就是调用StreamOperator进行snapshotState的入口方法</span><br><span class="line">		for (StreamOperator&lt;?&gt; op : allOperators) &#123;</span><br><span class="line">			checkpointStreamOperator(op);</span><br><span class="line">		&#125;</span><br><span class="line"></span><br><span class="line">		&#x2F;&#x2F; we are transferring ownership over snapshotInProgressList for cleanup to the thread, active on submit</span><br><span class="line">		AsyncCheckpointRunnable asyncCheckpointRunnable &#x3D; new AsyncCheckpointRunnable(</span><br><span class="line">			owner,</span><br><span class="line">			operatorSnapshotsInProgress,</span><br><span class="line">			checkpointMetaData,</span><br><span class="line">			checkpointMetrics,</span><br><span class="line">			startAsyncPartNano);</span><br><span class="line"></span><br><span class="line">		owner.cancelables.registerCloseable(asyncCheckpointRunnable);</span><br><span class="line">		&#x2F;&#x2F;这里注册了一个Runnable，在执行完checkpoint之后向JobManager发出CompletedCheckPoint消息，这也是fault tolerant两阶段提交的一部分</span><br><span class="line">		owner.asyncOperationsThreadPool.submit(asyncCheckpointRunnable);</span><br><span class="line">		</span><br><span class="line">		......</span><br><span class="line">	</span><br><span class="line">	&#125; </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
说到checkpoint，我们印象里最直观的感受肯定是我们的一些做聚合的操作符的状态保存，比如sum的和以及count的值等等。这些内容就是StreamOperator部分将要触发保存的内容。可以看到，除了我们直观的这些操作符的状态保存外，flink的checkpoint做了大量的其他工作。</li>
</ul>
<p>接下来，我们就把目光转向操作符的checkpoint机制。</p>
<h4 id="5-2-3-操作符的状态保存及barrier传递"><a href="#5-2-3-操作符的状态保存及barrier传递" class="headerlink" title="5.2.3 操作符的状态保存及barrier传递"></a>5.2.3 操作符的状态保存及barrier传递</h4><p>第四章时，我们已经了解了StreamOperator的类关系，这里，我们就直接接着上一节的<code>checkpointStreamOperator(op)</code>方法往下讲。<br>顺便，前面也提到了，在进行checkpoint之前，operator初始化时，会执行一个<code>initializeState</code>方法，在该方法中，如果task是从失败中恢复的话，其保存的state也会被restore进来。</p>
<p>传递barrier是在进行本operator的statesnapshot之前完成的，我们先来看看其逻辑，其实和传递一条数据是类似的，就是生成一个<code>CheckpointBarrier</code>对象，然后向每个streamOutput写进去：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">   public void broadcastCheckpointBarrier(long id, long timestamp, CheckpointOptions checkpointOptions) throws IOException &#123;</span><br><span class="line">	try &#123;</span><br><span class="line">		CheckpointBarrier barrier &#x3D; new CheckpointBarrier(id, timestamp, checkpointOptions);</span><br><span class="line">		for (RecordWriterOutput&lt;?&gt; streamOutput : streamOutputs) &#123;</span><br><span class="line">			streamOutput.broadcastEvent(barrier);</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">	catch (InterruptedException e) &#123;</span><br><span class="line">		throw new IOException(&quot;Interrupted while broadcasting checkpoint barrier&quot;);</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>下游的operator接收到本barrier，就会触发其自身的checkpoint。</p>
<p>StreamTask在执行完broadcastCheckpointBarrier之后，<br>我们当前的wordcount程序里有两个operator chain，分别是：</p>
<ul>
<li>kafka source -&gt; flatmap</li>
<li>keyed aggregation -&gt; sink</li>
</ul>
<p>我们就按这个顺序来捋一下checkpoint的过程。</p>
<p>1.kafka source的checkpoint过程</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line">public final void snapshotState(FunctionSnapshotContext context) throws Exception &#123;</span><br><span class="line">	if (!running) &#123;</span><br><span class="line">		LOG.debug(&quot;snapshotState() called on closed source&quot;);</span><br><span class="line">	&#125; else &#123;</span><br><span class="line">		unionOffsetStates.clear();</span><br><span class="line"></span><br><span class="line">		final AbstractFetcher&lt;?, ?&gt; fetcher &#x3D; this.kafkaFetcher;</span><br><span class="line">		if (fetcher &#x3D;&#x3D; null) &#123;</span><br><span class="line">			&#x2F;&#x2F; the fetcher has not yet been initialized, which means we need to return the</span><br><span class="line">			&#x2F;&#x2F; originally restored offsets or the assigned partitions</span><br><span class="line">			for (Map.Entry&lt;KafkaTopicPartition, Long&gt; subscribedPartition : subscribedPartitionsToStartOffsets.entrySet()) &#123;</span><br><span class="line">				unionOffsetStates.add(Tuple2.of(subscribedPartition.getKey(), subscribedPartition.getValue()));</span><br><span class="line">			&#125;</span><br><span class="line"></span><br><span class="line">			if (offsetCommitMode &#x3D;&#x3D; OffsetCommitMode.ON_CHECKPOINTS) &#123;</span><br><span class="line">				&#x2F;&#x2F; the map cannot be asynchronously updated, because only one checkpoint call can happen</span><br><span class="line">				&#x2F;&#x2F; on this function at a time: either snapshotState() or notifyCheckpointComplete()</span><br><span class="line">				pendingOffsetsToCommit.put(context.getCheckpointId(), restoredState);</span><br><span class="line">			&#125;</span><br><span class="line">		&#125; else &#123;</span><br><span class="line">			HashMap&lt;KafkaTopicPartition, Long&gt; currentOffsets &#x3D; fetcher.snapshotCurrentState();</span><br><span class="line"></span><br><span class="line">			if (offsetCommitMode &#x3D;&#x3D; OffsetCommitMode.ON_CHECKPOINTS) &#123;</span><br><span class="line">				&#x2F;&#x2F; the map cannot be asynchronously updated, because only one checkpoint call can happen</span><br><span class="line">				&#x2F;&#x2F; on this function at a time: either snapshotState() or notifyCheckpointComplete()</span><br><span class="line">				pendingOffsetsToCommit.put(context.getCheckpointId(), currentOffsets);</span><br><span class="line">			&#125;</span><br><span class="line"></span><br><span class="line">			for (Map.Entry&lt;KafkaTopicPartition, Long&gt; kafkaTopicPartitionLongEntry : currentOffsets.entrySet()) &#123;</span><br><span class="line">				unionOffsetStates.add(</span><br><span class="line">						Tuple2.of(kafkaTopicPartitionLongEntry.getKey(), kafkaTopicPartitionLongEntry.getValue()));</span><br><span class="line">			&#125;</span><br><span class="line">		&#125;</span><br><span class="line"></span><br><span class="line">		if (offsetCommitMode &#x3D;&#x3D; OffsetCommitMode.ON_CHECKPOINTS) &#123;</span><br><span class="line">			&#x2F;&#x2F; truncate the map of pending offsets to commit, to prevent infinite growth</span><br><span class="line">			while (pendingOffsetsToCommit.size() &gt; MAX_NUM_PENDING_CHECKPOINTS) &#123;</span><br><span class="line">				pendingOffsetsToCommit.remove(0);</span><br><span class="line">			&#125;</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>kafka的snapshot逻辑就是记录一下当前消费的offsets，然后做成tuple（partitiion，offset）放进一个<code>StateBackend</code>里。StateBackend是flink抽象出来的一个用于保存状态的接口。</p>
<p>2.<strong>FlatMap算子的checkpoint过程</strong><br>没什么可说的，就是调用了snapshotState()方法而已。</p>
<p>3.<strong>本operator chain的state保存过程</strong><br>细心的同学应该注意到了，各个算子的snapshot方法只把自己的状态保存到了StateBackend里，没有写入的持久化操作。这部分操作被放到了<code>AbstractStreamOperator</code>中，由flink统一负责持久化。其实不需要看源码我们也能想出来，持久化无非就是把这些数据用一个流写到磁盘或者别的地方，接下来我们来看看是不是这样：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">         &#x2F;&#x2F;还是AbstractStreamOperator.java的snapshotState方法</span><br><span class="line">if (null !&#x3D; operatorStateBackend) &#123;</span><br><span class="line">	snapshotInProgress.setOperatorStateManagedFuture(</span><br><span class="line">		operatorStateBackend.snapshot(checkpointId, timestamp, factory, checkpointOptions));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>那么这个operatorStateBackend是怎么保存状态的呢？</p>
<ul>
<li>首先把各个算子的state做了一份深拷贝；</li>
<li>然后以异步的方式执行了一个内部类的runnable，该内部类的run方法实现了一个模版方法，首先打开stream，然后写入数据，然后再关闭stream。</li>
</ul>
<p>我们来看看这个写入数据的方法：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">            public SnapshotResult&lt;OperatorStateHandle&gt; performOperation() throws Exception &#123;</span><br><span class="line">	long asyncStartTime &#x3D; System.currentTimeMillis();</span><br><span class="line"></span><br><span class="line">	CheckpointStreamFactory.CheckpointStateOutputStream localOut &#x3D; this.out;</span><br><span class="line"></span><br><span class="line">	&#x2F;&#x2F; get the registered operator state infos ...</span><br><span class="line">	List&lt;RegisteredOperatorBackendStateMetaInfo.Snapshot&lt;?&gt;&gt; operatorMetaInfoSnapshots &#x3D;</span><br><span class="line">		new ArrayList&lt;&gt;(registeredOperatorStatesDeepCopies.size());</span><br><span class="line"></span><br><span class="line">	for (Map.Entry&lt;String, PartitionableListState&lt;?&gt;&gt; entry : registeredOperatorStatesDeepCopies.entrySet()) &#123;</span><br><span class="line">		operatorMetaInfoSnapshots.add(entry.getValue().getStateMetaInfo().snapshot());</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	&#x2F;&#x2F; ... write them all in the checkpoint stream ...</span><br><span class="line">	DataOutputView dov &#x3D; new DataOutputViewStreamWrapper(localOut);</span><br><span class="line"></span><br><span class="line">	OperatorBackendSerializationProxy backendSerializationProxy &#x3D;</span><br><span class="line">		new OperatorBackendSerializationProxy(operatorMetaInfoSnapshots, broadcastMetaInfoSnapshots);</span><br><span class="line"></span><br><span class="line">	backendSerializationProxy.write(dov);</span><br><span class="line"></span><br><span class="line">    ......</span><br><span class="line">    </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>注释写的很清楚，我就不多说了。</p>
<p>4.<strong>后继operatorChain的checkpoint过程</strong><br>前面说到，在flink的流中，barrier流过时会触发checkpoint。在上面第1步中，上游节点已经发出了Barrier，所以在我们的keyed aggregation -&gt; sink 这个operatorchain中，我们将首先捕获这个barrier。</p>
<p>捕获barrier的过程其实就是处理input数据的过程，对应着<code>StreamInputProcessor.processInput()</code>方法，该方法我们在第四章已经讲过，这里我们简单回顾一下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">         &#x2F;&#x2F;每个元素都会触发这一段逻辑，如果下一个数据是buffer，则从外围的while循环里进入处理用户数据的逻辑；这个方法里默默的处理了barrier的逻辑</span><br><span class="line">         final BufferOrEvent bufferOrEvent &#x3D; barrierHandler.getNextNonBlocked();</span><br><span class="line">if (bufferOrEvent !&#x3D; null) &#123;</span><br><span class="line">	if (bufferOrEvent.isBuffer()) &#123;</span><br><span class="line">		currentChannel &#x3D; bufferOrEvent.getChannelIndex();</span><br><span class="line">		currentRecordDeserializer &#x3D; recordDeserializers[currentChannel];</span><br><span class="line">		currentRecordDeserializer.setNextBuffer(bufferOrEvent.getBuffer());</span><br><span class="line">	&#125;</span><br><span class="line">	else &#123;</span><br><span class="line">		&#x2F;&#x2F; Event received</span><br><span class="line">		final AbstractEvent event &#x3D; bufferOrEvent.getEvent();</span><br><span class="line">		if (event.getClass() !&#x3D; EndOfPartitionEvent.class) &#123;</span><br><span class="line">			throw new IOException(&quot;Unexpected event: &quot; + event);</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>处理barrier的过程在这段代码里没有体现，因为被包含在了<code>getNextNonBlocked()</code>方法中，我们看下这个方法的核心逻辑：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">         &#x2F;&#x2F;BarrierBuffer.getNextNonBlocked方法</span><br><span class="line">else if (bufferOrEvent.getEvent().getClass() &#x3D;&#x3D; CheckpointBarrier.class) &#123;</span><br><span class="line">	if (!endOfStream) &#123;</span><br><span class="line">		&#x2F;&#x2F; process barriers only if there is a chance of the checkpoint completing</span><br><span class="line">		processBarrier((CheckpointBarrier) bufferOrEvent.getEvent(), bufferOrEvent.getChannelIndex());</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br><span class="line">else if (bufferOrEvent.getEvent().getClass() &#x3D;&#x3D; CancelCheckpointMarker.class) &#123;</span><br><span class="line">	processCancellationBarrier((CancelCheckpointMarker) bufferOrEvent.getEvent());</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>先提一嘴，大家还记得之前的部分也提到过CheckpointMarker吧，这里正好也对上了。</p>
<p>处理barrier也是个麻烦事，大家回想一下5.1节提到的屏障的原理图，一个opertor必须收到从每个inputchannel发过来的同一序号的barrier之后才能发起本节点的checkpoint，如果有的channel的数据处理的快了，那该barrier后的数据还需要缓存起来，如果有的inputchannel被关闭了，那它就不会再发送barrier过来了：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line">private void processBarrier(CheckpointBarrier receivedBarrier, int channelIndex) throws Exception &#123;</span><br><span class="line">		final long barrierId &#x3D; receivedBarrier.getId();</span><br><span class="line"></span><br><span class="line">		&#x2F;&#x2F; fast path for single channel cases</span><br><span class="line">		if (totalNumberOfInputChannels &#x3D;&#x3D; 1) &#123;</span><br><span class="line">			if (barrierId &gt; currentCheckpointId) &#123;</span><br><span class="line">				&#x2F;&#x2F; new checkpoint</span><br><span class="line">				currentCheckpointId &#x3D; barrierId;</span><br><span class="line">				notifyCheckpoint(receivedBarrier);</span><br><span class="line">			&#125;</span><br><span class="line">			return;</span><br><span class="line">		&#125;</span><br><span class="line"></span><br><span class="line">		&#x2F;&#x2F; -- general code path for multiple input channels --</span><br><span class="line"></span><br><span class="line">		if (numBarriersReceived &gt; 0) &#123;</span><br><span class="line">			&#x2F;&#x2F; this is only true if some alignment is already progress and was not canceled</span><br><span class="line"></span><br><span class="line">			if (barrierId &#x3D;&#x3D; currentCheckpointId) &#123;</span><br><span class="line">				&#x2F;&#x2F; regular case</span><br><span class="line">				onBarrier(channelIndex);</span><br><span class="line">			&#125;</span><br><span class="line">			else if (barrierId &gt; currentCheckpointId) &#123;</span><br><span class="line">				&#x2F;&#x2F; we did not complete the current checkpoint, another started before</span><br><span class="line">				LOG.warn(&quot;Received checkpoint barrier for checkpoint &#123;&#125; before completing current checkpoint &#123;&#125;. &quot; +</span><br><span class="line">						&quot;Skipping current checkpoint.&quot;, barrierId, currentCheckpointId);</span><br><span class="line"></span><br><span class="line">				&#x2F;&#x2F; let the task know we are not completing this</span><br><span class="line">				notifyAbort(currentCheckpointId, new CheckpointDeclineSubsumedException(barrierId));</span><br><span class="line"></span><br><span class="line">				&#x2F;&#x2F; abort the current checkpoint</span><br><span class="line">				releaseBlocksAndResetBarriers();</span><br><span class="line"></span><br><span class="line">				&#x2F;&#x2F; begin a the new checkpoint</span><br><span class="line">				beginNewAlignment(barrierId, channelIndex);</span><br><span class="line">			&#125;</span><br><span class="line">			else &#123;</span><br><span class="line">				&#x2F;&#x2F; ignore trailing barrier from an earlier checkpoint (obsolete now)</span><br><span class="line">				return;</span><br><span class="line">			&#125;</span><br><span class="line">		&#125;</span><br><span class="line">		else if (barrierId &gt; currentCheckpointId) &#123;</span><br><span class="line">			&#x2F;&#x2F; first barrier of a new checkpoint</span><br><span class="line">			beginNewAlignment(barrierId, channelIndex);</span><br><span class="line">		&#125;</span><br><span class="line">		else &#123;</span><br><span class="line">			&#x2F;&#x2F; either the current checkpoint was canceled (numBarriers &#x3D;&#x3D; 0) or</span><br><span class="line">			&#x2F;&#x2F; this barrier is from an old subsumed checkpoint</span><br><span class="line">			return;</span><br><span class="line">		&#125;</span><br><span class="line"></span><br><span class="line">		&#x2F;&#x2F; check if we have all barriers - since canceled checkpoints always have zero barriers</span><br><span class="line">		&#x2F;&#x2F; this can only happen on a non canceled checkpoint</span><br><span class="line">		if (numBarriersReceived + numClosedChannels &#x3D;&#x3D; totalNumberOfInputChannels) &#123;</span><br><span class="line">			&#x2F;&#x2F; actually trigger checkpoint</span><br><span class="line">			if (LOG.isDebugEnabled()) &#123;</span><br><span class="line">				LOG.debug(&quot;Received all barriers, triggering checkpoint &#123;&#125; at &#123;&#125;&quot;,</span><br><span class="line">						receivedBarrier.getId(), receivedBarrier.getTimestamp());</span><br><span class="line">			&#125;</span><br><span class="line"></span><br><span class="line">			releaseBlocksAndResetBarriers();</span><br><span class="line">			notifyCheckpoint(receivedBarrier);</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br></pre></td></tr></table></figure>
<p>总之，当收到全部的barrier之后，就会触发<code>notifyCheckpoint()</code>，该方法又会调用StreamTask的<code>triggerCheckpoint</code>，和之前的operator是一样的。</p>
<p>如果还有后续的operator的话，就是完全相同的循环，不再赘述。</p>
<p>5.<strong>报告完成checkpoint事件</strong><br>当一个operator保存完checkpoint数据后，就会启动一个异步对象<code>AsyncCheckpointRunnable</code>，用以报告该检查点已完成，其具体逻辑在reportCompletedSnapshotStates中。这个方法把任务又最终委托给了<code>RpcCheckpointResponder</code>这个类：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">checkpointResponder.acknowledgeCheckpoint(</span><br><span class="line">			jobId,</span><br><span class="line">			executionAttemptID,</span><br><span class="line">			checkpointId,</span><br><span class="line">			checkpointMetrics,</span><br><span class="line">			acknowledgedState);</span><br></pre></td></tr></table></figure>
<p>从这个类也可以看出来，它的逻辑是通过rpc的方式远程调JobManager的相关方法完成报告事件，底层也是通过akka实现的。<br>那么，谁响应了这个rpc调用呢？是该任务的JobMaster。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">   &#x2F;&#x2F;JobMaster.java</span><br><span class="line">public void acknowledgeCheckpoint(</span><br><span class="line">		final JobID jobID,</span><br><span class="line">		final ExecutionAttemptID executionAttemptID,</span><br><span class="line">		final long checkpointId,</span><br><span class="line">		final CheckpointMetrics checkpointMetrics,</span><br><span class="line">		final TaskStateSnapshot checkpointState) &#123;</span><br><span class="line"></span><br><span class="line">	final CheckpointCoordinator checkpointCoordinator &#x3D; executionGraph.getCheckpointCoordinator();</span><br><span class="line">	final AcknowledgeCheckpoint ackMessage &#x3D; new AcknowledgeCheckpoint(</span><br><span class="line">		jobID,</span><br><span class="line">		executionAttemptID,</span><br><span class="line">		checkpointId,</span><br><span class="line">		checkpointMetrics,</span><br><span class="line">		checkpointState);</span><br><span class="line"></span><br><span class="line">	if (checkpointCoordinator !&#x3D; null) &#123;</span><br><span class="line">		getRpcService().execute(() -&gt; &#123;</span><br><span class="line">			try &#123;</span><br><span class="line">				checkpointCoordinator.receiveAcknowledgeMessage(ackMessage);</span><br><span class="line">			&#125; catch (Throwable t) &#123;</span><br><span class="line">				log.warn(&quot;Error while processing checkpoint acknowledgement message&quot;);</span><br><span class="line">			&#125;</span><br><span class="line">		&#125;);</span><br><span class="line">	&#125; else &#123;</span><br><span class="line">		log.error(&quot;Received AcknowledgeCheckpoint message for job &#123;&#125; with no CheckpointCoordinator&quot;,</span><br><span class="line">				jobGraph.getJobID());</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>JobMaster反手<del>就是一巴掌</del>就把任务又rpc给了<code>CheckpointCoordinator.receiveAcknowledgeMessage()</code>方法。</p>
<p>之前提到，coordinator在触发checkpoint时，生成了一个<code>PendingCheckpoint</code>，保存了所有operator的id。</p>
<p>当PendingCheckpoint收到一个operator的完成checkpoint的消息时，它就把这个operator从未完成checkpoint的节点集合移动到已完成的集合。当所有的operator都报告完成了checkpoint时，CheckpointCoordinator会触发<code>completePendingCheckpoint()</code>方法，该方法做了以下事情：</p>
<ul>
<li>把pendinCgCheckpoint转换为CompletedCheckpoint</li>
<li>把CompletedCheckpoint加入已完成的检查点集合，并从未完成检查点集合删除该检查点</li>
<li>再度向各个operator发出rpc，通知该检查点已完成</li>
</ul>
<p>本文里，收到这个远程调用的就是那两个operator chain，我们来看看其逻辑:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">public void notifyCheckpointComplete(long checkpointId) throws Exception &#123;</span><br><span class="line">	synchronized (lock) &#123;</span><br><span class="line">		if (isRunning) &#123;</span><br><span class="line">			LOG.debug(&quot;Notification of complete checkpoint for task &#123;&#125;&quot;, getName());</span><br><span class="line"></span><br><span class="line">			for (StreamOperator&lt;?&gt; operator : operatorChain.getAllOperators()) &#123;</span><br><span class="line">				if (operator !&#x3D; null) &#123;</span><br><span class="line">					operator.notifyCheckpointComplete(checkpointId);</span><br><span class="line">				&#125;</span><br><span class="line">			&#125;</span><br><span class="line">		&#125;</span><br><span class="line">		else &#123;</span><br><span class="line">			LOG.debug(&quot;Ignoring notification of complete checkpoint for not-running task &#123;&#125;&quot;, getName());</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>再接下来无非就是层层通知对应的算子做出响应罢了。</p>
<p>至此，flink的两阶段提交的checkpoint逻辑全部完成。</p>
<h3 id="5-3-承载checkpoint数据的抽象：State-amp-StateBackend"><a href="#5-3-承载checkpoint数据的抽象：State-amp-StateBackend" class="headerlink" title="5.3 承载checkpoint数据的抽象：State &amp; StateBackend"></a>5.3 承载checkpoint数据的抽象：State &amp; StateBackend</h3><p>State是快照数据的载体，StateBackend是快照如何被保存的抽象。</p>
<p>State分为 KeyedState和OperatorState，从名字就可以看出来分别对应着keyedStream和其他的oeprator。从State由谁管理上，也可以区分为raw state和Managed state。Flink管理的就是Managed state，用户自己管理的就是raw state。Managed State又分为ValueState、ListState、ReducingState、AggregatingState、FoldingState、MapState这么几种，看名字知用途。</p>
<p>StateBackend目前提供了三个backend，MemoryStateBackend，FsStateBackend，RocksDBStateBackend，都是看名字知用途系列。</p>
<p>State接口、StateBackend接口及其实现都比较简单，代码就不贴了， 尤其State本质上就是一层容器封装。</p>
<p>贴个别人写的状态管理的文章吧：<a target="_blank" rel="noopener" href="https://yq.aliyun.com/articles/225623?spm=a2c4e.11153940.blogcont225624.12.7c797f6bZo3tiM">详解Flink中的状态管理</a></p>
<h2 id="6-数据流转——Flink的数据抽象及数据交换过程"><a href="#6-数据流转——Flink的数据抽象及数据交换过程" class="headerlink" title="6.数据流转——Flink的数据抽象及数据交换过程"></a>6.数据流转——Flink的数据抽象及数据交换过程</h2><p>本章打算讲一下flink底层是如何定义和在操作符之间传递数据的。</p>
<h3 id="6-1-flink的数据抽象"><a href="#6-1-flink的数据抽象" class="headerlink" title="6.1 flink的数据抽象"></a>6.1 flink的数据抽象</h3><h4 id="6-1-1-MemorySegment"><a href="#6-1-1-MemorySegment" class="headerlink" title="6.1.1 MemorySegment"></a>6.1.1 MemorySegment</h4><p>Flink作为一个高效的流框架，为了避免JVM的固有缺陷（java对象存储密度低，FGC影响吞吐和响应等），必然走上自主管理内存的道路。</p>
<p>这个<code>MemorySegment</code>就是Flink的内存抽象。默认情况下，一个MemorySegment可以被看做是一个32kb大的内存块的抽象。这块内存既可以是JVM里的一个byte[]，也可以是堆外内存（DirectByteBuffer）。</p>
<p>如果说byte[]数组和direct memory是最底层的存储，那么memorysegment就是在其上覆盖的一层统一抽象。它定义了一系列抽象方法，用于控制和底层内存的交互，如：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">public abstract class MemorySegment &#123;</span><br><span class="line"></span><br><span class="line">    public abstract byte get(int index);</span><br><span class="line">    </span><br><span class="line">    public abstract void put(int index, byte b);</span><br><span class="line">    </span><br><span class="line">    public int size() ;</span><br><span class="line">    </span><br><span class="line">    public abstract ByteBuffer wrap(int offset, int length);</span><br><span class="line">    </span><br><span class="line">    ......</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>我们可以看到，它在提供了诸多直接操作内存的方法外，还提供了一个<code>wrap()</code>方法，将自己包装成一个ByteBuffer，我们待会儿讲这个ByteBuffer。</p>
<p>Flink为MemorySegment提供了两个实现类：<code>HeapMemorySegment</code>和<code>HybridMemorySegment</code>。他们的区别在于前者只能分配堆内存，而后者能用来分配堆内和堆外内存。事实上，Flink框架里，只使用了后者。这是为什么呢？</p>
<p>如果HybridMemorySegment只能用于分配堆外内存的话，似乎更合常理。但是在JVM的世界中，如果一个方法是一个虚方法，那么每次调用时，JVM都要花时间去确定调用的到底是哪个子类实现的该虚方法（方法重写机制，不明白的去看JVM的invokeVirtual指令），也就意味着每次都要去翻方法表；而如果该方法虽然是个虚方法，但实际上整个JVM里只有一个实现（就是说只加载了一个子类进来），那么JVM会很聪明的把它去虚化处理，这样就不用每次调用方法时去找方法表了，能够大大提升性能。但是只分配堆内或者堆外内存不能满足我们的需要，所以就出现了HybridMemorySegment同时可以分配两种内存的设计。</p>
<p>我们可以看看HybridMemorySegment的构造代码：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">HybridMemorySegment(ByteBuffer buffer, Object owner) &#123;</span><br><span class="line">	super(checkBufferAndGetAddress(buffer), buffer.capacity(), owner);</span><br><span class="line">	this.offHeapBuffer &#x3D; buffer;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">	HybridMemorySegment(byte[] buffer, Object owner) &#123;</span><br><span class="line">	super(buffer, owner);</span><br><span class="line">	this.offHeapBuffer &#x3D; null;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>其中，第一个构造函数的<code>checkBufferAndGetAddress()</code>方法能够得到direct buffer的内存地址，因此可以操作堆外内存。</p>
<h4 id="6-1-2-ByteBuffer与NetworkBufferPool"><a href="#6-1-2-ByteBuffer与NetworkBufferPool" class="headerlink" title="6.1.2 ByteBuffer与NetworkBufferPool"></a>6.1.2 ByteBuffer与NetworkBufferPool</h4><p>在<code>MemorySegment</code>这个抽象之上，Flink在数据从operator内的数据对象在向TaskManager上转移，预备被发给下个节点的过程中，使用的抽象或者说内存对象是<code>Buffer</code>。</p>
<p><strong>注意</strong>，这个Buffer是个flink接口，不是java.nio提供的那个Buffer抽象类。Flink在这一层面同时使用了这两个同名概念，用来存储对象，直接看代码时到处都是各种xxxBuffer很容易混淆：</p>
<ul>
<li>java提供的那个Buffer抽象类在这一层主要用于构建<code>HeapByteBuffer</code>，这个主要是当数据从jvm里的一个对象被序列化成字节数组时用的；</li>
<li>Flink的这个Buffer接口主要是一种flink层面用于传输数据和事件的统一抽象，其实现类是<code>NetworkBuffer</code>，是对<code>MemorySegment</code>的包装。Flink在各个TaskManager之间传递数据时，使用的是这一层的抽象。</li>
</ul>
<p>因为Buffer的底层是MemorySegment，这可能不是JVM所管理的，所以为了知道什么时候一个Buffer用完了可以回收，Flink引入了引用计数的概念，当确认这个buffer没有人引用，就可以回收这一片MemorySegment用于别的地方了（JVM的垃圾回收为啥不用引用计数？读者思考一下）：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">public abstract class AbstractReferenceCountedByteBuf extends AbstractByteBuf &#123;</span><br><span class="line"></span><br><span class="line">    private volatile int refCnt &#x3D; 1;</span><br><span class="line">    </span><br><span class="line">    ......</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>为了方便管理<code>NetworkBuffer</code>，Flink提供了<code>BufferPoolFactory</code>，并且提供了唯一实现<code>NetworkBufferPool</code>，这是个工厂模式的应用。</p>
<p>NetworkBufferPool在每个TaskManager上只有一个，负责所有子task的内存管理。其实例化时就会尝试获取所有可由它管理的内存（对于堆内存来说，直接获取所有内存并放入老年代，并令用户对象只在新生代存活，可以极大程度的减少Full GC），我们看看其构造方法：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">public NetworkBufferPool(int numberOfSegmentsToAllocate, int segmentSize) &#123;</span><br><span class="line"></span><br><span class="line">		......</span><br><span class="line">		</span><br><span class="line">		try &#123;</span><br><span class="line">			this.availableMemorySegments &#x3D; new ArrayBlockingQueue&lt;&gt;(numberOfSegmentsToAllocate);</span><br><span class="line">		&#125;</span><br><span class="line">		catch (OutOfMemoryError err) &#123;</span><br><span class="line">			throw new OutOfMemoryError(&quot;Could not allocate buffer queue of length &quot;</span><br><span class="line">					+ numberOfSegmentsToAllocate + &quot; - &quot; + err.getMessage());</span><br><span class="line">		&#125;</span><br><span class="line"></span><br><span class="line">		try &#123;</span><br><span class="line">			for (int i &#x3D; 0; i &lt; numberOfSegmentsToAllocate; i++) &#123;</span><br><span class="line">				ByteBuffer memory &#x3D; ByteBuffer.allocateDirect(segmentSize);</span><br><span class="line">				availableMemorySegments.add(MemorySegmentFactory.wrapPooledOffHeapMemory(memory, null));</span><br><span class="line">			&#125;</span><br><span class="line">		&#125;</span><br><span class="line"></span><br><span class="line">        ......</span><br><span class="line">        </span><br><span class="line">		long allocatedMb &#x3D; (sizeInLong * availableMemorySegments.size()) &gt;&gt; 20;</span><br><span class="line"></span><br><span class="line">		LOG.info(&quot;Allocated &#123;&#125; MB for network buffer pool (number of memory segments: &#123;&#125;, bytes per segment: &#123;&#125;).&quot;,</span><br><span class="line">				allocatedMb, availableMemorySegments.size(), segmentSize);</span><br><span class="line">	&#125;</span><br></pre></td></tr></table></figure>
<p>由于NetworkBufferPool只是个工厂，实际的内存池是<code>LocalBufferPool</code>。每个TaskManager都只有一个NetworkBufferPool工厂，但是上面运行的每个task都要有一个和其他task隔离的LocalBufferPool池，这从逻辑上很好理解。另外，NetworkBufferPool会计算自己所拥有的所有内存分片数，在分配新的内存池时对每个内存池应该占有的内存分片数重分配，步骤是：</p>
<ul>
<li>首先，从整个工厂管理的内存片中拿出所有的内存池所需要的最少Buffer数目总和</li>
<li>如果正好分配完，就结束</li>
<li>其次，把所有的剩下的没分配的内存片，按照每个LocalBufferPool内存池的剩余想要容量大小进行按比例分配</li>
<li>剩余想要容量大小是这么个东西：如果该内存池至少需要3个buffer，最大需要10个buffer，那么它的剩余想要容量就是7</li>
</ul>
<p>实现代码如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line">   private void redistributeBuffers() throws IOException &#123;</span><br><span class="line">	assert Thread.holdsLock(factoryLock);</span><br><span class="line"></span><br><span class="line">	&#x2F;&#x2F; All buffers, which are not among the required ones</span><br><span class="line">	final int numAvailableMemorySegment &#x3D; totalNumberOfMemorySegments - numTotalRequiredBuffers;</span><br><span class="line"></span><br><span class="line">	if (numAvailableMemorySegment &#x3D;&#x3D; 0) &#123;</span><br><span class="line">		&#x2F;&#x2F; in this case, we need to redistribute buffers so that every pool gets its minimum</span><br><span class="line">		for (LocalBufferPool bufferPool : allBufferPools) &#123;</span><br><span class="line">			bufferPool.setNumBuffers(bufferPool.getNumberOfRequiredMemorySegments());</span><br><span class="line">		&#125;</span><br><span class="line">		return;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	long totalCapacity &#x3D; 0; &#x2F;&#x2F; long to avoid int overflow</span><br><span class="line"></span><br><span class="line">	for (LocalBufferPool bufferPool : allBufferPools) &#123;</span><br><span class="line">		int excessMax &#x3D; bufferPool.getMaxNumberOfMemorySegments() -</span><br><span class="line">			bufferPool.getNumberOfRequiredMemorySegments();</span><br><span class="line">		totalCapacity +&#x3D; Math.min(numAvailableMemorySegment, excessMax);</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	&#x2F;&#x2F; no capacity to receive additional buffers?</span><br><span class="line">	if (totalCapacity &#x3D;&#x3D; 0) &#123;</span><br><span class="line">		return; &#x2F;&#x2F; necessary to avoid div by zero when nothing to re-distribute</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	final int memorySegmentsToDistribute &#x3D; MathUtils.checkedDownCast(</span><br><span class="line">			Math.min(numAvailableMemorySegment, totalCapacity));</span><br><span class="line"></span><br><span class="line">	long totalPartsUsed &#x3D; 0; &#x2F;&#x2F; of totalCapacity</span><br><span class="line">	int numDistributedMemorySegment &#x3D; 0;</span><br><span class="line">	for (LocalBufferPool bufferPool : allBufferPools) &#123;</span><br><span class="line">		int excessMax &#x3D; bufferPool.getMaxNumberOfMemorySegments() -</span><br><span class="line">			bufferPool.getNumberOfRequiredMemorySegments();</span><br><span class="line"></span><br><span class="line">		&#x2F;&#x2F; shortcut</span><br><span class="line">		if (excessMax &#x3D;&#x3D; 0) &#123;</span><br><span class="line">			continue;</span><br><span class="line">		&#125;</span><br><span class="line"></span><br><span class="line">		totalPartsUsed +&#x3D; Math.min(numAvailableMemorySegment, excessMax);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">		final int mySize &#x3D; MathUtils.checkedDownCast(</span><br><span class="line">				memorySegmentsToDistribute * totalPartsUsed &#x2F; totalCapacity - numDistributedMemorySegment);</span><br><span class="line"></span><br><span class="line">		numDistributedMemorySegment +&#x3D; mySize;</span><br><span class="line">		bufferPool.setNumBuffers(bufferPool.getNumberOfRequiredMemorySegments() + mySize);</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	assert (totalPartsUsed &#x3D;&#x3D; totalCapacity);</span><br><span class="line">	assert (numDistributedMemorySegment &#x3D;&#x3D; memorySegmentsToDistribute);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>接下来说说这个<code>LocalBufferPool</code>内存池。<br>LocalBufferPool的逻辑想想无非是<del>增删改查</del>，值得说的是其fields：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;** 该内存池需要的最少内存片数目*&#x2F;</span><br><span class="line">private final int numberOfRequiredMemorySegments;</span><br><span class="line"></span><br><span class="line">&#x2F;**</span><br><span class="line"> * 当前已经获得的内存片中，还没有写入数据的空白内存片</span><br><span class="line"> *&#x2F;</span><br><span class="line">private final ArrayDeque&lt;MemorySegment&gt; availableMemorySegments &#x3D; new ArrayDeque&lt;MemorySegment&gt;();</span><br><span class="line"></span><br><span class="line">&#x2F;**</span><br><span class="line"> * 注册的所有监控buffer可用性的监听器</span><br><span class="line"> *&#x2F;</span><br><span class="line">private final ArrayDeque&lt;BufferListener&gt; registeredListeners &#x3D; new ArrayDeque&lt;&gt;();</span><br><span class="line"></span><br><span class="line">&#x2F;** 能给内存池分配的最大分片数*&#x2F;</span><br><span class="line">private final int maxNumberOfMemorySegments;</span><br><span class="line"></span><br><span class="line">&#x2F;** 当前内存池大小 *&#x2F;</span><br><span class="line">private int currentPoolSize;</span><br><span class="line"></span><br><span class="line">&#x2F;**</span><br><span class="line"> * 所有经由NetworkBufferPool分配的，被本内存池引用到的（非直接获得的）分片数</span><br><span class="line"> *&#x2F;</span><br><span class="line">private int numberOfRequestedMemorySegments;</span><br></pre></td></tr></table></figure>
<p>承接NetworkBufferPool的重分配方法，我们来看看LocalBufferPool的<code>setNumBuffers()</code>方法，代码很短，逻辑也相当简单，就不展开说了：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">public void setNumBuffers(int numBuffers) throws IOException &#123;</span><br><span class="line">	synchronized (availableMemorySegments) &#123;</span><br><span class="line">		checkArgument(numBuffers &gt;&#x3D; numberOfRequiredMemorySegments,</span><br><span class="line">				&quot;Buffer pool needs at least %s buffers, but tried to set to %s&quot;,</span><br><span class="line">				numberOfRequiredMemorySegments, numBuffers);</span><br><span class="line"></span><br><span class="line">		if (numBuffers &gt; maxNumberOfMemorySegments) &#123;</span><br><span class="line">			currentPoolSize &#x3D; maxNumberOfMemorySegments;</span><br><span class="line">		&#125; else &#123;</span><br><span class="line">			currentPoolSize &#x3D; numBuffers;</span><br><span class="line">		&#125;</span><br><span class="line"></span><br><span class="line">		returnExcessMemorySegments();</span><br><span class="line"></span><br><span class="line">		&#x2F;&#x2F; If there is a registered owner and we have still requested more buffers than our</span><br><span class="line">		&#x2F;&#x2F; size, trigger a recycle via the owner.</span><br><span class="line">		if (owner !&#x3D; null &amp;&amp; numberOfRequestedMemorySegments &gt; currentPoolSize) &#123;</span><br><span class="line">			owner.releaseMemory(numberOfRequestedMemorySegments - numBuffers);</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="6-1-3-RecordWriter与Record"><a href="#6-1-3-RecordWriter与Record" class="headerlink" title="6.1.3 RecordWriter与Record"></a>6.1.3 RecordWriter与Record</h4><p>我们接着往高层抽象走，刚刚提到了最底层内存抽象是MemorySegment，用于数据传输的是Buffer，那么，承上启下对接从Java对象转为Buffer的中间对象是什么呢？是<code>StreamRecord</code>。</p>
<p>从<code>StreamRecord&lt;T&gt;</code>这个类名字就可以看出来，这个类就是个wrap，里面保存了原始的Java对象。另外，StreamRecord还保存了一个timestamp。</p>
<p>那么这个对象是怎么变成LocalBufferPool内存池里的一个大号字节数组的呢？借助了<code>StreamWriter</code>这个类。</p>
<p>我们直接来看把数据序列化交出去的方法：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">private void sendToTarget(T record, int targetChannel) throws IOException, InterruptedException &#123;</span><br><span class="line">	RecordSerializer&lt;T&gt; serializer &#x3D; serializers[targetChannel];</span><br><span class="line"></span><br><span class="line">	SerializationResult result &#x3D; serializer.addRecord(record);</span><br><span class="line"></span><br><span class="line">	while (result.isFullBuffer()) &#123;</span><br><span class="line">		if (tryFinishCurrentBufferBuilder(targetChannel, serializer)) &#123;</span><br><span class="line">			&#x2F;&#x2F; If this was a full record, we are done. Not breaking</span><br><span class="line">			&#x2F;&#x2F; out of the loop at this point will lead to another</span><br><span class="line">			&#x2F;&#x2F; buffer request before breaking out (that would not be</span><br><span class="line">			&#x2F;&#x2F; a problem per se, but it can lead to stalls in the</span><br><span class="line">			&#x2F;&#x2F; pipeline).</span><br><span class="line">			if (result.isFullRecord()) &#123;</span><br><span class="line">				break;</span><br><span class="line">			&#125;</span><br><span class="line">		&#125;</span><br><span class="line">		BufferBuilder bufferBuilder &#x3D; requestNewBufferBuilder(targetChannel);</span><br><span class="line"></span><br><span class="line">		result &#x3D; serializer.continueWritingWithNextBufferBuilder(bufferBuilder);</span><br><span class="line">	&#125;</span><br><span class="line">	checkState(!serializer.hasSerializedData(), &quot;All data should be written at once&quot;);</span><br><span class="line">       </span><br><span class="line">       </span><br><span class="line">       </span><br><span class="line">	if (flushAlways) &#123;</span><br><span class="line">		targetPartition.flush(targetChannel);</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>先说最后一行，如果配置为flushAlways，那么会立刻把元素发送出去，但是这样吞吐量会下降；Flink的默认设置其实也不是一个元素一个元素的发送，是单独起了一个线程，每隔固定时间flush一次所有channel，较真起来也算是mini batch了。</p>
<p>再说序列化那一句:<code>SerializationResult result = serializer.addRecord(record);</code>。在这行代码中，Flink把对象调用该对象所属的序列化器序列化为字节数组。</p>
<h3 id="6-2-数据流转过程"><a href="#6-2-数据流转过程" class="headerlink" title="6.2 数据流转过程"></a>6.2 数据流转过程</h3><p>上一节讲了各层数据的抽象，这一节讲讲数据在各个task之间exchange的过程。</p>
<h4 id="6-2-1-整体过程"><a href="#6-2-1-整体过程" class="headerlink" title="6.2.1 整体过程"></a>6.2.1 整体过程</h4><p>看这张图：<br><img src="http://static.zybuluo.com/bethunebtj/e5m0ggy1t6z8tjgfn52cr31r/image_1cetavukjja42ce1261v5k57i9.png" alt="image_1cetavukjja42ce1261v5k57i9.png-821.8kB"></p>
<ol>
<li>第一步必然是准备一个ResultPartition；</li>
<li>通知JobMaster；</li>
<li>JobMaster通知下游节点；如果下游节点尚未部署，则部署之；</li>
<li>下游节点向上游请求数据</li>
<li>开始传输数据</li>
</ol>
<h4 id="6-2-2-数据跨task传递"><a href="#6-2-2-数据跨task传递" class="headerlink" title="6.2.2 数据跨task传递"></a>6.2.2 数据跨task传递</h4><p>本节讲一下算子之间具体的数据传输过程。也先上一张图：<br><img src="http://static.zybuluo.com/bethunebtj/d9pmni04fg8i11xotv4xqxh7/image_1cfmpba9v15anggtvsba2o1277m.png" alt="image_1cfmpba9v15anggtvsba2o1277m.png-357.5kB"><br>数据在task之间传递有如下几步：</p>
<ol>
<li>数据在本operator处理完后，交给<code>RecordWriter</code>。每条记录都要选择一个下游节点，所以要经过<code>ChannelSelector</code>。</li>
<li>每个channel都有一个serializer（我认为这应该是为了避免多线程写的麻烦），把这条Record序列化为ByteBuffer</li>
<li>接下来数据被写入ResultPartition下的各个subPartition里，此时该数据已经存入DirectBuffer（MemorySegment）</li>
<li>单独的线程控制数据的flush速度，一旦触发flush，则通过Netty的nio通道向对端写入</li>
<li>对端的netty client接收到数据，decode出来，把数据拷贝到buffer里，然后通知<code>InputChannel</code></li>
<li>有可用的数据时，下游算子从阻塞醒来，从InputChannel取出buffer，再解序列化成record，交给算子执行用户代码</li>
</ol>
<p>数据在不同机器的算子之间传递的步骤就是以上这些。</p>
<p>了解了步骤之后，再来看一下部分关键代码：<br>首先是把数据交给recordwriter。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">   &#x2F;&#x2F;RecordWriterOutput.java</span><br><span class="line">@Override</span><br><span class="line">public void collect(StreamRecord&lt;OUT&gt; record) &#123;</span><br><span class="line">	if (this.outputTag !&#x3D; null) &#123;</span><br><span class="line">		&#x2F;&#x2F; we are only responsible for emitting to the main input</span><br><span class="line">		return;</span><br><span class="line">	&#125;</span><br><span class="line">       &#x2F;&#x2F;这里可以看到把记录交给了recordwriter</span><br><span class="line">	pushToRecordWriter(record);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>然后recordwriter把数据发送到对应的通道。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">   &#x2F;&#x2F;RecordWriter.java</span><br><span class="line">public void emit(T record) throws IOException, InterruptedException &#123;</span><br><span class="line">    &#x2F;&#x2F;channelselector登场了</span><br><span class="line">	for (int targetChannel : channelSelector.selectChannels(record, numChannels)) &#123;</span><br><span class="line">		sendToTarget(record, targetChannel);</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">	private void sendToTarget(T record, int targetChannel) throws IOException, InterruptedException &#123;</span><br><span class="line">	</span><br><span class="line">	&#x2F;&#x2F;选择序列化器并序列化数据</span><br><span class="line">	RecordSerializer&lt;T&gt; serializer &#x3D; serializers[targetChannel];</span><br><span class="line"></span><br><span class="line">	SerializationResult result &#x3D; serializer.addRecord(record);</span><br><span class="line"></span><br><span class="line">	while (result.isFullBuffer()) &#123;</span><br><span class="line">		if (tryFinishCurrentBufferBuilder(targetChannel, serializer)) &#123;</span><br><span class="line">			&#x2F;&#x2F; If this was a full record, we are done. Not breaking</span><br><span class="line">			&#x2F;&#x2F; out of the loop at this point will lead to another</span><br><span class="line">			&#x2F;&#x2F; buffer request before breaking out (that would not be</span><br><span class="line">			&#x2F;&#x2F; a problem per se, but it can lead to stalls in the</span><br><span class="line">			&#x2F;&#x2F; pipeline).</span><br><span class="line">			if (result.isFullRecord()) &#123;</span><br><span class="line">				break;</span><br><span class="line">			&#125;</span><br><span class="line">		&#125;</span><br><span class="line">		BufferBuilder bufferBuilder &#x3D; requestNewBufferBuilder(targetChannel);</span><br><span class="line"></span><br><span class="line">           &#x2F;&#x2F;写入channel</span><br><span class="line">		result &#x3D; serializer.continueWritingWithNextBufferBuilder(bufferBuilder);</span><br><span class="line">	&#125;</span><br><span class="line">	checkState(!serializer.hasSerializedData(), &quot;All data should be written at once&quot;);</span><br><span class="line"></span><br><span class="line">	if (flushAlways) &#123;</span><br><span class="line">		targetPartition.flush(targetChannel);</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>接下来是把数据推给底层设施（netty）的过程：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">   &#x2F;&#x2F;ResultPartition.java</span><br><span class="line">@Override</span><br><span class="line">public void flushAll() &#123;</span><br><span class="line">	for (ResultSubpartition subpartition : subpartitions) &#123;</span><br><span class="line">		subpartition.flush();</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">    &#x2F;&#x2F;PartitionRequestQueue.java</span><br><span class="line">	void notifyReaderNonEmpty(final NetworkSequenceViewReader reader) &#123;</span><br><span class="line">	&#x2F;&#x2F;这里交给了netty server线程去推</span><br><span class="line">	ctx.executor().execute(new Runnable() &#123;</span><br><span class="line">		@Override</span><br><span class="line">		public void run() &#123;</span><br><span class="line">			ctx.pipeline().fireUserEventTriggered(reader);</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>netty相关的部分：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F;AbstractChannelHandlerContext.java</span><br><span class="line">public ChannelHandlerContext fireUserEventTriggered(final Object event) &#123;</span><br><span class="line">    if (event &#x3D;&#x3D; null) &#123;</span><br><span class="line">        throw new NullPointerException(&quot;event&quot;);</span><br><span class="line">    &#125; else &#123;</span><br><span class="line">        final AbstractChannelHandlerContext next &#x3D; this.findContextInbound();</span><br><span class="line">        EventExecutor executor &#x3D; next.executor();</span><br><span class="line">        if (executor.inEventLoop()) &#123;</span><br><span class="line">            next.invokeUserEventTriggered(event);</span><br><span class="line">        &#125; else &#123;</span><br><span class="line">            executor.execute(new OneTimeTask() &#123;</span><br><span class="line">                public void run() &#123;</span><br><span class="line">                    next.invokeUserEventTriggered(event);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        return this;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>最后真实的写入：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line">   &#x2F;&#x2F;PartittionRequesetQueue.java</span><br><span class="line">private void enqueueAvailableReader(final NetworkSequenceViewReader reader) throws Exception &#123;</span><br><span class="line">	if (reader.isRegisteredAsAvailable() || !reader.isAvailable()) &#123;</span><br><span class="line">		return;</span><br><span class="line">	&#125;</span><br><span class="line">	&#x2F;&#x2F; Queue an available reader for consumption. If the queue is empty,</span><br><span class="line">	&#x2F;&#x2F; we try trigger the actual write. Otherwise this will be handled by</span><br><span class="line">	&#x2F;&#x2F; the writeAndFlushNextMessageIfPossible calls.</span><br><span class="line">	boolean triggerWrite &#x3D; availableReaders.isEmpty();</span><br><span class="line">	registerAvailableReader(reader);</span><br><span class="line"></span><br><span class="line">	if (triggerWrite) &#123;</span><br><span class="line">		writeAndFlushNextMessageIfPossible(ctx.channel());</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">private void writeAndFlushNextMessageIfPossible(final Channel channel) throws IOException &#123;</span><br><span class="line">	</span><br><span class="line">       ......</span><br><span class="line"></span><br><span class="line">			next &#x3D; reader.getNextBuffer();</span><br><span class="line">			if (next &#x3D;&#x3D; null) &#123;</span><br><span class="line">				if (!reader.isReleased()) &#123;</span><br><span class="line">					continue;</span><br><span class="line">				&#125;</span><br><span class="line">				markAsReleased(reader.getReceiverId());</span><br><span class="line"></span><br><span class="line">				Throwable cause &#x3D; reader.getFailureCause();</span><br><span class="line">				if (cause !&#x3D; null) &#123;</span><br><span class="line">					ErrorResponse msg &#x3D; new ErrorResponse(</span><br><span class="line">						new ProducerFailedException(cause),</span><br><span class="line">						reader.getReceiverId());</span><br><span class="line"></span><br><span class="line">					ctx.writeAndFlush(msg);</span><br><span class="line">				&#125;</span><br><span class="line">			&#125; else &#123;</span><br><span class="line">				&#x2F;&#x2F; This channel was now removed from the available reader queue.</span><br><span class="line">				&#x2F;&#x2F; We re-add it into the queue if it is still available</span><br><span class="line">				if (next.moreAvailable()) &#123;</span><br><span class="line">					registerAvailableReader(reader);</span><br><span class="line">				&#125;</span><br><span class="line"></span><br><span class="line">				BufferResponse msg &#x3D; new BufferResponse(</span><br><span class="line">					next.buffer(),</span><br><span class="line">					reader.getSequenceNumber(),</span><br><span class="line">					reader.getReceiverId(),</span><br><span class="line">					next.buffersInBacklog());</span><br><span class="line"></span><br><span class="line">				if (isEndOfPartitionEvent(next.buffer())) &#123;</span><br><span class="line">					reader.notifySubpartitionConsumed();</span><br><span class="line">					reader.releaseAllResources();</span><br><span class="line"></span><br><span class="line">					markAsReleased(reader.getReceiverId());</span><br><span class="line">				&#125;</span><br><span class="line"></span><br><span class="line">				&#x2F;&#x2F; Write and flush and wait until this is done before</span><br><span class="line">				&#x2F;&#x2F; trying to continue with the next buffer.</span><br><span class="line">				channel.writeAndFlush(msg).addListener(writeListener);</span><br><span class="line"></span><br><span class="line">				return;</span><br><span class="line">			&#125;</span><br><span class="line">	</span><br><span class="line">	......</span><br><span class="line">	</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>上面这段代码里第二个方法中调用的<code>writeAndFlush(msg)</code>就是真正往netty的nio通道里写入的地方了。在这里，写入的是一个RemoteInputChannel，对应的就是下游节点的InputGate的channels。</p>
<p>有写就有读，nio通道的另一端需要读入buffer，代码如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">   &#x2F;&#x2F;CreditBasedPartitionRequestClientHandler.java</span><br><span class="line">private void decodeMsg(Object msg) throws Throwable &#123;</span><br><span class="line">	final Class&lt;?&gt; msgClazz &#x3D; msg.getClass();</span><br><span class="line"></span><br><span class="line">	&#x2F;&#x2F; ---- Buffer --------------------------------------------------------</span><br><span class="line">	if (msgClazz &#x3D;&#x3D; NettyMessage.BufferResponse.class) &#123;</span><br><span class="line">		NettyMessage.BufferResponse bufferOrEvent &#x3D; (NettyMessage.BufferResponse) msg;</span><br><span class="line"></span><br><span class="line">		RemoteInputChannel inputChannel &#x3D; inputChannels.get(bufferOrEvent.receiverId);</span><br><span class="line">		if (inputChannel &#x3D;&#x3D; null) &#123;</span><br><span class="line">			bufferOrEvent.releaseBuffer();</span><br><span class="line"></span><br><span class="line">			cancelRequestFor(bufferOrEvent.receiverId);</span><br><span class="line"></span><br><span class="line">			return;</span><br><span class="line">		&#125;</span><br><span class="line"></span><br><span class="line">		decodeBufferOrEvent(inputChannel, bufferOrEvent);</span><br><span class="line"></span><br><span class="line">	&#125; </span><br><span class="line">	</span><br><span class="line">	......</span><br><span class="line">	</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>插一句，Flink其实做阻塞和获取数据的方式非常自然，利用了生产者和消费者模型，当获取不到数据时，消费者自然阻塞；当数据被加入队列，消费者被notify。Flink的背压机制也是借此实现。</p>
<p>然后在这里又反序列化成<code>StreamRecord</code>：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">   &#x2F;&#x2F;StreamElementSerializer.java</span><br><span class="line">public StreamElement deserialize(DataInputView source) throws IOException &#123;</span><br><span class="line">	int tag &#x3D; source.readByte();</span><br><span class="line">	if (tag &#x3D;&#x3D; TAG_REC_WITH_TIMESTAMP) &#123;</span><br><span class="line">		long timestamp &#x3D; source.readLong();</span><br><span class="line">		return new StreamRecord&lt;T&gt;(typeSerializer.deserialize(source), timestamp);</span><br><span class="line">	&#125;</span><br><span class="line">	else if (tag &#x3D;&#x3D; TAG_REC_WITHOUT_TIMESTAMP) &#123;</span><br><span class="line">		return new StreamRecord&lt;T&gt;(typeSerializer.deserialize(source));</span><br><span class="line">	&#125;</span><br><span class="line">	else if (tag &#x3D;&#x3D; TAG_WATERMARK) &#123;</span><br><span class="line">		return new Watermark(source.readLong());</span><br><span class="line">	&#125;</span><br><span class="line">	else if (tag &#x3D;&#x3D; TAG_STREAM_STATUS) &#123;</span><br><span class="line">		return new StreamStatus(source.readInt());</span><br><span class="line">	&#125;</span><br><span class="line">	else if (tag &#x3D;&#x3D; TAG_LATENCY_MARKER) &#123;</span><br><span class="line">		return new LatencyMarker(source.readLong(), new OperatorID(source.readLong(), source.readLong()), source.readInt());</span><br><span class="line">	&#125;</span><br><span class="line">	else &#123;</span><br><span class="line">		throw new IOException(&quot;Corrupt stream, found tag: &quot; + tag);</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>然后再次在<code>StreamInputProcessor.processInput()</code>循环中得到处理。</p>
<p>至此，数据在跨jvm的节点之间的流转过程就讲完了。</p>
<h3 id="6-3-Credit漫谈"><a href="#6-3-Credit漫谈" class="headerlink" title="6.3 Credit漫谈"></a>6.3 Credit漫谈</h3><p>在看上一部分的代码时，有一个小细节不知道读者有没有注意到，我们的数据发送端的代码叫做<code>PartittionRequesetQueue.java</code>，而我们的接收端却起了一个完全不相干的名字：<code>CreditBasedPartitionRequestClientHandler.java</code>。为什么前面加了CreditBased的前缀呢？</p>
<h4 id="6-3-1-背压问题"><a href="#6-3-1-背压问题" class="headerlink" title="6.3.1 背压问题"></a>6.3.1 背压问题</h4><p>在流模型中，我们期待数据是像水流一样平滑的流过我们的引擎，但现实生活不会这么美好。数据的上游可能因为各种原因数据量暴增，远远超出了下游的瞬时处理能力（回忆一下98年大洪水），导致系统崩溃。<br>那么框架应该怎么应对呢？和人类处理自然灾害的方式类似，我们修建了三峡大坝，当洪水来临时把大量的水囤积在大坝里；对于Flink来说，就是在数据的接收端和发送端放置了缓存池，用以缓冲数据，并且设置闸门阻止数据向下流。</p>
<p>那么Flink又是如何处理背压的呢？答案也是靠这些缓冲池。<br><img src="http://static.zybuluo.com/bethunebtj/1r40q9nbeuxh4j0omiic5tob/image_1cfksrl5cd4m1lbqqqgvc811349.png" alt="image_1cfksrl5cd4m1lbqqqgvc811349.png-43.1kB"><br>这张图说明了Flink在生产和消费数据时的大致情况。<code>ResultPartition</code>和<code>InputGate</code>在输出和输入数据时，都要向<code>NetworkBufferPool</code>申请一块<code>MemorySegment</code>作为缓存池。<br>接下来的情况和生产者消费者很类似。当数据发送太多，下游处理不过来了，那么首先InputChannel会被填满，然后是InputChannel能申请到的内存达到最大，于是下游停止读取数据，上游负责发送数据的nettyServer会得到响应，停止从ResultSubPartition读取缓存，那么ResultPartition很快也将存满数据不能被消费，从而生产数据的逻辑被阻塞在获取新buffer上，非常自然地形成背压的效果。</p>
<p>Flink自己做了个试验用以说明这个机制的效果：<br><img src="http://static.zybuluo.com/bethunebtj/xxqpmehf1w4un8leyc9itr9y/image_1cfkta54rkdd1od4aau1e3n7nhm.png" alt="image_1cfkta54rkdd1od4aau1e3n7nhm.png-240.6kB"><br>我们首先设置生产者的发送速度为60%，然后下游的算子以同样的速度处理数据。然后我们将下游算子的处理速度降低到30%，可以看到上游的生产者的数据产生曲线几乎与消费者同步下滑。而后当我们解除限速，整个流的速度立刻提高到了100%。</p>
<h4 id="6-3-2-使用Credit实现ATM网络流控"><a href="#6-3-2-使用Credit实现ATM网络流控" class="headerlink" title="6.3.2 使用Credit实现ATM网络流控"></a>6.3.2 使用Credit实现ATM网络流控</h4><p>上文已经提到，对于流量控制，一个朴素的思路就是在<del>长江上建三峡</del>链路上建立一个拦截的dam，如下图所示：<br><img src="http://static.zybuluo.com/bethunebtj/1wc3o2qo6ozsyxqebnn2xw0j/image_1cfku114lf7hpqf3lmcl0116c13.png" alt="image_1cfku114lf7hpqf3lmcl0116c13.png-22.7kB"><br>基于Credit的流控就是这样一种建立在信用（消费数据的能力)上的，面向每个虚链路（而非端到端的）流模型，如下图所示：<br><img src="http://static.zybuluo.com/bethunebtj/on4kd4bzvoozbo6yk6n2but6/image_1cfku4g4g174d7gb5ecbfcib71g.png" alt="image_1cfku4g4g174d7gb5ecbfcib71g.png-22.5kB"><br>首先，下游会向上游发送一条credit message，用以通知其目前的信用（可联想信用卡的可用额度），然后上游会根据这个信用消息来决定向下游发送多少数据。当上游把数据发送给下游时，它就从下游的信用卡上划走相应的额度（credit balance）：<br><img src="http://static.zybuluo.com/bethunebtj/i8t1qvlib162x1i6lm0qruju/image_1cfkug5sm1v4l15pbgj4jntc7q1t.png" alt="image_1cfkug5sm1v4l15pbgj4jntc7q1t.png-12.9kB"><br>下游总共获得的credit数目是Buf_Alloc，已经消费的数据是Fwd_Cnt，上游发送出来的数据是Tx_Cnt，那么剩下的那部分就是Crd_Bal:<br>Crd_Bal = Buf_Alloc - ( Tx_Cnt - Fwd_Cnt )<br>上面这个式子应该很好理解。</p>
<p>可以看到，Credit Based Flow Control的关键是buffer分配。这种分配可以在数据的发送端完成，也可以在接收端完成。对于下游可能有多个上游节点的情况（比如Flink），使用接收端的credit分配更加合理：<br><img src="http://static.zybuluo.com/bethunebtj/o09mav0lfnk7iqar98iphr7o/image_1cfkvpmlh1gl31ef41cvh1c903a19.png" alt="image_1cfkvpmlh1gl31ef41cvh1c903a19.png-13.1kB"><br>上图中，接收者可以观察到每个上游连接的带宽情况，而上游的节点Snd1却不可能轻易知道发往同一个下游节点的其他Snd2的带宽情况，从而如果在上游控制流量将会很困难，而在下游控制流量将会很方便。</p>
<p>因此，这就是为何Flink在接收端有一个基于Credit的Client，而不是在发送端有一个CreditServer的原因。</p>
<p>最后，再讲一下Credit的面向虚链路的流设计和端到端的流设计的区别：<br><img src="http://static.zybuluo.com/bethunebtj/1mm2eqnuop9rcccap915qrzx/image_1cfl05d2f1ub879c1lc5qsq14n9m.png" alt="image_1cfl05d2f1ub879c1lc5qsq14n9m.png-13.4kB"><br>如上图所示，a是面向连接的流设计，b是端到端的流设计。其中，a的设计使得当下游节点3因某些情况必须缓存数据暂缓处理时，每个上游节点（1和2）都可以利用其缓存保存数据；而端到端的设计b里，只有节点3的缓存才可以用于保存数据（读者可以从如何实现上想想为什么）。</p>
<p>对流控制感兴趣的读者，可以看这篇文章：<a target="_blank" rel="noopener" href="https://www.nap.edu/read/5769/chapter/1">Traffic Management For High-Speed Networks</a>。</p>
<h2 id="7-其他核心概念"><a href="#7-其他核心概念" class="headerlink" title="7.其他核心概念"></a>7.其他核心概念</h2><p>截至第六章，和执行过程相关的部分就全部讲完，告一段落了。第七章主要讲一点杂七杂八的内容，有时间就不定期更新。</p>
<h3 id="7-1-EventTime时间模型"><a href="#7-1-EventTime时间模型" class="headerlink" title="7.1 EventTime时间模型"></a>7.1 EventTime时间模型</h3><p>flink有三种时间模型：ProcessingTime，EventTime和IngestionTime。<br>关于时间模型看这张图：<br><img src="http://static.zybuluo.com/bethunebtj/kcp52h1se5xzocfqcigcv9oh/image_1cdbotdcmoe11q961st5lbn1j4n9.png" alt="image_1cdbotdcmoe11q961st5lbn1j4n9.png-38.4kB"><br>从这张图里可以很清楚的看到三种Time模型的区别。</p>
<ul>
<li>EventTime是数据被生产出来的时间，可以是比如传感器发出信号的时间等（此时数据还没有被传输给flink）。</li>
<li>IngestionTime是数据进入flink的时间，也就是从Source进入flink流的时间（此时数据刚刚被传给flink）</li>
<li>ProcessingTime是针对当前算子的系统时间，是指该数据已经进入某个operator时，operator所在系统的当前时间</li>
</ul>
<p>例如，我在写这段话的时间是2018年5月13日03点47分，但是我引用的这张EventTime的图片，是2015年画出来的，那么这张图的EventTime是2015年，而ProcessingTime是现在。<br>Flink官网对于时间戳的解释非常详细：<a target="_blank" rel="noopener" href="https://ci.apache.org/projects/flink/flink-docs-master/dev/event_time.html">点我</a><br>Flink对于EventTime模型的实现，依赖的是一种叫做<code>watermark</code>的对象。watermark是携带有时间戳的一个对象，会按照程序的要求被插入到数据流中，用以标志某个事件在该时间发生了。<br>我再做一点简短的说明，还是以官网的图为例：<br><img src="http://static.zybuluo.com/bethunebtj/f4k8110qo8arjey5zbp75xz3/image_1cdbt8v5jl2ujn91uu1joh1p4gm.png" alt="image_1cdbt8v5jl2ujn91uu1joh1p4gm.png-11.3kB"><br>对于有序到来的数据，假设我们在timestamp为11的元素后加入一个watermark，时间记录为11，则下个元素收到该watermark时，认为所有早于11的元素均已到达。这是非常理想的情况。<br><img src="http://static.zybuluo.com/bethunebtj/3aqwmrc5hg054b09z47lwsvp/image_1cdbtcc5c1a6i1tuaadb1rd5136913.png" alt="image_1cdbtcc5c1a6i1tuaadb1rd5136913.png-11.6kB"><br>而在现实生活中，经常会遇到乱序的数据。这时，我们虽然在timestamp为7的元素后就收到了11，但是我们一直等到了收到元素12之后，才插入了watermark为11的元素。与上面的图相比，如果我们仍然在11后就插入11的watermark，那么元素9就会被丢弃，造成数据丢失。而我们在12之后插入watermark11，就保证了9仍然会被下一个operator处理。当然，我们不可能无限制的永远等待迟到元素，所以要在哪个元素后插入11需要根据实际场景权衡。</p>
<p>对于来自多个数据源的watermark，可以看这张图：<br><img src="http://static.zybuluo.com/bethunebtj/pu1cr48mq9340g5embaig9b5/image_1cdbufp4a1opmsit5n61mial4520.png" alt="image_1cdbufp4a1opmsit5n61mial4520.png-72kB"><br>可以看到，当一个operator收到多个watermark时，它遵循最小原则（或者说最早），即算子的当前watermark是流经该算子的最小watermark，以容许来自不同的source的乱序数据到来。<br>关于事件时间模型，更多内容可以参考<a target="_blank" rel="noopener" href="https://www.oreilly.com/ideas/the-world-beyond-batch-streaming-101">Stream 101</a> 和谷歌的这篇论文：<a target="_blank" rel="noopener" href="https://research.google.com/pubs/archive/43864.pdf">Dataflow Model paper</a></p>
<h3 id="7-2-FLIP-6-部署及处理模型演进"><a href="#7-2-FLIP-6-部署及处理模型演进" class="headerlink" title="7.2 FLIP-6 部署及处理模型演进"></a>7.2 FLIP-6 部署及处理模型演进</h3><p>就在老白写这篇blog的时候，Flink发布了其1.5 RELEASE版本，号称实现了其部署及处理模型（也就是FLIP-6)，所以打算简略地说一下FLIP-6的主要内容。</p>
<h4 id="7-2-1-现有模型不足"><a href="#7-2-1-现有模型不足" class="headerlink" title="7.2.1 现有模型不足"></a>7.2.1 现有模型不足</h4><p>1.5之前的Flink模型有很多不足，包括：</p>
<ul>
<li>只能静态分配计算资源</li>
<li>在YARN上所有的资源分配都是一碗水端平的</li>
<li>与Docker/k8s的集成非常之蠢，颇有脱裤子放屁的神韵</li>
<li>JobManager没有任务调度逻辑</li>
<li>任务在YARN上执行结束后web dashboard就不可用</li>
<li>集群的session模式和per job模式混淆难以理解</li>
</ul>
<p>就我个人而言，我觉得Flink有一个这里完全没提到的不足才是最应该修改的：针对任务的完全的资源隔离。尤其是如果用Standalone集群，一个用户的task跑挂了TaskManager，然后拖垮了整个集群的情况简直不要太多。</p>
<h4 id="7-2-2-核心变更"><a href="#7-2-2-核心变更" class="headerlink" title="7.2.2 核心变更"></a>7.2.2 核心变更</h4><p><strong>Single Job JobManager</strong><br>最重要的变更是一个JobManager只处理一个job。当我们生成JobGraph时就顺便起一个JobManager，这显然更加自然。</p>
<p><strong>ResourceManager</strong><br>其职责包括获取新的TM和slot，通知失败，释放资源以及缓存TM以用于重用等。重要的是，这个组件要能做到挂掉时不要搞垮正在运行的好好的任务。其职责和与JobManager、TaskManager的交互图如下：<br><img src="http://static.zybuluo.com/bethunebtj/pzuvevivascmk2xky450ll87/image_1cfl9453k1gld4acr1m13j3195sg.png" alt="image_1cfl9453k1gld4acr1m13j3195sg.png-23.9kB"></p>
<p><strong>TaskManager</strong><br>TM要与上面的两个组件交互。与JobManager交互时，要能提供slot，要能与所有给出slot的JM交互。丢失与JM的连接时要能试图把本TM上的slot的情况通告给新JM，如果这一步失败，就要能重新分配slot。<br>与ResourceManager交互时，要通知RM自己的资源和当前的Job分配情况，能按照RM的要求分配资源或者关闭自身。</p>
<p><strong>JobManager Slot Pool</strong><br>这个pool要持有所有分配给当前job的slot资源，并且能在RM挂掉的情况下管理当前已经持有的slot。</p>
<p><strong>Dispatcher</strong><br>需要一个Job的分发器的主要原因是在有的集群环境下我们可能需要一个统一的提交和监控点，以及替代之前的Standalone模式下的JobManager。将来对分发器的期望可能包括权限控制等。<br><img src="http://static.zybuluo.com/bethunebtj/on7x5expzpyvtyqvkjm1si9e/image_1cfl9ju2617bh1s191mar1jsp12vot.png" alt="image_1cfl9ju2617bh1s191mar1jsp12vot.png-31.4kB"></p>
<h4 id="7-2-3-Cluster-Manager的架构"><a href="#7-2-3-Cluster-Manager的架构" class="headerlink" title="7.2.3 Cluster Manager的架构"></a>7.2.3 Cluster Manager的架构</h4><p><strong>YARN</strong><br>新的基于YARN的架构主要包括不再需要先在容器里启动集群，然后提交任务；用户代码不再使用动态ClassLoader加载；不用的资源可以释放；可以按需分配不同大小的容器等。其执行过程如下：<br>无Dispatcher时<br><img src="http://static.zybuluo.com/bethunebtj/w3z5qz98tq5q4jtndka8kdhp/image_1cfla0n7u1lg21n3o36uu0c1o5h1a.png" alt="image_1cfla0n7u1lg21n3o36uu0c1o5h1a.png-46.2kB"><br>有Dispatcher时<br><img src="http://static.zybuluo.com/bethunebtj/ukhd6f3480du2nsx2wnl56g3/image_1cfla15os15i3qcsu6c4p4clk1n.png" alt="image_1cfla15os15i3qcsu6c4p4clk1n.png-50.7kB"></p>
<p><strong>Mesos</strong><br>与基于YARN的模式很像，但是只有带Dispatcher模式，因为只有这样才能在Mesos集群里跑其RM。<br><img src="http://static.zybuluo.com/bethunebtj/k0b95bqzs9crsj2jwk8oy33n/image_1cfla4tka101n18bf1mno4npu9s24.png" alt="image_1cfla4tka101n18bf1mno4npu9s24.png-49.2kB"><br>Mesos的Fault Tolerance是类似这样的：<br><img src="http://static.zybuluo.com/bethunebtj/app8m86al53shk2a83w14x0r/image_1cfla6eka1ph71mu1pll1q0mgqq2h.png" alt="image_1cfla6eka1ph71mu1pll1q0mgqq2h.png-12.1kB"><br>必须用类似Marathon之类的技术保证Dispatcher的HA。</p>
<p><strong>Standalone</strong><br>其实没啥可说的，把以前的JobManager的职责换成现在的Dispatcher就行了。<br><img src="http://static.zybuluo.com/bethunebtj/nn4vbn25yojf3vq80yffr20v/image_1cflaaim2ih2v54umsmq01lqc2u.png" alt="image_1cflaaim2ih2v54umsmq01lqc2u.png-36.8kB"><br>将来可能会实现一个类似于轻量级Yarn的模式。</p>
<p><strong>Docker/k8s</strong><br>用户定义好容器，至少有一个是job specific的（不然怎么启动任务）；还有用于启动TM的，可以不是job specific的。启动过程如下<br><img src="http://static.zybuluo.com/bethunebtj/vcow51koxy17wd3qxj60y4lj/image_1cflafs2o1trgicjmdbndn1bdq3b.png" alt="image_1cflafs2o1trgicjmdbndn1bdq3b.png-24.2kB"></p>
<h4 id="7-2-4-组件设计及细节"><a href="#7-2-4-组件设计及细节" class="headerlink" title="7.2.4 组件设计及细节"></a>7.2.4 组件设计及细节</h4><p><strong>分配slot相关细节</strong><br>从新的TM取slot过程：<br><img src="http://static.zybuluo.com/bethunebtj/r1anoecf2er16nuh3h9r9jb8/image_1cflakoadvjm8pf6nt1k331qj33o.png" alt="image_1cflakoadvjm8pf6nt1k331qj33o.png-77.2kB"></p>
<p>从Cached TM取slot过程：<br><img src="http://static.zybuluo.com/bethunebtj/2uyr1ynvj8ieqi8rth8h8bub/image_1cflambu91ufi5fl1cg9gimdff45.png" alt="image_1cflambu91ufi5fl1cg9gimdff45.png-63.4kB"></p>
<p><strong>失败处理</strong></p>
<ol>
<li><p>TM失败<br>TM失败时，RM要能检测到失败，更新自己的状态，发送消息给JM，重启一份TM；JM要能检测到失败，从状态移除失效slot，标记该TM的task为失败，并在没有足够slot继续任务时调整规模；TM自身则要能从Checkpoint恢复</p>
</li>
<li><p>RM失败<br>此时TM要能检测到失败，并准备向新的RM注册自身，并且向新的RM传递自身的资源情况；JM要能检测到失败并且等待新的RM可用，重新请求需要的资源；丢失的数据要能从Container、TM等处恢复。</p>
</li>
<li><p>JM失败<br>TM释放所有task，向新JM注册资源，并且如果不成功，就向RM报告这些资源可用于重分配；RM坐等；JM丢失的数据从持久化存储中获得，已完成的checkpoints从HA恢复，从最近的checkpoint重启task，并申请资源。</p>
</li>
<li><p>JM &amp; RM 失败<br>TM将在一段时间内试图把资源交给新上任的JM，如果失败，则把资源交给新的RM</p>
</li>
<li><p>TM &amp; RM失败<br>JM如果正在申请资源，则要等到新的RM启动后才能获得；JM可能需要调整其规模，因为损失了TM的slot。</p>
</li>
</ol>
<h2 id="8-后记"><a href="#8-后记" class="headerlink" title="8.后记"></a>8.后记</h2><p>Flink是当前流处理领域的优秀框架，其设计思想和代码实现都蕴含着许多人的智慧结晶。这篇解读花了很多时间，篇幅也写了很长，也仍然不能能覆盖Flink的方方面面，也肯定有很多错误之处，欢迎大家批评指正！Flink生态里中文资料确实不多，对Flink源码有兴趣的读者，可以参考<a target="_blank" rel="noopener" href="https://blog.csdn.net/yanghua_kobe/article/category/6170573/4">VinoYang的专栏</a>，继续学习之旅。</p>
<p>本文至此结束。</p>
<p>最后，欢迎关注我的微信公众号，一起交流技术，或者职业生涯？<br><img src="http://static.zybuluo.com/bethunebtj/daydmugl837tmw92klc6rqxz/image_1cfhqfqgt17r89b4156i1bni1hqq9.png" alt="老白讲互联网"></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://example.com/bigdata/Flink/Flink-metrics/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="aaronzhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Guadazi">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/bigdata/Flink/Flink-metrics/" class="post-title-link" itemprop="url">Flink:指标监控</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2019-08-10 14:58:00" itemprop="dateCreated datePublished" datetime="2019-08-10T14:58:00+08:00">2019-08-10</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-06-01 20:52:39" itemprop="dateModified" datetime="2021-06-01T20:52:39+08:00">2021-06-01</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/bigdata/" itemprop="url" rel="index"><span itemprop="name">bigdata</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="flink-metrics"><a href="#flink-metrics" class="headerlink" title="flink-metrics"></a>flink-metrics</h1><p>[参考文献]</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_21653785/article/details/79625601">Flink源码系列-指标监控</a></li>
<li><a target="_blank" rel="noopener" href="https://www.cnblogs.com/0x12345678/p/10561039.html">自定义metric-report</a></li>
<li><a target="_blank" rel="noopener" href="http://www.mamicode.com/info-detail-2317943.html">深入理解Flink之metrics</a></li>
<li><a target="_blank" rel="noopener" href="https://my.oschina.net/go4it/blog/3023586">聊聊Flink的MertricsQueryServiceGateway</a></li>
<li><a target="_blank" rel="noopener" href="https://www.jianshu.com/p/e50586fff515">Flink指标</a></li>
</ul>
<p>Flink Metrics是通过引入<code>com.codahale.metrics</code>包实现的，它将收集的metrics分为四大类：<code>Counter</code>，<code>Gauge</code>，<code>Histogram</code>和<code>Meter</code>下面分别说明：</p>
<ul>
<li><code>Counter计数器</code><br>  用来统计一个metrics的总量。<br>  拿flink中的指标来举例，像Task/Operator中的numRecordsIn（此task或者operator接收到的record总量）和numRecordsOut（此task或者operator发送的record总量）就属于Counter。</li>
<li><code>Gauge指标值</code><br>  用来记录一个metrics的瞬间值。<br>  拿flink中的指标举例，像JobManager或者TaskManager中的<code>JVM.Heap.Used</code>就属于<code>Gauge</code>，记录某个时刻JobManager或者TaskManager所在机器的JVM堆使用量。</li>
<li><code>Histogram直方图</code><br>  有的时候我们不满足于只拿到metrics的总量或者瞬时值，当想得到metrics的最大值，最小值，中位数等信息时，我们就能用到Histogram了。<br>  Flink中属于Histogram的指标很少，但是最重要的一个是属于operator的latency。此项指标会记录数据处理的延迟信息，对任务监控起到很重要的作用。</li>
<li><code>Meter平均值</code><br>   用来记录一个metrics某个时间段内平均值。<br>   flink中类似指标有task/operator中的numRecordsInPerSecond，字面意思就可以理解，指的是此task或者operator每秒接收的记录数。</li>
</ul>
<h3 id="com-tencent-oceanus-metastore-metrics-CustomMetricsRegistry"><a href="#com-tencent-oceanus-metastore-metrics-CustomMetricsRegistry" class="headerlink" title="com.tencent.oceanus.metastore.metrics.CustomMetricsRegistry"></a>com.tencent.oceanus.metastore.metrics.CustomMetricsRegistry</h3><h3 id="org-apache-flink-metrics-MeterView"><a href="#org-apache-flink-metrics-MeterView" class="headerlink" title="org.apache.flink.metrics.MeterView"></a>org.apache.flink.metrics.MeterView</h3>
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://example.com/bigdata/Flink/Flink-DataTypesAndSerialization/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="aaronzhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Guadazi">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/bigdata/Flink/Flink-DataTypesAndSerialization/" class="post-title-link" itemprop="url">Flink:数据类型与序列化</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2019-08-10 14:58:00" itemprop="dateCreated datePublished" datetime="2019-08-10T14:58:00+08:00">2019-08-10</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-06-01 20:52:39" itemprop="dateModified" datetime="2021-06-01T20:52:39+08:00">2021-06-01</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/bigdata/" itemprop="url" rel="index"><span itemprop="name">bigdata</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="Flink-Data-Types-amp-Serialization"><a href="#Flink-Data-Types-amp-Serialization" class="headerlink" title="Flink Data Types &amp; Serialization"></a>Flink Data Types &amp; Serialization</h1><h2 id="使用case-class的坑"><a href="#使用case-class的坑" class="headerlink" title="使用case class的坑"></a>使用case class的坑</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Event</span>(<span class="params">id: <span class="type">Int</span></span>) </span>&#123;</span><br><span class="line">  <span class="keyword">val</span> lb = <span class="keyword">new</span> <span class="type">ListBuffer</span>[<span class="type">Int</span>]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">13:00:43,342 INFO  org.apache.flink.api.java.typeutils.TypeExtractor             - class org.myorg.quickstart.Event does not contain a setter for field id</span><br><span class="line">13:00:43,343 INFO  org.apache.flink.api.java.typeutils.TypeExtractor             - Class class org.myorg.quickstart.Event cannot be used as a POJO type because not all fields are valid POJO fields, and must be processed as GenericType. Please read the Flink documentation on &quot;Data Types &amp; Serialization&quot; for details of the effect on performance.</span><br></pre></td></tr></table></figure>
<p><strong>提示信息：找不到setter，对于POJO类型必须所有的字段必须要有setter和getter</strong><br>命名是case class啊</p>
<p>再看生产环境的例子： 折腾了一下午</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Event</span>(<span class="params">uin: <span class="type">String</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">                 sPid: <span class="type">String</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">                 applyId: <span class="type">String</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">                 bankType: <span class="type">Long</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">                 transactionId: <span class="type">String</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">                 amount: <span class="type">Long</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">                 createTime: <span class="type">Long</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">                 bizType: <span class="type">Long</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">                 modifyTime: <span class="type">String</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">                 equ: <span class="type">ListBuffer</span>[<span class="type">Int</span>]</span></span></span><br><span class="line"><span class="class"><span class="params">                </span>) </span>&#123;</span><br><span class="line"></span><br><span class="line">   <span class="function"><span class="keyword">def</span> <span class="title">this</span></span>(uin: <span class="type">String</span>,</span><br><span class="line">           sPid: <span class="type">String</span>,</span><br><span class="line">           applyId: <span class="type">String</span>,</span><br><span class="line">           bankType: <span class="type">Long</span>,</span><br><span class="line">           transactionId: <span class="type">String</span>,</span><br><span class="line">           amount: <span class="type">Long</span>,</span><br><span class="line">           createTime: <span class="type">Long</span>,</span><br><span class="line">           bizType: <span class="type">Long</span>,</span><br><span class="line">           modifyTime: <span class="type">String</span>,</span><br><span class="line">           equ: <span class="type">List</span>[<span class="type">Int</span>]</span><br><span class="line">          ) = &#123;</span><br><span class="line">    <span class="keyword">this</span>(uin, sPid, applyId, bankType, transactionId, amount, createTime, bizType, modifyTime)</span><br><span class="line">    <span class="keyword">if</span> (equ != <span class="literal">null</span>) &#123;</span><br><span class="line">      equities.appendAll(equ)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">var</span> _active = <span class="literal">false</span></span><br><span class="line">  <span class="keyword">val</span> equities: <span class="type">ListBuffer</span>[<span class="type">Int</span>] = <span class="keyword">new</span> <span class="type">ListBuffer</span>[<span class="type">Int</span>]</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">addEquity</span></span>(id: <span class="type">Int</span>): <span class="type">Event</span> = &#123;</span><br><span class="line">    equities.append(id)</span><br><span class="line">    <span class="keyword">this</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">equityString</span></span>(separator: <span class="type">String</span>): <span class="type">String</span> = &#123;</span><br><span class="line">    equities.mkString(separator)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">setActive</span></span>(): <span class="type">Event</span> = &#123;</span><br><span class="line">    _active = <span class="literal">true</span></span><br><span class="line">    <span class="keyword">this</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">setActive</span></span>(active: <span class="type">String</span>): <span class="type">Event</span> = &#123;</span><br><span class="line">    _active = <span class="string">&quot;1&quot;</span>.equals(active)</span><br><span class="line">    <span class="keyword">this</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">isActive</span></span>() = &#123;</span><br><span class="line">    _active</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>使用的是flink 1.6版本的，case class识别出来了，但是equities没有传递到下一个算子中，始终没有值</p>
<p>老老实实的修改成普通类</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Event</span>(<span class="params">_uin: <span class="type">String</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">            _sPid: <span class="type">String</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">            _applyId: <span class="type">String</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">            _bankType: <span class="type">Long</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">            _transactionId: <span class="type">String</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">            _amount: <span class="type">Long</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">            _createTime: <span class="type">Long</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">            _bizType: <span class="type">Long</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">            _modifyTime: <span class="type">String</span></span></span></span><br><span class="line"><span class="class"><span class="params">           </span>) <span class="keyword">extends</span> <span class="title">Serializable</span> </span>&#123;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">var</span> equities: <span class="type">ListBuffer</span>[<span class="type">Int</span>] = <span class="keyword">new</span> <span class="type">ListBuffer</span>[<span class="type">Int</span>]</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">var</span> uin: <span class="type">String</span> = _uin</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">var</span> sPid: <span class="type">String</span> = _sPid</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">var</span> applyId: <span class="type">String</span> = _applyId</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">var</span> bankType: <span class="type">Long</span> = _bankType</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">var</span> transactionId: <span class="type">String</span> = _transactionId</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">var</span> amount: <span class="type">Long</span> = _amount</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">var</span> createTime: <span class="type">Long</span> = _createTime</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">var</span> bizType: <span class="type">Long</span> = _bizType</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">var</span> modifyTime: <span class="type">String</span> = _modifyTime</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">var</span> _active = <span class="literal">false</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">this</span></span>(uin: <span class="type">String</span>,</span><br><span class="line">           sPid: <span class="type">String</span>,</span><br><span class="line">           applyId: <span class="type">String</span>,</span><br><span class="line">           bankType: <span class="type">Long</span>,</span><br><span class="line">           transactionId: <span class="type">String</span>,</span><br><span class="line">           amount: <span class="type">Long</span>,</span><br><span class="line">           createTime: <span class="type">Long</span>,</span><br><span class="line">           bizType: <span class="type">Long</span>,</span><br><span class="line">           modifyTime: <span class="type">String</span>,</span><br><span class="line">           equ: <span class="type">List</span>[<span class="type">Int</span>]</span><br><span class="line">          ) = &#123;</span><br><span class="line">    <span class="keyword">this</span>(uin, sPid, applyId, bankType, transactionId, amount, createTime, bizType, modifyTime)</span><br><span class="line">    <span class="keyword">if</span> (equ != <span class="literal">null</span>) &#123;</span><br><span class="line">      equities.appendAll(equ)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">getUin</span></span>: <span class="type">String</span> = uin</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">setUin</span></span>(<span class="type">Uin</span>: <span class="type">String</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">this</span>.uin = <span class="type">Uin</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">getSPid</span></span>: <span class="type">String</span> = sPid</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">setSPid</span></span>(<span class="type">SPid</span>: <span class="type">String</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">this</span>.sPid = <span class="type">SPid</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">getApplyId</span></span>: <span class="type">String</span> = applyId</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">setApplyId</span></span>(<span class="type">ApplyId</span>: <span class="type">String</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">this</span>.applyId = <span class="type">ApplyId</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">getBankType</span></span>: <span class="type">Long</span> = bankType</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">setBankType</span></span>(<span class="type">BankType</span>: <span class="type">Long</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">this</span>.bankType = <span class="type">BankType</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">getTransactionId</span></span>: <span class="type">String</span> = transactionId</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">setTransactionId</span></span>(<span class="type">TransactionId</span>: <span class="type">String</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">this</span>.transactionId = <span class="type">TransactionId</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">getAmount</span></span>: <span class="type">Long</span> = amount</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">setAmount</span></span>(<span class="type">Amount</span>: <span class="type">Long</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">this</span>.amount = <span class="type">Amount</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">getCreateTime</span></span>: <span class="type">Long</span> = createTime</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">setCreateTime</span></span>(<span class="type">CreateTime</span>: <span class="type">Long</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">this</span>.createTime = <span class="type">CreateTime</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">getBizType</span></span>: <span class="type">Long</span> = bizType</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">setBizType</span></span>(<span class="type">BizType</span>: <span class="type">Long</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">this</span>.bizType = <span class="type">BizType</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">getModifyTime</span></span>: <span class="type">String</span> = modifyTime</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">setModifyTime</span></span>(<span class="type">ModifyTime</span>: <span class="type">String</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">this</span>.modifyTime = <span class="type">ModifyTime</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">addEquity</span></span>(id: <span class="type">Int</span>): <span class="type">Event</span> = &#123;</span><br><span class="line">    equities.append(id)</span><br><span class="line">    <span class="keyword">this</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">equityString</span></span>(separator: <span class="type">String</span>): <span class="type">String</span> = &#123;</span><br><span class="line">    equities.mkString(separator)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">setActive</span></span>(): <span class="type">Event</span> = &#123;</span><br><span class="line">    _active = <span class="literal">true</span></span><br><span class="line">    <span class="keyword">this</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">setActive</span></span>(active: <span class="type">String</span>): <span class="type">Event</span> = &#123;</span><br><span class="line">    _active = <span class="string">&quot;1&quot;</span>.equals(active)</span><br><span class="line">    <span class="keyword">this</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">isActive</span></span>() = &#123;</span><br><span class="line">    _active</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">rights</span></span>(split: <span class="type">String</span>) = &#123;</span><br><span class="line">    <span class="type">RightEvent</span>(<span class="keyword">this</span>, equities.mkString(split))</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">getEquities</span> </span>= equities</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">setEquities</span></span>(equities: <span class="type">ListBuffer</span>[<span class="type">Int</span>]) = &#123;</span><br><span class="line">    <span class="keyword">this</span>.equities = equities</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>可以了</p>
<p>初步估计，序列化除了问题</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://example.com/bigdata/Flink/Flink-on-yarn/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="aaronzhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Guadazi">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/bigdata/Flink/Flink-on-yarn/" class="post-title-link" itemprop="url">Flink on yarn</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2019-08-10 14:58:00" itemprop="dateCreated datePublished" datetime="2019-08-10T14:58:00+08:00">2019-08-10</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-06-01 20:52:39" itemprop="dateModified" datetime="2021-06-01T20:52:39+08:00">2021-06-01</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/bigdata/" itemprop="url" rel="index"><span itemprop="name">bigdata</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="Flink-on-yarn-任务提交"><a href="#Flink-on-yarn-任务提交" class="headerlink" title="Flink-on-yarn 任务提交"></a>Flink-on-yarn 任务提交</h2><p><img src="https://gitee.com/averyzhang/pic-go/raw/master/img/20210422173135.png" alt="这里写图片描述"></p>
<h2 id="flink-on-yarn部署"><a href="#flink-on-yarn部署" class="headerlink" title="flink on yarn部署"></a>flink on yarn部署</h2><p>flink on yarn需要的组件与版本如下</p>
<ol>
<li>Zookeeper 3.4.9 用于做Flink的JobManager的HA服务</li>
<li>hadoop 2.7.2 搭建HDFS和Yarn</li>
<li>flink 1.3.2 或者 1.4.1版本（scala 2.11）</li>
</ol>
<p>Zookeeper, HDFS 和 Yarn 的组件的安装可以参照网上的教程。</p>
<p>在zookeeper，HDFS 和Yarn的组件的安装好的前提下，在客户机上提交Flink任务，具体流程如下：</p>
<ul>
<li>在启动Yarn-Session 之前， 设置好HADOOP_HOME,YARN_CONF_DIR ， HADOOP_CONF_DIR环境变量中三者的一个。如下所示， 根据具体的hadoop 路径来设置<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">export</span> HADOOP_HOME=/usr/<span class="built_in">local</span>/hadoop-current</span></span><br></pre></td></tr></table></figure></li>
<li>配置flink 目录下的flink-conf.yaml, 如下所示<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">jobmanager.rpc.address:</span> <span class="string">localhost</span></span><br><span class="line"><span class="attr">jobmanager.rpc.port:</span> <span class="number">6123</span></span><br><span class="line"><span class="attr">jobmanager.heap.mb:</span> <span class="number">256</span></span><br><span class="line"><span class="attr">taskmanager.heap.mb:</span> <span class="number">512</span></span><br><span class="line"><span class="attr">taskmanager.numberOfTaskSlots:</span> <span class="number">1</span></span><br><span class="line"><span class="attr">taskmanager.memory.preallocate:</span> <span class="literal">false</span></span><br><span class="line"><span class="attr">parallelism.default:</span> <span class="number">1</span></span><br><span class="line"><span class="attr">jobmanager.web.port:</span> <span class="number">8081</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># yarn</span></span><br><span class="line"><span class="attr">yarn.maximum-failed-containers:</span> <span class="number">99999</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#akka config</span></span><br><span class="line"><span class="attr">akka.watch.heartbeat.interval:</span> <span class="number">5</span> <span class="string">s</span></span><br><span class="line"><span class="attr">akka.watch.heartbeat.pause:</span> <span class="number">20</span> <span class="string">s</span></span><br><span class="line"><span class="attr">akka.ask.timeout:</span> <span class="number">60</span> <span class="string">s</span></span><br><span class="line"><span class="attr">akka.framesize:</span> <span class="string">20971520b</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#high-avaliability</span></span><br><span class="line"><span class="attr">high-availability:</span> <span class="string">zookeeper</span></span><br><span class="line"><span class="comment">## 根据安装的zookeeper信息填写</span></span><br><span class="line"><span class="attr">high-availability.zookeeper.quorum:</span> <span class="number">10.141</span><span class="number">.61</span><span class="number">.226</span><span class="string">:2181,10.141.53.244:2181,10.141.18.219:2181</span></span><br><span class="line"><span class="attr">high-availability.zookeeper.path.root:</span> <span class="string">/flink</span></span><br><span class="line"><span class="comment">## HA 信息存储到HDFS的目录，根据各自的Hdfs情况修改</span></span><br><span class="line"><span class="attr">high-availability.zookeeper.storageDir:</span> <span class="string">hdfs://hdcluster/flink/recovery/</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#checkpoint config</span></span><br><span class="line"><span class="attr">state.backend:</span> <span class="string">rocksdb</span></span><br><span class="line"><span class="comment">## checkpoint到HDFS的目录 根据各自安装的HDFS情况修改</span></span><br><span class="line"><span class="attr">state.backend.fs.checkpointdir:</span> <span class="string">hdfs://hdcluster/flink/checkpoint</span></span><br><span class="line"><span class="comment">## 对外checkpoint到HDFS的目录</span></span><br><span class="line"><span class="attr">state.checkpoints.dir:</span> <span class="string">hdfs://hdcluster/flink/savepoint</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#memory config</span></span><br><span class="line"><span class="attr">env.java.opts:</span> <span class="string">-XX:+UseConcMarkSweepGC</span> <span class="string">-XX:CMSInitiatingOccupancyFraction=75</span> <span class="string">-XX:+UseCMSInitiatingOccupancyOnly</span> <span class="string">-XX:+AlwaysPreTouch</span> <span class="string">-server</span> <span class="string">-XX:+HeapDumpOnOutOfMemoryError</span></span><br><span class="line"><span class="attr">yarn.heap-cutoff-ratio:</span> <span class="number">0.2</span></span><br><span class="line"><span class="attr">taskmanager.memory.off-heap:</span> <span class="literal">true</span></span><br><span class="line"></span><br></pre></td></tr></table></figure></li>
<li>提交Yarn-Session，切换到flink的bin 目录下,提交命令如下<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> ./yarn-session.sh -n 2 -s 6 -jm 3072 -tm 6144 -nm <span class="built_in">test</span> -d</span></span><br></pre></td></tr></table></figure>
启动yarn-session的参数解释如下</li>
</ul>
<table>
<thead>
<tr>
<th>参数</th>
<th>参数解释</th>
<th>设置推荐</th>
</tr>
</thead>
<tbody><tr>
<td>-n(–container)</td>
<td>taskmanager的数量</td>
<td></td>
</tr>
<tr>
<td>-s(–slots)</td>
<td>用启动应用所需的slot数量/ -s 的值向上取整，有时可以多一些taskmanager，做冗余 每个taskmanager的slot数量，默认一个slot一个core，默认每个taskmanager的slot的个数为1</td>
<td>6～10</td>
</tr>
<tr>
<td>-jm</td>
<td>jobmanager的内存（单位MB)</td>
<td>3072</td>
</tr>
<tr>
<td>-tm</td>
<td>每个taskmanager的内存（单位MB)</td>
<td>根据core 与内存的比例来设置，-s的值＊ （core与内存的比）来算</td>
</tr>
<tr>
<td>-nm</td>
<td>yarn 的appName(现在yarn的ui上的名字)｜</td>
<td></td>
</tr>
<tr>
<td>-d</td>
<td>后台执行</td>
<td></td>
</tr>
</tbody></table>
<ul>
<li>提交yarn－session 后，可以在yarn的ui上看到一个应用（应用有一个appId）, 切换到flink的bin目录下，提交flink 应用。命令如下<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> ./flink -run file:///home/yarn/test.jar -a 1 -p 12 -yid appId -nm flink-test -d</span></span><br></pre></td></tr></table></figure>
启动flink 应用的参数解释如下</li>
</ul>
<table>
<thead>
<tr>
<th>参数</th>
<th>参数解释</th>
</tr>
</thead>
<tbody><tr>
<td>-j</td>
<td>运行flink 应用的jar所在的目录</td>
</tr>
<tr>
<td>-a</td>
<td>运行flink 应用的主方法的参数</td>
</tr>
<tr>
<td>-p</td>
<td>运行flink应用的并行度</td>
</tr>
<tr>
<td>-c</td>
<td>运行flink应用的主类, 可以通过在打包设置主类</td>
</tr>
<tr>
<td>-nm</td>
<td>flink 应用名字，在flink-ui 上面展示</td>
</tr>
<tr>
<td>-d</td>
<td>后台执行</td>
</tr>
<tr>
<td>–fromsavepoint</td>
<td>flink 应用启动的状态恢复点</td>
</tr>
</tbody></table>
<ul>
<li>启动flink应用成功，即可在yarn ui 点击对应应用的ApplicationMaster链接,既可以查看flink-ui ，并查看flink 应用运行情况。</li>
</ul>
<p>注：在安装部署遇到任何问题，可以在小象问答，微信群以及私聊提出，我们一般会在晚上作答（由于白天要上班，作答不及时请谅解。）</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://example.com/bigdata/Flink/Flink-VS-Spark/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="aaronzhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Guadazi">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/bigdata/Flink/Flink-VS-Spark/" class="post-title-link" itemprop="url">Flink VS Spark</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2019-08-10 14:58:00" itemprop="dateCreated datePublished" datetime="2019-08-10T14:58:00+08:00">2019-08-10</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-06-01 20:52:39" itemprop="dateModified" datetime="2021-06-01T20:52:39+08:00">2021-06-01</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/bigdata/" itemprop="url" rel="index"><span itemprop="name">bigdata</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="Flink-VS-Spark"><a href="#Flink-VS-Spark" class="headerlink" title="Flink VS Spark"></a>Flink VS Spark</h1><p>Spark Structure Streaming 是什么？</p>
<h3 id="1、抽象-Abstraction"><a href="#1、抽象-Abstraction" class="headerlink" title="1、抽象 Abstraction"></a>1、抽象 Abstraction</h3><p>　　Spark中，对于批处理我们有RDD,对于流式，我们有DStream，不过内部实际还是RDD.所以所有的数据表示本质上还是RDD抽象。在Flink中，对于批处理有DataSet，对于流式我们有DataStreams。看起来和Spark类似，他们的不同点在于：</p>
<p>　　<strong>（一）DataSet在运行时是表现为运行计划(runtime plans)的</strong></p>
<p>　　在Spark中，RDD在运行时是表现为java objects的。通过引入Tungsten，这块有了些许的改变。但是在Flink中是被表现为logical plan(逻辑计划)的, 就是类似于Spark中的dataframes。所以在Flink中你使用的类Dataframe api是被作为第一优先级来优化的。但是相对来说在Spark RDD中就没有了这块的优化了。<br>　　Flink中的Dataset，对标Spark中的Dataframe，在运行前会经过优化。在Spark 1.6，dataset API已经被引入Spark了，也许最终会取代RDD 抽象。</p>
<p>　　<strong>(二）Dataset和DataStream是独立的API</strong></p>
<p>　　在Spark中，所有不同的API，例如DStream，Dataframe都是基于RDD抽象的。但是在Flink中，Dataset和DataStream是同一个公用的引擎之上两个独立的抽象。所以你不能把这两者的行为合并在一起操作，当然，Flink社区目前在朝这个方向努力(<code>https://issues.apache.org/jira/browse/Flink-2320</code>)，但是目前还不能轻易断言最后的结果。</p>
<h3 id="2、内存管理"><a href="#2、内存管理" class="headerlink" title="2、内存管理"></a>2、内存管理</h3><p>　　一直到1.5版本，Spark都是试用java的内存管理来做数据缓存，明显很容易导致OOM或者gc。所以从1.5开始，Spark开始转向精确的控制内存的使用，这就是tungsten项目了。</p>
<p>　　而Flink从第一天开始就坚持自己控制内存试用。这个也是启发了Spark走这条路的原因之一。Flink除了把数据存在自己管理的内存以外，还直接操作二进制数据。在Spark中，从1.5开始，所有的dataframe操作都是直接作用在tungsten的二进制数据上。</p>
<h3 id="3、语言实现"><a href="#3、语言实现" class="headerlink" title="3、语言实现"></a>3、语言实现</h3><ul>
<li><p>实现语言</p>
<p>Spark和Flink均有Scala/Java混合编程实现，Spark的核心逻辑由Scala完成，Flink的主要核心逻辑由Java完成</p>
</li>
<li><p>支持应用语言<br> Flink主要支持Scala，和Java编程，部分API支持python应用<br> Spark主要支持Scala，Java，Python,R语言编程，部分API暂不支持Python和R</p>
</li>
</ul>
<h3 id="4、API"><a href="#4、API" class="headerlink" title="4、API"></a>4、API</h3><table>
<thead>
<tr>
<th>API对比</th>
<th>Flink</th>
<th></th>
<th>Spark</th>
<th></th>
<th></th>
</tr>
</thead>
<tbody><tr>
<td>应用类型</td>
<td>Batch</td>
<td>Streaming</td>
<td>Batch</td>
<td>Structed Streaming</td>
<td>SparkStreaming</td>
</tr>
<tr>
<td>数据表示</td>
<td>Dataset</td>
<td>datastream</td>
<td>RDD,Dataset</td>
<td>Dataset</td>
<td>Dtream</td>
</tr>
<tr>
<td>主要支持API</td>
<td>map,filter,flatMap等</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>转换后数据类型</td>
<td>Dataset</td>
<td>datastream</td>
<td>RDD,Dataset</td>
<td>Dataset</td>
<td>Dtream</td>
</tr>
</tbody></table>
<h4 id="批处理："><a href="#批处理：" class="headerlink" title="批处理："></a>批处理：</h4><p>Spark批处理的数据表示经历了从<code>RDD -&gt; DataFrame -&gt; Dataset</code>的变化，均具有不可变，lazy执行，可分区等特性，是Spark框架的核心，rdd经过map等函数操作后，并没有改变而是生成新的RDD，Spark的Dataset（DataFrame是一种特殊的Dataset，已经不推荐使用）还包含数据类型信息</p>
<p>Flink批处理的API是Dataset,同样具有不可变，lazy执行，可分区等特性，是Flink框架的核心，Dataset经过map等函数操作后，并没有改变而是生成新的Dataset</p>
<h4 id="流处理"><a href="#流处理" class="headerlink" title="流处理"></a>流处理</h4><ul>
<li><p>Spark Streaming</p>
<p>Spark在1.*版本引入的spark streaming作为流处理模块，抽象出Dstream的API来进行流数据处理，同时抽象出通过receiver获取消息数据，然后启动task处理的模式，以及直接启动task消费处理两种方式的流式数据处理。receiver模式由于稳定性不足被遗弃，推荐使用的是直接消费模式；然而本质上讲，Sparkstreaming的流处理是micro-batch的处理模式，将一定时间的流数据作为一个block/RDD，然后使用批处理的rdd的api来完成数据的处理。</p>
</li>
<li><p>Structed streaming</p>
<p>随着Spark在2.*版本的Structed streaming的推出，Spark streaming模块进入了维护模式，从Spark2.*版本以来没有已经没有更新，当前社区主推使用Structed streaming进行流处理。Structed streaming在流处理中有两种流处理模式，一种是microbatch模式；一种是continuous模式；</p>
<ul>
<li><p>microbatch模式与spark streaming的microbatch模式大致相当，分批处理消息，但可通过设置连续的批次处理，即一个批次执行完之后立即进入下一个批次的处理</p>
</li>
<li><p>continuous模式，可以实现真正的流数据处理，端到端的毫秒级，当前处于Experiment状态，也只能支持简单的map,filter操作，当前不支持聚合，<code>current_timestamp</code>，<code>current_date</code>等操作</p>
</li>
<li><p>PS : microbatch &lt;—-&gt; continuous 两种模式可以相互切换且无需改动代码</p>
</li>
</ul>
</li>
</ul>
<ul>
<li><p>Flink Streaming</p>
<p>Flink Streaming以流的方式处理流数据，可以实现简单map,fliter等操作，也可以实现复杂的聚合，关联操作，以完善的处理模型及high throughout得到了广泛的应用。</p>
</li>
</ul>
<p>　　Spark和Flink都在模仿scala的collection API.所以从表面看起来，两者都很类似。下面是分别用RDD和DataSet API实现的word count</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Spark wordcount</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">WordCount</span> </span>&#123;</span><br><span class="line"> </span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]) &#123;</span><br><span class="line">    <span class="keyword">val</span> env = <span class="keyword">new</span> <span class="type">SparkContext</span>(<span class="string">&quot;local&quot;</span>,<span class="string">&quot;wordCount&quot;</span>)</span><br><span class="line">    <span class="keyword">val</span> data = <span class="type">List</span>(<span class="string">&quot;hi&quot;</span>,<span class="string">&quot;how are you&quot;</span>,<span class="string">&quot;hi&quot;</span>)</span><br><span class="line">    <span class="keyword">val</span> dataSet = env.parallelize(data)</span><br><span class="line">    <span class="keyword">val</span> words = dataSet.flatMap(value =&gt; value.split(<span class="string">&quot;\\s+&quot;</span>))</span><br><span class="line">    <span class="keyword">val</span> mappedWords = words.map(value =&gt; (value,<span class="number">1</span>))</span><br><span class="line">    <span class="keyword">val</span> sum = mappedWords.reduceByKey(_+_)</span><br><span class="line">    println(sum.collect())</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line"><span class="comment">// Flink wordcount</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">WordCount</span> </span>&#123;</span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]) &#123;</span><br><span class="line">　　<span class="keyword">val</span> env = <span class="type">ExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">　　<span class="keyword">val</span> data = <span class="type">List</span>(<span class="string">&quot;hi&quot;</span>,<span class="string">&quot;how are you&quot;</span>,<span class="string">&quot;hi&quot;</span>)</span><br><span class="line">　　<span class="keyword">val</span> dataSet = env.fromCollection(data)</span><br><span class="line">　　<span class="keyword">val</span> words = dataSet.flatMap(value =&gt; value.split(<span class="string">&quot;\\s+&quot;</span>))</span><br><span class="line">　　<span class="keyword">val</span> mappedWords = words.map(value =&gt; (value,<span class="number">1</span>))</span><br><span class="line">　　<span class="keyword">val</span> grouped = mappedWords.groupBy(<span class="number">0</span>)</span><br><span class="line">　　<span class="keyword">val</span> sum = grouped.sum(<span class="number">1</span>)</span><br><span class="line">　　println(sum.collect())</span><br><span class="line">　&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>　　不知道是偶然还是故意的，API都长得很像，这样很方便开发者从一个引擎切换到另外一个引擎。我感觉以后这种Collection API会成为写data pipeline的标配。</p>
<h3 id="5、Steaming"><a href="#5、Steaming" class="headerlink" title="5、Steaming"></a>5、Steaming</h3><p>　　Spark把streaming看成是更快的批处理，而Flink把批处理看成streaming的special case。这里面的思路决定了各自的方向，其中两者的差异点有如下这些：</p>
<p><strong>实时 vs 近实时的角度</strong></p>
<p>　　Flink提供了基于每个事件的流式处理机制，所以可以被认为是一个真正的流式计算。它非常像storm的model。而Spark，不是基于事件的粒度，而是用小批量来模拟流式，也就是多个事件的集合。所以Spark被认为是近实时的处理系统。</p>
<p>　　Spark streaming 是更快的批处理，而Flink Batch是有限数据的流式计算。虽然大部分应用对准实时是可以接受的，但是也还是有很多应用需要event level的流式计算。这些应用更愿意选择storm而非Spark streaming，现在，Flink也许是一个更好的选择。</p>
<p><strong>流式计算和批处理计算的表示</strong></p>
<p>　　Spark对于批处理和流式计算，都是用的相同的抽象：RDD，这样很方便这两种计算合并起来表示。而Flink这两者分为了DataSet和DataStream，相比Spark，这个设计算是一个糟糕的设计。</p>
<p><strong>对 windowing 的支持</strong></p>
<p>　　因为Spark的小批量机制，Spark对于windowing的支持非常有限。只能基于process time，且只能对batches来做window。而Flink对window的支持非常到位，且Flink对windowing API的支持是相当给力的，允许基于process time,data time,record 来做windowing。我不太确定Spark是否能引入这些API，不过到目前为止，Flink的windowing支持是要比Spark好的。Steaming这部分Flink胜</p>
<table>
<thead>
<tr>
<th>Window 类型</th>
<th>Window 含义</th>
<th>Flink Streaming</th>
<th>SparkStreaming</th>
<th>Structed Streaming</th>
<th>备注</th>
</tr>
</thead>
<tbody><tr>
<td>tumblingWindow</td>
<td>一个滚动的window</td>
<td>支持</td>
<td>支持</td>
<td>支持</td>
<td></td>
</tr>
<tr>
<td>Sliding window</td>
<td>滑动的window</td>
<td>支持</td>
<td>支持</td>
<td>支持</td>
<td></td>
</tr>
<tr>
<td>Global window</td>
<td>全局window</td>
<td>支持</td>
<td>间接实现</td>
<td>间接支持</td>
<td>间接支持的含义是可以时间类似功能，但没有抽象出该window</td>
</tr>
<tr>
<td>Session window</td>
<td>以接收到数据开始，一定时间没有接收到数据，则结束</td>
<td>支持</td>
<td>不支持</td>
<td>不支持</td>
<td></td>
</tr>
</tbody></table>
<p><strong>流join分析：</strong></p>
<p>由于Spark streaming中不支持event time的概念，其只能支持window不同Dstream的RDD的join，不同window间无法join</p>
<table>
<thead>
<tr>
<th>模块</th>
<th>event-time</th>
<th>流join</th>
<th>join实现方式</th>
<th>处理方式</th>
<th>备注</th>
</tr>
</thead>
<tbody><tr>
<td>Spark streaming</td>
<td>不支持</td>
<td>支持</td>
<td>window内</td>
<td>processingTime</td>
<td>micro-batch处理</td>
</tr>
<tr>
<td>FLink1.5之前</td>
<td>支持</td>
<td>支持</td>
<td>window内</td>
<td>native处理，join时(window触发)，watermark灵活</td>
<td>Processing Time／ EventTime ／ element Number</td>
</tr>
<tr>
<td>FLink1.6之后</td>
<td>支持</td>
<td>支持</td>
<td>window内，跨window</td>
<td>native处理，join时(window触发)，watermark灵活</td>
<td>Processing Time／ EventTime ／ element Number</td>
</tr>
<tr>
<td>Structed Streaming 2.2</td>
<td>支持</td>
<td>不支持</td>
<td>仅支持流数据和静态数据的join</td>
<td>native处理，join时(window触发)，watermark灵活</td>
<td>Processing Time／ EventTime</td>
</tr>
<tr>
<td>Structed Streaming 2.3+</td>
<td>支持</td>
<td>支持</td>
<td>跨window</td>
<td>native处理，join时（proocessingTime（interval）触发）</td>
<td>Processing Time／ EventTime</td>
</tr>
</tbody></table>
<p>PS:</p>
<ul>
<li>Flink／structed streaming开发难度相当，FLink略复杂，但灵活度更高</li>
<li>Flink的inteval join</li>
<li>Structed Streaming支持数据去重（同个imsi的数据的多个不同join结果的去重）</li>
<li>FLink的窗口操作相当于structed streaming的update模式</li>
<li>Flink的单流的watermark更新时实时的，有专门线程处理</li>
<li>Structed streaming的watermark更新时间基于批的，每个批次共用同一个watermark，如果有多个流，多个流共用一个watermark</li>
<li>structed Streaming的watermark更新方法：<br> 基于每个流找出该流的watermark：Max_event_time - lateness<br> 找出所有流中最小/最大的watermark设置为batch的watermark</li>
<li>Flink专门抽象了类以便不同场景下使用自定义的eventTime的waterMark获取/设置方法,且提供了一般场景下的的类以便使用</li>
<li>Flink抽象了trigger和evictor来实现触发计算和清理数据的逻辑，以便自定义相关逻辑</li>
<li>FLink 支持sideoutput输出，如迟到的数据可以单独输出</li>
</ul>
<h3 id="6、SQL-interface"><a href="#6、SQL-interface" class="headerlink" title="6、SQL interface"></a>6、SQL interface</h3><p>　　目前Spark-sql是Spark里面最活跃的组件之一，Spark提供了类似Hive的sql和Dataframe这种DSL来查询结构化数据，API很成熟，在流式计算中使用很广，预计在流式计算中也会发展得很快。至于Flink，到目前为止，Flink Table API只支持类似DataFrame这种DSL，并且还是处于beta状态，社区有计划增加SQL 的interface，但是目前还不确定什么时候才能在框架中用上。所以这个部分，Spark胜出。目前Flink已经支持SQL API</p>
<h3 id="7、外部数据源的整合"><a href="#7、外部数据源的整合" class="headerlink" title="7、外部数据源的整合"></a>7、外部数据源的整合</h3><p>　　Spark的数据源 API是整个框架中最好的，支持的数据源包括NoSql db,parquet,ORC等，并且支持一些高级的操作，例如predicate push down。Flink目前还依赖map/reduce InputFormat来做数据源聚合。这一场Spark胜，目前已经提供 </p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">env.readTextFile(path_i)</span><br><span class="line">env.writeTextFile(path_i)</span><br></pre></td></tr></table></figure>


<h3 id="8、Iterative-processing"><a href="#8、Iterative-processing" class="headerlink" title="8、Iterative processing"></a>8、Iterative processing</h3><p>![Flink 迭代处理](_v_images/20190723102241286_1833770582.png =519x)<br>![Spark迭代处理](_v_images/20190723102318593_811318029.png =519x)<br>　　Spark对机器学习的支持较好，因为利用内存cache来加速机器学习算法。然而大部分机器学习算法其实是一个有环的数据流，但是在Spark中，实际是用无环图来表示的，一般的分布式处理引擎都是不鼓励试用有环图的。但是Flink这里又有点不一样，Flink支持在runtime中的有环数据流，这样表示机器学习算法更有效而且更有效率。这一点Flink胜出。</p>
<h3 id="9、Stream-as-platform-vs-Batch-as-Platform"><a href="#9、Stream-as-platform-vs-Batch-as-Platform" class="headerlink" title="9、Stream as platform vs Batch as Platform"></a>9、Stream as platform vs Batch as Platform</h3><ul>
<li><p>Spark诞生在Map/Reduce的时代，数据都是以文件的形式保存在磁盘中，这样非常方便做容错处理。</p>
</li>
<li><p>Flink把纯流式数据计算引入大数据时代，无疑给业界带来了一股清新的空气。这个idea非常类似akka-streams这种。</p>
</li>
</ul>
<p>【参考文献】</p>
<ol>
<li><a target="_blank" rel="noopener" href="https://www.iteblog.com/archives/1624.html">Apache Flink vs Apache Spark</a></li>
<li><a target="_blank" rel="noopener" href="https://www.jianshu.com/p/da1910535f73">Flink vs Spark</a></li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://example.com/Java/multithread/08.Thread-Native/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="aaronzhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Guadazi">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/Java/multithread/08.Thread-Native/" class="post-title-link" itemprop="url">Java Thread与系统线程对应关系</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2019-07-03 00:00:00" itemprop="dateCreated datePublished" datetime="2019-07-03T00:00:00+08:00">2019-07-03</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-06-01 20:52:39" itemprop="dateModified" datetime="2021-06-01T20:52:39+08:00">2021-06-01</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Java/" itemprop="url" rel="index"><span itemprop="name">Java</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">SecurityManager securityManager = <span class="keyword">new</span> SecurityManager();</span><br><span class="line">ThreadInfo[] infos = ManagementFactory.getThreadMXBean().dumpAllThreads(<span class="keyword">true</span>,<span class="keyword">true</span>);</span><br><span class="line">Stream.of(infos).forEach(info-&gt;&#123; System.out.println(info.getThreadName()+<span class="string">&quot;\t&quot;</span>+info.getThreadId()+<span class="string">&quot;\t&quot;</span>+info.getThreadState());</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure>



<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="[参考文献]"></a>[参考文献]</h2><ol>
<li><a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_27035123/article/details/77651534">Java线程与内核线程</a></li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/10/"><i class="fa fa-angle-left" aria-label="Previous page"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/10/">10</a><span class="page-number current">11</span><a class="page-number" href="/page/12/">12</a><span class="space">&hellip;</span><a class="page-number" href="/page/25/">25</a><a class="extend next" rel="next" href="/page/12/"><i class="fa fa-angle-right" aria-label="Next page"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">aaronzhang</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">242</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">16</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">128</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">aaronzhang</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  

</body>
</html>
