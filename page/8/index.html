<!DOCTYPE html>
<html lang="zh-Hans">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta property="og:type" content="website">
<meta property="og:title" content="Guadazi">
<meta property="og:url" content="http://example.com/page/8/index.html">
<meta property="og:site_name" content="Guadazi">
<meta property="og:locale">
<meta property="article:author" content="aaronzhang">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://example.com/page/8/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'zh-Hans'
  };
</script>

  <title>Guadazi</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Guadazi</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://example.com/bigdata/Spark/Spark-usage/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="aaronzhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Guadazi">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/bigdata/Spark/Spark-usage/" class="post-title-link" itemprop="url">Spark 使用</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2019-08-10 14:58:00" itemprop="dateCreated datePublished" datetime="2019-08-10T14:58:00+08:00">2019-08-10</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-04-11 16:41:41" itemprop="dateModified" datetime="2021-04-11T16:41:41+08:00">2021-04-11</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/bigdata/" itemprop="url" rel="index"><span itemprop="name">bigdata</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="Spark-使用"><a href="#Spark-使用" class="headerlink" title="Spark 使用"></a>Spark 使用</h1><h2 id="copy-file"><a href="#copy-file" class="headerlink" title="copy file"></a>copy file</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.&#123;<span class="type">FileAlreadyExistsException</span>, <span class="type">FileSystem</span>, <span class="type">FileUtil</span>, <span class="type">Path</span>&#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> srcFileSystem: <span class="type">FileSystem</span> = <span class="type">FileSystemUtil</span></span><br><span class="line">      .apply(spark.sparkContext.hadoopConfiguration)</span><br><span class="line">      .getFileSystem(sourceFile)</span><br><span class="line">    <span class="keyword">val</span> dstFileSystem: <span class="type">FileSystem</span> = <span class="type">FileSystemUtil</span></span><br><span class="line">      .apply(spark.sparkContext.hadoopConfiguration)</span><br><span class="line">      .getFileSystem(sourceFile)</span><br><span class="line"></span><br><span class="line">    <span class="type">FileUtil</span>.copy(</span><br><span class="line">      srcFileSystem,</span><br><span class="line">      <span class="keyword">new</span> <span class="type">Path</span>(<span class="keyword">new</span> <span class="type">URI</span>(sourceFile)),</span><br><span class="line">      dstFileSystem,</span><br><span class="line">      <span class="keyword">new</span> <span class="type">Path</span>(<span class="keyword">new</span> <span class="type">URI</span>(targetFile)),</span><br><span class="line">      <span class="literal">true</span>,</span><br><span class="line">      spark.sparkContext.hadoopConfiguration)</span><br></pre></td></tr></table></figure>
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://example.com/bigdata/Flink/Flink-checkpoint-savepoint-2pc/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="aaronzhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Guadazi">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/bigdata/Flink/Flink-checkpoint-savepoint-2pc/" class="post-title-link" itemprop="url">Flink-checkpoint与高可用</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2019-08-10 14:58:00" itemprop="dateCreated datePublished" datetime="2019-08-10T14:58:00+08:00">2019-08-10</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-04-11 16:41:41" itemprop="dateModified" datetime="2021-04-11T16:41:41+08:00">2021-04-11</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/bigdata/" itemprop="url" rel="index"><span itemprop="name">bigdata</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>Flink Checkpoint 受 Chandy-Lamport 分布式快照启发，可以保证数据的高可用。但是有些情况下，不见得一定有效:</p>
<p>Flink On Yarn 模式，某个 Container 发生 OOM 异常，这种情况程序直接变成失败状态，此时 Flink 程序虽然开启 Checkpoint 也无法恢复，因为程序已经变成失败状态，所以此时可以借助外部参与启动程序，比如外部程序检测到实时任务失败时，从新对实时任务进行拉起。</p>
<h2 id="1-1-2PC"><a href="#1-1-2PC" class="headerlink" title="1.1. 2PC"></a>1.1. 2PC</h2><h3 id="1-1-1-Exactly-once-VS-At-least-once"><a href="#1-1-1-Exactly-once-VS-At-least-once" class="headerlink" title="1.1.1. Exactly-once VS At-least-once"></a>1.1.1. Exactly-once VS At-least-once</h3><p>算子做快照时，如果等所有输入端的barrier都到了才开始做快照，可保证算子的exactly-once；如果为了降低延时而跳过对齐，从而继续处理数据，那么等barrier都到齐后做快照就是at-least-once了，因为这次的快照掺杂了下一次快照的数据，当作业失败恢复的时候，这些数据会重复作用系统，就好像这些数据被消费了两遍。</p>
<p>注：对齐只会发生在算子的上端是join操作以及上游存在partition或者shuffle的情况，对于直连操作类似map、flatMap、filter等还是会保证exactly-once的语义。</p>
<h3 id="1-1-2-端到端的Exactly-once实现"><a href="#1-1-2-端到端的Exactly-once实现" class="headerlink" title="1.1.2. 端到端的Exactly once实现"></a>1.1.2. 端到端的Exactly once实现</h3><p>2PC分为几个阶段: 开始事务-&gt;预提交-&gt;提交(或回滚)</p>
<p>为了保证Exactly once, source和sink必须支持flink的2PC</p>
<p>当状态涉及到外部系统时，需要外部系统支持事务操作来配合flink实现2PC协议，从而保证数据的exatly-once。<br>这个时候，sink算子除了将自己的state写到后段，还必须准备好事务提交。</p>
<ul>
<li>一旦所有的算子完成了它们的pre-commit，它们会要求一个commit。</li>
<li>如果存在一个算子pre-commit失败了，本次事务失败，我们回滚到上次的checkpoint。</li>
<li>一旦master做出了commit的决定，那么这个commit必须得到执行，就算宕机恢复也有继续执行。</li>
</ul>
<h4 id="1-1-2-1-pre-commit"><a href="#1-1-2-1-pre-commit" class="headerlink" title="1.1.2.1. pre-commit"></a>1.1.2.1. pre-commit</h4><p>pre-commit阶段起始于一次快照的开始，即master节点将checkpoint的barrier注入source端，barrier随着数据向下流动直到sink端。barrier每到一个算子，都会出发算子做本地快照。</p>
<p><img src="_v_images/20200710154629243_751269158.png" alt="precommit"></p>
<p>当所有的算子都做完了本地快照并且回复到master节点时，pre-commit阶段才算结束。这个时候，checkpoint已经成功，并且包含了外部系统的状态。如果作业失败，可以进行恢复。</p>
<p><img src="_v_images/20200710154750060_1524377793.png" alt="precommit-success"></p>
<h4 id="1-1-2-2-commit"><a href="#1-1-2-2-commit" class="headerlink" title="1.1.2.2. commit"></a>1.1.2.2. commit</h4><p>通知所有的算子这次checkpoint成功了，即2PC的commit阶段。source节点和window节点没有外部状态，所以这时它们不需要做任何操作。<br>而对于sink节点，需要commit这次事务，将数据写到外部系统。</p>
<p><img src="_v_images/20200710154844838_737658241.png" alt="commit"></p>
<h4 id="1-1-2-3-rollback"><a href="#1-1-2-3-rollback" class="headerlink" title="1.1.2.3. rollback"></a>1.1.2.3. rollback</h4><p>一旦任何一个算子的快照保存失败，则触发回滚，同样的sink算子也需要取消写入外部的数据</p>
<h3 id="1-1-3-TwoPhaseCommitSinkFunction"><a href="#1-1-3-TwoPhaseCommitSinkFunction" class="headerlink" title="1.1.3. TwoPhaseCommitSinkFunction"></a>1.1.3. TwoPhaseCommitSinkFunction</h3><p>为了简化2PC的实现成本，flink抽象了TwoPhaseCommitSinkFunction</p>
<ul>
<li>beginTransaction。开始一次事务，在目的文件系统创建一个临时文件。接下来我们就可以将数据写到这个文件。</li>
<li>preCommit。在这个阶段，将文件flush掉，同时重起一个文件写入，作为下一次事务的开始。</li>
<li>commit。这个阶段，将文件写到真正的目的目录。值得注意的是，这会增加数据可视的延时。</li>
<li>abort。如果回滚，那么删除临时文件。</li>
</ul>
<p>如果pre-commit成功了，但是commit没有到达算子旧宕机了，flink会将算子恢复到pre-commit时的状态，然后继续commit。</p>
<p>我们需要做的还有就是保证commit的幂等性，这可以通过检查临时文件是否还在来实现。</p>
<h2 id="1-2-checkpoint"><a href="#1-2-checkpoint" class="headerlink" title="1.2. checkpoint"></a>1.2. checkpoint</h2><p><strong>保留策略</strong>:</p>
<ul>
<li>DELETE_ON_CANCELLATION 表示当程序取消时，删除 Checkpoint 存储文件。</li>
<li>RETAIN_ON_CANCELLATION 表示当程序取消时，保存之前的 Checkpoint 存储文件</li>
</ul>
<p>默认情况下，Flink不会触发一次 Checkpoint 当系统有其他 Checkpoint 在进行时，也就是说 Checkpoint 默认的并发为1。</p>
<p><strong>CheckpointCoordinator</strong> :</p>
<p>针对 Flink DataStream 任务，程序需要经历从 StreamGraph -&gt; JobGraph -&gt; ExecutionGraph -&gt; 物理执行图四个步骤，其中在 ExecutionGraph 构建时，会初始化 CheckpointCoordinator。ExecutionGraph通过ExecutionGraphBuilder.buildGraph方法构建，在构建完时，会调用 ExecutionGraph 的enableCheckpointing方法创建CheckpointCoordinator</p>
<p><strong>Flink Checkpoint 参数配置及建议</strong>:</p>
<ul>
<li>当 Checkpoint 时间比设置的 Checkpoint 间隔时间要长时，可以设置 Checkpoint 间最小时间间隔 。这样在上次 Checkpoint 完成时，不会立马进行下一次 Checkpoint，而是会等待一个最小时间间隔，然后在进行该次 Checkpoint。否则，每次 Checkpoint 完成时，就会立马开始下一次 Checkpoint，系统会有很多资源消耗 Checkpoint。</li>
<li>如果Flink状态很大，在进行恢复时，需要从远程存储读取状态恢复，此时可能导致任务恢复很慢，可以设置 Flink Task 本地状态恢复。任务状态本地恢复默认没有开启，可以设置参数state.backend.local-recovery值为true进行激活。</li>
<li>Checkpoint保存数，Checkpoint 保存数默认是1，也就是保存最新的 Checkpoint 文件，当进行状态恢复时，如果最新的Checkpoint文件不可用时(比如HDFS文件所有副本都损坏或者其他原因)，那么状态恢复就会失败，如果设置 Checkpoint 保存数2，即使最新的Checkpoint恢复失败，那么Flink 会回滚到之前那一次Checkpoint进行恢复。考虑到这种情况，用户可以增加 Checkpoint 保存数。</li>
<li>建议设置的 Checkpoint 的间隔时间最好大于 Checkpoint 的完成时间。</li>
</ul>
<p>下图是不设置 Checkpoint 最小时间间隔示例图，可以看到，系统一致在进行 Checkpoint，可能对运行的任务产生一定影响：<br><img src="_v_images/20200714095846469_121820659.png"></p>
<h2 id="1-3-savepoint"><a href="#1-3-savepoint" class="headerlink" title="1.3. savepoint"></a>1.3. savepoint</h2><blockquote>
<p>注意:<br>使用DataStream进行开发，建议为每个算子定义一个 uid，这样我们在修改作业时，即使导致程序拓扑图改变，由于相关算子 uid 没有变，那么这些算子还能够继续使用之前的状态，如果用户没有定义 uid ， Flink 会为每个算子自动生成 uid，如果用户修改了程序，可能导致之前的状态程序不能再进行复用。</p>
</blockquote>
<p>Flink 在触发Savepoint 或者 Checkpoint时，会根据这次触发的类型计算出在HDFS上面的目录:</p>
<p>如果类型是 Savepoint，那么 其 HDFS 上面的目录为：Savepoint 根目录+savepoint-jobid前六位+随机数字，具体如下格式：</p>
<p><img src="_v_images/20200714100459823_887900222.png"></p>
<p>Checkpoint 目录为 chk-checkpoint ID,具体格式如下：</p>
<p><img src="_v_images/20200714100516223_630729421.png"></p>
<ul>
<li>使用 flink cancel -s 命令取消作业同时触发 Savepoint 时，会有一个问题，可能存在触发 Savepoint 失败。比如实时程序处于异常状态(比如 Checkpoint失败)，而此时你停止作业，同时触发 Savepoint,这次 Savepoint 就会失败，这种情况会导致，在实时平台上面看到任务已经停止，但是实际实时作业在 Yarn 还在运行。针对这种情况，需要捕获触发 Savepoint 失败的异常，当抛出异常时，可以直接在 Yarn 上面 Kill 掉该任务。</li>
<li>使用 DataStream 程序开发时，最好为每个算子分配 uid,这样即使作业拓扑图变了，相关算子还是能够从之前的状态进行恢复，默认情况下，Flink 会为每个算子分配 uid,这种情况下，当你改变了程序的某些逻辑时，可能导致算子的 uid 发生改变，那么之前的状态数据，就不能进行复用，程序在启动的时候，就会报错。</li>
<li>由于 Savepoint 是程序的全局状态，对于某些状态很大的实时任务，当我们触发 Savepoint，可能会对运行着的实时任务产生影响，个人建议如果对于状态过大的实时任务，触发 Savepoint 的时间，不要太过频繁。根据状态的大小，适当的设置触发时间。</li>
<li>当我们从 Savepoint 进行恢复时，需要检查这次 Savepoint 目录文件是否可用。可能存在你上次触发 Savepoint 没有成功，导致 HDFS 目录上面 Savepoint 文件不可用或者缺少数据文件等，这种情况下，如果在指定损坏的 Savepoint 的状态目录进行状态恢复，任务会启动不起来。</li>
</ul>
<h2 id="1-4-snapshot保存到哪里-应该需要汇总到jobManager？"><a href="#1-4-snapshot保存到哪里-应该需要汇总到jobManager？" class="headerlink" title="1.4. snapshot保存到哪里? 应该需要汇总到jobManager？"></a>1.4. snapshot保存到哪里? 应该需要汇总到jobManager？</h2><h2 id="1-5-state-backend"><a href="#1-5-state-backend" class="headerlink" title="1.5. state backend"></a>1.5. state backend</h2><p><img src="_v_images/20200713183839429_2053265091.png"></p>
<h3 id="FsStateBackend"><a href="#FsStateBackend" class="headerlink" title="FsStateBackend"></a>FsStateBackend</h3><p>构造方法:<br><code>FsStateBackend(URI checkpointDataUri,boolean asynchronousSnapshots)</code></p>
<p>1 基于文件系统的状态管理器<br>2 如果使用，默认是异步<br>3 比较稳定，3个副本，比较安全。不会出现任务无法恢复等问题<br>4 状态大小受磁盘容量限制</p>
<p>存储方式:</p>
<ul>
<li>State: TaskManager内存</li>
<li>checkpoint: 外部文件系统(本地或HDFS)</li>
</ul>
<p>容量限制:</p>
<ul>
<li>单TaskManager上State总量不超过它的内存</li>
<li>总大小不超过配置的文件系统容量</li>
</ul>
<p>推荐使用场景:</p>
<ul>
<li>常规使用状态的作业，例如分钟级窗口聚合、join、窗口比较长、kv状态大；需要开启HA的作业</li>
<li>可以用于生产场景</li>
</ul>
<h3 id="RocksDBStateBackend"><a href="#RocksDBStateBackend" class="headerlink" title="RocksDBStateBackend"></a>RocksDBStateBackend</h3><p>状态数据先写入RocksDB，然后异步的将状态数据写入文件系统。正在进行计算的热数据存储在RocksDB，长时间才更新的数据写入磁盘中（文件系统）存储，体量比较小的元数据状态写入JobManager内存中（将工作state保存在RocksDB中，并且默认将checkpoint数据存在文件系统中）</p>
<p>目前唯一支持incremental的checkpoints的策略</p>
<p>构造方法:<br><code>RocksDBStateBackend(URI checkpointDataUri,boolean enableIncrementalCheckpointing)</code></p>
<p>存储方式:</p>
<ul>
<li>State: TaskManager上的KV数据库(实际使用内存+硬盘)</li>
<li>Checkpoint: 外部文件系统(本地或HDFS)</li>
</ul>
<p>容量限制:</p>
<ul>
<li>单TaskManager上的State总量不超过他的内存+磁盘</li>
<li>单key最大2G</li>
<li>总大小不超过配置的文件系统容量</li>
</ul>
<p>推荐使用的场景:</p>
<ul>
<li>超大状态的作业，例如天级别窗口聚合；需要开启HA的作业；对状态读写性能要求不高的作业</li>
<li>可以在生产环境使用</li>
</ul>
<h3 id="MemoryStateBackend"><a href="#MemoryStateBackend" class="headerlink" title="MemoryStateBackend"></a>MemoryStateBackend</h3><p>构造方法:<br><code>MemoryStateBackend(int maxStateSize, boolean asynchronousSnapshots)</code></p>
<p>主机内存中的数据可能会丢失，任务可能无法恢复</p>
<p>存储方式:</p>
<ul>
<li>State: TaskManager内存</li>
<li>Checkpoint: JobManager内存</li>
</ul>
<p>容量限制</p>
<ul>
<li>单个State maxStateSize默认5M</li>
<li>maxStateSize &lt;= akka.frameSize 默认10M</li>
<li>总大小不超过JobManager的内存</li>
</ul>
<p>推荐使用场景：</p>
<ul>
<li>本地测试；几乎无状态的作业，比如ETL；JobManager不容易挂，或挂掉影响不大的情况</li>
<li>不推荐在生产环境使用</li>
</ul>
<h2 id="1-6-checkpoint-与-savepoint"><a href="#1-6-checkpoint-与-savepoint" class="headerlink" title="1.6. checkpoint 与 savepoint"></a>1.6. checkpoint 与 savepoint</h2><p>Checkpoint指定触发生成时间间隔后，每当需要触发Checkpoint时，会向Flink程序运行时的多个分布式的Stream Source中插入一个Barrier标记，这些Barrier会根据Stream中的数据记录一起流向下游的各个Operator。<br>当一个Operator接收到一个Barrier时，它会暂停处理Steam中新接收到的数据记录。<br>因为一个Operator可能存在多个输入的Stream，而每个Stream中都会存在对应的Barrier，该Operator要等到所有的输入Stream中的Barrier都到达。(<strong>对齐</strong>)<br>当所有Stream中的Barrier都已经到达该Operator，这时所有的Barrier在时间上看来是同一个时刻点（表示已经对齐），在等待所有Barrier到达的过程中，<br>Operator的Buffer中可能已经缓存了一些比Barrier早到达Operator的数据记录（Outgoing Records），这时该Operator会将数据记录（Outgoing Records）发射（Emit）出去，作为下游Operator的输入，<br>最后将Barrier对应Snapshot发射（Emit）出去作为此次Checkpoint的结果数据。</p>
<p>Checkpoint 是增量做的，每次的时间较短，数据量较小，只要在程序里面启用后会自动触发，用户无须感知；Checkpoint 是作业 failover 的时候自动使用，不需要用户指定。</p>
<p>Savepoint 是全量做的，每次的时间较长，数据量较大，需要用户主动去触发。Savepoint 一般用于程序的版本更新（详见文档），Bug 修复，A/B Test 等场景，需要用户指定。</p>
<p><strong>保存的内容</strong></p>
<ul>
<li>首先，Savepoint 包含了一个目录，其中包含（通常很大的）二进制文件，这些文件表示了整个流应用在 Checkpoint/Savepoint 时的状态。</li>
<li>以及一个（相对较小的）元数据文件，包含了指向 Savapoint 各个文件的指针，并存储在所选的分布式文件系统或数据存储中。</li>
</ul>
<p><strong>目标</strong></p>
<p>Savepoint 和 Checkpoint 的不同之处很像传统数据库中备份与恢复日志之间的区别。Checkpoint 的主要目标是充当 Flink 中的恢复机制，确保能从潜在的故障中恢复。相反，Savepoint 的主要目标是充当手动备份、恢复暂停作业的方法。</p>
<p><strong>实现</strong></p>
<p>Checkpoint 被设计成轻量和快速的机制。它们可能（但不一定必须）利用底层状态后端的不同功能尽可能快速地恢复数据。例如，基于 RocksDB 状态后端的增量检查点，能够加速 RocksDB 的 checkpoint 过程，这使得 checkpoint 机制变得更加轻量。相反，Savepoint 旨在更多地关注数据的可移植性，并支持对作业做任何更改而状态能保持兼容，这使得生成和恢复的成本更高</p>
<p><strong>状态文件保留策略</strong></p>
<p>Checkpoint默认程序删除，可以设置CheckpointConfig中的参数进行保留 。Savepoint会一直保存，除非用户删除 。</p>
<p><strong>应用</strong></p>
<ul>
<li>部署流应用的一个新版本，包括新功能、BUG 修复、或者一个更好的机器学习模型</li>
<li>引入 A/B 测试，使用相同的源数据测试程序的不同版本，从同一时间点开始测试而不牺牲先前的状态</li>
<li>在需要更多资源时扩容应用程序</li>
<li>迁移流应用程序到 Flink 的新版本上，或者迁移到另一个集群</li>
</ul>
<h1 id="Flink数据一致性"><a href="#Flink数据一致性" class="headerlink" title="Flink数据一致性"></a>Flink数据一致性</h1><h2 id="一、综述"><a href="#一、综述" class="headerlink" title="一、综述"></a>一、综述</h2><p><strong>flink 通过内部依赖checkpoint 并且可以通过设置其参数exactly-once 实现其内部的一致性</strong>。但要实现其端到端的一致性，还必须保证<br>1、source 外部数据源可重设数据的读取位置<br>2、sink端 需要保证数据从故障恢复时，数据不会重复写入外部系统（或者可以逻辑实现写入多次，但只有一次生效的数据sink端）</p>
<h2 id="二、sink-端到端实现方式"><a href="#二、sink-端到端实现方式" class="headerlink" title="二、sink 端到端实现方式"></a>二、sink 端到端实现方式</h2><p><strong>幂等操作：</strong><br>一个操作，可以重复执行多次，但只导致一次结果更改，豁免重复操作执行就不起作用了，他的瑕疵 （在系统恢复的过程中，如果这段时间内多个更新或者插入导致状态不一致，但当数据追上就可以了）<br>（逻辑与、逻辑或等）具体理解参照自己以前写的文章。<br><strong>事务写入：</strong><br>事务应该具有四个属性：原子性、一致性、隔离性、持久性等。其具体的实现方式有两种<br><strong>（1）、预写日志</strong><br>简单易于实现，由于数据提前在状态后端中做了缓存，所以无论什么sink系统，都能用这种方式一批搞定，DataStream API提供了一个模板类：GenericWriteAheadSink，来实现这种事务性sink；<br>缺点：<br>1）、sink系统没说他支持事务。有可能出现一部分写入集群了。一部分没有写进去（如果实表，再写一次就写重复了）<br>2）、checkpoint做完了sink才去真正的写入（但其实得等sink都写完checkpoint才能生效，所以WAL这个机制jobmanager确定它写完还不算真正写完，还得有一个外部系统已经确认 完成的checkpoint）<br>（<strong>2）、两阶段提交。 flink 真正实现exactle-once</strong><br>对于每个checkpoint,sink 任务会启动一个事务，并将接下来所有接收的数据添加到事务中，然后将这些数据写入外部sink系统，但不提交他们（这里是预提交）。当checkpoint完成时的通知，它才正式提交事务，实现结果的真正写入；这种方式真正实现了exactly-once,它需要一个提供事务支持的外部sink系统，Flink提供了其具体实现（TwoPhaseCommitSinkFunction接口）</p>
<h2 id="三、-2pc-对外部-sink的要求"><a href="#三、-2pc-对外部-sink的要求" class="headerlink" title="三、 2pc 对外部 sink的要求"></a>三、 2pc 对外部 sink的要求</h2><p>1、外部sink系统必须事务支持，或者sink任务必须能够模拟外部系统上的事务；<br>2、在checkpoint的间隔期间里，必须能够开启一个事务，并接受数据写入。<br>3、在收到checkpoint完成通知之前，事务必须是“等待提交”的状态，在故障恢复的情况线，这可能需要一些时间。如果个时候sink系统关闭事务（例如超时了），那么未提交的数据就会丢失；<br>4、四年任务必选能够在进程失败后恢复事务<br>5、提交事务必须是幂等操作；</p>
<p>四、综上不同Source和sink的一致性保证：<br><img src="_v_images/20201208154240979_1471214802.png" alt="在这里插入图片描述"></p>
<h2 id="五、应用（flinK-kafka-端到端一致性保证）"><a href="#五、应用（flinK-kafka-端到端一致性保证）" class="headerlink" title="五、应用（flinK+kafka 端到端一致性保证）"></a>五、应用（flinK+kafka 端到端一致性保证）</h2><p>flink 和kafka 端到端一致性(kafka(source+flink+kafka(sink)))<br>1、内部 – 利用checkpoint机制，把状态存盘，发生故障的时候可以恢复，保证内部的状态一致性<br>2、source – kafka consumer作为source，可以将偏移量保存下来，如果后续任务出现了故障，恢复的时候可以由连接器重置偏移量，重新消费数据，保证一致性；</p>
<figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-tag">kafka</span> 0.8 和<span class="selector-tag">kafka</span> 0.11 之后 通过以下配置将偏移量保存，恢复时候重新消费</span><br><span class="line"> <span class="selector-tag">kafka</span><span class="selector-class">.setStartFromLatest</span>();</span><br><span class="line"> <span class="selector-tag">kafka</span><span class="selector-class">.setCommitOffsetsOnCheckpoints</span>(<span class="selector-tag">false</span>);</span><br><span class="line"> <span class="selector-tag">kafka</span> 0.9 和<span class="selector-tag">kafka0</span>.10 未验证是否支持这两个参数(<span class="selector-tag">todo</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>3、sink FlinkkafkaProducer作为Sink，采用两阶段提交的sink，由下图可以看出flink 0.11 已经默认继承了TwoPhaseCommitSinkFunction<br><img src="_v_images/20201208154240548_914126344.png" alt="在这里插入图片描述"><br>但我们需要在参数种传入指定语义，它默认时还是at-least-once<br>此外我们还需要进行一些producer的容错配置：<br>（1）除了启用Flink的检查点之外，还可以通过将适当的semantic参数传递给FlinkKafkaProducer011（FlinkKafkaProducer对于Kafka&gt; = 1.0.0版本）<br>（2）来选择三种不同的操作模式<br>1）、Semantic.NONE 代表at-mostly-once语义<br>2）、Semantic.AT_LEAST_ONCE（Flink默认设置<br>3）、Semantic.EXACTLY_ONCE 使用Kafka事务提供一次精确的语义，每当您使用事务写入Kafka时<br>（3）、请不要忘记消费kafka记录任何应用程序设置所需的设置isolation.leva（read_committed 或者read_uncommitted-后者是默认）<br>read_committed，只是读取已经提交的数据。</p>
<p>应用；<br>Semantic.EXACTLY_ONCE依赖与下游系统能支持事务操作.以0.11kafka为例.<br>transaction.max.timeout.ms 最大超市时长，默认15分钟，如果需要用exactly语义，需要增加这个值。（因为它小于transaction.timeout.ms ）<br>isolation.level 如果需要用到exactly语义，需要在下级consumerConfig中设置read-commited [read-uncommited(默认值)]<br>transaction.timeout.ms 默认为1hour</p>
<p><strong>其参数对应关系为 和一些报错问题<br>checkpoint间隔&lt;transaction.timeout.ms&lt;transaction.max.timeout.ms</strong></p>
<p><strong>参考：<a target="_blank" rel="noopener" href="https://www.cnblogs.com/createweb/p/11971846.html">https://www.cnblogs.com/createweb/p/11971846.html</a></strong></p>
<p>注意：<br>1、Semantic.EXACTLY_ONCE 模式每个FlinkKafkaProducer011实例使用一个固定大小的KafkaProducers池。每个检查点使用这些生产者中的每一个。如果并发检查点的数量超过池大小，FlinkKafkaProducer011 将引发异常，并使整个应用程序失败。请相应地配置最大池大小和最大并发检查点数。</p>
<p>2、Semantic.EXACTLY_ONCE采取所有可能的措施，不要留下任何挥之不去的数据，否则这将有碍于消费者更多地阅读Kafka主题。但是，如果flink应用程序在第一个检查点之前失败，则在重新启动此类应用程序后，系统种将没有有关先前池大小信息，因此，在第一个检查点完成前按比例缩小Flink应用程序的FlinkKafkaProducer011.SAFE_SCALE_DOWN_FACTOR</p>
<figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">//1。设置最大允许的并行<span class="selector-tag">checkpoint</span>数，防止超过<span class="selector-tag">producer</span>池的个数发生异常</span><br><span class="line"><span class="selector-tag">env</span><span class="selector-class">.getCheckpointConfig</span><span class="selector-class">.setMaxConcurrentCheckpoints</span>(5) </span><br><span class="line">//2。设置<span class="selector-tag">producer</span>的<span class="selector-tag">ack</span>传输配置</span><br><span class="line">// 设置超市时长，默认15分钟，建议1个小时以上</span><br><span class="line"><span class="selector-tag">producerConfig</span><span class="selector-class">.put</span>(<span class="selector-tag">ProducerConfig</span><span class="selector-class">.ACKS_CONFIG</span>, 1) </span><br><span class="line"><span class="selector-tag">producerConfig</span><span class="selector-class">.put</span>(<span class="selector-tag">ProducerConfig</span><span class="selector-class">.TRANSACTION_TIMEOUT_CONFIG</span>, 15000) </span><br><span class="line"></span><br><span class="line">//3。在下一个<span class="selector-tag">kafka</span> <span class="selector-tag">consumer</span>的配置文件，或者代码中设置<span class="selector-tag">ISOLATION_LEVEL_CONFIG-read-commited</span></span><br><span class="line">//<span class="selector-tag">Note</span>:必须在下一个<span class="selector-tag">consumer</span>中指定，当前指定是没用用的</span><br><span class="line"><span class="selector-tag">kafkaonfigs</span><span class="selector-class">.setProperty</span>(<span class="selector-tag">ConsumerConfig</span><span class="selector-class">.ISOLATION_LEVEL_CONFIG</span>,&quot;<span class="selector-tag">read_commited</span>&quot;)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>完整应用代码：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.shufang.flink.connectors</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.Properties</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.serialization.SimpleStringSchema</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.TimeCharacteristic</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.functions.timestamps.BoundedOutOfOrdernessTimestampExtractor</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala._</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.windowing.time.Time</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer.Semantic</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.connectors.kafka._</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.util.serialization.KeyedSerializationSchemaWrapper</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.ConsumerConfig</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.ProducerConfig</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.common.serialization.StringDeserializer</span><br><span class="line"></span><br><span class="line">object KafkaSource01 &#123;</span><br><span class="line">  <span class="function">def <span class="title">main</span><span class="params">(args: Array[String])</span>: Unit </span>= &#123;</span><br><span class="line">    val env: StreamExecutionEnvironment = StreamExecutionEnvironment.getExecutionEnvironment</span><br><span class="line"></span><br><span class="line">    env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//这是checkpoint的超时时间</span></span><br><span class="line">    <span class="comment">//env.getCheckpointConfig.setCheckpointTimeout()</span></span><br><span class="line">    <span class="comment">//设置最大并行的chekpoint</span></span><br><span class="line">    env.getCheckpointConfig.setMaxConcurrentCheckpoints(<span class="number">5</span>)</span><br><span class="line">    env.getCheckpointConfig.setCheckpointInterval(<span class="number">1000</span>) <span class="comment">//增加checkpoint的中间时长，保证可靠性</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 为了保证数据的一致性，我们开启Flink的checkpoint一致性检查点机制，保证容错</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    env.enableCheckpointing(<span class="number">60000</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 从kafka获取数据，一定要记得添加checkpoint，能保证offset的状态可以重置，从数据源保证数据的一致性</span></span><br><span class="line"><span class="comment">     * 保证kafka代理的offset与checkpoint备份中保持状态一致</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line"></span><br><span class="line">    val kafkaonfigs = <span class="keyword">new</span> Properties()</span><br><span class="line"></span><br><span class="line">    <span class="comment">//指定kafka的启动集群</span></span><br><span class="line">    kafkaonfigs.setProperty(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, <span class="string">&quot;localhost:9092&quot;</span>)</span><br><span class="line">    <span class="comment">//指定消费者组</span></span><br><span class="line">    kafkaonfigs.setProperty(ConsumerConfig.GROUP_ID_CONFIG, <span class="string">&quot;flinkConsumer&quot;</span>)</span><br><span class="line">    <span class="comment">//指定key的反序列化类型</span></span><br><span class="line">    kafkaonfigs.setProperty(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, classOf[StringDeserializer].getName)</span><br><span class="line">    <span class="comment">//指定value的反序列化类型</span></span><br><span class="line">    kafkaonfigs.setProperty(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, classOf[StringDeserializer].getName)</span><br><span class="line">    <span class="comment">//指定自动消费offset的起点配置</span></span><br><span class="line">    <span class="comment">//    kafkaonfigs.setProperty(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, &quot;latest&quot;)</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 自定义kafkaConsumer，同时可以指定从哪里开始消费</span></span><br><span class="line"><span class="comment">     * 开启了Flink的检查点之后，我们还要开启kafka-offset的检查点，通过kafkaConsumer.setCommitOffsetsOnCheckpoints(true)开启，</span></span><br><span class="line"><span class="comment">     * 一旦这个检查点开启，那么之前配置的 auto-commit-enable = true的配置就会自动失效</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    val kafkaConsumer = <span class="keyword">new</span> FlinkKafkaConsumer[String](</span><br><span class="line">      <span class="string">&quot;console-topic&quot;</span>,</span><br><span class="line">      <span class="keyword">new</span> SimpleStringSchema(), <span class="comment">// 这个schema是将kafka的数据应设成Flink中的String类型</span></span><br><span class="line">      kafkaonfigs</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 开启kafka-offset检查点状态保存机制</span></span><br><span class="line">    kafkaConsumer.setCommitOffsetsOnCheckpoints(<span class="keyword">true</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//    kafkaConsumer.setStartFromEarliest()//</span></span><br><span class="line">    <span class="comment">//    kafkaConsumer.setStartFromTimestamp(1010003794)</span></span><br><span class="line">    <span class="comment">//    kafkaConsumer.setStartFromLatest()</span></span><br><span class="line">    <span class="comment">//    kafkaConsumer.setStartFromSpecificOffsets(Map[KafkaTopicPartition,Long]()</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 添加source数据源</span></span><br><span class="line">    val kafkaStream: DataStream[String] = env.addSource(kafkaConsumer)</span><br><span class="line"></span><br><span class="line">    kafkaStream.print()</span><br><span class="line"></span><br><span class="line">    val sinkStream: DataStream[String] = kafkaStream.assignTimestampsAndWatermarks(<span class="keyword">new</span> BoundedOutOfOrdernessTimestampExtractor[String](Time.seconds(<span class="number">5</span>)) &#123;</span><br><span class="line">      <span class="function">override def <span class="title">extractTimestamp</span><span class="params">(element: String)</span>: Long </span>= &#123;</span><br><span class="line">        element.split(<span class="string">&quot;,&quot;</span>)(<span class="number">1</span>).toLong</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 通过FlinkkafkaProduccer API将stream的数据写入到kafka的&#x27;sink-topic&#x27;中</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="comment">//    val brokerList = &quot;localhost:9092&quot;</span></span><br><span class="line">    val topic = <span class="string">&quot;sink-topic&quot;</span></span><br><span class="line">    val producerConfig = <span class="keyword">new</span> Properties()</span><br><span class="line">    producerConfig.put(ProducerConfig.ACKS_CONFIG, <span class="keyword">new</span> Integer(<span class="number">1</span>)) <span class="comment">// 设置producer的ack传输配置</span></span><br><span class="line">    producerConfig.put(ProducerConfig.TRANSACTION_TIMEOUT_CONFIG, Time.hours(<span class="number">2</span>)) <span class="comment">//设置超市时长，默认1小时，建议1个小时以上</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 自定义producer，可以通过不同的构造器创建</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    val producer: FlinkKafkaProducer[String] = <span class="keyword">new</span> FlinkKafkaProducer[String](</span><br><span class="line">      topic,</span><br><span class="line">      <span class="keyword">new</span> KeyedSerializationSchemaWrapper[String](SimpleStringSchema),</span><br><span class="line">      producerConfig,</span><br><span class="line">      Semantic.EXACTLY_ONCE</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment">//    FlinkKafkaProducer.SAFE_SCALE_DOWN_FACTOR</span></span><br><span class="line">    <span class="comment">/** *****************************************************************************************************************</span></span><br><span class="line"><span class="comment">     * * 出了要开启flink的checkpoint功能，同时还要设置相关配置功能。</span></span><br><span class="line"><span class="comment">     * * 因在0.9或者0.10，默认的FlinkKafkaProducer只能保证at-least-once语义，假如需要满足at-least-once语义，我们还需要设置</span></span><br><span class="line"><span class="comment">     * * setLogFailuresOnly(boolean)    默认false</span></span><br><span class="line"><span class="comment">     * * setFlushOnCheckpoint(boolean)  默认true</span></span><br><span class="line"><span class="comment">     * * come from 官网 below：</span></span><br><span class="line"><span class="comment">     * * Besides enabling Flink’s checkpointing，you should also configure the setter methods setLogFailuresOnly(boolean)</span></span><br><span class="line"><span class="comment">     * * and setFlushOnCheckpoint(boolean) appropriately.</span></span><br><span class="line"><span class="comment">     * ******************************************************************************************************************/</span></span><br><span class="line"></span><br><span class="line">    producer.setLogFailuresOnly(<span class="keyword">false</span>) <span class="comment">//默认是false</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 除了启用Flink的检查点之外，还可以通过将适当的semantic参数传递给FlinkKafkaProducer011（FlinkKafkaProducer对于Kafka&gt; = 1.0.0版本）</span></span><br><span class="line"><span class="comment">     * 来选择三种不同的操作模式：</span></span><br><span class="line"><span class="comment">     * Semantic.NONE  代表at-mostly-once语义</span></span><br><span class="line"><span class="comment">     * Semantic.AT_LEAST_ONCE（Flink默认设置）</span></span><br><span class="line"><span class="comment">     * Semantic.EXACTLY_ONCE：使用Kafka事务提供一次精确的语义，每当您使用事务写入Kafka时，</span></span><br><span class="line"><span class="comment">     * 请不要忘记为使用Kafka记录的任何应用程序设置所需的设置isolation.level（read_committed 或read_uncommitted-后者是默认值)</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line"></span><br><span class="line">    sinkStream.addSink(producer)</span><br><span class="line"></span><br><span class="line">    env.execute(<span class="string">&quot;kafka source &amp; sink&quot;</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="[参考文献]"></a>[参考文献]</h2><ol>
<li><a target="_blank" rel="noopener" href="http://shiyanjun.cn/archives/1855.html">Flink Checkpoint、Savepoint配置与实践</a></li>
<li><a target="_blank" rel="noopener" href="http://wuchong.me/blog/2018/11/04/how-apache-flink-manages-kafka-consumer-offsets/">Flink 小贴士 (2)：Flink 如何管理 Kafka 消费位点</a></li>
<li><a target="_blank" rel="noopener" href="https://www.jianshu.com/p/4bcbcda0e2f4">Flink实时计算-深入理解Checkpoint和Savepoint</a></li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1506.08603">Lightweight Asynchronous Snapshots for Distributed Dataflows: 分布式数据流轻量级异步快照</a></li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://example.com/bigdata/Flink/Flink-backpress-ext/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="aaronzhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Guadazi">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/bigdata/Flink/Flink-backpress-ext/" class="post-title-link" itemprop="url">【转发】咱们从头到尾讲一次 Flink 网络流控和反压剖析</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2019-08-10 14:58:00" itemprop="dateCreated datePublished" datetime="2019-08-10T14:58:00+08:00">2019-08-10</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-04-11 16:41:41" itemprop="dateModified" datetime="2021-04-11T16:41:41+08:00">2021-04-11</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/bigdata/" itemprop="url" rel="index"><span itemprop="name">bigdata</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>本文根据 Apache Flink 系列直播整理而成，由 Apache Flink Contributor、OPPO 大数据平台研发负责人张俊老师分享。主要内容如下：</p>
<ul>
<li><p>网络流控的概念与背景</p>
</li>
<li><p>TCP的流控机制</p>
</li>
<li><p>Flink TCP-based 反压机制（before V1.5）</p>
</li>
<li><p>Flink Credit-based 反压机制 （since V1.5）</p>
</li>
<li><p>总结与思考</p>
</li>
</ul>
<h2 id="网络流控的概念与背景"><a href="#网络流控的概念与背景" class="headerlink" title="网络流控的概念与背景"></a>网络流控的概念与背景</h2><h3 id="为什么需要网络流控"><a href="#为什么需要网络流控" class="headerlink" title="为什么需要网络流控"></a>为什么需要网络流控</h3><p><img src="_v_images/20200714171454820_132283117"></p>
<p>首先我们可以看下这张最精简的网络流控的图，Producer 的吞吐率是 2MB/s，Consumer 是 1MB/s，这个时候我们就会发现在网络通信的时候我们的 Producer 的速度是比 Consumer 要快的，有 1MB/s 的这样的速度差，假定我们两端都有一个 Buffer，Producer 端有一个发送用的 Send Buffer，Consumer 端有一个接收用的 Receive Buffer，在网络端的吞吐率是 2MB/s，过了 5s 后我们的 Receive Buffer 可能就撑不住了，这时候会面临两种情况：</p>
<ul>
<li><p>1.如果 Receive Buffer 是有界的，这时候新到达的数据就只能被丢弃掉了。</p>
</li>
<li><p>2.如果 Receive Buffer 是无界的，Receive Buffer 会持续的扩张，最终会导致 Consumer 的内存耗尽。</p>
</li>
</ul>
<h3 id="网络流控的实现：静态限速"><a href="#网络流控的实现：静态限速" class="headerlink" title="网络流控的实现：静态限速"></a>网络流控的实现：静态限速</h3><p><img src="_v_images/20200714171454414_685240728"></p>
<p>为了解决这个问题，我们就需要网络流控来解决上下游速度差的问题，传统的做法可以在 Producer 端实现一个类似 Rate Limiter 这样的静态限流，Producer 的发送速率是 2MB/s，但是经过限流这一层后，往 Send Buffer 去传数据的时候就会降到 1MB/s 了，这样的话 Producer 端的发送速率跟 Consumer 端的处理速率就可以匹配起来了，就不会导致上述问题。但是这个解决方案有两点限制：</p>
<ul>
<li><p>1、事先无法预估 Consumer 到底能承受多大的速率</p>
</li>
<li><p>2、 Consumer 的承受能力通常会动态地波动</p>
</li>
</ul>
<h3 id="网络流控的实现：动态反馈-自动反压"><a href="#网络流控的实现：动态反馈-自动反压" class="headerlink" title="网络流控的实现：动态反馈/自动反压"></a>网络流控的实现：动态反馈/自动反压</h3><p><img src="_v_images/20200714171454209_1020595047"></p>
<p>针对静态限速的问题我们就演进到了动态反馈（自动反压）的机制，我们需要 Consumer 能够及时的给 Producer 做一个 feedback，即告知 Producer 能够承受的速率是多少。动态反馈分为两种：</p>
<ul>
<li><p>1、负反馈：接受速率小于发送速率时发生，告知 Producer 降低发送速率</p>
</li>
<li><p>2、正反馈：发送速率小于接收速率时发生，告知 Producer 可以把发送速率提上来</p>
</li>
</ul>
<p>让我们来看几个经典案例</p>
<h3 id="案例一：Storm-反压实现"><a href="#案例一：Storm-反压实现" class="headerlink" title="案例一：Storm 反压实现"></a>案例一：Storm 反压实现</h3><p><img src="_v_images/20200714171454004_1684651238"></p>
<p>上图就是 Storm 里实现的反压机制，可以看到 Storm 在每一个 Bolt 都会有一个监测反压的线程（Backpressure Thread），这个线程一但检测到 Bolt 里的接收队列（recv queue）出现了严重阻塞就会把这个情况写到 ZooKeeper 里，ZooKeeper 会一直被 Spout 监听，监听到有反压的情况就会停止发送，通过这样的方式匹配上下游的发送接收速率。</p>
<h3 id="案例二：Spark-Streaming-反压实现"><a href="#案例二：Spark-Streaming-反压实现" class="headerlink" title="案例二：Spark Streaming 反压实现"></a>案例二：Spark Streaming 反压实现</h3><p><img src="_v_images/20200714171453699_1486221085"></p>
<p>Spark Streaming 里也有做类似这样的 feedback 机制，上图 Fecher 会实时的从 Buffer、Processing 这样的节点收集一些指标然后通过 Controller 把速度接收的情况再反馈到 Receiver，实现速率的匹配。</p>
<h3 id="疑问：为什么-Flink（before-V1-5）里没有用类似的方式实现-feedback-机制？"><a href="#疑问：为什么-Flink（before-V1-5）里没有用类似的方式实现-feedback-机制？" class="headerlink" title="疑问：为什么 Flink（before V1.5）里没有用类似的方式实现 feedback 机制？"></a>疑问：为什么 Flink（before V1.5）里没有用类似的方式实现 feedback 机制？</h3><p>首先在解决这个疑问之前我们需要先了解一下 Flink 的网络传输是一个什么样的架构。</p>
<p><img src="_v_images/20200714171453393_758272414"></p>
<p>这张图就体现了 Flink 在做网络传输的时候基本的数据的流向，发送端在发送网络数据前要经历自己内部的一个流程，会有一个自己的 Network Buffer，在底层用 Netty 去做通信，Netty 这一层又有属于自己的 ChannelOutbound Buffer，因为最终是要通过 Socket 做网络请求的发送，所以在 Socket 也有自己的 Send Buffer，同样在接收端也有对应的三级 Buffer。学过计算机网络的时候我们应该了解到，TCP 是自带流量控制的。实际上 Flink （before V1.5）就是通过 TCP 的流控机制来实现 feedback 的。</p>
<h2 id="TCP-流控机制"><a href="#TCP-流控机制" class="headerlink" title="TCP 流控机制"></a>TCP 流控机制</h2><p>根据下图我们来简单的回顾一下 TCP 包的格式结构。首先，他有 Sequence number 这样一个机制给每个数据包做一个编号，还有 ACK number 这样一个机制来确保 TCP 的数据传输是可靠的，除此之外还有一个很重要的部分就是 Window Size，接收端在回复消息的时候会通过 Window Size 告诉发送端还可以发送多少数据。</p>
<p><img src="_v_images/20200714171452987_650184722"></p>
<p>接下来我们来简单看一下这个过程。</p>
<h3 id="TCP-流控：滑动窗口"><a href="#TCP-流控：滑动窗口" class="headerlink" title="TCP 流控：滑动窗口"></a>TCP 流控：滑动窗口</h3><p><img src="_v_images/20200714171452683_1953611527"></p>
<p>TCP 的流控就是基于滑动窗口的机制，现在我们有一个 Socket 的发送端和一个 Socket 的接收端，目前我们的发送端的速率是我们接收端的 3 倍，这样会发生什么样的一个情况呢？假定初始的时候我们发送的 window 大小是 3，然后我们接收端的 window 大小是固定的，就是接收端的 Buffer 大小为 5。</p>
<p><img src="_v_images/20200714171452278_1991948370"></p>
<p>首先，发送端会一次性发 3 个 packets，将 1，2，3 发送给接收端，接收端接收到后会将这 3 个 packets 放到 Buffer 里去。</p>
<p><img src="_v_images/20200714171452072_41416741"></p>
<p>接收端一次消费 1 个 packet，这时候 1 就已经被消费了，然后我们看到接收端的滑动窗口会往前滑动一格，这时候 2，3 还在 Buffer 当中 而 4，5，6 是空出来的，所以接收端会给发送端发送 ACK = 4 ，代表发送端可以从 4 开始发送，同时会将 window 设置为 3 （Buffer 的大小 5 减去已经存下的 2 和 3），发送端接收到回应后也会将他的滑动窗口向前移动到 4，5，6。</p>
<p><img src="_v_images/20200714171451767_719493078"></p>
<p>这时候发送端将 4，5，6 发送，接收端也能成功的接收到 Buffer 中去。</p>
<p><img src="_v_images/20200714171451562_313469730"></p>
<p>到这一阶段后，接收端就消费到 2 了，同样他的窗口也会向前滑动一个，这时候他的 Buffer 就只剩一个了，于是向发送端发送 ACK = 7、window = 1。发送端收到之后滑动窗口也向前移，但是这个时候就不能移动 3 格了，虽然发送端的速度允许发 3 个 packets 但是 window 传值已经告知只能接收一个，所以他的滑动窗口就只能往前移一格到 7 ，这样就达到了限流的效果，发送端的发送速度从 3 降到 1。</p>
<p><img src="_v_images/20200714171451057_482639053"></p>
<p><img src="_v_images/20200714171450752_1192910727"></p>
<p>我们再看一下这种情况，这时候发送端将 7 发送后，接收端接收到，但是由于接收端的消费出现问题，一直没有从 Buffer 中去取，这时候接收端向发送端发送 ACK = 8、window = 0 ，由于这个时候 window = 0，发送端是不能发送任何数据，也就会使发送端的发送速度降为 0。这个时候发送端不发送任何数据了，接收端也不进行任何的反馈了，那么如何知道消费端又开始消费了呢？</p>
<p><img src="_v_images/20200714171450147_1390602232"></p>
<p><img src="_v_images/20200714171449642_832142513"></p>
<p><img src="_v_images/20200714171449037_394744327"></p>
<p>TCP 当中有一个 ZeroWindowProbe 的机制，发送端会定期的发送 1 个字节的探测消息，这时候接收端就会把 window 的大小进行反馈。当接收端的消费恢复了之后，接收到探测消息就可以将 window 反馈给发送端端了从而恢复整个流程。TCP 就是通过这样一个滑动窗口的机制实现 feedback。</p>
<h2 id="Flink-TCP-based-反压机制（before-V1-5）"><a href="#Flink-TCP-based-反压机制（before-V1-5）" class="headerlink" title="Flink TCP-based 反压机制（before V1.5）"></a>Flink TCP-based 反压机制（before V1.5）</h2><h3 id="示例：WindowWordCount"><a href="#示例：WindowWordCount" class="headerlink" title="示例：WindowWordCount"></a>示例：WindowWordCount</h3><p><img src="_v_images/20200714171448732_1736435542"></p>
<p>大体的逻辑就是从 Socket 里去接收数据，每 5s 去进行一次 WordCount，将这个代码提交后就进入到了编译阶段。</p>
<h3 id="编译阶段：生成-JobGraph"><a href="#编译阶段：生成-JobGraph" class="headerlink" title="编译阶段：生成 JobGraph"></a>编译阶段：生成 JobGraph</h3><p><img src="_v_images/20200714171448227_116055159"></p>
<p>这时候还没有向集群去提交任务，在 Client 端会将 StreamGraph 生成 JobGraph，JobGraph 就是做为向集群提交的最基本的单元。在生成 JobGrap 的时候会做一些优化，将一些没有 Shuffle 机制的节点进行合并。有了 JobGraph 后就会向集群进行提交，进入运行阶段。</p>
<h3 id="运行阶段：调度-ExecutionGraph"><a href="#运行阶段：调度-ExecutionGraph" class="headerlink" title="运行阶段：调度 ExecutionGraph"></a>运行阶段：调度 ExecutionGraph</h3><p><img src="_v_images/20200714171447922_1674684296"></p>
<p>JobGraph 提交到集群后会生成 ExecutionGraph ，这时候就已经具备基本的执行任务的雏形了，把每个任务拆解成了不同的 SubTask，上图 ExecutionGraph 中的 Intermediate Result Partition 就是用于发送数据的模块，最终会将 ExecutionGraph 交给 JobManager 的调度器，将整个 ExecutionGraph 调度起来。然后我们概念化这样一张物理执行图，可以看到每个 Task 在接收数据时都会通过这样一个 InputGate 可以认为是负责接收数据的，再往前有这样一个 ResultPartition 负责发送数据，在 ResultPartition 又会去做分区跟下游的 Task 保持一致，就形成了 ResultSubPartition 和 InputChannel 的对应关系。这就是从逻辑层上来看的网络传输的通道，基于这么一个概念我们可以将反压的问题进行拆解。</p>
<h3 id="问题拆解：反压传播两个阶段"><a href="#问题拆解：反压传播两个阶段" class="headerlink" title="问题拆解：反压传播两个阶段"></a>问题拆解：反压传播两个阶段</h3><p><img src="_v_images/20200714171447717_722983245"></p>
<p>反压的传播实际上是分为两个阶段的，对应着上面的执行图，我们一共涉及 3 个 TaskManager，在每个 TaskManager 里面都有相应的 Task 在执行，还有负责接收数据的 InputGate，发送数据的 ResultPartition，这就是一个最基本的数据传输的通道。在这时候假设最下游的 Task （Sink）出现了问题，处理速度降了下来这时候是如何将这个压力反向传播回去呢？这时候就分为两种情况：</p>
<ul>
<li><p>跨 TaskManager ，反压如何从 InputGate 传播到 ResultPartition</p>
</li>
<li><p>TaskManager 内，反压如何从 ResultPartition 传播到 InputGate</p>
</li>
</ul>
<h3 id="跨-TaskManager-数据传输"><a href="#跨-TaskManager-数据传输" class="headerlink" title="跨 TaskManager 数据传输"></a>跨 TaskManager 数据传输</h3><p><img src="_v_images/20200714171447411_1651413338"></p>
<p>前面提到，发送数据需要 ResultPartition，在每个 ResultPartition 里面会有分区 ResultSubPartition，中间还会有一些关于内存管理的 Buffer。 对于一个 TaskManager 来说会有一个统一的 Network BufferPool 被所有的 Task 共享，在初始化时会从 Off-heap Memory 中申请内存，申请到内存的后续内存管理就是同步 Network BufferPool 来进行的，不需要依赖 JVM GC 的机制去释放。有了 Network BufferPool 之后可以为每一个 ResultSubPartition 创建 Local BufferPool 。 如上图左边的 TaskManager 的 Record Writer 写了 &lt;1，2&gt; 这个两个数据进来，因为 ResultSubPartition 初始化的时候为空，没有 Buffer 用来接收，就会向 Local BufferPool 申请内存，这时 Local BufferPool 也没有足够的内存于是将请求转到 Network BufferPool，最终将申请到的 Buffer 按原链路返还给 ResultSubPartition，&lt;1，2&gt; 这个两个数据就可以被写入了。之后会将 ResultSubPartition 的 Buffer 拷贝到 Netty 的 Buffer 当中最终拷贝到 Socket 的 Buffer 将消息发送出去。然后接收端按照类似的机制去处理将消息消费掉。 接下来我们来模拟上下游处理速度不匹配的场景，发送端的速率为 2，接收端的速率为 1，看一下反压的过程是怎样的。</p>
<h3 id="跨-TaskManager-反压过程"><a href="#跨-TaskManager-反压过程" class="headerlink" title="跨 TaskManager 反压过程"></a>跨 TaskManager 反压过程</h3><p><img src="_v_images/20200714171447006_680735181"></p>
<p>因为速度不匹配就会导致一段时间后 InputChannel 的 Buffer 被用尽，于是他会向 Local BufferPool 申请新的 Buffer ，这时候可以看到 Local BufferPool 中的一个 Buffer 就会被标记为 Used。</p>
<p><img src="_v_images/20200714171446799_1194236091"></p>
<p>发送端还在持续以不匹配的速度发送数据，然后就会导致 InputChannel 向 Local BufferPool 申请 Buffer 的时候发现没有可用的 Buffer 了，这时候就只能向 Network BufferPool 去申请，当然每个 Local BufferPool 都有最大的可用的 Buffer，防止一个 Local BufferPool 把 Network BufferPool 耗尽。这时候看到 Network BufferPool 还是有可用的 Buffer 可以向其申请。</p>
<p><img src="_v_images/20200714171445992_941635712"></p>
<p>一段时间后，发现 Network BufferPool 没有可用的 Buffer，或是 Local BufferPool 的最大可用 Buffer 到了上限无法向 Network BufferPool 申请，没有办法去读取新的数据，这时 Netty AutoRead 就会被禁掉，Netty 就不会从 Socket 的 Buffer 中读取数据了。</p>
<p><img src="_v_images/20200714171444186_160162504"></p>
<p>显然，再过不久 Socket 的 Buffer 也被用尽，这时就会将 Window = 0 发送给发送端（前文提到的 TCP 滑动窗口的机制）。这时发送端的 Socket 就会停止发送。</p>
<p><img src="_v_images/20200714171443780_1304730499"></p>
<p>很快发送端的 Socket 的 Buffer 也被用尽，Netty 检测到 Socket 无法写了之后就会停止向 Socket 写数据。</p>
<p><img src="_v_images/20200714171443573_720011262"></p>
<p>Netty 停止写了之后，所有的数据就会阻塞在 Netty 的 Buffer 当中了，但是 Netty 的 Buffer 是无界的，可以通过 Netty 的水位机制中的 high watermark 控制他的上界。当超过了 high watermark，Netty 就会将其 channel 置为不可写，ResultSubPartition 在写之前都会检测 Netty 是否可写，发现不可写就会停止向 Netty 写数据。</p>
<p><img src="_v_images/20200714171443267_566801867"></p>
<p>这时候所有的压力都来到了 ResultSubPartition，和接收端一样他会不断的向 Local BufferPool 和 Network BufferPool 申请内存。</p>
<p><img src="_v_images/20200714171442761_1041985779"></p>
<p>Local BufferPool 和 Network BufferPool 都用尽后整个 Operator 就会停止写数据，达到跨 TaskManager 的反压。</p>
<h3 id="TaskManager-内反压过程"><a href="#TaskManager-内反压过程" class="headerlink" title="TaskManager 内反压过程"></a>TaskManager 内反压过程</h3><p>了解了跨 TaskManager 反压过程后再来看 TaskManager 内反压过程就更好理解了，下游的 TaskManager 反压导致本 TaskManager 的 ResultSubPartition 无法继续写入数据，于是 Record Writer 的写也被阻塞住了，因为 Operator 需要有输入才能有计算后的输出，输入跟输出都是在同一线程执行， Record Writer 阻塞了，Record Reader 也停止从 InputChannel 读数据，这时上游的 TaskManager 还在不断地发送数据，最终将这个 TaskManager 的 Buffer 耗尽。具体流程可以参考下图，这就是 TaskManager 内的反压过程。</p>
<p><img src="_v_images/20200714171442455_926070537"></p>
<p><img src="_v_images/20200714171442149_493428448"></p>
<p><img src="_v_images/20200714171441744_1291490922"></p>
<p><img src="_v_images/20200714171441439_2105575637"></p>
<h2 id="Flink-Credit-based-反压机制（since-V1-5）"><a href="#Flink-Credit-based-反压机制（since-V1-5）" class="headerlink" title="Flink Credit-based 反压机制（since V1.5）"></a>Flink Credit-based 反压机制（since V1.5）</h2><h3 id="TCP-based-反压的弊端"><a href="#TCP-based-反压的弊端" class="headerlink" title="TCP-based 反压的弊端"></a>TCP-based 反压的弊端</h3><p><img src="_v_images/20200714171441134_1354461165"></p>
<p>在介绍 Credit-based 反压机制之前，先分析下 TCP 反压有哪些弊端。</p>
<ul>
<li><p>在一个 TaskManager 中可能要执行多个 Task，如果多个 Task 的数据最终都要传输到下游的同一个 TaskManager 就会复用同一个 Socket 进行传输，这个时候如果单个 Task 产生反压，就会导致复用的 Socket 阻塞，其余的 Task 也无法使用传输，checkpoint barrier 也无法发出导致下游执行 checkpoint 的延迟增大。</p>
</li>
<li><p>依赖最底层的 TCP 去做流控，会导致反压传播路径太长，导致生效的延迟比较大。</p>
</li>
</ul>
<h3 id="引入-Credit-based-反压"><a href="#引入-Credit-based-反压" class="headerlink" title="引入 Credit-based 反压"></a>引入 Credit-based 反压</h3><p>这个机制简单的理解起来就是在 Flink 层面实现类似 TCP 流控的反压机制来解决上述的弊端，Credit 可以类比为 TCP 的 Window 机制。</p>
<h3 id="Credit-based-反压过程"><a href="#Credit-based-反压过程" class="headerlink" title="Credit-based 反压过程"></a>Credit-based 反压过程</h3><p><img src="_v_images/20200714171440828_629774125"></p>
<p>如图所示在 Flink 层面实现反压机制，就是每一次 ResultSubPartition 向 InputChannel 发送消息的时候都会发送一个 backlog size 告诉下游准备发送多少消息，下游就会去计算有多少的 Buffer 去接收消息，算完之后如果有充足的 Buffer 就会返还给上游一个 Credit 告知他可以发送消息（图上两个 ResultSubPartition 和 InputChannel 之间是虚线是因为最终还是要通过 Netty 和 Socket 去通信），下面我们看一个具体示例。</p>
<p><img src="_v_images/20200714171440422_2073806088"></p>
<p>假设我们上下游的速度不匹配，上游发送速率为 2，下游接收速率为 1，可以看到图上在 ResultSubPartition 中累积了两条消息，10 和 11， backlog 就为 2，这时就会将发送的数据 &lt;8,9&gt; 和 backlog = 2 一同发送给下游。下游收到了之后就会去计算是否有 2 个 Buffer 去接收，可以看到 InputChannel 中已经不足了这时就会从 Local BufferPool 和 Network BufferPool 申请，好在这个时候 Buffer 还是可以申请到的。</p>
<p><img src="_v_images/20200714171440216_190878629"></p>
<p>过了一段时间后由于上游的发送速率要大于下游的接受速率，下游的 TaskManager 的 Buffer 已经到达了申请上限，这时候下游就会向上游返回 Credit = 0，ResultSubPartition 接收到之后就不会向 Netty 去传输数据，上游 TaskManager 的 Buffer 也很快耗尽，达到反压的效果，这样在 ResultSubPartition 层就能感知到反压，不用通过 Socket 和 Netty 一层层地向上反馈，降低了反压生效的延迟。同时也不会将 Socket 去阻塞，解决了由于一个 Task 反压导致 TaskManager 和 TaskManager 之间的 Socket 阻塞的问题。</p>
<h2 id="总结与思考"><a href="#总结与思考" class="headerlink" title="总结与思考"></a>总结与思考</h2><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><ul>
<li><p>网络流控是为了在上下游速度不匹配的情况下，防止下游出现过载</p>
</li>
<li><p>网络流控有静态限速和动态反压两种手段</p>
</li>
<li><p>Flink 1.5 之前是基于 TCP 流控 + bounded buffer 实现反压</p>
</li>
<li><p>Flink 1.5 之后实现了自己托管的 credit - based 流控机制，在应用层模拟 TCP 的流控机制</p>
</li>
</ul>
<h3 id="思考"><a href="#思考" class="headerlink" title="思考"></a>思考</h3><p>有了动态反压，静态限速是不是完全没有作用了？</p>
<p><img src="_v_images/20200714171439509_423419298"></p>
<p>实际上动态反压不是万能的，我们流计算的结果最终是要输出到一个外部的存储（Storage），外部数据存储到 Sink 端的反压是不一定会触发的，这要取决于外部存储的实现，像 Kafka 这样是实现了限流限速的消息中间件可以通过协议将反压反馈给 Sink 端，但是像 ES 无法将反压进行传播反馈给 Sink 端，这种情况下为了防止外部存储在大的数据量下被打爆，我们就可以通过静态限速的方式在 Source 端去做限流。所以说动态反压并不能完全替代静态限速的，需要根据合适的场景去选择处理方案。</p>
<p>作者：阿里云云栖号<br>链接：<a target="_blank" rel="noopener" href="https://juejin.im/post/5dce4b265188254a2b1faddf">https://juejin.im/post/5dce4b265188254a2b1faddf</a><br>来源：掘金<br>著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://example.com/bigdata/Impala/01.Impala/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="aaronzhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Guadazi">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/bigdata/Impala/01.Impala/" class="post-title-link" itemprop="url">Impala基础与痛点</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2019-08-10 14:58:00" itemprop="dateCreated datePublished" datetime="2019-08-10T14:58:00+08:00">2019-08-10</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-04-11 16:41:41" itemprop="dateModified" datetime="2021-04-11T16:41:41+08:00">2021-04-11</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/bigdata/" itemprop="url" rel="index"><span itemprop="name">bigdata</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="01-Impala"><a href="#01-Impala" class="headerlink" title="01.Impala"></a>01.Impala</h1><h2 id="components"><a href="#components" class="headerlink" title="components"></a>components</h2><p><a target="_blank" rel="noopener" href="https://impala.apache.org/docs/build/html/topics/impala_components.html">impala components</a></p>
<h3 id="impalad的部署"><a href="#impalad的部署" class="headerlink" title="impalad的部署"></a>impalad的部署</h3><p>Impala daemons can be deployed in one of the following ways:</p>
<ul>
<li>HDFS and Impala are co-located, and each Impala daemon runs on the same host as a DataNode.</li>
<li>Impala is deployed separately in a compute cluster and reads remotely from HDFS, S3, ADLS, etc.</li>
</ul>
<h3 id="hive连接表需要刷新impalad"><a href="#hive连接表需要刷新impalad" class="headerlink" title="hive连接表需要刷新impalad"></a>hive连接表需要刷新impalad</h3><p>The catalog service avoids the need to issue REFRESH and INVALIDATE METADATA statements when the metadata changes are performed by statements issued through Impala. When you create a table, load data, and so on through Hive, you do need to issue REFRESH or INVALIDATE METADATA on an Impala daemon before executing a query there.</p>
<p>impala 自己执行修改元数据的请求时，不需要刷新和重载元数据，当通过hive创建表和加载数据时，在执行查询之前，需要在Impala守护进程上发出REFRESH或INVALIDATE元数据。</p>
<p>The REFRESH and INVALIDATE METADATA statements are not needed when the CREATE TABLE, INSERT, or other table-changing or data-changing operation is performed through Impala. These statements are still needed if such operations are done through Hive or by manipulating data files directly in HDFS, but in those cases the statements only need to be issued on one Impala daemon rather than on all daemons. </p>
<p>当通过Impala执行创建表、插入或其他表更改或数据更改操作时，不需要刷新和无效元数据语句。<br>如果通过Hive或直接在HDFS中操作数据文件，仍然需要，但是在这种情况下，只需要在一个Impala守护进程上发出这些语句，而不是在所有守护进程上。</p>
<h3 id="元数据加载与对查询的影响"><a href="#元数据加载与对查询的影响" class="headerlink" title="元数据加载与对查询的影响"></a>元数据加载与对查询的影响</h3><p><code>‑‑load_catalog_in_background</code> option to control when the metadata of a table is loaded.<br>If set to false, the metadata of a table is loaded when it is referenced for the first time. This means that the first run of a particular query can be slower than subsequent runs. Starting in Impala 2.2, the default for ‑‑load_catalog_in_background is false.</p>
<p>表的元数据在第一次引用时加载。这意味着特定查询的第一次运行可能比后续运行慢</p>
<p>If set to true, the catalog service attempts to load metadata for a table even if no query needed that metadata. So metadata will possibly be already loaded when the first query that would need it is run. However, for the following reasons, we recommend not to set the option to true.</p>
<p>catalogd尝试加载表的元数据，即使没有查询需要该元数据。因此，在运行第一个需要元数据的查询时，元数据可能已经被加载</p>
<p>Background load can interfere with query-specific metadata loading. This can happen on startup or after invalidating metadata, with a duration depending on the amount of metadata, and can lead to a seemingly random long running queries that are difficult to diagnose.</p>
<p>后台加载可能会干扰特定查询的元数据加载。这种情况可能在启动时发生，也可能在元数据失效后发生，持续时间取决于元数据的数量，并可能导致看似随机的长时间运行查询，而这些查询很难诊断。</p>
<p>Impala may load metadata for tables that are possibly never used, potentially increasing catalog size and consequently memory usage for both catalog service and Impala Daemon.</p>
<p>Impala可以为可能从未使用过的表加载元数据，这可能会增加目录大小，从而增加目录服务和Impala守护进程的内存使用量。</p>
<h3 id="元数据刷新耗时"><a href="#元数据刷新耗时" class="headerlink" title="元数据刷新耗时"></a>元数据刷新耗时</h3><p>For tables with a large volume of data and/or many partitions, retrieving all the metadata for a table can be time-consuming, taking minutes in some cases. Thus, each Impala node caches all of this metadata to reuse for future queries against the same table.<br>对于具有大量数据和/或许多分区的表，检索表的所有元数据可能很耗时，在某些情况下会花费几分钟。<br>因此，每个Impala节点都会缓存所有这些元数据，以供将来针对同一表的查询重用。</p>
<p>If the table definition or the data in the table is updated, all other Impala daemons in the cluster must receive the latest metadata, replacing the obsolete cached metadata, before issuing a query against that table</p>
<p>For DDL and DML issued through Hive, or changes made manually to files in HDFS, you still use the REFRESH statement (when new data files are added to existing tables) or the INVALIDATE METADATA statement (for entirely new tables, or after dropping a table, performing an HDFS rebalance operation, or deleting data files). Issuing INVALIDATE METADATA by itself retrieves metadata for all the tables tracked by the metastore. If you know that only specific tables have been changed outside of Impala, you can issue REFRESH table_name for each affected table to only retrieve the latest metadata for those tables.</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://example.com/bigdata/Flink/metrics/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="aaronzhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Guadazi">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/bigdata/Flink/metrics/" class="post-title-link" itemprop="url">Flink:指标监控</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2019-08-10 14:58:00" itemprop="dateCreated datePublished" datetime="2019-08-10T14:58:00+08:00">2019-08-10</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-04-11 16:41:41" itemprop="dateModified" datetime="2021-04-11T16:41:41+08:00">2021-04-11</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/bigdata/" itemprop="url" rel="index"><span itemprop="name">bigdata</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="flink-metrics"><a href="#flink-metrics" class="headerlink" title="flink-metrics"></a>flink-metrics</h1><p>[参考文献]</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_21653785/article/details/79625601">Flink源码系列-指标监控</a></li>
<li><a target="_blank" rel="noopener" href="https://www.cnblogs.com/0x12345678/p/10561039.html">自定义metric-report</a></li>
<li><a target="_blank" rel="noopener" href="http://www.mamicode.com/info-detail-2317943.html">深入理解Flink之metrics</a></li>
<li><a target="_blank" rel="noopener" href="https://my.oschina.net/go4it/blog/3023586">聊聊Flink的MertricsQueryServiceGateway</a></li>
<li><a target="_blank" rel="noopener" href="https://www.jianshu.com/p/e50586fff515">Flink指标</a></li>
</ul>
<p>Flink Metrics是通过引入<code>com.codahale.metrics</code>包实现的，它将收集的metrics分为四大类：<code>Counter</code>，<code>Gauge</code>，<code>Histogram</code>和<code>Meter</code>下面分别说明：</p>
<ul>
<li><code>Counter计数器</code><br>  用来统计一个metrics的总量。<br>  拿flink中的指标来举例，像Task/Operator中的numRecordsIn（此task或者operator接收到的record总量）和numRecordsOut（此task或者operator发送的record总量）就属于Counter。</li>
<li><code>Gauge指标值</code><br>  用来记录一个metrics的瞬间值。<br>  拿flink中的指标举例，像JobManager或者TaskManager中的<code>JVM.Heap.Used</code>就属于<code>Gauge</code>，记录某个时刻JobManager或者TaskManager所在机器的JVM堆使用量。</li>
<li><code>Histogram直方图</code><br>  有的时候我们不满足于只拿到metrics的总量或者瞬时值，当想得到metrics的最大值，最小值，中位数等信息时，我们就能用到Histogram了。<br>  Flink中属于Histogram的指标很少，但是最重要的一个是属于operator的latency。此项指标会记录数据处理的延迟信息，对任务监控起到很重要的作用。</li>
<li><code>Meter平均值</code><br>   用来记录一个metrics某个时间段内平均值。<br>   flink中类似指标有task/operator中的numRecordsInPerSecond，字面意思就可以理解，指的是此task或者operator每秒接收的记录数。</li>
</ul>
<h3 id="com-tencent-oceanus-metastore-metrics-CustomMetricsRegistry"><a href="#com-tencent-oceanus-metastore-metrics-CustomMetricsRegistry" class="headerlink" title="com.tencent.oceanus.metastore.metrics.CustomMetricsRegistry"></a>com.tencent.oceanus.metastore.metrics.CustomMetricsRegistry</h3>
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://example.com/bigdata/Flink/flink-on-yarn/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="aaronzhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Guadazi">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/bigdata/Flink/flink-on-yarn/" class="post-title-link" itemprop="url">Flink on yarn</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2019-08-10 14:58:00" itemprop="dateCreated datePublished" datetime="2019-08-10T14:58:00+08:00">2019-08-10</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-04-11 16:41:41" itemprop="dateModified" datetime="2021-04-11T16:41:41+08:00">2021-04-11</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/bigdata/" itemprop="url" rel="index"><span itemprop="name">bigdata</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="flink-on-yarn部署"><a href="#flink-on-yarn部署" class="headerlink" title="flink on yarn部署"></a>flink on yarn部署</h2><p>flink on yarn需要的组件与版本如下</p>
<ol>
<li>Zookeeper 3.4.9 用于做Flink的JobManager的HA服务</li>
<li>hadoop 2.7.2 搭建HDFS和Yarn</li>
<li>flink 1.3.2 或者 1.4.1版本（scala 2.11）</li>
</ol>
<p>Zookeeper, HDFS 和 Yarn 的组件的安装可以参照网上的教程。</p>
<p>在zookeeper，HDFS 和Yarn的组件的安装好的前提下，在客户机上提交Flink任务，具体流程如下：</p>
<ul>
<li>在启动Yarn-Session 之前， 设置好HADOOP_HOME,YARN_CONF_DIR ， HADOOP_CONF_DIR环境变量中三者的一个。如下所示， 根据具体的hadoop 路径来设置<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">export</span> HADOOP_HOME=/usr/<span class="built_in">local</span>/hadoop-current</span></span><br></pre></td></tr></table></figure></li>
<li>配置flink 目录下的flink-conf.yaml, 如下所示<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">jobmanager.rpc.address:</span> <span class="string">localhost</span></span><br><span class="line"><span class="attr">jobmanager.rpc.port:</span> <span class="number">6123</span></span><br><span class="line"><span class="attr">jobmanager.heap.mb:</span> <span class="number">256</span></span><br><span class="line"><span class="attr">taskmanager.heap.mb:</span> <span class="number">512</span></span><br><span class="line"><span class="attr">taskmanager.numberOfTaskSlots:</span> <span class="number">1</span></span><br><span class="line"><span class="attr">taskmanager.memory.preallocate:</span> <span class="literal">false</span></span><br><span class="line"><span class="attr">parallelism.default:</span> <span class="number">1</span></span><br><span class="line"><span class="attr">jobmanager.web.port:</span> <span class="number">8081</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># yarn</span></span><br><span class="line"><span class="attr">yarn.maximum-failed-containers:</span> <span class="number">99999</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#akka config</span></span><br><span class="line"><span class="attr">akka.watch.heartbeat.interval:</span> <span class="number">5</span> <span class="string">s</span></span><br><span class="line"><span class="attr">akka.watch.heartbeat.pause:</span> <span class="number">20</span> <span class="string">s</span></span><br><span class="line"><span class="attr">akka.ask.timeout:</span> <span class="number">60</span> <span class="string">s</span></span><br><span class="line"><span class="attr">akka.framesize:</span> <span class="string">20971520b</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#high-avaliability</span></span><br><span class="line"><span class="attr">high-availability:</span> <span class="string">zookeeper</span></span><br><span class="line"><span class="comment">## 根据安装的zookeeper信息填写</span></span><br><span class="line"><span class="attr">high-availability.zookeeper.quorum:</span> <span class="number">10.141</span><span class="number">.61</span><span class="number">.226</span><span class="string">:2181,10.141.53.244:2181,10.141.18.219:2181</span></span><br><span class="line"><span class="attr">high-availability.zookeeper.path.root:</span> <span class="string">/flink</span></span><br><span class="line"><span class="comment">## HA 信息存储到HDFS的目录，根据各自的Hdfs情况修改</span></span><br><span class="line"><span class="attr">high-availability.zookeeper.storageDir:</span> <span class="string">hdfs://hdcluster/flink/recovery/</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#checkpoint config</span></span><br><span class="line"><span class="attr">state.backend:</span> <span class="string">rocksdb</span></span><br><span class="line"><span class="comment">## checkpoint到HDFS的目录 根据各自安装的HDFS情况修改</span></span><br><span class="line"><span class="attr">state.backend.fs.checkpointdir:</span> <span class="string">hdfs://hdcluster/flink/checkpoint</span></span><br><span class="line"><span class="comment">## 对外checkpoint到HDFS的目录</span></span><br><span class="line"><span class="attr">state.checkpoints.dir:</span> <span class="string">hdfs://hdcluster/flink/savepoint</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#memory config</span></span><br><span class="line"><span class="attr">env.java.opts:</span> <span class="string">-XX:+UseConcMarkSweepGC</span> <span class="string">-XX:CMSInitiatingOccupancyFraction=75</span> <span class="string">-XX:+UseCMSInitiatingOccupancyOnly</span> <span class="string">-XX:+AlwaysPreTouch</span> <span class="string">-server</span> <span class="string">-XX:+HeapDumpOnOutOfMemoryError</span></span><br><span class="line"><span class="attr">yarn.heap-cutoff-ratio:</span> <span class="number">0.2</span></span><br><span class="line"><span class="attr">taskmanager.memory.off-heap:</span> <span class="literal">true</span></span><br><span class="line"></span><br></pre></td></tr></table></figure></li>
<li>提交Yarn-Session，切换到flink的bin 目录下,提交命令如下<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> ./yarn-session.sh -n 2 -s 6 -jm 3072 -tm 6144 -nm <span class="built_in">test</span> -d</span></span><br></pre></td></tr></table></figure>
启动yarn-session的参数解释如下</li>
</ul>
<table>
<thead>
<tr>
<th>参数</th>
<th>参数解释</th>
<th>设置推荐</th>
</tr>
</thead>
<tbody><tr>
<td>-n(–container)</td>
<td>taskmanager的数量</td>
<td></td>
</tr>
<tr>
<td>-s(–slots)</td>
<td>用启动应用所需的slot数量/ -s 的值向上取整，有时可以多一些taskmanager，做冗余 每个taskmanager的slot数量，默认一个slot一个core，默认每个taskmanager的slot的个数为1</td>
<td>6～10</td>
</tr>
<tr>
<td>-jm</td>
<td>jobmanager的内存（单位MB)</td>
<td>3072</td>
</tr>
<tr>
<td>-tm</td>
<td>每个taskmanager的内存（单位MB)</td>
<td>根据core 与内存的比例来设置，-s的值＊ （core与内存的比）来算</td>
</tr>
<tr>
<td>-nm</td>
<td>yarn 的appName(现在yarn的ui上的名字)｜</td>
<td></td>
</tr>
<tr>
<td>-d</td>
<td>后台执行</td>
<td></td>
</tr>
</tbody></table>
<ul>
<li>提交yarn－session 后，可以在yarn的ui上看到一个应用（应用有一个appId）, 切换到flink的bin目录下，提交flink 应用。命令如下<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> ./flink -run file:///home/yarn/test.jar -a 1 -p 12 -yid appId -nm flink-test -d</span></span><br></pre></td></tr></table></figure>
启动flink 应用的参数解释如下</li>
</ul>
<table>
<thead>
<tr>
<th>参数</th>
<th>参数解释</th>
</tr>
</thead>
<tbody><tr>
<td>-j</td>
<td>运行flink 应用的jar所在的目录</td>
</tr>
<tr>
<td>-a</td>
<td>运行flink 应用的主方法的参数</td>
</tr>
<tr>
<td>-p</td>
<td>运行flink应用的并行度</td>
</tr>
<tr>
<td>-c</td>
<td>运行flink应用的主类, 可以通过在打包设置主类</td>
</tr>
<tr>
<td>-nm</td>
<td>flink 应用名字，在flink-ui 上面展示</td>
</tr>
<tr>
<td>-d</td>
<td>后台执行</td>
</tr>
<tr>
<td>–fromsavepoint</td>
<td>flink 应用启动的状态恢复点</td>
</tr>
</tbody></table>
<ul>
<li>启动flink应用成功，即可在yarn ui 点击对应应用的ApplicationMaster链接,既可以查看flink-ui ，并查看flink 应用运行情况。</li>
</ul>
<p>注：在安装部署遇到任何问题，可以在小象问答，微信群以及私聊提出，我们一般会在晚上作答（由于白天要上班，作答不及时请谅解。）</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://example.com/Java/tool/01.%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="aaronzhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Guadazi">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/Java/tool/01.%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F/" class="post-title-link" itemprop="url">正则表达式</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2019-08-10 14:58:00" itemprop="dateCreated datePublished" datetime="2019-08-10T14:58:00+08:00">2019-08-10</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-04-11 16:41:41" itemprop="dateModified" datetime="2021-04-11T16:41:41+08:00">2021-04-11</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/tool/" itemprop="url" rel="index"><span itemprop="name">tool</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="01-正则表达式"><a href="#01-正则表达式" class="headerlink" title="01.正则表达式"></a>01.正则表达式</h1><p>To use regular expressions effectively in Java, you need to know the syntax. The syntax is extensive, enabling you to write very advanced regular expressions. It may take a lot of exercise to fully master the syntax.</p>
<p>In this text I will go through the basics of the syntax with examples. I will not cover every little detail of the syntax, but focus on the main concepts you need to understand, in order to work with regular expressions. For a full explanation, see the <a target="_blank" rel="noopener" href="http://docs.oracle.com/javase/8/docs/api/java/util/regex/Pattern.html"><code>Pattern</code> class JavaDoc page</a>.</p>
<p>Before showing all the advanced options you can use in Java regular expressions, I will give you a quick run-down of the Java regular expression syntax basics.</p>
<p>The most basic form of regular expressions is an expression that simply matches certain characters. Here is an example:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"></span><br></pre></td></tr></table></figure>
<p>This simple regular expression will match occurences of the text “John” in a given input text.</p>
<p>You can use any characters in the alphabet in a regular expression.</p>
<p>You can also refer to characters via their octal, hexadecimal or unicode codes. Here are two examples:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"></span><br></pre></td></tr></table></figure>
<p>These three expressions all refer to the uppercase <code>A</code> character. The first uses the octal code (<code>101</code>) for <code>A</code>, the second uses the hexadecimal code (<code>41</code>) and the third uses the unicode code (<code>0041</code>).</p>
<p>Character classes are constructst that enable you to specify a match against multiple characters instead of just one. In other words, a character class matches a single character in the input text against multiple allowed characters in the character class. For instance, you can match either of the characters <code>a</code>, <code>b</code> or <code>c</code> like this:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"></span><br></pre></td></tr></table></figure>
<p>Character classes are nested inside a pair of square brackets <code>[]</code>. The brackets themselves are not part of what is being matched.</p>
<p>You can use character classes for many things. For instance, this example finds all occurrences of the word <code>John</code>, with either a lowercase or uppercase <code>J</code>:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"></span><br></pre></td></tr></table></figure>
<p>The character class <code>[Jj]</code> will match either a <code>J</code> or a <code>j</code>, and the rest of the expression will match the characters <code>ohn</code> in that exact sequence.</p>
<p>There are several other character classes you can use. See the character class table later in this text.</p>
<p>The Java regular expression syntax has a few predefined character classes you can use. For instance, the <code>\d</code> character class matches any digit, the <code>\s</code> character class matches any white space character, and the <code>\w</code> character matches any word character.</p>
<p>The predefined character classes do not have to be enclosed in square brackets, but you can if you want to combine them. Here are a few examples:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"></span><br></pre></td></tr></table></figure>
<p>The first example matches any digit character. The second example matches any digit or any white space character.</p>
<p>The predefined character classes are listed in a table later in this text.</p>
<p>The syntax also include matchers for matching boundaries, like boundaries between words, the beginning and end of the input text etc. For instance, the <code>\w</code> matches boundaries between words, the <code>^</code> matches the beginning of a line, and the <code>$</code> matches the end of a line.</p>
<p>Here is a boundary matcher example:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"></span><br></pre></td></tr></table></figure>
<p>This expression matches a line of text with only the text <code>This is a single line</code>. Notice the start-of-line and end-of-line matchers in the expression. These state that there can be nothing before or after the text, except the beginning and end of a line.</p>
<p>There is a full list of boundary matchers later in this text.</p>
<p>Quantifiers enables you to match a given expression or subexpression multiple times. For instance, the following expression matches the letter <code>A</code> zero or more times:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"></span><br></pre></td></tr></table></figure>
<p>The <code>*</code> character is a quantifier that means “zero or more times”. There is also a <code>+</code> quantifier meaning “one or more times”, a <code>?</code> quantifier meaning “zero or one time”, and a few others which you can see in the quantifier table later in this text.</p>
<p>Quantifiers can be either “reluctant”, “greedy” or “possesive”. A reluctant quantifier will match as little as possible of the input text. A greedy quantifier will match as much as possible of the input text. A possesive quantifier will match as much as possible, even if it makes the rest of the expression not match anything, and the expression to fail finding a match.</p>
<p>I will illustrate the difference between reluctant, greedy and possesive quantifiers with an example. Here is an input text:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"></span><br></pre></td></tr></table></figure>
<p>Then look at the following expression with a reluctant quantifier:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"></span><br></pre></td></tr></table></figure>
<p>This expression will match the word <code>John</code> followed by zero or more characters The <code>.</code> means “any character”, and the <code>*</code> means “zero or more times”. The <code>?</code> after the <code>*</code> makes the <code>*</code> a reluctant quantifier.</p>
<p>Being a reluctant quantifier, the quantifier will match as little as possible, meaning zero characters. The expression will thus find the word <code>John</code> with zero characters after, 3 times in the above input text.</p>
<p>If we change the quantifier to a greedy quantifier, the expression will look like this:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"></span><br></pre></td></tr></table></figure>
<p>The greedy quantifier will match as many characters as possible. Now the expression will only match the first occurrence of <code>John</code>, and the greedy quantifier will match the rest of the characters in the input text. Thus, only a single match is found.</p>
<p>Finally, lets us change the expression a bit to contain a possesive quantifier:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"></span><br></pre></td></tr></table></figure>
<p>The <code>+</code> after the <code>*</code> makes it a possesive quantifier.</p>
<p>This expression will not match the input text given above, even if both the words <code>John</code> and <code>hurt</code> are found in the input text. Why is that? Because the <code>.*+</code> is possesive. Instead of matching as much as possible to make the expression match, as a greedy quantifier would have done, the possesive quantifier matches as much as possible, regardless of whether the expression will match or not.</p>
<p>The <code>.*+</code> will match all characters after the first occurrence of <code>John</code> in the input text, including the word <code>hurt</code>. Thus, there is no <code>hurt</code> word left to match, when the possesive quantifier has claimed its match.</p>
<p>If you change the quantifier to a greedy quantifier, the expression will match the input text one time. Here is how the expression looks with a greedy quantifier:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"></span><br></pre></td></tr></table></figure>
<p>You will have to play around with the different quantifiers and types to understand how they work. See the table later in this text for a full list of quantifiers.</p>
<p>The Java regular expression syntax also has support for a few logical operators (and, or, not).</p>
<p>The <code>and</code> operator is implicit. When you write the expression <code>John</code>, then it means “<code>J</code> and <code>o</code> and <code>h</code> and <code>n</code>“.</p>
<p>The <code>or</code> operator is explicit, and is written with a <code>|</code>. For instance, the expression <code>John|hurt</code> will match either the word <code>John</code>, or the word <code>hurt</code>.</p>
<table>
<thead>
<tr>
<th>Construct</th>
<th>Matches</th>
</tr>
</thead>
<tbody><tr>
<td><code>x</code></td>
<td>The character x. Any character in the alphabet can be used in place of x.</td>
</tr>
<tr>
<td><code>\\</code></td>
<td>The backslash character. A single backslash is used as escape character in conjunction with other characters to signal special matching, so to match just the backslash character itself, you need to escape with a backslash character. Hence the double backslash to match a single backslash character.</td>
</tr>
<tr>
<td><code>\0n</code></td>
<td>The character with octal value <code>0n</code>. n has to be between 0 and 7.</td>
</tr>
<tr>
<td><code>\0nn</code></td>
<td>The character with octal value <code>0nn</code>. n has to be between 0 and 7.</td>
</tr>
<tr>
<td><code>\0mnn</code></td>
<td>The character with octal value <code>0mnn</code>. m has to be between 0 and 3, n has to be between 0 and 7.</td>
</tr>
<tr>
<td><code>\xhh</code></td>
<td>The character with the hexadecimal value <code>0xhh</code>.</td>
</tr>
<tr>
<td><code>\uhhhh</code></td>
<td>The character with the hexadecimal value <code>0xhhhh</code>. This construct is used to match unicode characters.</td>
</tr>
<tr>
<td><code>\t</code></td>
<td>The tab character.</td>
</tr>
<tr>
<td><code>\n</code></td>
<td>The newline (line feed) character (unicode: <code>&#39;\u000A&#39;</code>).</td>
</tr>
<tr>
<td><code>\r</code></td>
<td>The carriage-return character (unicode: <code>&#39;\u000D&#39;</code>).</td>
</tr>
<tr>
<td><code>\f</code></td>
<td>The form-feed character (unicode: <code>&#39;\u000C&#39;</code>).</td>
</tr>
<tr>
<td><code>\a</code></td>
<td>The alert (bell) character (unicode: <code>&#39;\u0007&#39;</code>).</td>
</tr>
<tr>
<td><code>\e</code></td>
<td>The escape character (unicode: <code>&#39;\u001B&#39;</code>).</td>
</tr>
<tr>
<td><code>\cx</code></td>
<td>The control character corresponding to <code>x</code></td>
</tr>
<tr>
<td>``</td>
<td></td>
</tr>
</tbody></table>
<table>
<thead>
<tr>
<th>Construct</th>
<th>Matches</th>
</tr>
</thead>
<tbody><tr>
<td><code>[abc]</code></td>
<td>Matches <code>a</code>, or <code>b</code> or <code>c</code>. This is called a simple class, and it matches any of the characters in the class.</td>
</tr>
<tr>
<td><code>[^abc]</code></td>
<td>Matches any character except <code>a</code>, <code>b</code>, and <code>c</code>. This is a negation.</td>
</tr>
<tr>
<td><code>[a-zA-Z]</code></td>
<td>Matches any character from <code>a</code> to <code>z</code>, or <code>A</code> to <code>Z</code>, including <code>a</code>, <code>A</code>, <code>z</code> and <code>Z</code>. This called a range.</td>
</tr>
<tr>
<td><code>[a-d[m-p]]</code></td>
<td>Matches any character from <code>a</code> to <code>d</code>, or from <code>m</code> to <code>p</code>. This is called a union.</td>
</tr>
<tr>
<td><code>[a-z&amp;&amp;[def]]</code></td>
<td>Matches <code>d</code>, <code>e</code>, or <code>f</code>. This is called an intersection (here between the range <code>a-z</code> and the characters <code>def</code>).</td>
</tr>
<tr>
<td><code>[a-z&amp;&amp;[^bc]]</code></td>
<td>Matches all characters from <code>a</code> to <code>z</code> except <code>b</code> and <code>c</code>. This is called a subtraction.</td>
</tr>
<tr>
<td><code>[a-z&amp;&amp;[^m-p]]</code></td>
<td>Matches all characters from <code>a</code> to <code>z</code> except the characters from <code>m</code> to <code>p</code>. This is also called a subtraction.</td>
</tr>
</tbody></table>
<table>
<thead>
<tr>
<th>Construct</th>
<th>Matches</th>
</tr>
</thead>
<tbody><tr>
<td><code>.</code></td>
<td>Matches any single character. May or may not match line terminators, depending on what flags were used to compile the <code>Pattern</code>.</td>
</tr>
<tr>
<td><code>\d</code></td>
<td>Matches any digit [0-9]</td>
</tr>
<tr>
<td><code>\D</code></td>
<td>Matches any non-digit character [^0-9]</td>
</tr>
<tr>
<td><code>\s</code></td>
<td>Matches any white space character (space, tab, line break, carriage return)</td>
</tr>
<tr>
<td><code>\S</code></td>
<td>Matches any non-white space character.</td>
</tr>
<tr>
<td><code>\w</code></td>
<td>Matches any word character.</td>
</tr>
<tr>
<td><code>\W</code></td>
<td>Matches any non-word character.</td>
</tr>
</tbody></table>
<table>
<thead>
<tr>
<th>Construct</th>
<th>Matches</th>
</tr>
</thead>
<tbody><tr>
<td><code>^</code></td>
<td>Matches the beginning of a line.</td>
</tr>
<tr>
<td><code>$</code></td>
<td>Matches then end of a line.</td>
</tr>
<tr>
<td><code>\b</code></td>
<td>Matches a word boundary.</td>
</tr>
<tr>
<td><code>\B</code></td>
<td>Matches a non-word boundary.</td>
</tr>
<tr>
<td><code>\A</code></td>
<td>Matches the beginning of the input text.</td>
</tr>
<tr>
<td><code>\G</code></td>
<td>Matches the end of the previous match</td>
</tr>
<tr>
<td><code>\Z</code></td>
<td>Matches the end of the input text except the final terminator if any.</td>
</tr>
<tr>
<td><code>\z</code></td>
<td>Matches the end of the input text.</td>
</tr>
</tbody></table>
<table>
<thead>
<tr>
<th>Greedy</th>
<th>Reluctant</th>
<th>Possessive</th>
<th>Matches</th>
</tr>
</thead>
<tbody><tr>
<td><code>X?</code></td>
<td><code>X??</code></td>
<td><code>X?+</code></td>
<td>Matches X once, or not at all (0 or 1 time).</td>
</tr>
<tr>
<td><code>X*</code></td>
<td><code>X*?</code></td>
<td><code>X*+</code></td>
<td>Matches X zero or more times.</td>
</tr>
<tr>
<td><code>X+</code></td>
<td><code>X+?</code></td>
<td><code>X++</code></td>
<td>Matches X one or more times.</td>
</tr>
<tr>
<td><code>X&#123;n&#125;</code></td>
<td><code>X&#123;n&#125;?</code></td>
<td><code>X&#123;n&#125;+</code></td>
<td>Matches X exactly n times.</td>
</tr>
<tr>
<td><code>X&#123;n,&#125;</code></td>
<td><code>X&#123;n,&#125;?</code></td>
<td><code>X&#123;n,&#125;+</code></td>
<td>Matches X at least n times.</td>
</tr>
<tr>
<td><code>X&#123;n, m)</code></td>
<td><code>X&#123;n, m)?</code></td>
<td><code>X&#123;n, m)+</code></td>
<td>Matches X at least n time, but at most m times.</td>
</tr>
</tbody></table>
<table>
<thead>
<tr>
<th>Construct</th>
<th>Matches</th>
</tr>
</thead>
<tbody><tr>
<td><code>XY</code></td>
<td>Matches X and Y (X followed by Y).</td>
</tr>
<tr>
<td>`X</td>
<td>Y`</td>
</tr>
</tbody></table>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://example.com/Java/tool/02.RAID/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="aaronzhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Guadazi">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/Java/tool/02.RAID/" class="post-title-link" itemprop="url">磁盘Raid机制</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2019-08-10 14:58:00" itemprop="dateCreated datePublished" datetime="2019-08-10T14:58:00+08:00">2019-08-10</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-04-11 16:41:41" itemprop="dateModified" datetime="2021-04-11T16:41:41+08:00">2021-04-11</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/tool/" itemprop="url" rel="index"><span itemprop="name">tool</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="Raid"><a href="#Raid" class="headerlink" title="Raid"></a>Raid</h1>
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://example.com/Java/tool/04.Mac/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="aaronzhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Guadazi">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/Java/tool/04.Mac/" class="post-title-link" itemprop="url">MacOS操作经验</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2019-08-10 14:58:00" itemprop="dateCreated datePublished" datetime="2019-08-10T14:58:00+08:00">2019-08-10</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-04-11 16:41:41" itemprop="dateModified" datetime="2021-04-11T16:41:41+08:00">2021-04-11</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/tool/" itemprop="url" rel="index"><span itemprop="name">tool</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="MySQL"><a href="#MySQL" class="headerlink" title="MySQL"></a>MySQL</h1><p>因为苹果在OS X 10.11中引入的SIP特性使得即使加了sudo（也就是具有root权限）也无法修改系统级的目录，其中就包括了/usr/bin。要解决这个问题有两种做法：</p>
<p>一种是比较不安全的就是关闭SIP，也就是rootless特性；</p>
<p>另一种是将本要链接到/usr/bin下的改链接到/usr/local/bin下就好了。</p>
<p>解决办法</p>
<p>sudo ln -s /usr/local/mysql/bin/mysql /usr/local/bin</p>
<p>mysql 启动</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mysqld --user mysql</span><br></pre></td></tr></table></figure>
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://example.com/Java/metric/JMeter/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="aaronzhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Guadazi">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/Java/metric/JMeter/" class="post-title-link" itemprop="url">JMeter与性能压测</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2019-08-10 14:58:00" itemprop="dateCreated datePublished" datetime="2019-08-10T14:58:00+08:00">2019-08-10</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-04-11 16:41:40" itemprop="dateModified" datetime="2021-04-11T16:41:40+08:00">2021-04-11</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/java/" itemprop="url" rel="index"><span itemprop="name">java</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="JMeter与性能压测"><a href="#JMeter与性能压测" class="headerlink" title="JMeter与性能压测"></a>JMeter与性能压测</h1><p> jmeter是一款纯java的性能测试工具，跨平台运行方便、提供图形化界面设置、简单易用。</p>
<p>在性能测试方法论中，很典型的方法就是二八原则，量化业务需求。</p>
<p>二八原则：指80%的业务量在20%的时间里完成。</p>
<p>如何理解，下面我们来个例子吧</p>
<p>用户登录场景：早高峰时段，8：50—9：10，5000坐席上线登陆。</p>
<pre><code>  业务量：5000个 

  时间：20x60=1200秒

吞吐量=80%x业务量/(20%*时间)=4000/240=16.7/秒</code></pre>
<p>而并非5000/1200=4.1/秒</p>
<p>实际上，登录请求数分布是一个正态分布，最高峰时肯定比4.1/秒更高，高峰段实际上完成了80%的业务量，却只花了20%的时间。</p>
<p>温馨提示：</p>
<p>1.二八原则计算的结果并非在线并发用户数，是系统要达到的处理能力（吞吐量），初学者容易被误导，那这这个数据就去设置并发数，这是错误滴。</p>
<p>2.如果你的系统性能要求更高，也可以选择一九原则或更严格的算法，二八原则比较通用，一般系统性能比较接近这个算法而已，大家应该活用。</p>
<p>3.tps、响应时间、在线并发数三者关系详解：<a target="_blank" rel="noopener" href="http://blog.csdn.net/musen518/article/details/43795047">点击打开链接</a></p>
<p>  三者关系图</p>
<p><img src="_v_images/20200121162958712_1262110967"></p>
<ol start="2">
<li> 结论</li>
</ol>
<ul>
<li>小并发数区间测试，找拐点（如：100-300并发持续5分钟，可以发现上图中200并发时出现拐点）</li>
<li>大并发数区间测试，找符合需求的最大并发数（如：1800-2200并发持续5分钟，可以找到满足响应时间在3秒内的最大并发数2000）</li>
<li>利用最大并发数，压测环境在极限时的资源消耗（压测时间1小时以内）</li>
<li>80%最大并发数，进行稳定性测试（压测时间1小时以上）</li>
</ul>
<p>注：执行机资源消耗必须监控上，保证能提供稳定的并发负载。</p>
<p>注：这里的响应时间是90%响应时间</p>
<p>tps:</p>
<p>每秒事务处理量 - <a target="_blank" rel="noopener" href="https://baike.baidu.com/item/%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95">性能测试</a>的术语介绍</p>
<p>TPS(Transaction Per Second)</p>
<p>每秒钟系统能够处理的交易或事务的数量。它是衡量系统处理能力的重要指标。TPS是<a target="_blank" rel="noopener" href="https://baike.baidu.com/item/LoadRunner">LoadRunner</a>中重要的性能参数指标。</p>
<p> 1.下载安装</p>
<p>仅仅需要从apache的网站找到下载包，解压到本地文件目录即可。</p>
<p><a target="_blank" rel="noopener" href="http://jmeter.apache.org/download_jmeter.cgi">http://jmeter.apache.org/download_jmeter.cgi</a></p>
<p>2.启动</p>
<p>解压目录中存在一个bin的目录，里面有很多批处理文件和脚本文件，window系统运行jmeter.bat即可。需要关注的是bin目录中的jmeter.properties文件，这是运行相关的配置文件. 特别是TCP Sampler configuration部分几个配置会和后面内容相关</p>
<p>3.建立一种类型测试</p>
<p>这里只描述简单的tcp测试建立步骤，因为目前支持的测试类型很多，无法一一陈述，功能细节部分可以参考JMeter文档</p>
<p>1）创建测试线程组</p>
<p><strong>1. 启动测试用接口</strong><br>首先我们写一段 php 代码，通过 PHP 内置的 Server 启动它。</p>
<figure class="highlight php"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="variable">$user_id</span> = <span class="variable">$_GET</span>[<span class="string">&#x27;user_id&#x27;</span>];</span><br><span class="line">file_put_contents(<span class="string">&#x27;/tmp/1.log&#x27;</span>, <span class="variable">$user_id</span>.PHP_EOL,  FILE_APPEND);</span><br><span class="line"><span class="keyword">echo</span> <span class="variable">$user_id</span>;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>以上代码保存为 <code>index.php</code></p>
<p>命令中执行 <code>php -S 127.0.0.1:8080</code></p>
<p>在浏览器访问 <code>http://127.0.0.1:8080/index.php?user_id=1</code> , 输出 <code>1</code> 说明服务接口正常</p>
<p><strong>2. 创建线程组</strong><br>使用 JMeter 测试应用性能首先要创建一个线程组<br>右键 “Text Plan”, 在弹出的菜单栏选择 “Add-&gt;Threads(Users)-&gt;Thread Group”</p>
<p>就创建了一个线程组：</p>
<p><img src="_v_images/20200121162958609_236107942.png"></p>
<p>“Number of Threads (users): ” 即并发用户数，相当于 ab 命令的 -c 参数<br>“Loop Count:” 循环请求次数， 即每个线程请求多少次， 这个数据乘以线程数相当于 ab 命令的 -n 参数</p>
<p>我们设置了 “Number of Threads (users)” 为 5 ， “Loop Count” 为 60 ， 相当于ab 命令</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ab -c 5 -n 300 http:&#x2F;&#x2F;xxx.com</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p><strong>2. 创建测试请求</strong><br>右键我们刚刚创建的线程组“Thread Group”, 选择 “Add-&gt; Sampler-&gt; HTTP Request”</p>
<p><img src="_v_images/20200121162958391_1716814489.png"></p>
<p>这一步相当于通过多个参数拼出要测试的接口地址。</p>
<p>注意<code>Path</code>中， <code>$&#123;__counter(false)&#125;</code> 为 JMeter 内置的函数， 它的返回值为当前请求次数<br>**这样保证了我们每次向服务器请求的 <code>user_id</code> 的值都不一样 **</p>
<p>此时我们将要进行的测试等同于 ab 测试命令：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ab -c 5 -n 300 http:&#x2F;&#x2F;127.0.0.1&#x2F;index.php?user_id&#x3D;1</span><br></pre></td></tr></table></figure>
<h2 id="、-Counter-函数"><a href="#、-Counter-函数" class="headerlink" title="、_Counter 函数"></a>、_Counter 函数</h2><p>每次调用计数器函数都会产生一个新值，从1开始每次加1。计数器既可以被配置成针对每个虚拟用户是独立的，也可以被配置成所有虚拟用户公用的。如果每个虚拟用户的计数器是独立增长的，那么通常被用于记录测试计划运行了多少遍。全局计数器通常被用于记录发送了多少次请求。</p>
<p>计数器使用一个整数值来记录，允许的最大值为2,147,483,647。</p>
<p>功能：这个函数是一个计数器，用于统计函数的使用次数，它从1开始，每调用这个函数一次它就会自动加1，它有两个参数，第一个参数是布尔型的，只能设置成“TRUE”或者“FALSE”，如果是TRUE，那么每个用户有自己的计数器，可以用于统计每个线程歌执行了多少次。如果是FALSE，那就使用全局计数器，可以统计出这次测试共运行了多少次。第二个参数是“函数名称”</p>
<p><strong>格式：</strong>${__counter(FALSE,test)}</p>
<p><strong>使用：</strong>我们将“_counter”函数生成的参数复制到某个参数下面，如果为TRUE格式，则每个线程各自统计，最大数为循环数，如果为FALSE，则所有线程一起统计，最大数为线程数乘以循环数</p>
<p><strong>参数：</strong></p>
<p>第一个参数：True，如果测试人员希望每个虚拟用户的计数器保持独立，与其他用户的计数器相区别。False，全局计数器</p>
<p>第二个参数：重用计数器函数创建值的引用名。测试人员可以这样引用计数器的值：${test}。这样一来，测试人员就可以创建一个计数器后，在多个地方引用它的值。</p>
<p>以上，摘自网络（不知道怎么用，只好摘抄，记录下来等灵感~~~~(&gt;_&lt;)~~~~ ）。</p>
<p>目前，我测试_Counter函数，就是在参数列表加一个参数，值填写为${__counter(FALSE,test)}</p>
<p>）  </p>
<p><strong>3.开始测试</strong><br>右键线程组 “Thread Group”， 选择 “Add-&gt; Listener-&gt;Summary Report “, 创建一个结果报表</p>
<p>然后点击， 菜单栏中的绿色按钮, 开始测试：</p>
<p><img src="_v_images/20200121162958170_1920589116.png"></p>
<p>结果如图:</p>
<p> <img src="_v_images/20200121162957964_563024753.png"></p>
<p>打开 ‘/tmp/1.log’ 可以看到，每次请求的 user_id的值都是不同的。</p>
<h1 id="Thread-Group-线程组"><a href="#Thread-Group-线程组" class="headerlink" title="Thread Group(线程组)"></a>Thread Group(线程组)</h1><blockquote>
<p>1.线程组，或者可以叫用户组，进行性能测试时的用户资源池。</p>
<p>2.是任何一个测试计划执行的开始点。</p>
<p>3.上一篇提到的“控制器”和“HTTP请求”(采集器)必须在线程组内；监听器等其他组件，可以直接放在测试计划下。</p>
<p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/linglingyuese/archive/2013/03/06/linglingyuese-three.html">https://www.cnblogs.com/linglingyuese/archive/2013/03/06/linglingyuese-three.html</a></p>
<p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/hait1234/p/6767212.html">https://www.cnblogs.com/hait1234/p/6767212.html</a></p>
</blockquote>
<p>二、Thread Group线程组功能分区</p>
<p>总的来说，一个线程组有三个功能分区，这里分别标注为区域1、区域2、区域3。</p>
<p><img src="_v_images/20200121162957741_1278470356.png"></p>
<p>1.区域1：在取样器错误后要执行的动作，这个区域的主要作用很明显，在线程内的采样器失败后，接下来做什么。</p>
<pre><code> （1）继续：选择此项，将继续执行接下来的操作。

 （2）Start Next Loop：忽略错误，执行下一个循环。

 （3）停止线程：退出该线程（不再进行此线程的任何操作）。

 （4）停止测试：等待当前执行的采样器结束后，结束整个测试。

 （5）Stop Test Now：直接停止整个测试。（注意与4的“停止测试”进行区分）。</code></pre>
<p>2.区域2：线程属性，这里可以设置线程数（模拟的用户数）和循环次数。含义如下图所示：</p>
<p><img src="_v_images/20200121162957533_2123434422.png"></p>
<p>ramp up:斜坡上升; [动词短语] 加强，加大;</p>
<p> 相当于warm up的一个词,包含准备,热身,加速的意思,可用在生产中小批量的试制中, 也可以指人初入公司的锻炼. 在项目初始阶段要做许多准备工作。</p>
<p>3.区域3：调度器配置（全部都在调度器复选框被选中的前提下，下面的选项才会生效。）</p>
<p><img src="_v_images/20200121162957230_2144403715.png"></p>
<p>最重要的Tcp Sampler:tcp取样器</p>
<h4 id="TCPClient-classname"><a href="#TCPClient-classname" class="headerlink" title="TCPClient classname"></a>TCPClient classname</h4><p>TCP Sampler提供了3个Sampler的实现，分别是</p>
<p>org.apache.jmeter.protocol.tcp.sampler.TCPClientImpl </p>
<p>org.apache.jmeter.protocol.tcp.sampler.BinaryTCPClientImpl和<br>org.apache.jmeter.protocol.tcp.sampler.LengthPrefixedBinaryTCPClientImpl。</p>
<p>其中TCPClientImpl实现了以文本编辑器中所编辑的纯文本为内容进行发送，BinaryTCPClientImpl则以文本编辑器中所编辑的16进制字符（hex）内容为基础转换为二进制的字节内容进行发送，LengthPrefixedBinaryTCPClientImpl则会在BinaryTCPClientImpl基础上默认以发送内容的长度以字节前缀进行填充。</p>
<p>我们可以通过配置jmeter.properties文件中tcp.handler属性来设置默认的TCPClient。</p>
<h2 id="测试基于文本套接字应用"><a href="#测试基于文本套接字应用" class="headerlink" title="测试基于文本套接字应用"></a>测试基于文本套接字应用</h2><p>被测应用的源码请参见<a target="_blank" rel="noopener" href="https://link.jianshu.com/?t=https://github.com/XMeterSaaSService/Blog_sample_project/blob/master/socket_echo/src/main/java/net/xmeter/echo/TextServer.java">这里</a>. 如果想运行该程序，请点击该链接下载socket_echo-0.0.1-SNAPSHOT.jar，并且在命令行下执行:</p>
<p><a target="_blank" rel="noopener" href="https://github.com/XMeterSaaSService/Blog/_sample/_project/tree/master/socket_echo">https://github.com/XMeterSaaSService/Blog\_sample\_project/tree/master/socket_echo</a> </p>
<p>（javac 和java可以去掉包名后再在命令行执行）</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">java -cp socket_echo-0.0.1-SNAPSHOT.jar net.xmeter.echo.TextServer这个程序源码：</span><br></pre></td></tr></table></figure>
<p><img src="_v_images/20200121162957026_1366283370.gif"></p>
<p><a href="javascript:void(0);" title="复制代码"><img src="_v_images/20200121162956724_1452766871.gif" alt="复制代码"></a></p>
<p>import java.io.BufferedReader; import java.io.IOException; import java.io.InputStreamReader; import java.io.PrintWriter; import java.net.ServerSocket; import java.net.Socket; import java.util.concurrent.ExecutorService; import java.util.concurrent.Executors; import java.util.concurrent.atomic.AtomicInteger; public class TextServer { public static AtomicInteger sessions = new AtomicInteger(0); public void handleRequest(final Socket socket) {<br>        ExecutorService executor = Executors.newSingleThreadExecutor();</p>
<pre><code>    executor.submit(new Runnable() &#123;
        @Override public void run() &#123; try &#123;
                BufferedReader is = new BufferedReader(new InputStreamReader(socket.getInputStream()));
                PrintWriter os = new PrintWriter(socket.getOutputStream()); while(true) &#123;
                    String line = is.readLine(); if(line == null) &#123;
                        System.out.println(&quot;Probably the client side closed the connection, now close me as well.&quot;);
                        socket.close(); break;
                    &#125;
                    System.out.println(&quot;Received message: &quot; + line);
                    os.println(&quot;Echo: &quot; + line);
                    os.flush(); if(&quot;bye&quot;.equals(line)) &#123; break;
                    &#125;
                &#125;
            &#125; catch(Exception ex) &#123;
                ex.printStackTrace();
            &#125; finally &#123; try &#123;
                    socket.close(); int num = sessions.decrementAndGet();
                    System.out.println(&quot;Now totally has &quot; + num + &quot; of conn.&quot;);
                &#125; catch (IOException e) &#123;
                    e.printStackTrace();
                &#125;
            &#125;
        &#125;

    &#125;);

&#125; public static void main(String\[\] args) &#123; try &#123;
        ServerSocket server = new ServerSocket(4700); while(true) &#123;
            Socket socket = server.accept();
            TextServer srv = new TextServer();
            srv.handleRequest(socket); int num = sessions.incrementAndGet();
            System.out.println(&quot;Received new conn, now totally has &quot; + num + &quot; of conn.&quot;);
        &#125;
    &#125; catch (Exception e) &#123;
        e.printStackTrace();
    &#125;
&#125;</code></pre>
<p>}</p>
<p><a href="javascript:void(0);" title="复制代码"><img src="_v_images/20200121162956522_679224056.gif" alt="复制代码"></a></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(这个程序测试：</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">注意几图：hello后面有个换行， ENDof line Byte value 填写的是10.LF (NL line feed, new line) 换行键 ，ascill是10.os.println(&quot;Echo: &quot; + line); 用的是println，服务端返回的最后是一个换行符。如果不填写EOF byte value,那么客户端将会一直阻塞没有返回。</span><br></pre></td></tr></table></figure>
<p>我们发<strong>现EOL原来是与读数据相关的，就是设定来自于服务器数据流的一个结束标识字节。没有设置EOL将会一直读到输入流结束为</strong>止。</p>
<p>这里值得注意的是，这是个十进制的值（千万不要写成hex），比如你可以查询ASCII表，来确认一个表示结束字符的十进制值，我们以$作为案例，改造一下Mock TCP Server，输出结尾为$，如下面代码：</p>
<p>)</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"></span><br></pre></td></tr></table></figure>
<p>（请确保您的机器上已经安装了Java）。 该程序会在4700端口建立一个ServerSocket，等待来自客户端的请求，客户端如果发送了一个字符串，服务器端返回“Echo: “ + 客户端发送的字符串。如下图所示，如果我们使用telnet连接到服务器端的套接字应用，双方就可以直接进行通信了。</p>
<h5 id="TCPClientImpl"><a href="#TCPClientImpl" class="headerlink" title="TCPClientImpl"></a>TCPClientImpl</h5><p>我们使用TCPClientImpl对Mock TCP Server进行测试，配置参考下图：</p>
<p><img src="_v_images/20200121162956318_161208051.png"></p>
<p>点击运行测试，你会发现测试发生了阻塞，原因是服务器使用了readLine获取客户端的发送数据，需要根据发送数据中的CRLF（\r或\n）判断一行的<strong>结束。而我们制作的发送内容并不包括CRLF标识内容，因此，服务器阻塞在了读数据，测试客户端得不到服务器响应，同样</strong>也阻塞在了读数据，正确的配置需要添加一个“回车”（不能是”\r”或”\n”，因为TCPClientImpl会自动将其转换为对应的两个字符而不是CRLF标识）参考下图</p>
<p>TCP 取样器通过TCP/IP来连接特定服务器，连上服务器之后发送消息，然后等待服务器回复。</p>
<p>如果“Re-use connection”(重复使用连接) 复选框被选中了，在同一个线程中Samplers(取样器)共享连接，包含相同主机名和端口，不同主机/端口合并将会使用不同线程。如果“Re-use connection” 和 “Close connection”(关闭连接)同时被选中，这个套接字在运行完当前Samplers将会关闭。再下一个Sampler将会另外创建一个新套接字。你可能想要在每次线程循环结束之后关闭套接字。</p>
<p>如果一个错误被检测到或者“Re-use connection” 没有被选中，这个套接字将会关闭，另外套接字将会在接下Samplers被再一次打开。</p>
<p>详细看这篇文章：</p>
<h1 id="Apache-JMeter-TCPSampler的使用及自定义"><a href="#Apache-JMeter-TCPSampler的使用及自定义" class="headerlink" title="Apache JMeter TCPSampler的使用及自定义"></a><a target="_blank" rel="noopener" href="https://blog.csdn.net/xreztento/article/details/73741697">Apache JMeter TCPSampler的使用及自定义</a></h1><p> 还有这篇文章：<a target="_blank" rel="noopener" href="https://www.jianshu.com/p/63e08071075e">https://www.jianshu.com/p/63e08071075e</a></p>
<h1 id="JMeter—–TCP-Sampler（TCP-取样器）"><a href="#JMeter—–TCP-Sampler（TCP-取样器）" class="headerlink" title="JMeter—–TCP Sampler（TCP 取样器）"></a><a target="_blank" rel="noopener" href="https://blog.csdn.net/m0_37355951/article/details/74779977">JMeter—–TCP Sampler（TCP 取样器）</a></h1><p>jmeter报告结果中会出现三个时间</p>
<ol>
<li><p>Elapsed time    经过的时间(= Sample time = Load time = Response time ) </p>
<p>   这个时间是我们测试常用的时间，也是整个请求的消耗时间，从发送到接收完成全程消耗的时间</p>
</li>
<li><p>Latency time  延迟时间</p>
<p>  不常用，表示请求发送到刚开始接收响应时，这个时间&lt;Elapsed time</p>
</li>
</ol>
<p>3. Connection time  建立连接时间 （2.13新增参数）</p>
<pre><code>   不常用，请求连接建立的时间，这个时间 &lt; Latency time &lt; Elapsed time</code></pre>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/7/"><i class="fa fa-angle-left" aria-label="Previous page"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/7/">7</a><span class="page-number current">8</span><a class="page-number" href="/page/9/">9</a><span class="space">&hellip;</span><a class="page-number" href="/page/24/">24</a><a class="extend next" rel="next" href="/page/9/"><i class="fa fa-angle-right" aria-label="Next page"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">aaronzhang</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">235</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">17</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">125</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">aaronzhang</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  

</body>
</html>
