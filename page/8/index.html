<!DOCTYPE html>
<html lang="zh-Hans">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta property="og:type" content="website">
<meta property="og:title" content="Guadazi">
<meta property="og:url" content="http://example.com/page/8/index.html">
<meta property="og:site_name" content="Guadazi">
<meta property="og:locale">
<meta property="article:author" content="aaronzhang">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://example.com/page/8/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'zh-Hans'
  };
</script>

  <title>Guadazi</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Guadazi</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://example.com/bigdata/Flink/Flink-checkpoint-savepoint-2pc/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="aaronzhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Guadazi">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/bigdata/Flink/Flink-checkpoint-savepoint-2pc/" class="post-title-link" itemprop="url">Flink-checkpoint与高可用</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2019-08-10 14:58:00" itemprop="dateCreated datePublished" datetime="2019-08-10T14:58:00+08:00">2019-08-10</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-04-19 15:02:11" itemprop="dateModified" datetime="2021-04-19T15:02:11+08:00">2021-04-19</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/bigdata/" itemprop="url" rel="index"><span itemprop="name">bigdata</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>Flink Checkpoint 受 Chandy-Lamport 分布式快照启发，可以保证数据的高可用。但是有些情况下，不见得一定有效:</p>
<p>Flink On Yarn 模式，某个 Container 发生 OOM 异常，这种情况程序直接变成失败状态，此时 Flink 程序虽然开启 Checkpoint 也无法恢复，因为程序已经变成失败状态，所以此时可以借助外部参与启动程序，比如外部程序检测到实时任务失败时，从新对实时任务进行拉起。</p>
<h2 id="1-1-2PC"><a href="#1-1-2PC" class="headerlink" title="1.1. 2PC"></a>1.1. 2PC</h2><h3 id="1-1-1-Exactly-once-VS-At-least-once"><a href="#1-1-1-Exactly-once-VS-At-least-once" class="headerlink" title="1.1.1. Exactly-once VS At-least-once"></a>1.1.1. Exactly-once VS At-least-once</h3><p>算子做快照时，如果等所有输入端的barrier都到了才开始做快照，可保证算子的exactly-once；如果为了降低延时而跳过对齐，从而继续处理数据，那么等barrier都到齐后做快照就是at-least-once了，因为这次的快照掺杂了下一次快照的数据，当作业失败恢复的时候，这些数据会重复作用系统，就好像这些数据被消费了两遍。</p>
<p>注：对齐只会发生在算子的上端是join操作以及上游存在partition或者shuffle的情况，对于直连操作类似map、flatMap、filter等还是会保证exactly-once的语义。</p>
<h3 id="1-1-2-端到端的Exactly-once实现"><a href="#1-1-2-端到端的Exactly-once实现" class="headerlink" title="1.1.2. 端到端的Exactly once实现"></a>1.1.2. 端到端的Exactly once实现</h3><p>2PC分为几个阶段: 开始事务-&gt;预提交-&gt;提交(或回滚)</p>
<p>为了保证Exactly once, source和sink必须支持flink的2PC</p>
<p>当状态涉及到外部系统时，需要外部系统支持事务操作来配合flink实现2PC协议，从而保证数据的exatly-once。<br>这个时候，sink算子除了将自己的state写到后段，还必须准备好事务提交。</p>
<ul>
<li>一旦所有的算子完成了它们的pre-commit，它们会要求一个commit。</li>
<li>如果存在一个算子pre-commit失败了，本次事务失败，我们回滚到上次的checkpoint。</li>
<li>一旦master做出了commit的决定，那么这个commit必须得到执行，就算宕机恢复也有继续执行。</li>
</ul>
<h4 id="1-1-2-1-pre-commit"><a href="#1-1-2-1-pre-commit" class="headerlink" title="1.1.2.1. pre-commit"></a>1.1.2.1. pre-commit</h4><p>pre-commit阶段起始于一次快照的开始，即master节点将checkpoint的barrier注入source端，barrier随着数据向下流动直到sink端。barrier每到一个算子，都会出发算子做本地快照。</p>
<p><img src="_v_images/20200710154629243_751269158.png" alt="precommit"></p>
<p>当所有的算子都做完了本地快照并且回复到master节点时，pre-commit阶段才算结束。这个时候，checkpoint已经成功，并且包含了外部系统的状态。如果作业失败，可以进行恢复。</p>
<p><img src="_v_images/20200710154750060_1524377793.png" alt="precommit-success"></p>
<h4 id="1-1-2-2-commit"><a href="#1-1-2-2-commit" class="headerlink" title="1.1.2.2. commit"></a>1.1.2.2. commit</h4><p>通知所有的算子这次checkpoint成功了，即2PC的commit阶段。source节点和window节点没有外部状态，所以这时它们不需要做任何操作。<br>而对于sink节点，需要commit这次事务，将数据写到外部系统。</p>
<p><img src="_v_images/20200710154844838_737658241.png" alt="commit"></p>
<h4 id="1-1-2-3-rollback"><a href="#1-1-2-3-rollback" class="headerlink" title="1.1.2.3. rollback"></a>1.1.2.3. rollback</h4><p>一旦任何一个算子的快照保存失败，则触发回滚，同样的sink算子也需要取消写入外部的数据</p>
<h3 id="1-1-3-TwoPhaseCommitSinkFunction"><a href="#1-1-3-TwoPhaseCommitSinkFunction" class="headerlink" title="1.1.3. TwoPhaseCommitSinkFunction"></a>1.1.3. TwoPhaseCommitSinkFunction</h3><p>为了简化2PC的实现成本，flink抽象了TwoPhaseCommitSinkFunction</p>
<ul>
<li>beginTransaction。开始一次事务，在目的文件系统创建一个临时文件。接下来我们就可以将数据写到这个文件。</li>
<li>preCommit。在这个阶段，将文件flush掉，同时重起一个文件写入，作为下一次事务的开始。</li>
<li>commit。这个阶段，将文件写到真正的目的目录。值得注意的是，这会增加数据可视的延时。</li>
<li>abort。如果回滚，那么删除临时文件。</li>
</ul>
<p>如果pre-commit成功了，但是commit没有到达算子旧宕机了，flink会将算子恢复到pre-commit时的状态，然后继续commit。</p>
<p>我们需要做的还有就是保证commit的幂等性，这可以通过检查临时文件是否还在来实现。</p>
<h2 id="1-2-checkpoint"><a href="#1-2-checkpoint" class="headerlink" title="1.2. checkpoint"></a>1.2. checkpoint</h2><p><strong>保留策略</strong>:</p>
<ul>
<li>DELETE_ON_CANCELLATION 表示当程序取消时，删除 Checkpoint 存储文件。</li>
<li>RETAIN_ON_CANCELLATION 表示当程序取消时，保存之前的 Checkpoint 存储文件</li>
</ul>
<p>默认情况下，Flink不会触发一次 Checkpoint 当系统有其他 Checkpoint 在进行时，也就是说 Checkpoint 默认的并发为1。</p>
<p><strong>CheckpointCoordinator</strong> :</p>
<p>针对 Flink DataStream 任务，程序需要经历从 StreamGraph -&gt; JobGraph -&gt; ExecutionGraph -&gt; 物理执行图四个步骤，其中在 ExecutionGraph 构建时，会初始化 CheckpointCoordinator。ExecutionGraph通过ExecutionGraphBuilder.buildGraph方法构建，在构建完时，会调用 ExecutionGraph 的enableCheckpointing方法创建CheckpointCoordinator</p>
<p><strong>Flink Checkpoint 参数配置及建议</strong>:</p>
<ul>
<li>当 Checkpoint 时间比设置的 Checkpoint 间隔时间要长时，可以设置 Checkpoint 间最小时间间隔 。这样在上次 Checkpoint 完成时，不会立马进行下一次 Checkpoint，而是会等待一个最小时间间隔，然后在进行该次 Checkpoint。否则，每次 Checkpoint 完成时，就会立马开始下一次 Checkpoint，系统会有很多资源消耗 Checkpoint。</li>
<li>如果Flink状态很大，在进行恢复时，需要从远程存储读取状态恢复，此时可能导致任务恢复很慢，可以设置 Flink Task 本地状态恢复。任务状态本地恢复默认没有开启，可以设置参数state.backend.local-recovery值为true进行激活。</li>
<li>Checkpoint保存数，Checkpoint 保存数默认是1，也就是保存最新的 Checkpoint 文件，当进行状态恢复时，如果最新的Checkpoint文件不可用时(比如HDFS文件所有副本都损坏或者其他原因)，那么状态恢复就会失败，如果设置 Checkpoint 保存数2，即使最新的Checkpoint恢复失败，那么Flink 会回滚到之前那一次Checkpoint进行恢复。考虑到这种情况，用户可以增加 Checkpoint 保存数。</li>
<li>建议设置的 Checkpoint 的间隔时间最好大于 Checkpoint 的完成时间。</li>
</ul>
<p>下图是不设置 Checkpoint 最小时间间隔示例图，可以看到，系统一致在进行 Checkpoint，可能对运行的任务产生一定影响：<br><img src="_v_images/20200714095846469_121820659.png"></p>
<h2 id="1-3-savepoint"><a href="#1-3-savepoint" class="headerlink" title="1.3. savepoint"></a>1.3. savepoint</h2><blockquote>
<p>注意:<br>使用DataStream进行开发，建议为每个算子定义一个 uid，这样我们在修改作业时，即使导致程序拓扑图改变，由于相关算子 uid 没有变，那么这些算子还能够继续使用之前的状态，如果用户没有定义 uid ， Flink 会为每个算子自动生成 uid，如果用户修改了程序，可能导致之前的状态程序不能再进行复用。</p>
</blockquote>
<p>Flink 在触发Savepoint 或者 Checkpoint时，会根据这次触发的类型计算出在HDFS上面的目录:</p>
<p>如果类型是 Savepoint，那么 其 HDFS 上面的目录为：Savepoint 根目录+savepoint-jobid前六位+随机数字，具体如下格式：</p>
<p><img src="_v_images/20200714100459823_887900222.png"></p>
<p>Checkpoint 目录为 chk-checkpoint ID,具体格式如下：</p>
<p><img src="_v_images/20200714100516223_630729421.png"></p>
<ul>
<li>使用 flink cancel -s 命令取消作业同时触发 Savepoint 时，会有一个问题，可能存在触发 Savepoint 失败。比如实时程序处于异常状态(比如 Checkpoint失败)，而此时你停止作业，同时触发 Savepoint,这次 Savepoint 就会失败，这种情况会导致，在实时平台上面看到任务已经停止，但是实际实时作业在 Yarn 还在运行。针对这种情况，需要捕获触发 Savepoint 失败的异常，当抛出异常时，可以直接在 Yarn 上面 Kill 掉该任务。</li>
<li>使用 DataStream 程序开发时，最好为每个算子分配 uid,这样即使作业拓扑图变了，相关算子还是能够从之前的状态进行恢复，默认情况下，Flink 会为每个算子分配 uid,这种情况下，当你改变了程序的某些逻辑时，可能导致算子的 uid 发生改变，那么之前的状态数据，就不能进行复用，程序在启动的时候，就会报错。</li>
<li>由于 Savepoint 是程序的全局状态，对于某些状态很大的实时任务，当我们触发 Savepoint，可能会对运行着的实时任务产生影响，个人建议如果对于状态过大的实时任务，触发 Savepoint 的时间，不要太过频繁。根据状态的大小，适当的设置触发时间。</li>
<li>当我们从 Savepoint 进行恢复时，需要检查这次 Savepoint 目录文件是否可用。可能存在你上次触发 Savepoint 没有成功，导致 HDFS 目录上面 Savepoint 文件不可用或者缺少数据文件等，这种情况下，如果在指定损坏的 Savepoint 的状态目录进行状态恢复，任务会启动不起来。</li>
</ul>
<h2 id="1-4-snapshot保存到哪里-应该需要汇总到jobManager？"><a href="#1-4-snapshot保存到哪里-应该需要汇总到jobManager？" class="headerlink" title="1.4. snapshot保存到哪里? 应该需要汇总到jobManager？"></a>1.4. snapshot保存到哪里? 应该需要汇总到jobManager？</h2><h2 id="1-5-state-backend"><a href="#1-5-state-backend" class="headerlink" title="1.5. state backend"></a>1.5. state backend</h2><p><img src="_v_images/20200713183839429_2053265091.png"></p>
<h3 id="FsStateBackend"><a href="#FsStateBackend" class="headerlink" title="FsStateBackend"></a>FsStateBackend</h3><p>构造方法:<br><code>FsStateBackend(URI checkpointDataUri,boolean asynchronousSnapshots)</code></p>
<p>1 基于文件系统的状态管理器<br>2 如果使用，默认是异步<br>3 比较稳定，3个副本，比较安全。不会出现任务无法恢复等问题<br>4 状态大小受磁盘容量限制</p>
<p>存储方式:</p>
<ul>
<li>State: TaskManager内存</li>
<li>checkpoint: 外部文件系统(本地或HDFS)</li>
</ul>
<p>容量限制:</p>
<ul>
<li>单TaskManager上State总量不超过它的内存</li>
<li>总大小不超过配置的文件系统容量</li>
</ul>
<p>推荐使用场景:</p>
<ul>
<li>常规使用状态的作业，例如分钟级窗口聚合、join、窗口比较长、kv状态大；需要开启HA的作业</li>
<li>可以用于生产场景</li>
</ul>
<h3 id="RocksDBStateBackend"><a href="#RocksDBStateBackend" class="headerlink" title="RocksDBStateBackend"></a>RocksDBStateBackend</h3><p>状态数据先写入RocksDB，然后异步的将状态数据写入文件系统。正在进行计算的热数据存储在RocksDB，长时间才更新的数据写入磁盘中（文件系统）存储，体量比较小的元数据状态写入JobManager内存中（将工作state保存在RocksDB中，并且默认将checkpoint数据存在文件系统中）</p>
<p>目前唯一支持incremental的checkpoints的策略</p>
<p>构造方法:<br><code>RocksDBStateBackend(URI checkpointDataUri,boolean enableIncrementalCheckpointing)</code></p>
<p>存储方式:</p>
<ul>
<li>State: TaskManager上的KV数据库(实际使用内存+硬盘)</li>
<li>Checkpoint: 外部文件系统(本地或HDFS)</li>
</ul>
<p>容量限制:</p>
<ul>
<li>单TaskManager上的State总量不超过他的内存+磁盘</li>
<li>单key最大2G</li>
<li>总大小不超过配置的文件系统容量</li>
</ul>
<p>推荐使用的场景:</p>
<ul>
<li>超大状态的作业，例如天级别窗口聚合；需要开启HA的作业；对状态读写性能要求不高的作业</li>
<li>可以在生产环境使用</li>
</ul>
<h3 id="MemoryStateBackend"><a href="#MemoryStateBackend" class="headerlink" title="MemoryStateBackend"></a>MemoryStateBackend</h3><p>构造方法:<br><code>MemoryStateBackend(int maxStateSize, boolean asynchronousSnapshots)</code></p>
<p>主机内存中的数据可能会丢失，任务可能无法恢复</p>
<p>存储方式:</p>
<ul>
<li>State: TaskManager内存</li>
<li>Checkpoint: JobManager内存</li>
</ul>
<p>容量限制</p>
<ul>
<li>单个State maxStateSize默认5M</li>
<li>maxStateSize &lt;= akka.frameSize 默认10M</li>
<li>总大小不超过JobManager的内存</li>
</ul>
<p>推荐使用场景：</p>
<ul>
<li>本地测试；几乎无状态的作业，比如ETL；JobManager不容易挂，或挂掉影响不大的情况</li>
<li>不推荐在生产环境使用</li>
</ul>
<h2 id="1-6-checkpoint-与-savepoint"><a href="#1-6-checkpoint-与-savepoint" class="headerlink" title="1.6. checkpoint 与 savepoint"></a>1.6. checkpoint 与 savepoint</h2><p>Checkpoint指定触发生成时间间隔后，每当需要触发Checkpoint时，会向Flink程序运行时的多个分布式的Stream Source中插入一个Barrier标记，这些Barrier会根据Stream中的数据记录一起流向下游的各个Operator。<br>当一个Operator接收到一个Barrier时，它会暂停处理Steam中新接收到的数据记录。<br>因为一个Operator可能存在多个输入的Stream，而每个Stream中都会存在对应的Barrier，该Operator要等到所有的输入Stream中的Barrier都到达。(<strong>对齐</strong>)<br>当所有Stream中的Barrier都已经到达该Operator，这时所有的Barrier在时间上看来是同一个时刻点（表示已经对齐），在等待所有Barrier到达的过程中，<br>Operator的Buffer中可能已经缓存了一些比Barrier早到达Operator的数据记录（Outgoing Records），这时该Operator会将数据记录（Outgoing Records）发射（Emit）出去，作为下游Operator的输入，<br>最后将Barrier对应Snapshot发射（Emit）出去作为此次Checkpoint的结果数据。</p>
<p>Checkpoint 是增量做的，每次的时间较短，数据量较小，只要在程序里面启用后会自动触发，用户无须感知；Checkpoint 是作业 failover 的时候自动使用，不需要用户指定。</p>
<p>Savepoint 是全量做的，每次的时间较长，数据量较大，需要用户主动去触发。Savepoint 一般用于程序的版本更新（详见文档），Bug 修复，A/B Test 等场景，需要用户指定。</p>
<p><strong>保存的内容</strong></p>
<ul>
<li>首先，Savepoint 包含了一个目录，其中包含（通常很大的）二进制文件，这些文件表示了整个流应用在 Checkpoint/Savepoint 时的状态。</li>
<li>以及一个（相对较小的）元数据文件，包含了指向 Savapoint 各个文件的指针，并存储在所选的分布式文件系统或数据存储中。</li>
</ul>
<p><strong>目标</strong></p>
<p>Savepoint 和 Checkpoint 的不同之处很像传统数据库中备份与恢复日志之间的区别。Checkpoint 的主要目标是充当 Flink 中的恢复机制，确保能从潜在的故障中恢复。相反，Savepoint 的主要目标是充当手动备份、恢复暂停作业的方法。</p>
<p><strong>实现</strong></p>
<p>Checkpoint 被设计成轻量和快速的机制。它们可能（但不一定必须）利用底层状态后端的不同功能尽可能快速地恢复数据。例如，基于 RocksDB 状态后端的增量检查点，能够加速 RocksDB 的 checkpoint 过程，这使得 checkpoint 机制变得更加轻量。相反，Savepoint 旨在更多地关注数据的可移植性，并支持对作业做任何更改而状态能保持兼容，这使得生成和恢复的成本更高</p>
<p><strong>状态文件保留策略</strong></p>
<p>Checkpoint默认程序删除，可以设置CheckpointConfig中的参数进行保留 。Savepoint会一直保存，除非用户删除 。</p>
<p><strong>应用</strong></p>
<ul>
<li>部署流应用的一个新版本，包括新功能、BUG 修复、或者一个更好的机器学习模型</li>
<li>引入 A/B 测试，使用相同的源数据测试程序的不同版本，从同一时间点开始测试而不牺牲先前的状态</li>
<li>在需要更多资源时扩容应用程序</li>
<li>迁移流应用程序到 Flink 的新版本上，或者迁移到另一个集群</li>
</ul>
<h1 id="Flink数据一致性"><a href="#Flink数据一致性" class="headerlink" title="Flink数据一致性"></a>Flink数据一致性</h1><h2 id="一、综述"><a href="#一、综述" class="headerlink" title="一、综述"></a>一、综述</h2><p><strong>flink 通过内部依赖checkpoint 并且可以通过设置其参数exactly-once 实现其内部的一致性</strong>。但要实现其端到端的一致性，还必须保证<br>1、source 外部数据源可重设数据的读取位置<br>2、sink端 需要保证数据从故障恢复时，数据不会重复写入外部系统（或者可以逻辑实现写入多次，但只有一次生效的数据sink端）</p>
<h2 id="二、sink-端到端实现方式"><a href="#二、sink-端到端实现方式" class="headerlink" title="二、sink 端到端实现方式"></a>二、sink 端到端实现方式</h2><p><strong>幂等操作：</strong><br>一个操作，可以重复执行多次，但只导致一次结果更改，豁免重复操作执行就不起作用了，他的瑕疵 （在系统恢复的过程中，如果这段时间内多个更新或者插入导致状态不一致，但当数据追上就可以了）<br>（逻辑与、逻辑或等）具体理解参照自己以前写的文章。<br><strong>事务写入：</strong><br>事务应该具有四个属性：原子性、一致性、隔离性、持久性等。其具体的实现方式有两种<br><strong>（1）、预写日志</strong><br>简单易于实现，由于数据提前在状态后端中做了缓存，所以无论什么sink系统，都能用这种方式一批搞定，DataStream API提供了一个模板类：GenericWriteAheadSink，来实现这种事务性sink；<br>缺点：<br>1）、sink系统没说他支持事务。有可能出现一部分写入集群了。一部分没有写进去（如果实表，再写一次就写重复了）<br>2）、checkpoint做完了sink才去真正的写入（但其实得等sink都写完checkpoint才能生效，所以WAL这个机制jobmanager确定它写完还不算真正写完，还得有一个外部系统已经确认 完成的checkpoint）<br>（<strong>2）、两阶段提交。 flink 真正实现exactle-once</strong><br>对于每个checkpoint,sink 任务会启动一个事务，并将接下来所有接收的数据添加到事务中，然后将这些数据写入外部sink系统，但不提交他们（这里是预提交）。当checkpoint完成时的通知，它才正式提交事务，实现结果的真正写入；这种方式真正实现了exactly-once,它需要一个提供事务支持的外部sink系统，Flink提供了其具体实现（TwoPhaseCommitSinkFunction接口）</p>
<h2 id="三、-2pc-对外部-sink的要求"><a href="#三、-2pc-对外部-sink的要求" class="headerlink" title="三、 2pc 对外部 sink的要求"></a>三、 2pc 对外部 sink的要求</h2><p>1、外部sink系统必须事务支持，或者sink任务必须能够模拟外部系统上的事务；<br>2、在checkpoint的间隔期间里，必须能够开启一个事务，并接受数据写入。<br>3、在收到checkpoint完成通知之前，事务必须是“等待提交”的状态，在故障恢复的情况线，这可能需要一些时间。如果个时候sink系统关闭事务（例如超时了），那么未提交的数据就会丢失；<br>4、四年任务必选能够在进程失败后恢复事务<br>5、提交事务必须是幂等操作；</p>
<p>四、综上不同Source和sink的一致性保证：<br><img src="_v_images/20201208154240979_1471214802.png" alt="在这里插入图片描述"></p>
<h2 id="五、应用（flinK-kafka-端到端一致性保证）"><a href="#五、应用（flinK-kafka-端到端一致性保证）" class="headerlink" title="五、应用（flinK+kafka 端到端一致性保证）"></a>五、应用（flinK+kafka 端到端一致性保证）</h2><p>flink 和kafka 端到端一致性(kafka(source+flink+kafka(sink)))<br>1、内部 – 利用checkpoint机制，把状态存盘，发生故障的时候可以恢复，保证内部的状态一致性<br>2、source – kafka consumer作为source，可以将偏移量保存下来，如果后续任务出现了故障，恢复的时候可以由连接器重置偏移量，重新消费数据，保证一致性；</p>
<figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-tag">kafka</span> 0.8 和<span class="selector-tag">kafka</span> 0.11 之后 通过以下配置将偏移量保存，恢复时候重新消费</span><br><span class="line"> <span class="selector-tag">kafka</span><span class="selector-class">.setStartFromLatest</span>();</span><br><span class="line"> <span class="selector-tag">kafka</span><span class="selector-class">.setCommitOffsetsOnCheckpoints</span>(<span class="selector-tag">false</span>);</span><br><span class="line"> <span class="selector-tag">kafka</span> 0.9 和<span class="selector-tag">kafka0</span>.10 未验证是否支持这两个参数(<span class="selector-tag">todo</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>3、sink FlinkkafkaProducer作为Sink，采用两阶段提交的sink，由下图可以看出flink 0.11 已经默认继承了TwoPhaseCommitSinkFunction<br><img src="_v_images/20201208154240548_914126344.png" alt="在这里插入图片描述"><br>但我们需要在参数种传入指定语义，它默认时还是at-least-once<br>此外我们还需要进行一些producer的容错配置：<br>（1）除了启用Flink的检查点之外，还可以通过将适当的semantic参数传递给FlinkKafkaProducer011（FlinkKafkaProducer对于Kafka&gt; = 1.0.0版本）<br>（2）来选择三种不同的操作模式<br>1）、Semantic.NONE 代表at-mostly-once语义<br>2）、Semantic.AT_LEAST_ONCE（Flink默认设置<br>3）、Semantic.EXACTLY_ONCE 使用Kafka事务提供一次精确的语义，每当您使用事务写入Kafka时<br>（3）、请不要忘记消费kafka记录任何应用程序设置所需的设置isolation.leva（read_committed 或者read_uncommitted-后者是默认）<br>read_committed，只是读取已经提交的数据。</p>
<p>应用；<br>Semantic.EXACTLY_ONCE依赖与下游系统能支持事务操作.以0.11kafka为例.<br>transaction.max.timeout.ms 最大超市时长，默认15分钟，如果需要用exactly语义，需要增加这个值。（因为它小于transaction.timeout.ms ）<br>isolation.level 如果需要用到exactly语义，需要在下级consumerConfig中设置read-commited [read-uncommited(默认值)]<br>transaction.timeout.ms 默认为1hour</p>
<p><strong>其参数对应关系为 和一些报错问题<br>checkpoint间隔&lt;transaction.timeout.ms&lt;transaction.max.timeout.ms</strong></p>
<p><strong>参考：<a target="_blank" rel="noopener" href="https://www.cnblogs.com/createweb/p/11971846.html">https://www.cnblogs.com/createweb/p/11971846.html</a></strong></p>
<p>注意：<br>1、Semantic.EXACTLY_ONCE 模式每个FlinkKafkaProducer011实例使用一个固定大小的KafkaProducers池。每个检查点使用这些生产者中的每一个。如果并发检查点的数量超过池大小，FlinkKafkaProducer011 将引发异常，并使整个应用程序失败。请相应地配置最大池大小和最大并发检查点数。</p>
<p>2、Semantic.EXACTLY_ONCE采取所有可能的措施，不要留下任何挥之不去的数据，否则这将有碍于消费者更多地阅读Kafka主题。但是，如果flink应用程序在第一个检查点之前失败，则在重新启动此类应用程序后，系统种将没有有关先前池大小信息，因此，在第一个检查点完成前按比例缩小Flink应用程序的FlinkKafkaProducer011.SAFE_SCALE_DOWN_FACTOR</p>
<figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">//1。设置最大允许的并行<span class="selector-tag">checkpoint</span>数，防止超过<span class="selector-tag">producer</span>池的个数发生异常</span><br><span class="line"><span class="selector-tag">env</span><span class="selector-class">.getCheckpointConfig</span><span class="selector-class">.setMaxConcurrentCheckpoints</span>(5) </span><br><span class="line">//2。设置<span class="selector-tag">producer</span>的<span class="selector-tag">ack</span>传输配置</span><br><span class="line">// 设置超市时长，默认15分钟，建议1个小时以上</span><br><span class="line"><span class="selector-tag">producerConfig</span><span class="selector-class">.put</span>(<span class="selector-tag">ProducerConfig</span><span class="selector-class">.ACKS_CONFIG</span>, 1) </span><br><span class="line"><span class="selector-tag">producerConfig</span><span class="selector-class">.put</span>(<span class="selector-tag">ProducerConfig</span><span class="selector-class">.TRANSACTION_TIMEOUT_CONFIG</span>, 15000) </span><br><span class="line"></span><br><span class="line">//3。在下一个<span class="selector-tag">kafka</span> <span class="selector-tag">consumer</span>的配置文件，或者代码中设置<span class="selector-tag">ISOLATION_LEVEL_CONFIG-read-commited</span></span><br><span class="line">//<span class="selector-tag">Note</span>:必须在下一个<span class="selector-tag">consumer</span>中指定，当前指定是没用用的</span><br><span class="line"><span class="selector-tag">kafkaonfigs</span><span class="selector-class">.setProperty</span>(<span class="selector-tag">ConsumerConfig</span><span class="selector-class">.ISOLATION_LEVEL_CONFIG</span>,&quot;<span class="selector-tag">read_commited</span>&quot;)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>完整应用代码：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.shufang.flink.connectors</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.Properties</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.serialization.SimpleStringSchema</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.TimeCharacteristic</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.functions.timestamps.BoundedOutOfOrdernessTimestampExtractor</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala._</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.windowing.time.Time</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer.Semantic</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.connectors.kafka._</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.util.serialization.KeyedSerializationSchemaWrapper</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.ConsumerConfig</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.ProducerConfig</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.common.serialization.StringDeserializer</span><br><span class="line"></span><br><span class="line">object KafkaSource01 &#123;</span><br><span class="line">  <span class="function">def <span class="title">main</span><span class="params">(args: Array[String])</span>: Unit </span>= &#123;</span><br><span class="line">    val env: StreamExecutionEnvironment = StreamExecutionEnvironment.getExecutionEnvironment</span><br><span class="line"></span><br><span class="line">    env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//这是checkpoint的超时时间</span></span><br><span class="line">    <span class="comment">//env.getCheckpointConfig.setCheckpointTimeout()</span></span><br><span class="line">    <span class="comment">//设置最大并行的chekpoint</span></span><br><span class="line">    env.getCheckpointConfig.setMaxConcurrentCheckpoints(<span class="number">5</span>)</span><br><span class="line">    env.getCheckpointConfig.setCheckpointInterval(<span class="number">1000</span>) <span class="comment">//增加checkpoint的中间时长，保证可靠性</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 为了保证数据的一致性，我们开启Flink的checkpoint一致性检查点机制，保证容错</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    env.enableCheckpointing(<span class="number">60000</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 从kafka获取数据，一定要记得添加checkpoint，能保证offset的状态可以重置，从数据源保证数据的一致性</span></span><br><span class="line"><span class="comment">     * 保证kafka代理的offset与checkpoint备份中保持状态一致</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line"></span><br><span class="line">    val kafkaonfigs = <span class="keyword">new</span> Properties()</span><br><span class="line"></span><br><span class="line">    <span class="comment">//指定kafka的启动集群</span></span><br><span class="line">    kafkaonfigs.setProperty(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, <span class="string">&quot;localhost:9092&quot;</span>)</span><br><span class="line">    <span class="comment">//指定消费者组</span></span><br><span class="line">    kafkaonfigs.setProperty(ConsumerConfig.GROUP_ID_CONFIG, <span class="string">&quot;flinkConsumer&quot;</span>)</span><br><span class="line">    <span class="comment">//指定key的反序列化类型</span></span><br><span class="line">    kafkaonfigs.setProperty(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, classOf[StringDeserializer].getName)</span><br><span class="line">    <span class="comment">//指定value的反序列化类型</span></span><br><span class="line">    kafkaonfigs.setProperty(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, classOf[StringDeserializer].getName)</span><br><span class="line">    <span class="comment">//指定自动消费offset的起点配置</span></span><br><span class="line">    <span class="comment">//    kafkaonfigs.setProperty(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, &quot;latest&quot;)</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 自定义kafkaConsumer，同时可以指定从哪里开始消费</span></span><br><span class="line"><span class="comment">     * 开启了Flink的检查点之后，我们还要开启kafka-offset的检查点，通过kafkaConsumer.setCommitOffsetsOnCheckpoints(true)开启，</span></span><br><span class="line"><span class="comment">     * 一旦这个检查点开启，那么之前配置的 auto-commit-enable = true的配置就会自动失效</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    val kafkaConsumer = <span class="keyword">new</span> FlinkKafkaConsumer[String](</span><br><span class="line">      <span class="string">&quot;console-topic&quot;</span>,</span><br><span class="line">      <span class="keyword">new</span> SimpleStringSchema(), <span class="comment">// 这个schema是将kafka的数据应设成Flink中的String类型</span></span><br><span class="line">      kafkaonfigs</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 开启kafka-offset检查点状态保存机制</span></span><br><span class="line">    kafkaConsumer.setCommitOffsetsOnCheckpoints(<span class="keyword">true</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//    kafkaConsumer.setStartFromEarliest()//</span></span><br><span class="line">    <span class="comment">//    kafkaConsumer.setStartFromTimestamp(1010003794)</span></span><br><span class="line">    <span class="comment">//    kafkaConsumer.setStartFromLatest()</span></span><br><span class="line">    <span class="comment">//    kafkaConsumer.setStartFromSpecificOffsets(Map[KafkaTopicPartition,Long]()</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 添加source数据源</span></span><br><span class="line">    val kafkaStream: DataStream[String] = env.addSource(kafkaConsumer)</span><br><span class="line"></span><br><span class="line">    kafkaStream.print()</span><br><span class="line"></span><br><span class="line">    val sinkStream: DataStream[String] = kafkaStream.assignTimestampsAndWatermarks(<span class="keyword">new</span> BoundedOutOfOrdernessTimestampExtractor[String](Time.seconds(<span class="number">5</span>)) &#123;</span><br><span class="line">      <span class="function">override def <span class="title">extractTimestamp</span><span class="params">(element: String)</span>: Long </span>= &#123;</span><br><span class="line">        element.split(<span class="string">&quot;,&quot;</span>)(<span class="number">1</span>).toLong</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 通过FlinkkafkaProduccer API将stream的数据写入到kafka的&#x27;sink-topic&#x27;中</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="comment">//    val brokerList = &quot;localhost:9092&quot;</span></span><br><span class="line">    val topic = <span class="string">&quot;sink-topic&quot;</span></span><br><span class="line">    val producerConfig = <span class="keyword">new</span> Properties()</span><br><span class="line">    producerConfig.put(ProducerConfig.ACKS_CONFIG, <span class="keyword">new</span> Integer(<span class="number">1</span>)) <span class="comment">// 设置producer的ack传输配置</span></span><br><span class="line">    producerConfig.put(ProducerConfig.TRANSACTION_TIMEOUT_CONFIG, Time.hours(<span class="number">2</span>)) <span class="comment">//设置超市时长，默认1小时，建议1个小时以上</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 自定义producer，可以通过不同的构造器创建</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    val producer: FlinkKafkaProducer[String] = <span class="keyword">new</span> FlinkKafkaProducer[String](</span><br><span class="line">      topic,</span><br><span class="line">      <span class="keyword">new</span> KeyedSerializationSchemaWrapper[String](SimpleStringSchema),</span><br><span class="line">      producerConfig,</span><br><span class="line">      Semantic.EXACTLY_ONCE</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment">//    FlinkKafkaProducer.SAFE_SCALE_DOWN_FACTOR</span></span><br><span class="line">    <span class="comment">/** *****************************************************************************************************************</span></span><br><span class="line"><span class="comment">     * * 出了要开启flink的checkpoint功能，同时还要设置相关配置功能。</span></span><br><span class="line"><span class="comment">     * * 因在0.9或者0.10，默认的FlinkKafkaProducer只能保证at-least-once语义，假如需要满足at-least-once语义，我们还需要设置</span></span><br><span class="line"><span class="comment">     * * setLogFailuresOnly(boolean)    默认false</span></span><br><span class="line"><span class="comment">     * * setFlushOnCheckpoint(boolean)  默认true</span></span><br><span class="line"><span class="comment">     * * come from 官网 below：</span></span><br><span class="line"><span class="comment">     * * Besides enabling Flink’s checkpointing，you should also configure the setter methods setLogFailuresOnly(boolean)</span></span><br><span class="line"><span class="comment">     * * and setFlushOnCheckpoint(boolean) appropriately.</span></span><br><span class="line"><span class="comment">     * ******************************************************************************************************************/</span></span><br><span class="line"></span><br><span class="line">    producer.setLogFailuresOnly(<span class="keyword">false</span>) <span class="comment">//默认是false</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 除了启用Flink的检查点之外，还可以通过将适当的semantic参数传递给FlinkKafkaProducer011（FlinkKafkaProducer对于Kafka&gt; = 1.0.0版本）</span></span><br><span class="line"><span class="comment">     * 来选择三种不同的操作模式：</span></span><br><span class="line"><span class="comment">     * Semantic.NONE  代表at-mostly-once语义</span></span><br><span class="line"><span class="comment">     * Semantic.AT_LEAST_ONCE（Flink默认设置）</span></span><br><span class="line"><span class="comment">     * Semantic.EXACTLY_ONCE：使用Kafka事务提供一次精确的语义，每当您使用事务写入Kafka时，</span></span><br><span class="line"><span class="comment">     * 请不要忘记为使用Kafka记录的任何应用程序设置所需的设置isolation.level（read_committed 或read_uncommitted-后者是默认值)</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line"></span><br><span class="line">    sinkStream.addSink(producer)</span><br><span class="line"></span><br><span class="line">    env.execute(<span class="string">&quot;kafka source &amp; sink&quot;</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="[参考文献]"></a>[参考文献]</h2><ol>
<li><a target="_blank" rel="noopener" href="http://shiyanjun.cn/archives/1855.html">Flink Checkpoint、Savepoint配置与实践</a></li>
<li><a target="_blank" rel="noopener" href="http://wuchong.me/blog/2018/11/04/how-apache-flink-manages-kafka-consumer-offsets/">Flink 小贴士 (2)：Flink 如何管理 Kafka 消费位点</a></li>
<li><a target="_blank" rel="noopener" href="https://www.jianshu.com/p/4bcbcda0e2f4">Flink实时计算-深入理解Checkpoint和Savepoint</a></li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1506.08603">Lightweight Asynchronous Snapshots for Distributed Dataflows: 分布式数据流轻量级异步快照</a></li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://example.com/bigdata/Flink/Oceanus/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="aaronzhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Guadazi">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/bigdata/Flink/Oceanus/" class="post-title-link" itemprop="url">Oceanus: table meta API</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2019-08-10 14:58:00" itemprop="dateCreated datePublished" datetime="2019-08-10T14:58:00+08:00">2019-08-10</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-04-19 15:02:11" itemprop="dateModified" datetime="2021-04-19T15:02:11+08:00">2021-04-19</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/bigdata/" itemprop="url" rel="index"><span itemprop="name">bigdata</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="Oceanus"><a href="#Oceanus" class="headerlink" title="Oceanus"></a>Oceanus</h1><h2 id="拉取库表信息"><a href="#拉取库表信息" class="headerlink" title="拉取库表信息"></a>拉取库表信息</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">curl -o tank_pos_meta.json &#x27;http://&lt;host&gt;:&lt;port&gt;/ec/v1/listTable?pageNum=1&amp;pageSize=999&amp;type=hippo&amp;dbName=bank-pos-info&amp;name=pos_yyyymmdd&#x27;</span><br></pre></td></tr></table></figure>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    <span class="attr">&quot;result_code&quot;</span>: <span class="string">&quot;0&quot;</span>,</span><br><span class="line">    <span class="attr">&quot;result_msg&quot;</span>: <span class="string">&quot;操作成功&quot;</span>,</span><br><span class="line">    <span class="attr">&quot;result_content&quot;</span>: &#123;</span><br><span class="line">        <span class="attr">&quot;tables&quot;</span>: [</span><br><span class="line">            &#123;</span><br><span class="line">                <span class="attr">&quot;id&quot;</span>: <span class="number">2255</span>,</span><br><span class="line">                <span class="attr">&quot;dbName&quot;</span>: <span class="string">&quot;info&quot;</span>,</span><br><span class="line">                <span class="attr">&quot;name&quot;</span>: <span class="string">&quot;pos_yyyymmdd&quot;</span>,</span><br><span class="line">                <span class="attr">&quot;type&quot;</span>: <span class="string">&quot;hippo&quot;</span>,</span><br><span class="line">                <span class="attr">&quot;principals&quot;</span>: <span class="string">&quot;user_name&quot;</span>,</span><br><span class="line">                <span class="attr">&quot;fields&quot;</span>: <span class="string">&quot;db_name,String,db_name: tb_name,String,tb_name: op_name,String,op_name: exp_time_stample,String,exp_time_stample: exp_time_stample_order,String,exp_time_stample_order: Fbill_no,String,Fbill_no: Fbank_type,Long,Fbank_type: Fbiz_type,Long,Fbiz_type: Fpos_status,Long,Fpos_status: Freverse_status,Long,Freverse_status: Freverse_times,Long,Freverse_times: Ftransaction_id,String,Ftransaction_id: Freal_bill_no,String,Freal_bill_no: Ftrace_no,String,Ftrace_no: Ftx_date,String,Ftx_date: Famount,Long,Famount: Fcur_type,Long,Fcur_type: Fuin,String,Fuin: Fuid,Long,Fuid: Fuser_name,String,Fuser_name: Fuser_id_type,Long,Fuser_id_type: Fuser_id,String,Fuser_id: Fuser_phone,String,Fuser_phone: Fcard_no,String,&quot;</span>,</span><br><span class="line">                <span class="attr">&quot;content&quot;</span>: <span class="string">&quot;&quot;</span>,</span><br><span class="line">                <span class="attr">&quot;description&quot;</span>: <span class="string">&quot;银行pos流水&quot;</span>,</span><br><span class="line">                <span class="attr">&quot;attributes&quot;</span>: &#123;</span><br><span class="line">                    <span class="attr">&quot;table.topic&quot;</span>: <span class="string">&quot;sample_pos&quot;</span>,</span><br><span class="line">                    <span class="attr">&quot;table.bid&quot;</span>: <span class="string">&quot;sample_pos&quot;</span>,</span><br><span class="line">                    <span class="attr">&quot;data.encode&quot;</span>: <span class="string">&quot;UTF8&quot;</span>,</span><br><span class="line">                    <span class="attr">&quot;table.hippo.addrlist&quot;</span>: <span class="string">&quot;&lt;hippo_master&gt;&quot;</span>,</span><br><span class="line">                    <span class="attr">&quot;table.package&quot;</span>: <span class="string">&quot;true&quot;</span>,</span><br><span class="line">                    <span class="attr">&quot;table.interfaceId&quot;</span>: <span class="string">&quot;t_sample_pos_yyyymmdd&quot;</span>,</span><br><span class="line">                    <span class="attr">&quot;source.data.type&quot;</span>: <span class="string">&quot;data.type.default&quot;</span>,</span><br><span class="line">                    <span class="attr">&quot;table.needwatermark&quot;</span>: <span class="string">&quot;false&quot;</span>,</span><br><span class="line">                    <span class="attr">&quot;table.kv&quot;</span>: <span class="string">&quot;false&quot;</span>,</span><br><span class="line">                    <span class="attr">&quot;table.field.splitter&quot;</span>: <span class="string">&quot;0x1&quot;</span>,</span><br><span class="line">                    <span class="attr">&quot;is.temporal.table&quot;</span>: <span class="string">&quot;false&quot;</span>,</span><br><span class="line">                    <span class="attr">&quot;table.field.splitter.other&quot;</span>: <span class="string">&quot;1&quot;</span>,</span><br><span class="line">                    <span class="attr">&quot;table.usage&quot;</span>: <span class="string">&quot;table&quot;</span>,</span><br><span class="line">                    <span class="attr">&quot;table.used&quot;</span>: <span class="string">&quot;false&quot;</span>,</span><br><span class="line">                    <span class="attr">&quot;table.running.used&quot;</span>: <span class="string">&quot;false&quot;</span></span><br><span class="line">                &#125;,</span><br><span class="line">                <span class="attr">&quot;modifier&quot;</span>: <span class="string">&quot;&lt;table_owner&gt;&quot;</span>,</span><br><span class="line">                <span class="attr">&quot;modify_time&quot;</span>: <span class="number">1563420065831</span>,</span><br><span class="line">                <span class="attr">&quot;create_time&quot;</span>: <span class="number">1563420065831</span>,</span><br><span class="line">                <span class="attr">&quot;tablesLogs&quot;</span>: <span class="literal">null</span></span><br><span class="line">            &#125;</span><br><span class="line">        ],</span><br><span class="line">        <span class="attr">&quot;total&quot;</span>: <span class="number">1</span>,</span><br><span class="line">        <span class="attr">&quot;pageNum&quot;</span>: <span class="number">1</span>,</span><br><span class="line">        <span class="attr">&quot;pageSize&quot;</span>: <span class="number">999</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="flink的优化"><a href="#flink的优化" class="headerlink" title="flink的优化"></a>flink的优化</h2><h3 id="JobManager-failover"><a href="#JobManager-failover" class="headerlink" title="JobManager failover"></a>JobManager failover</h3><p><img src="_v_images/20200601183746039_28243.png"></p>
<p>它的 standby 节点是冷备的，JobManager 的切换会导致它管理的所有 Job 都会被重启恢复，这一行为在我们现网环境中是不可接受的。所以，我们首先定制的第一个大特性就是<br>JobManager 的 failover 优化，让 standby 节点变成热备，这使得 JobManager 的切换对 TaskManager 上已经正在运行的 Job 不产生影响。我们已经对 Standalone 以及 Flink on YARN 这两种部署模式支持了这个特性，Flink on YARN 的支持还处于内部验证阶段。我们以对 Standalone 模式的优化为例来进行分析，它主要包含这么几个步骤：</p>
<ul>
<li>取消 JobManager 跟 TaskManager 因为心跳超时或 Leadership 变动就 cancel task 的行为；</li>
<li>对 ExecutionGraph 核心数据的快照；</li>
<li>通过 ExecutionGraphBuilder 重构空的 ExecutionGraph 加上快照重置来恢复出一个跟原先等价的 ExecutionGraph 对象；</li>
<li>TaskManager 跟新的 JobManager leader 建立连接后以心跳上报自己的状态和必要的信息；<br>新的 JobManager 确认在 reconcile 阶段 Job 的所有 task 是否正常运行。</li>
</ul>
<h3 id="checkpoint失败改进"><a href="#checkpoint失败改进" class="headerlink" title="checkpoint失败改进"></a>checkpoint失败改进</h3><p><img src="_v_images/20200601185553966_3881.png"><br>社区版当前的处理机制。JobMaster 中，每个 Job 会对应一个 Checkpoint Coordinator，它用来管理并协调 Job 检查点的执行。当到达一个检查点的触发周期，Coordinator 会对所有的 Source Task 下发 TriggerCheckpoint 消息，source task 会在自身完成快照后向下游广播 CheckpointBarrier，作为下游 task 触发的通知。其中，如果一个 task 在执行检查点时失败了，这取决于用户是否容忍这个失败（通过一个配置项），如果选择不容忍那么这个失败将变成一个异常导致 task 的失败，与此同时 task 的失败将会通知到 JobMaster，JobMaster 将会通知这个 Job 的其他 task 取消它们的执行。现有的机制存在一些问题：</p>
<ul>
<li>Coordinator 并不能控制 Job 是否容忍检查点失败，因为控制权在 task 端；</li>
<li>Coordinator 当前的失败处理代码逻辑混乱，区分出了触发阶段，却忽略了执行阶段；</li>
<li>无法实现容忍多少个连续的检查点失败则让 Job 失败的逻辑。</li>
</ul>
<p><img src="_v_images/20200601190109255_17290.png"><br>首先，我们对源码中 checkpoint package 下的相关类进行了重构，使得它不再区分触发阶段，引进了更多的检查点失败原因的枚举并重构了相关的代码。然后我们引入了 CheckpointFailureManager 组件，用来统一失败管理，同时为了适配更灵活的容忍失败的能力，我们引入了检查点失败计数器机制。现在，当我们遇到检查点失败后，这个失败信息会直接上报到 Coordinator，而是否要让 Job 失败具体的决策则由 CheckpointFailureManager 作出，这就<strong>使得 Coordinator 具有了完整的检查点控制权，而决策权转让给 CheckpointFailureManager，则充分实现了逻辑解耦</strong>。</p>
<h3 id="AsyncIO超时"><a href="#AsyncIO超时" class="headerlink" title="AsyncIO超时"></a>AsyncIO超时</h3><h3 id="sql-editor"><a href="#sql-editor" class="headerlink" title="sql editor"></a>sql editor</h3><p>ace editor</p>
<h3 id="yarn"><a href="#yarn" class="headerlink" title="yarn"></a>yarn</h3><p>[averyzhang@tdw-<ip> /data/yarnenv/local/usercache/u_teg_tdbank/appcache/application_1542247480019_1646]$ ll<br>总用量 32<br>drwxr-s—  2 u_teg_tdbank users 4096 6月   5 16:07 blobStore-b0fe739d-d5e0-4a87-9c34-2a2492fc84a6<br>drwxr-s—  2 u_teg_tdbank users 4096 6月   5 16:07 blobStore-f8cdfc9d-d7d2-4fb6-9836-da65b414b45a<br>drwxr-s—  3 u_teg_tdbank users 4096 6月   5 16:07 container_e03_1542247480019_1646_01_000002<br>drwx–x— 11 u_teg_tdbank users 4096 6月   5 16:07 filecache<br>drwxr-s—  2 u_teg_tdbank users 4096 6月   5 16:07 flink-dist-cache-d55b9eaa-87b9-404e-a015-c4c90c767ac9<br>drwxr-s—  8 u_teg_tdbank users 4096 6月   5 16:07 flink-io-0bd3c6be-79ef-41d6-95ec-f42a949a10f1<br>drwxr-s—  4 u_teg_tdbank users 4096 6月   5 16:07 localState<br>drwxr-s—  2 u_teg_tdbank users 4096 6月   5 16:07 rocksdb-lib-89c57e29c492279dc1e188992c87925e</p>
<p>[averyzhang@tdw-<ip> /data/yarnenv/local/usercache/u_teg_tdbank/appcache/application_1542247480019_1646]$ tree<br>.<br>|– blobStore-b0fe739d-d5e0-4a87-9c34-2a2492fc84a6<br>|– blobStore-f8cdfc9d-d7d2-4fb6-9836-da65b414b45a<br>|– container_e03_1542247480019_1646_01_000002<br>|   |– container_tokens<br>|   |– flink-conf.yaml -&gt; /data/yarnenv/local/usercache/u_teg_tdbank/appcache/application_1542247480019_1646/filecache/10/91db3812-bb43-492d-8c32-359b2c21a142-taskmanager-conf.yaml<br>|   |– flink-runtime-oceanus-dependency-lib-1.1.0-SNAPSHOT.jar -&gt; /data/yarnenv/local/usercache/u_teg_tdbank/appcache/application_1542247480019_1646/filecache/15/flink-runtime-oceanus-dependency-lib-1.1.0-SNAPSHOT.jar<br>|   |– kafka-connector-0.11-dependency-1.1.0-SNAPSHOT.jar -&gt; /data/yarnenv/local/usercache/u_teg_tdbank/appcache/application_1542247480019_1646/filecache/11/kafka-connector-0.11-dependency-1.1.0-SNAPSHOT.jar<br>|   |– launch_container.sh<br>|   |– lct-click-stat-1.0-SNAPSHOT-jar-with-dependencies-20200511152610.jar -&gt; /data/yarnenv/local/usercache/u_teg_tdbank/appcache/application_1542247480019_1646/filecache/12/lct-click-stat-1.0-SNAPSHOT-jar-with-dependencies-20200511152610.jar<br>|   |– log4j.properties -&gt; /data/yarnenv/local/usercache/u_teg_tdbank/appcache/application_1542247480019_1646/filecache/17/log4j.properties<br>|   |– log4j.zip -&gt; /data/yarnenv/local/usercache/u_teg_tdbank/appcache/application_1542247480019_1646/filecache/14/log4j.zip<br>|   |– oceanus-common-1.1.0-SNAPSHOT.jar -&gt; /data/yarnenv/local/usercache/u_teg_tdbank/appcache/application_1542247480019_1646/filecache/13/oceanus-common-1.1.0-SNAPSHOT.jar<br>|   |– oceanus-core-1.1.0-SNAPSHOT.jar -&gt; /data/yarnenv/local/usercache/u_teg_tdbank/appcache/application_1542247480019_1646/filecache/16/oceanus-core-1.1.0-SNAPSHOT.jar<br>|   |– oceanus-ml-1.1.0-SNAPSHOT.jar -&gt; /data/yarnenv/local/usercache/u_teg_tdbank/appcache/application_1542247480019_1646/filecache/18/oceanus-ml-1.1.0-SNAPSHOT.jar<br>|   <code>-- tmp |-- filecache [error opening dir] |-- flink-dist-cache-d55b9eaa-87b9-404e-a015-c4c90c767ac9 |-- flink-io-0bd3c6be-79ef-41d6-95ec-f42a949a10f1 |   |-- job_05606ccf34b1f48ff075d3092d9b81dd_op_WindowOperator_337adade1e207453ed3502e01d75fd03__1_8__uuid_6d0d5c24-bb66-4793-afa4-7a7e3c3ffd6a |   |   </code>– db<br>|   |       |– 000013.log<br>|   |       |– 024751.sst<br>|   |       |– 024753.sst<br>|   |       |– CURRENT<br>|   |       |– IDENTITY<br>|   |       |– LOCK<br>|   |       |– LOG<br>|   |       |– MANIFEST-000006<br>|   |       |– OPTIONS-000010<br>|   |       <code>-- OPTIONS-000012 |   |-- job_05606ccf34b1f48ff075d3092d9b81dd_op_WindowOperator_337adade1e207453ed3502e01d75fd03__2_8__uuid_18ea3048-b73d-482d-b6c9-615452280981 |   |   </code>– db<br>|   |       |– 000013.log<br>|   |       |– 024739.sst<br>|   |       |– 024741.sst<br>|   |       |– 024742.sst<br>|   |       |– 024743.sst<br>|   |       |– CURRENT<br>|   |       |– IDENTITY<br>|   |       |– LOCK<br>|   |       |– LOG<br>|   |       |– MANIFEST-000006<br>|   |       |– OPTIONS-000010<br>|   |       <code>-- OPTIONS-000012 |   |-- job_05606ccf34b1f48ff075d3092d9b81dd_op_WindowOperator_be465a1f98956392d0b820196e75d12b__1_8__uuid_efd7a85c-2a15-4d62-9929-60624dd6ea4a |   |   </code>– db<br>|   |       |– 000013.log<br>|   |       |– 024741.sst<br>|   |       |– 024743.sst<br>|   |       |– 024744.sst<br>|   |       |– 024745.sst<br>|   |       |– 024746.sst<br>|   |       |– 024747.sst<br>|   |       |– 024748.sst<br>|   |       |– 024749.sst<br>|   |       |– CURRENT<br>|   |       |– IDENTITY<br>|   |       |– LOCK<br>|   |       |– LOG<br>|   |       |– MANIFEST-000006<br>|   |       |– OPTIONS-000010<br>|   |       <code>-- OPTIONS-000012 |   |-- job_05606ccf34b1f48ff075d3092d9b81dd_op_WindowOperator_be465a1f98956392d0b820196e75d12b__2_8__uuid_e666a8b1-d26d-4688-b089-0a957e49f947 |   |   </code>– db<br>|   |       |– 000013.log<br>|   |       |– 024740.sst<br>|   |       |– 024742.sst<br>|   |       |– 024743.sst<br>|   |       |– CURRENT<br>|   |       |– IDENTITY<br>|   |       |– LOCK<br>|   |       |– LOG<br>|   |       |– MANIFEST-000006<br>|   |       |– OPTIONS-000010<br>|   |       <code>-- OPTIONS-000012 |   |-- job_05606ccf34b1f48ff075d3092d9b81dd_op_WindowOperator_eaecdab2f3df15db186c08e889659492__1_8__uuid_24efb767-bdf2-4309-a136-302cc4a18399 |   |   </code>– db<br>|   |       |– 000013.log<br>|   |       |– 024741.sst<br>|   |       |– 024743.sst<br>|   |       |– 024744.sst<br>|   |       |– 024745.sst<br>|   |       |– 024746.sst<br>|   |       |– 024747.sst<br>|   |       |– 024748.sst<br>|   |       |– CURRENT<br>|   |       |– IDENTITY<br>|   |       |– LOCK<br>|   |       |– LOG<br>|   |       |– MANIFEST-000006<br>|   |       |– OPTIONS-000010<br>|   |       <code>-- OPTIONS-000012 |   </code>– job_05606ccf34b1f48ff075d3092d9b81dd_op_WindowOperator_eaecdab2f3df15db186c08e889659492__2_8__uuid_776c72fa-68fa-493b-b9ab-03461406816a<br>|       <code>-- db |           |-- 000013.log |           |-- 024740.sst |           |-- 024742.sst |           |-- 024743.sst |           |-- CURRENT |           |-- IDENTITY |           |-- LOCK |           |-- LOG |           |-- MANIFEST-000006 |           |-- OPTIONS-000010 |           </code>– OPTIONS-000012<br>|– localState<br>|   |– aid_AllocationID{e1f41e5a166354d80ad6d6b8d3765fa1}<br>|   <code>-- aid_AllocationID&#123;e6e7f806bb078db66f50eeb3a48f0da9&#125; </code>– rocksdb-lib-89c57e29c492279dc1e188992c87925e<br>    `– librocksdbjni-linux64.so</p>
<p>23 directories, 87 files</p>
<p>[averyzhang@tdw-<ip> /data/yarnenv/local/usercache/u_teg_tdbank/appcache/application_1542247480019_1646/container_e03_1542247480019_1646_01_000002]$ ll<br>总用量 52<br>-rw——- 1 u_teg_tdbank users   69 6月   5 16:07 container_tokens<br>lrwxrwxrwx 1 u_teg_tdbank users  154 6月   5 16:07 flink-conf.yaml -&gt; /data/yarnenv/local/usercache/u_teg_tdbank/appcache/application_1542247480019_1646/filecache/10/91db3812-bb43-492d-8c32-359b2c21a142-taskmanager-conf.yaml<br>lrwxrwxrwx 1 u_teg_tdbank users  151 6月   5 16:07 flink-runtime-oceanus-dependency-lib-1.1.0-SNAPSHOT.jar -&gt; /data/yarnenv/local/usercache/u_teg_tdbank/appcache/application_1542247480019_1646/filecache/15/flink-runtime-oceanus-dependency-lib-1.1.0-SNAPSHOT.jar<br>lrwxrwxrwx 1 u_teg_tdbank users  146 6月   5 16:07 kafka-connector-0.11-dependency-1.1.0-SNAPSHOT.jar -&gt; /data/yarnenv/local/usercache/u_teg_tdbank/appcache/application_1542247480019_1646/filecache/11/kafka-connector-0.11-dependency-1.1.0-SNAPSHOT.jar<br>-rwx—— 1 u_teg_tdbank users 6328 6月   5 16:07 launch_container.sh<br>lrwxrwxrwx 1 u_teg_tdbank users  164 6月   5 16:07 lct-click-stat-1.0-SNAPSHOT-jar-with-dependencies-20200511152610.jar -&gt; /data/yarnenv/local/usercache/u_teg_tdbank/appcache/application_1542247480019_1646/filecache/12/lct-click-stat-1.0-SNAPSHOT-jar-with-dependencies-20200511152610.jar<br>lrwxrwxrwx 1 u_teg_tdbank users  112 6月   5 16:07 log4j.properties -&gt; /data/yarnenv/local/usercache/u_teg_tdbank/appcache/application_1542247480019_1646/filecache/17/log4j.properties<br>lrwxrwxrwx 1 u_teg_tdbank users  105 6月   5 16:07 log4j.zip -&gt; /data/yarnenv/local/usercache/u_teg_tdbank/appcache/application_1542247480019_1646/filecache/14/log4j.zip<br>lrwxrwxrwx 1 u_teg_tdbank users  129 6月   5 16:07 oceanus-common-1.1.0-SNAPSHOT.jar -&gt; /data/yarnenv/local/usercache/u_teg_tdbank/appcache/application_1542247480019_1646/filecache/13/oceanus-common-1.1.0-SNAPSHOT.jar<br>lrwxrwxrwx 1 u_teg_tdbank users  127 6月   5 16:07 oceanus-core-1.1.0-SNAPSHOT.jar -&gt; /data/yarnenv/local/usercache/u_teg_tdbank/appcache/application_1542247480019_1646/filecache/16/oceanus-core-1.1.0-SNAPSHOT.jar<br>lrwxrwxrwx 1 u_teg_tdbank users  125 6月   5 16:07 oceanus-ml-1.1.0-SNAPSHOT.jar -&gt; /data/yarnenv/local/usercache/u_teg_tdbank/appcache/application_1542247480019_1646/filecache/18/oceanus-ml-1.1.0-SNAPSHOT.jar<br>drwxr-s— 2 u_teg_tdbank users 4096 6月   5 16:07 tmp</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://example.com/bigdata/Flink/Flink-TableAPI/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="aaronzhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Guadazi">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/bigdata/Flink/Flink-TableAPI/" class="post-title-link" itemprop="url">Flink:Table API</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2019-08-10 14:58:00" itemprop="dateCreated datePublished" datetime="2019-08-10T14:58:00+08:00">2019-08-10</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-04-19 15:02:11" itemprop="dateModified" datetime="2021-04-19T15:02:11+08:00">2021-04-19</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/bigdata/" itemprop="url" rel="index"><span itemprop="name">bigdata</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="flink-table-api"><a href="#flink-table-api" class="headerlink" title="flink-table-api"></a>flink-table-api</h1><p><a target="_blank" rel="noopener" href="https://github.com/crestofwave1/oneFlink/blob/master/doc/table/Concept%20%26%20Common%20API.md">官方文档翻译</a></p>
<h2 id="Concept-amp-Common-API"><a href="#Concept-amp-Common-API" class="headerlink" title="Concept  &amp; Common API"></a>Concept  &amp; Common API</h2><p>Table API和SQL集成在一个联合的API中。这个API核心概念是Table，<br>Table可以作为查询的输入和输出。这篇文章展示了使用Table API和SQL查询的通用结构，<br>如何去进行表的注册，如何去进行表的查询，并且展示如何去进行表的输出。</p>
<h2 id="1-Structure-of-Table-API-and-SQL-Programs"><a href="#1-Structure-of-Table-API-and-SQL-Programs" class="headerlink" title="1. Structure of Table API and SQL Programs"></a>1. Structure of Table API and SQL Programs</h2><p>​    所有使用批量和流式相关的Table API和SQL的程序都有以下相同模式。下面的代码实例展示了Table API和SQL程序的通用结构。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 在批处理程序中使用ExecutionEnvironment代替StreamExecutionEnvironment</span></span><br><span class="line"><span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line"></span><br><span class="line"><span class="comment">// 创建TableEnvironment对象</span></span><br><span class="line"><span class="keyword">val</span> tableEnv = <span class="type">TableEnvironment</span>.getTableEnvironment(env)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 注册表</span></span><br><span class="line">tableEnv.registerTable(<span class="string">&quot;table1&quot;</span>, ...)           <span class="comment">// or</span></span><br><span class="line">tableEnv.registerTableSource(<span class="string">&quot;table2&quot;</span>, ...)     <span class="comment">// or</span></span><br><span class="line">tableEnv.registerExternalCatalog(<span class="string">&quot;extCat&quot;</span>, ...) </span><br><span class="line"></span><br><span class="line"><span class="comment">// 基于Table API的查询创建表</span></span><br><span class="line"><span class="keyword">val</span> tapiResult = tableEnv.scan(<span class="string">&quot;table1&quot;</span>).select(...)</span><br><span class="line"><span class="comment">// 从SQL查询创建表</span></span><br><span class="line"><span class="keyword">val</span> sqlResult  = tableEnv.sqlQuery(<span class="string">&quot;SELECT ... FROM table2 ...&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 将表操作API查询到的结果表输出到TableSink，SQL查询到的结果一样如此</span></span><br><span class="line">tapiResult.writeToSink(...)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 执行</span></span><br><span class="line">env.execute()</span><br></pre></td></tr></table></figure>
<p>注意：Table API和SQL查询很容易集成并被嵌入到DataStream或者DataSet程序中。查看<a target="_blank" rel="noopener" href="https://ci.apache.org/projects/flink/flink-docs-release-1.6/dev/table/common.html#integration-with-datastream-and-dataset-api">将DataStream和DataSet API进行整合</a>章节<br>学习DataSteams和DataSets是如何转换成Table以及Table是如何转换为DataStream或DataSet</p>
<h2 id="2-Create-a-TableEnvironment"><a href="#2-Create-a-TableEnvironment" class="headerlink" title="2. Create a TableEnvironment"></a>2. Create a TableEnvironment</h2><p>TableEnvironment是Table API与SQL整合的核心概念之一，它主要有如下功能：</p>
<ul>
<li>在internal catalog注册表</li>
<li>注册external catalog</li>
<li>执行SQL查询</li>
<li>注册UDF函数（user-defined function)，例如 标量, 表或聚合</li>
<li>将DataStream或者DataSet转换为表</li>
<li>保持ExecutionEnvironment或者StreamExecutionEnvironment的引用指向</li>
</ul>
<p>一个表总是与一个特定的TableEnvironment绑定在一块，<br>相同的查询不同的TableEnvironment是无法通过join、union合并在一起。</p>
<p>创建TableEnvironment的方法通常是通过StreamExecutionEnvironment，ExecutionEnvironment对象调用其中的静态方法TableEnvironment.getTableEnvironment()，或者是TableConfig来创建。<br>TableConfig可以用作配置TableEnvironment或是对自定义查询优化器或者是编译过程进行优化(详情查看<a target="_blank" rel="noopener" href="https://ci.apache.org/projects/flink/flink-docs-release-1.6/dev/table/common.html#query-optimization">查询优化</a>)</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// ***************</span></span><br><span class="line"><span class="comment">// 流式查询</span></span><br><span class="line"><span class="comment">// ***************</span></span><br><span class="line"><span class="keyword">val</span> sEnv = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line"><span class="comment">// 为流式查询创建一个TableEnvironment对象</span></span><br><span class="line"><span class="keyword">val</span> sTableEnv = <span class="type">TableEnvironment</span>.getTableEnvironment(sEnv)</span><br><span class="line"></span><br><span class="line"><span class="comment">// ***********</span></span><br><span class="line"><span class="comment">// 批量查询</span></span><br><span class="line"><span class="comment">// ***********</span></span><br><span class="line"><span class="keyword">val</span> bEnv = <span class="type">ExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line"><span class="comment">// 为批量查询创建一个TableEnvironment对象</span></span><br><span class="line"><span class="keyword">val</span> bTableEnv = <span class="type">TableEnvironment</span>.getTableEnvironment(bEnv)</span><br></pre></td></tr></table></figure>
<h2 id="Register-Tables-in-the-Catalog"><a href="#Register-Tables-in-the-Catalog" class="headerlink" title="Register Tables in the Catalog"></a>Register Tables in the Catalog</h2><p>TableEnvironment包含了通过名称注册表时的表的catalog信息。通常情况下有两种表，一种为输入表，<br>一种为输出表。输入表主要是在使用Table API和SQL查询时提供输入数据，输出表主要是将Table API和<br>SQL查询的结果作为输出结果对接到外部系统。</p>
<p>输入表有多种不同的输入源进行注册：</p>
<ul>
<li>已经存在的Table对象，通常是是作为Table API和SQL查询的结果</li>
<li>TableSource，可以访问外部数据如文件，数据库或者是消息系统</li>
<li>来自DataStream或是DataSet程序中的DataStream或DataSet，讨论DataStream或是DataSet<br>可以<a target="_blank" rel="noopener" href="https://ci.apache.org/projects/flink/flink-docs-release-1.6/dev/table/common.html#integration-with-datastream-and-dataset-api">整合DataStream和DataSet API</a>了解到</li>
</ul>
<p>输出表可使用TableSink进行注册</p>
<h2 id="Register-a-Table"><a href="#Register-a-Table" class="headerlink" title="Register a Table"></a>Register a Table</h2><p>Table是如何注册到TableEnvironment中如下所示：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 获取(创建)TableEnvironment对象</span></span><br><span class="line"><span class="keyword">val</span> tableEnv = <span class="type">TableEnvironment</span>.getTableEnvironment(env)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 从简单的查询结果中作为表</span></span><br><span class="line"><span class="keyword">val</span> projTable: <span class="type">Table</span> = tableEnv.scan(<span class="string">&quot;X&quot;</span>).select(...)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 将创建的表projTable命名为projectedTable注册到TableEnvironment中</span></span><br><span class="line">tableEnv.registerTable(<span class="string">&quot;projectedTable&quot;</span>, projTable)</span><br></pre></td></tr></table></figure>
<p>注意：一张注册过的Table就跟关系型数据库中的视图性质相同，定义表的查询未进行优化，但在另一个查询引用已注册的表时将进行内联。<br>如果多表查询引用了相同的Table，它就会将每一个引用进行内联并且多次执行，已注册的Table的结果之间不会进行共享。</p>
<h2 id="Register-a-TableSource"><a href="#Register-a-TableSource" class="headerlink" title="Register a TableSource"></a>Register a TableSource</h2><p>TableSource可以访问外部系统存储例如数据库（Mysql,HBase），特殊格式编码的文件(CSV, Apache [Parquet, Avro, ORC], …)<br>或者是消息系统 (Apache Kafka, RabbitMQ, …)中的数据。</p>
<p>Flink旨在为通用数据格式和存储系统提供TableSource。请查看<a target="_blank" rel="noopener" href="https://ci.apache.org/projects/flink/flink-docs-release-1.6/dev/table/sourceSinks.html">此处</a><br>了解支持的TableSource类型与如何去自定义TableSour。</p>
<p>TableSource是如何注册到TableEnvironment中如下所示：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 获取(创建)TableEnvironment对象</span></span><br><span class="line"><span class="keyword">val</span> tableEnv = <span class="type">TableEnvironment</span>.getTableEnvironment(env)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 创建TableSource对象</span></span><br><span class="line"><span class="keyword">val</span> csvSource: <span class="type">TableSource</span> = <span class="keyword">new</span> <span class="type">CsvTableSource</span>(<span class="string">&quot;/path/to/file&quot;</span>, ...)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 将创建的TableSource作为表并命名为csvTable注册到TableEnvironment中</span></span><br><span class="line">tableEnv.registerTableSource(<span class="string">&quot;CsvTable&quot;</span>, csvSource)</span><br></pre></td></tr></table></figure>
<h2 id="Register-a-TableSink"><a href="#Register-a-TableSink" class="headerlink" title="Register a TableSink"></a>Register a TableSink</h2><p>注册过的TableSink可以将SQL查询的结果以表的形式输出到外部的存储系统，例如关系型数据库，<br>Key-Value数据库(Nosql)，消息队列，或者是其他文件系统(使用不同的编码, 例如CSV, Apache [Parquet, Avro, ORC], …)</p>
<p>Flink使用TableSink的目的是为了将常用的数据进行清洗转换然后存储到不同的存储介质中。详情请查看<a target="_blank" rel="noopener" href="https://ci.apache.org/projects/flink/flink-docs-release-1.6/dev/table/sourceSinks.html">此处</a><br>去深入了解哪些sinks是可用的，并且如何去自定义TableSink。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 获取(创建)TableEnvironment对象</span></span><br><span class="line"><span class="keyword">val</span> tableEnv = <span class="type">TableEnvironment</span>.getTableEnvironment(env)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 创建TableSink对象</span></span><br><span class="line"><span class="keyword">val</span> csvSink: <span class="type">TableSink</span> = <span class="keyword">new</span> <span class="type">CsvTableSink</span>(<span class="string">&quot;/path/to/file&quot;</span>, ...)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 定义字段的名称和类型</span></span><br><span class="line"><span class="keyword">val</span> fieldNames: <span class="type">Array</span>[<span class="type">String</span>] = <span class="type">Array</span>(<span class="string">&quot;a&quot;</span>, <span class="string">&quot;b&quot;</span>, <span class="string">&quot;c&quot;</span>)</span><br><span class="line"><span class="keyword">val</span> fieldTypes: <span class="type">Array</span>[<span class="type">TypeInformation</span>[_]] = <span class="type">Array</span>(<span class="type">Types</span>.<span class="type">INT</span>, <span class="type">Types</span>.<span class="type">STRING</span>, <span class="type">Types</span>.<span class="type">LONG</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 将创建的TableSink作为表并命名为CsvSinkTable注册到TableEnvironment中</span></span><br><span class="line">tableEnv.registerTableSink(<span class="string">&quot;CsvSinkTable&quot;</span>, fieldNames, fieldTypes, csvSink)</span><br></pre></td></tr></table></figure>
<h2 id="Register-an-External-Catalog"><a href="#Register-an-External-Catalog" class="headerlink" title="Register an External Catalog"></a>Register an External Catalog</h2><p>外部目录可以提供有关外部数据库和表的信息，<br>例如其名称，模式，统计以及有关如何访问存储在外部数据库，表或文件中的数据的信息。</p>
<p>外部目录的创建方式可以通过实现ExternalCatalog接口，并且注册到TableEnvironment中，详情如下所示:</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 获取(创建)TableEnvironment对象</span></span><br><span class="line"><span class="keyword">val</span> tableEnv = <span class="type">TableEnvironment</span>.getTableEnvironment(env)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 创建一个External Catalog目录对象</span></span><br><span class="line"><span class="keyword">val</span> catalog: <span class="type">ExternalCatalog</span> = <span class="keyword">new</span> <span class="type">InMemoryExternalCatalog</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 将ExternalCatalog注册到TableEnvironment中</span></span><br><span class="line">tableEnv.registerExternalCatalog(<span class="string">&quot;InMemCatalog&quot;</span>, catalog)</span><br></pre></td></tr></table></figure>
<p>一旦将External Catalog注册到TableEnvironment中，所有在ExternalCatalog中<br>定义的表可以通过完整的路径如catalog.database.table进行Table API和SQL的查询操作 </p>
<p>目前，Flink提供InMemoryExternalCatalog对象用来做demo和测试，然而，<br>ExternalCatalog对象还可用作Table API来连接catalogs，例如HCatalog 或 Metastore</p>
<h2 id="Query-a-Table"><a href="#Query-a-Table" class="headerlink" title="Query a Table"></a>Query a Table</h2><h3 id="Table-API"><a href="#Table-API" class="headerlink" title="Table API"></a>Table API</h3><p>Table API是Scala和Java语言集成查询的API，与SQL查询不同之处在于，它的查询不是像<br>SQL一样使用字符串进行查询，而是在语言中使用语法进行逐步组合使用</p>
<p>Table API是基于展示表（流或批处理）的Table类，它提供一些列操作应用相关的操作。<br>这些方法返回一个新的Table对象，该对象表示在输入表上关系运算的结果。一些关系运算是<br>由多个方法组合而成的，例如 table.groupBy(…).select()，其中groupBy()指定<br>表的分组，select()表示在分组的结果上进行查询。</p>
<p><a target="_blank" rel="noopener" href="https://ci.apache.org/projects/flink/flink-docs-release-1.6/dev/table/tableApi.html">Table API</a><br>描述了所有支持表的流式或者批处理相关的操作。</p>
<p>下面给出一个简单的实例去说明如何去使用Table API进行聚合查询：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 获取(创建)TableEnvironment对象</span></span><br><span class="line"><span class="keyword">val</span> tableEnv = <span class="type">TableEnvironment</span>.getTableEnvironment(env)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 注册Orders表</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 扫描注册过的Orders表</span></span><br><span class="line"><span class="keyword">val</span> orders = tableEnv.scan(<span class="string">&quot;Orders&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 计算表中所有来自法国的客户的收入</span></span><br><span class="line"><span class="keyword">val</span> revenue = orders</span><br><span class="line">  .filter(<span class="symbol">&#x27;cCountry</span> === <span class="string">&quot;FRANCE&quot;</span>)</span><br><span class="line">  .groupBy(<span class="symbol">&#x27;cID</span>, <span class="symbol">&#x27;cName</span>)</span><br><span class="line">  .select(<span class="symbol">&#x27;cID</span>, <span class="symbol">&#x27;cName</span>, <span class="symbol">&#x27;revenue</span>.sum <span class="type">AS</span> <span class="symbol">&#x27;revSum</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 将结果输出成一张表或者是转换表</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 执行查询</span></span><br></pre></td></tr></table></figure>
<p>注意：Scala的Table API使用Scala符号，它使用单引号加字段(‘cID)来表示表的属性的引用，<br>如果使用Scala的隐式转换的话，确保引入了org.apache.flink.api.scala._ 和 org.apache.flink.table.api.scala._<br>来确保它们之间的转换。</p>
<h3 id="SQL"><a href="#SQL" class="headerlink" title="SQL"></a>SQL</h3><p>Flink的SQL操作基于实现了SQL标准的<a target="_blank" rel="noopener" href="https://calcite.apache.org/">Apache Calcite</a>，SQL查询通常是使用特殊且有规律的字符串。<br><a target="_blank" rel="noopener" href="https://ci.apache.org/projects/flink/flink-docs-release-1.6/dev/table/sql.html">SQL</a><br>描述了所有支持表的流式或者批处理相关的SQL操作。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 获取(创建)TableEnvironment对象</span></span><br><span class="line"><span class="keyword">val</span> tableEnv = <span class="type">TableEnvironment</span>.getTableEnvironment(env)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 注册Orders表</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 计算表中所有来自法国的客户的收入</span></span><br><span class="line"><span class="keyword">val</span> revenue = tableEnv.sqlQuery(<span class="string">&quot;&quot;</span><span class="string">&quot;</span></span><br><span class="line"><span class="string">  |SELECT cID, cName, SUM(revenue) AS revSum</span></span><br><span class="line"><span class="string">  |FROM Orders</span></span><br><span class="line"><span class="string">  |WHERE cCountry = &#x27;FRANCE&#x27;</span></span><br><span class="line"><span class="string">  |GROUP BY cID, cName</span></span><br><span class="line"><span class="string">  &quot;</span><span class="string">&quot;&quot;</span>.stripMargin)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 将结果输出成一张表或者是转换表</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 执行查询</span></span><br></pre></td></tr></table></figure>
<p>下面的例子展示了如何去使用更新查询去插入数据到已注册的表中</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 获取(创建)TableEnvironment对象</span></span><br><span class="line"><span class="keyword">val</span> tableEnv = <span class="type">TableEnvironment</span>.getTableEnvironment(env)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 注册&quot;Orders&quot;表</span></span><br><span class="line"><span class="comment">// 注册&quot;RevenueFrance&quot;输出表</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 计算表中所有来自法国的客户的收入并且将结果作为结果输出到&quot;RevenueFrance&quot;中</span></span><br><span class="line">tableEnv.sqlUpdate(<span class="string">&quot;&quot;</span><span class="string">&quot;</span></span><br><span class="line"><span class="string">  |INSERT INTO RevenueFrance</span></span><br><span class="line"><span class="string">  |SELECT cID, cName, SUM(revenue) AS revSum</span></span><br><span class="line"><span class="string">  |FROM Orders</span></span><br><span class="line"><span class="string">  |WHERE cCountry = &#x27;FRANCE&#x27;</span></span><br><span class="line"><span class="string">  |GROUP BY cID, cName</span></span><br><span class="line"><span class="string">  &quot;</span><span class="string">&quot;&quot;</span>.stripMargin)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 执行查询</span></span><br></pre></td></tr></table></figure>
<h2 id="Mixing-Table-API-and-SQL"><a href="#Mixing-Table-API-and-SQL" class="headerlink" title="Mixing Table API and SQL"></a>Mixing Table API and SQL</h2><p>Table API和SQL可以很轻松的混合使用因为他们两者返回的结果都为Table对象：</p>
<ul>
<li>可以在SQL查询返回的Table对象上定义Table API查询</li>
<li>通过在TableEnvironment中注册结果表并在SQL查询的FROM子句中引用它，<br>可以在Table API查询的结果上定义SQL查询。</li>
</ul>
<h2 id="Emit-a-Table"><a href="#Emit-a-Table" class="headerlink" title="Emit a Table"></a>Emit a Table</h2><p>通过将Table写入到TableSink来作为一张表的输出，TableSink是做为多种文件类型 (CSV, Apache Parquet, Apache Avro),<br>存储系统(JDBC, Apache HBase, Apache Cassandra, Elasticsearch), 或者是消息系统 (Apache Kafka, RabbitMQ).输出的通用接口，</p>
<p>Batch Table只能通过BatchTableSink来进行数据写入，而Streaming Table可以<br>选择AppendStreamTableSink，RetractStreamTableSink，UpsertStreamTableSink<br>中的任意一个来进行。</p>
<p>请查看<a target="_blank" rel="noopener" href="https://ci.apache.org/projects/flink/flink-docs-release-1.6/dev/table/sourceSinks.html">Table Source &amp; Sinks</a><br>来更详细的了解支持的Sinks并且如何去实现自定义的TableSink。</p>
<p>可以使用两种方式来输出一张表：</p>
<ul>
<li>Table.writeToSink(TableSink sink)方法使用提供的TableSink自动配置的表的schema来<br>进行表的输出</li>
<li>Table.insertInto（String sinkTable）方法查找在TableEnvironment目录中提供的名称下使用特定模式注册的TableSink。<br>将输出表的模式将根据已注册的TableSink的模式进行验证</li>
</ul>
<p>下面的例子展示了如何去查询结果作为一张表输出</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 获取(创建)TableEnvironment对象</span></span><br><span class="line"><span class="keyword">val</span> tableEnv = <span class="type">TableEnvironment</span>.getTableEnvironment(env)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 使用Table API或者SQL 查询来查找结果</span></span><br><span class="line"><span class="keyword">val</span> result: <span class="type">Table</span> = ...</span><br><span class="line"><span class="comment">// 创建TableSink对象</span></span><br><span class="line"><span class="keyword">val</span> sink: <span class="type">TableSink</span> = <span class="keyword">new</span> <span class="type">CsvTableSink</span>(<span class="string">&quot;/path/to/file&quot;</span>, fieldDelim = <span class="string">&quot;|&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 方法1: 使用TableSink的writeToSink()方法来将结果输出为一张表</span></span><br><span class="line">result.writeToSink(sink)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 方法2: 注册特殊schema的TableSink</span></span><br><span class="line"><span class="keyword">val</span> fieldNames: <span class="type">Array</span>[<span class="type">String</span>] = <span class="type">Array</span>(<span class="string">&quot;a&quot;</span>, <span class="string">&quot;b&quot;</span>, <span class="string">&quot;c&quot;</span>)</span><br><span class="line"><span class="keyword">val</span> fieldTypes: <span class="type">Array</span>[<span class="type">TypeInformation</span>] = <span class="type">Array</span>(<span class="type">Types</span>.<span class="type">INT</span>, <span class="type">Types</span>.<span class="type">STRING</span>, <span class="type">Types</span>.<span class="type">LONG</span>)</span><br><span class="line">tableEnv.registerTableSink(<span class="string">&quot;CsvSinkTable&quot;</span>, fieldNames, fieldTypes, sink)</span><br><span class="line"><span class="comment">// 调用注册过的TableSink中insertInto() 方法来将结果输出为一张表</span></span><br><span class="line">result.insertInto(<span class="string">&quot;CsvSinkTable&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 执行</span></span><br></pre></td></tr></table></figure>
<h2 id="Translate-and-Execute-a-Query"><a href="#Translate-and-Execute-a-Query" class="headerlink" title="Translate and Execute a Query"></a>Translate and Execute a Query</h2><p>Table API和SQL查询的结果转换为<a target="_blank" rel="noopener" href="https://ci.apache.org/projects/flink/flink-docs-release-1.6/dev/datastream_api.html">DataStream</a><br>或是<a target="_blank" rel="noopener" href="https://ci.apache.org/projects/flink/flink-docs-release-1.6/dev/batch/">DataSet</a><br>取决于它的输入是流式输入还是批处理输入。查询逻辑在内部表示为逻辑执行计划，并分为两个阶段进行转换：</p>
<ul>
<li>优化逻辑执行计划</li>
<li>转换为DataStream或DataSet</li>
</ul>
<p>Table API或SQL查询在下面请看下进行转换：</p>
<ul>
<li>当调用Table.writeToSink() 或 Table.insertInto()进行查询结果表输出的时候</li>
<li>当调用TableEnvironment.sqlUpdate()进行SQL更新查询时</li>
<li>当表转换为DataSteam或DataSet时，详情查看<a target="_blank" rel="noopener" href="https://ci.apache.org/projects/flink/flink-docs-release-1.6/dev/table/common.html#integration-with-dataStream-and-dataSet-api">Integration with DataStream and DataSet API</a></li>
</ul>
<p>一旦进行转换后，Table API或SQL查询的结果就会在StreamExecutionEnvironment.execute() 或 ExecutionEnvironment.execute()<br>被调用时被当做DataStream或DataSet一样被进行处理</p>
<h2 id="Integration-with-DataStream-and-DataSet-API"><a href="#Integration-with-DataStream-and-DataSet-API" class="headerlink" title="Integration with DataStream and DataSet API"></a>Integration with DataStream and DataSet API</h2><p>Table API或SQL查询的结果很容易被<a target="_blank" rel="noopener" href="https://ci.apache.org/projects/flink/flink-docs-release-1.6/dev/datastream_api.html">DataStream</a><br>或是<a target="_blank" rel="noopener" href="https://ci.apache.org/projects/flink/flink-docs-release-1.6/dev/batch/">DataSet</a>内嵌整合。举个例子，<br>我们会进行外部表的查询(像关系型数据库)，然后做像过滤，映射，聚合或者是元数据关联的一些预处理。<br>然后使用DataStream或是DataSet API(或者是基于这些基础库开发的上层API库, 例如CEP或Gelly)进一步对数据进行处理。<br>同样，Table API或SQL查询也可以应用于DataStream或DataSet程序的结果。</p>
<p>##implicit Conversion for Scala<br>Scala Table API具有DataSet，DataStream和Table Class之间的隐式转换，流式操作API中只要引入org.apache.flink.table.api.scala._<br>和 org.apache.flink.api.scala._ 便可以进行相应的隐式转换</p>
<h2 id="Register-a-DataStream-or-DataSet-as-Table"><a href="#Register-a-DataStream-or-DataSet-as-Table" class="headerlink" title="Register a DataStream or DataSet as Table"></a>Register a DataStream or DataSet as Table</h2><p>DataStream或DataSet也可以作为Table注册到TableEnvironment中。结果表的模式取决于已注册的DataStream或DataSet的数据类型，<br>详情请查看<a target="_blank" rel="noopener" href="https://ci.apache.org/projects/flink/flink-docs-release-1.6/dev/table/common.html#mapping-of-data-types-to-table-schema">mapping of data types to table schema</a></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 获取(创建)TableEnvironment对象</span></span><br><span class="line"><span class="comment">// 注册如表一样的DataSet</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> tableEnv = <span class="type">TableEnvironment</span>.getTableEnvironment(env)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> stream: <span class="type">DataStream</span>[(<span class="type">Long</span>, <span class="type">String</span>)] = ...</span><br><span class="line"></span><br><span class="line"><span class="comment">// 将DataStream作为具有&quot;f0&quot;, &quot;f1&quot;字段的&quot;myTable&quot;表注册到TableEnvironment中</span></span><br><span class="line">tableEnv.registerDataStream(<span class="string">&quot;myTable&quot;</span>, stream)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 将DataStream作为具有&quot;myLong&quot;, &quot;myString&quot;字段的&quot;myTable2&quot;表注册到TableEnvironment中</span></span><br><span class="line">tableEnv.registerDataStream(<span class="string">&quot;myTable2&quot;</span>, stream, <span class="symbol">&#x27;myLong</span>, <span class="symbol">&#x27;myString</span>)</span><br></pre></td></tr></table></figure>
<p>注意：DataStream表的名称必须与^ <em>DataStreamTable</em> [0-9] +模式不匹配，<br>并且DataSet表的名称必须与^ <em>DataSetTable</em> [0-9] +模式不匹配。<br>这些模式仅供内部使用。</p>
<h2 id="Convert-a-DataStream-or-DataSet-into-a-Table"><a href="#Convert-a-DataStream-or-DataSet-into-a-Table" class="headerlink" title="Convert a DataStream or DataSet into a Table"></a>Convert a DataStream or DataSet into a Table</h2><p>如果你使用Table API或是SQL查询，你可以直接将DataStream或DataSet直接转换为表而不需要<br>再将它们注册到TableEnvironment中。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 获取(创建)TableEnvironment对象</span></span><br><span class="line"><span class="comment">// 注册如表一样的DataSet</span></span><br><span class="line"><span class="keyword">val</span> tableEnv = <span class="type">TableEnvironment</span>.getTableEnvironment(env)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> stream: <span class="type">DataStream</span>[(<span class="type">Long</span>, <span class="type">String</span>)] = ...</span><br><span class="line"></span><br><span class="line"><span class="comment">// 使用默认的字段&#x27;_1, &#x27;_2将DataStram转换为Table</span></span><br><span class="line"><span class="keyword">val</span> table1: <span class="type">Table</span> = tableEnv.fromDataStream(stream)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 使用默认的字段&#x27;myLong, &#x27;myString将DataStram转换为Table</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> table2: <span class="type">Table</span> = tableEnv.fromDataStream(stream, <span class="symbol">&#x27;myLong</span>, <span class="symbol">&#x27;myString</span>)</span><br></pre></td></tr></table></figure>
<h2 id="Convert-a-Table-into-a-DataStream-or-DataSet"><a href="#Convert-a-Table-into-a-DataStream-or-DataSet" class="headerlink" title="Convert a Table into a DataStream or DataSet"></a>Convert a Table into a DataStream or DataSet</h2><p>表可以转换为DataStream或DataSet，通过这种方式，自定义DataStream或DataSet<br>同样也可以作为Table API或SQL查询结果的结果。<br>当把表转换为DataStream或DataSet时，你需要指定生成的DataStream或DataSet的数据类型。<br>例如，表格行所需转换的数据类型，通常最方便的转换类型也最常用的是Row。<br>以下列表概述了不同选项的功能：</p>
<ul>
<li>Row：字段按位置，任意数量的字段映射，支持空值，无类型安全访问。</li>
<li>POJO：字段按名称(POJO字段必须与Table字段保持一致)，任意数量的字段映射，支持空值，类型安全访问。</li>
<li>Case Class：字段按位置，任意数量的字段映射，不支持空值，类型安全访问。</li>
<li>Tuple：字段按位置，Scala支持22个字段，Java 25个字段映射，不支持空值，类型安全访问。</li>
<li>Atomic Type：表必须具有单个字段，不支持空值，类型安全访问。<h3 id="Convert-a-Table-into-a-DataStream"><a href="#Convert-a-Table-into-a-DataStream" class="headerlink" title="Convert a Table into a DataStream"></a>Convert a Table into a DataStream</h3>作为流式查询结果的表将动态更新，它随着新记录到达查询的输入流而改变，于是，转换到这样的动态查询DataStream<br>需要对表的更新进行编码。<br>将表转换为DataStream有两种模式：</li>
<li>Append Mode：这种模式仅用于动态表仅仅通过INSERT来进行表的更新，它是仅可追加模式，<br>并且之前输出的表不会进行更改</li>
<li>Retract Mode：这种模式经常用到。它使用布尔值的变量来对INSERT和DELETE对表的更新做标记<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 获取(创建)TableEnvironment对象 </span></span><br><span class="line"><span class="comment">// 注册如表一样的DataSet</span></span><br><span class="line"><span class="keyword">val</span> tableEnv = <span class="type">TableEnvironment</span>.getTableEnvironment(env)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 表中有两个字段(String name, Integet age)</span></span><br><span class="line"><span class="keyword">val</span> table: <span class="type">Table</span> = ...</span><br><span class="line"></span><br><span class="line"><span class="comment">// 将表转换为列的 append DataStream</span></span><br><span class="line"><span class="keyword">val</span> dsRow: <span class="type">DataStream</span>[<span class="type">Row</span>] = tableEnv.toAppendStream[<span class="type">Row</span>](table)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 将表转换为Tubple2[String,Int]的 append DataStream</span></span><br><span class="line"><span class="comment">// convert the Table into an append DataStream of Tuple2[String, Int]</span></span><br><span class="line"><span class="keyword">val</span> dsTuple: <span class="type">DataStream</span>[(<span class="type">String</span>, <span class="type">Int</span>)] dsTuple = </span><br><span class="line">  tableEnv.toAppendStream[(<span class="type">String</span>, <span class="type">Int</span>)](table)</span><br><span class="line"></span><br><span class="line"><span class="comment">// convert the Table into a retract DataStream of Row.</span></span><br><span class="line"><span class="comment">// Retract Mode下将表转换为列的 append DataStream</span></span><br><span class="line"><span class="comment">// 判断A retract stream X是否为DataStream[(Boolean, X)]</span></span><br><span class="line"><span class="comment">//  布尔只表示数据类型的变化,True代表为INSERT，false表示为删除</span></span><br><span class="line"><span class="keyword">val</span> retractStream: <span class="type">DataStream</span>[(<span class="type">Boolean</span>, <span class="type">Row</span>)] = tableEnv.toRetractStream[<span class="type">Row</span>](table)</span><br></pre></td></tr></table></figure>
注意：关于动态表和它的属性详情参考<a target="_blank" rel="noopener" href="https://ci.apache.org/projects/flink/flink-docs-release-1.6/dev/table/streaming.html">Streaming Queries</a></li>
</ul>
<h3 id="Convert-a-Table-into-a-DataSet"><a href="#Convert-a-Table-into-a-DataSet" class="headerlink" title="Convert a Table into a DataSet"></a>Convert a Table into a DataSet</h3><p>表转换为DataSet如下所示：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 获取(创建)TableEnvironment对象 </span></span><br><span class="line"><span class="comment">// 注册如表一样的DataSet</span></span><br><span class="line"><span class="keyword">val</span> tableEnv = <span class="type">TableEnvironment</span>.getTableEnvironment(env)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 表中有两个字段(String name, Integet age)</span></span><br><span class="line"><span class="keyword">val</span> table: <span class="type">Table</span> = ...</span><br><span class="line"></span><br><span class="line"><span class="comment">// 将表转换为列的DataSet</span></span><br><span class="line"><span class="keyword">val</span> dsRow: <span class="type">DataSet</span>[<span class="type">Row</span>] = tableEnv.toDataSet[<span class="type">Row</span>](table)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 将表转换为Tubple2[String,Int]的DataSet</span></span><br><span class="line"><span class="keyword">val</span> dsTuple: <span class="type">DataSet</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = tableEnv.toDataSet[(<span class="type">String</span>, <span class="type">Int</span>)](table)</span><br></pre></td></tr></table></figure>
<h3 id="Mapping-of-Data-Types-to-Table-Schema"><a href="#Mapping-of-Data-Types-to-Table-Schema" class="headerlink" title="Mapping of Data Types to Table Schema"></a>Mapping of Data Types to Table Schema</h3><p>Flink的DataStream和DataSet API支持多种类型。组合类型像Tuple(内置Scala元组和Flink Java元组)，<br>POJOs，Scala case classes和Flink中具有可在表表达式中访问的多个字段允许嵌套数据结构的Row类型，<br>其他类型都被视为原子类型。接下来，我们将会描述Table API是如何将这些类型转换为内部的列展现并且<br>举例说明如何将DataStream转换为Table</p>
<h4 id="Position-based-Mapping"><a href="#Position-based-Mapping" class="headerlink" title="Position-based Mapping"></a>Position-based Mapping</h4><p>基于位置的映射通常在保持顺序的情况下给字段一个更有意义的名称，这种映射可用于有固定顺序的组合数据类型，<br>也可用于原子类型。复合数据类型（如元组，行和Case Class）具有此类字段顺序.然而，POJO的字段必须与映射的<br>表的字段名相同。</p>
<p>当定义基于位置的映射，输入的数据类型不得存在指定的名称，不然API会认为这些映射应该按名称来进行映射。<br>如果未指定字段名称，则使用复合类型的默认字段名称和字段顺序，或者使用f0作为原子类型。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 获取(创建)TableEnvironment对象 </span></span><br><span class="line"><span class="keyword">val</span> tableEnv = <span class="type">TableEnvironment</span>.getTableEnvironment(env)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> stream: <span class="type">DataStream</span>[(<span class="type">Long</span>, <span class="type">Int</span>)] = ...</span><br><span class="line"></span><br><span class="line"><span class="comment">// 使用默认的字段&#x27;_1, &#x27;_2将DataStram转换为Table</span></span><br><span class="line"><span class="keyword">val</span> table1: <span class="type">Table</span> = tableEnv.fromDataStream(stream)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 使用默认的字段&#x27;myLong, &#x27;myInt将DataStram转换为Table</span></span><br><span class="line"><span class="keyword">val</span> table: <span class="type">Table</span> = tableEnv.fromDataStream(stream, <span class="symbol">&#x27;myLong</span> <span class="symbol">&#x27;myInt</span>)</span><br></pre></td></tr></table></figure>
<h4 id="Name-based-Mapping"><a href="#Name-based-Mapping" class="headerlink" title="Name-based Mapping"></a>Name-based Mapping</h4><p>基于名称的映射可用于一切数据类型包括POJOs，它是定义表模式映射最灵活的一种方式。虽然查询结果的字段可能会使用别名，但<br>这种模式下所有的字段都是使用名称进行映射的。使用别名的情况下会进行重排序。<br>如果未指定字段名称，则使用复合类型的默认字段名称和字段顺序，或者使用f0作为原子类型。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 获取(创建)TableEnvironment对象 </span></span><br><span class="line"><span class="keyword">val</span> tableEnv = <span class="type">TableEnvironment</span>.getTableEnvironment(env)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> stream: <span class="type">DataStream</span>[(<span class="type">Long</span>, <span class="type">Int</span>)] = ...</span><br><span class="line"></span><br><span class="line"><span class="comment">// 使用默认的字段&#x27;_1 和 &#x27;_2将DataStram转换为Table</span></span><br><span class="line"><span class="keyword">val</span> table: <span class="type">Table</span> = tableEnv.fromDataStream(stream)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 只使用&#x27;_2字段将DataStream转换为Table</span></span><br><span class="line"><span class="keyword">val</span> table: <span class="type">Table</span> = tableEnv.fromDataStream(stream, <span class="symbol">&#x27;_2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 交换字段将DataStream转换为Table</span></span><br><span class="line"><span class="keyword">val</span> table: <span class="type">Table</span> = tableEnv.fromDataStream(stream, <span class="symbol">&#x27;_2</span>, <span class="symbol">&#x27;_1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 交换后的字段给予别名&#x27;myInt, &#x27;myLong将DataStream转换为Table</span></span><br><span class="line"><span class="keyword">val</span> table: <span class="type">Table</span> = tableEnv.fromDataStream(stream, <span class="symbol">&#x27;_2</span> as <span class="symbol">&#x27;myInt</span>, <span class="symbol">&#x27;_1</span> as <span class="symbol">&#x27;myLong</span>)</span><br></pre></td></tr></table></figure>
<h4 id="Atomic-Types"><a href="#Atomic-Types" class="headerlink" title="Atomic Types"></a>Atomic Types</h4><p>Flink将基础类型(Integer, Double, String)和通用类型(不能被分析和拆分的类型)视为原子类型。<br>原子类型的DataStream或DataSet转换为只有单个属性的表。从原子类型推断属性的类型，并且可以指定属性的名称。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 获取(创建)TableEnvironment对象</span></span><br><span class="line"><span class="keyword">val</span> tableEnv = <span class="type">TableEnvironment</span>.getTableEnvironment(env)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> stream: <span class="type">DataStream</span>[<span class="type">Long</span>] = ...</span><br><span class="line"></span><br><span class="line"><span class="comment">// 将DataStream转换为带默认字段&quot;f0&quot;的表</span></span><br><span class="line"><span class="keyword">val</span> table: <span class="type">Table</span> = tableEnv.fromDataStream(stream)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 将DataStream转换为带字段&quot;myLong&quot;的表</span></span><br><span class="line"><span class="keyword">val</span> table: <span class="type">Table</span> = tableEnv.fromDataStream(stream, <span class="symbol">&#x27;myLong</span>)</span><br></pre></td></tr></table></figure>
<h4 id="Tuples-Scala-and-Java-and-Case-Classes-Scala-only"><a href="#Tuples-Scala-and-Java-and-Case-Classes-Scala-only" class="headerlink" title="Tuples (Scala and Java) and Case Classes (Scala only)"></a>Tuples (Scala and Java) and Case Classes (Scala only)</h4><p>Flink支持内建的Tuples并且提供了自己的Tuple类给Java进行使用。DataStreams和DataSet这两种<br>Tuple都可以转换为表。提供所有字段的名称(基于位置的映射)字段可以被重命名。如果没有指定字段的名称，<br>就使用默认的字段名称。如果原始字段名(f0, f1, … for Flink Tuples and _1, _2, … for Scala Tuples)被引用了的话，<br>API就会使用基于名称的映射来代替位置的映射。基于名称的映射可以起别名并且会进行重排序。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 获取(创建)TableEnvironment对象 </span></span><br><span class="line"><span class="keyword">val</span> tableEnv = <span class="type">TableEnvironment</span>.getTableEnvironment(env)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> stream: <span class="type">DataStream</span>[(<span class="type">Long</span>, <span class="type">String</span>)] = ...</span><br><span class="line"></span><br><span class="line"><span class="comment">// 将默认的字段重命名为&#x27;_1，&#x27;_2的DataStream转换为Table</span></span><br><span class="line"><span class="keyword">val</span> table: <span class="type">Table</span> = tableEnv.fromDataStream(stream)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 将字段名为&#x27;myLong，&#x27;myString的DataStream转换为Table(基于位置)</span></span><br><span class="line"><span class="keyword">val</span> table: <span class="type">Table</span> = tableEnv.fromDataStream(stream, <span class="symbol">&#x27;myLong</span>, <span class="symbol">&#x27;myString</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 将重排序后字段为&#x27;_2，&#x27;_1 的DataStream转换为Table(基于名称)</span></span><br><span class="line"><span class="keyword">val</span> table: <span class="type">Table</span> = tableEnv.fromDataStream(stream, <span class="symbol">&#x27;_2</span>, <span class="symbol">&#x27;_1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 将映射字段&#x27;_2的DataStream转换为Table(基于名称)</span></span><br><span class="line"><span class="keyword">val</span> table: <span class="type">Table</span> = tableEnv.fromDataStream(stream, <span class="symbol">&#x27;_2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 将重排序后字段为&#x27;_2给出别名&#x27;myString，&#x27;_1给出别名&#x27;myLong 的DataStream转换为Table(基于名称)</span></span><br><span class="line"><span class="keyword">val</span> table: <span class="type">Table</span> = tableEnv.fromDataStream(stream, <span class="symbol">&#x27;_2</span> as <span class="symbol">&#x27;myString</span>, <span class="symbol">&#x27;_1</span> as <span class="symbol">&#x27;myLong</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 定义 case class</span></span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Person</span>(<span class="params">name: <span class="type">String</span>, age: <span class="type">Int</span></span>)</span></span><br><span class="line"><span class="class"><span class="title">val</span> <span class="title">streamCC</span></span>: <span class="type">DataStream</span>[<span class="type">Person</span>] = ...</span><br><span class="line"></span><br><span class="line"><span class="comment">// 将默认字段&#x27;name, &#x27;age的DataStream转换为Table</span></span><br><span class="line"><span class="keyword">val</span> table = tableEnv.fromDataStream(streamCC)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 将字段名为&#x27;myName，&#x27;myAge的DataStream转换为Table(基于位置)</span></span><br><span class="line"><span class="keyword">val</span> table = tableEnv.fromDataStream(streamCC, <span class="symbol">&#x27;myName</span>, <span class="symbol">&#x27;myAge</span>)</span><br><span class="line"></span><br><span class="line">将重排序后字段为<span class="symbol">&#x27;_age</span>给出别名<span class="symbol">&#x27;myAge</span>，<span class="symbol">&#x27;_name</span>给出别名<span class="symbol">&#x27;myName</span> 的<span class="type">DataStream</span>转换为<span class="type">Table</span>(基于名称)</span><br><span class="line"><span class="keyword">val</span> table: <span class="type">Table</span> = tableEnv.fromDataStream(stream, <span class="symbol">&#x27;age</span> as <span class="symbol">&#x27;myAge</span>, <span class="symbol">&#x27;name</span> as <span class="symbol">&#x27;myName</span>)</span><br></pre></td></tr></table></figure>
<h4 id="POJO-Java-and-Scala"><a href="#POJO-Java-and-Scala" class="headerlink" title="POJO (Java and Scala)"></a>POJO (Java and Scala)</h4><p>Flink支持POJO作为符合类型。决定POJO规则的文档请参考<a target="_blank" rel="noopener" href="https://ci.apache.org/projects/flink/flink-docs-release-1.6/dev/api_concepts.html#pojos">这里</a></p>
<p>当将一个POJO类型的DataStream或者DataSet转换为Table而不指定字段名称时，Table的字段名称将采用JOPO原生的字段名称作为字段名称。<br>重命名原始的POJO字段需要关键字AS，因为POJO没有固定的顺序，名称映射需要原始名称并且不能通过位置来完成。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 获取(创建)TableEnvironment对象</span></span><br><span class="line"><span class="keyword">val</span> tableEnv = <span class="type">TableEnvironment</span>.getTableEnvironment(env)</span><br><span class="line"></span><br><span class="line"><span class="comment">// Person 是一个有两个字段&quot;name&quot;和&quot;age&quot;的POJO</span></span><br><span class="line"><span class="keyword">val</span> stream: <span class="type">DataStream</span>[<span class="type">Person</span>] = ...</span><br><span class="line"></span><br><span class="line"><span class="comment">// 将 DataStream 转换为带字段 &quot;age&quot;, &quot;name&quot; 的Table(字段通过名称进行排序)</span></span><br><span class="line"><span class="keyword">val</span> table: <span class="type">Table</span> = tableEnv.fromDataStream(stream)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 将DataStream转换为重命名为&quot;myAge&quot;, &quot;myName&quot;的Table(基于名称)</span></span><br><span class="line"><span class="keyword">val</span> table: <span class="type">Table</span> = tableEnv.fromDataStream(stream, <span class="symbol">&#x27;age</span> as <span class="symbol">&#x27;myAge</span>, <span class="symbol">&#x27;name</span> as <span class="symbol">&#x27;myName</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 将带映射字段&#x27;name的DataStream转换为Table(基于名称)</span></span><br><span class="line"><span class="keyword">val</span> table: <span class="type">Table</span> = tableEnv.fromDataStream(stream, <span class="symbol">&#x27;name</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 将带映射字段&#x27;name并重命名为&#x27;myName的DataStream转换为Table(基于名称)</span></span><br><span class="line"><span class="keyword">val</span> table: <span class="type">Table</span> = tableEnv.fromDataStream(stream, <span class="symbol">&#x27;name</span> as <span class="symbol">&#x27;myName</span>)</span><br></pre></td></tr></table></figure>
<h4 id="Row"><a href="#Row" class="headerlink" title="Row"></a>Row</h4><p>Row数据类型可以支持任意数量的字段，并且这些字段支持null值。当进行Row DataStream或Row DataSet<br>转换为Table时可以通过RowTypeInfo来指定字段的名称。Row Type支持基于位置和名称的两种映射方式。<br>通过提供所有字段的名称可以进行字段的重命名(基于位置)，或者是单独选择列来进行映射/重排序/重命名(基于名称)</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 获取(创建)TableEnvironment对象</span></span><br><span class="line"><span class="keyword">val</span> tableEnv = <span class="type">TableEnvironment</span>.getTableEnvironment(env)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 在`RowTypeInfo`中指定字段&quot;name&quot; 和 &quot;age&quot;的Row类型DataStream</span></span><br><span class="line"><span class="keyword">val</span> stream: <span class="type">DataStream</span>[<span class="type">Row</span>] = ...</span><br><span class="line"></span><br><span class="line"><span class="comment">// 将 DataStream 转换为带默认字段 &quot;age&quot;, &quot;name&quot; 的Table</span></span><br><span class="line"><span class="keyword">val</span> table: <span class="type">Table</span> = tableEnv.fromDataStream(stream)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 将 DataStream 转换为重命名字段 &#x27;myName, &#x27;myAge 的Table(基于位置)</span></span><br><span class="line"><span class="keyword">val</span> table: <span class="type">Table</span> = tableEnv.fromDataStream(stream, <span class="symbol">&#x27;myName</span>, <span class="symbol">&#x27;myAge</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 将 DataStream 转换为重命名字段 &#x27;myName, &#x27;myAge 的Table(基于名称)</span></span><br><span class="line"><span class="keyword">val</span> table: <span class="type">Table</span> = tableEnv.fromDataStream(stream, <span class="symbol">&#x27;name</span> as <span class="symbol">&#x27;myName</span>, <span class="symbol">&#x27;age</span> as <span class="symbol">&#x27;myAge</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 将 DataStream 转换为映射字段 &#x27;name的Table(基于名称)</span></span><br><span class="line"><span class="keyword">val</span> table: <span class="type">Table</span> = tableEnv.fromDataStream(stream, <span class="symbol">&#x27;name</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 将 DataStream 转换为映射字段 &#x27;name并重命名为&#x27;myName的Table(基于名称)</span></span><br><span class="line"><span class="keyword">val</span> table: <span class="type">Table</span> = tableEnv.fromDataStream(stream, <span class="symbol">&#x27;name</span> as <span class="symbol">&#x27;myName</span>)</span><br></pre></td></tr></table></figure>
<h4 id="Query-Optimization"><a href="#Query-Optimization" class="headerlink" title="Query Optimization"></a>Query Optimization</h4><p>Apache Flink 基于 Apache Calcite 来做转换和查询优化。当前的查询优化包括投影、过滤下推、<br>相关子查询和各种相关的查询重写。Flink不去做join优化，但是会让他们去顺序执行(FROM子句中表的顺序或者WHERE子句中连接谓词的顺序)</p>
<p>可以通过提供一个CalciteConfig对象来调整在不同阶段应用的优化规则集，<br>这个可以通过调用CalciteConfig.createBuilder())获得的builder来创建，<br>并且可以通过调用tableEnv.getConfig.setCalciteConfig(calciteConfig)来提供给TableEnvironment。</p>
<h4 id="Explaining-a-Table"><a href="#Explaining-a-Table" class="headerlink" title="Explaining a Table"></a>Explaining a Table</h4><p>Table API为计算Table提供了一个机制来解析逻辑和优化查询计划，这个可以通过TableEnvironment.explain(table)<br>来完成。它返回描述三个计划的字符串信息：</p>
<ul>
<li>关联查询抽象语法树，即未优化过的逻辑执行计划</li>
<li>优化过的逻辑执行计划</li>
<li>物理执行计划</li>
</ul>
<p>下面的实例展示了相应的输出：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line"><span class="keyword">val</span> tEnv = <span class="type">TableEnvironment</span>.getTableEnvironment(env)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> table1 = env.fromElements((<span class="number">1</span>, <span class="string">&quot;hello&quot;</span>)).toTable(tEnv, <span class="symbol">&#x27;count</span>, <span class="symbol">&#x27;word</span>)</span><br><span class="line"><span class="keyword">val</span> table2 = env.fromElements((<span class="number">1</span>, <span class="string">&quot;hello&quot;</span>)).toTable(tEnv, <span class="symbol">&#x27;count</span>, <span class="symbol">&#x27;word</span>)</span><br><span class="line"><span class="keyword">val</span> table = table1</span><br><span class="line">  .where(<span class="symbol">&#x27;word</span>.like(<span class="string">&quot;F%&quot;</span>))</span><br><span class="line">  .unionAll(table2)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> explanation: <span class="type">String</span> = tEnv.explain(table)</span><br><span class="line">println(explanation)</span><br></pre></td></tr></table></figure>
<p>对应的输出如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">&#x3D;&#x3D; 抽象语法树 &#x3D;&#x3D;</span><br><span class="line">LogicalUnion(all&#x3D;[true])</span><br><span class="line">  LogicalFilter(condition&#x3D;[LIKE($1, &#39;F%&#39;)])</span><br><span class="line">    LogicalTableScan(table&#x3D;[[_DataStreamTable_0]])</span><br><span class="line">  LogicalTableScan(table&#x3D;[[_DataStreamTable_1]])</span><br><span class="line"></span><br><span class="line">&#x3D;&#x3D; 优化后的逻辑执行计划 &#x3D;&#x3D;</span><br><span class="line">DataStreamUnion(union&#x3D;[count, word])</span><br><span class="line">  DataStreamCalc(select&#x3D;[count, word], where&#x3D;[LIKE(word, &#39;F%&#39;)])</span><br><span class="line">    DataStreamScan(table&#x3D;[[_DataStreamTable_0]])</span><br><span class="line">  DataStreamScan(table&#x3D;[[_DataStreamTable_1]])</span><br><span class="line"></span><br><span class="line">&#x3D;&#x3D; 物理执行计划 &#x3D;&#x3D;</span><br><span class="line">Stage 1 : Data Source</span><br><span class="line">  content : collect elements with CollectionInputFormat</span><br><span class="line"></span><br><span class="line">Stage 2 : Data Source</span><br><span class="line">  content : collect elements with CollectionInputFormat</span><br><span class="line"></span><br><span class="line">  Stage 3 : Operator</span><br><span class="line">    content : from: (count, word)</span><br><span class="line">    ship_strategy : REBALANCE</span><br><span class="line"></span><br><span class="line">    Stage 4 : Operator</span><br><span class="line">      content : where: (LIKE(word, &#39;F%&#39;)), select: (count, word)</span><br><span class="line">      ship_strategy : FORWARD</span><br><span class="line"></span><br><span class="line">      Stage 5 : Operator</span><br><span class="line">        content : from: (count, word)</span><br><span class="line">        ship_strategy : REBALANCE</span><br></pre></td></tr></table></figure>

<h1 id="Flink用户自定义函数"><a href="#Flink用户自定义函数" class="headerlink" title="Flink用户自定义函数"></a>Flink用户自定义函数</h1><p>用户自定义函数是非常重要的一个特征，因为他极大地扩展了查询的表达能力。</p>
<p>在大多数场景下，用户自定义函数在使用之前是必须要注册的。对于Scala的Table API，udf是不需要注册的。<br>调用TableEnvironment的registerFunction()方法来实现注册。Udf注册成功之后，会被插入TableEnvironment的function catalog，这样table API和sql就能解析他了。<br>本文会主要讲三种udf：</p>
<ul>
<li>ScalarFunction</li>
<li>TableFunction</li>
<li>AggregateFunction</li>
</ul>
<h2 id="1-Scalar-Functions-标量函数"><a href="#1-Scalar-Functions-标量函数" class="headerlink" title="1. Scalar Functions 标量函数"></a>1. Scalar Functions 标量函数</h2><p>标量函数，是指指返回一个值的函数。标量函数是实现讲0，1，或者多个标量值转化为一个新值。</p>
<p>实现一个标量函数需要继承ScalarFunction，并且实现一个或者多个evaluation方法。标量函数的行为就是通过evaluation方法来实现的。evaluation方法必须定义为public，命名为eval。evaluation方法的输入参数类型和返回值类型决定着标量函数的输入参数类型和返回值类型。evaluation方法也可以被重载实现多个eval。同时evaluation方法支持变参数，例如：eval(String… strs)。</p>
<p>下面给出一个标量函数的例子。例子实现的事一个hashcode方法。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">HashCode</span> <span class="keyword">extends</span> <span class="title">ScalarFunction</span> </span>&#123;</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">int</span> factor = <span class="number">12</span>;</span><br><span class="line">  </span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="title">HashCode</span><span class="params">(<span class="keyword">int</span> factor)</span> </span>&#123;</span><br><span class="line">      <span class="keyword">this</span>.factor = factor;</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">eval</span><span class="params">(String s)</span> </span>&#123;</span><br><span class="line">      <span class="keyword">return</span> s.hashCode() * factor;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">BatchTableEnvironment tableEnv = TableEnvironment.getTableEnvironment(env);</span><br><span class="line"></span><br><span class="line"><span class="comment">// register the function</span></span><br><span class="line">tableEnv.registerFunction(<span class="string">&quot;hashCode&quot;</span>, <span class="keyword">new</span> HashCode(<span class="number">10</span>));</span><br><span class="line"></span><br><span class="line"><span class="comment">// use the function in Java Table API</span></span><br><span class="line">myTable.select(<span class="string">&quot;string, string.hashCode(), hashCode(string)&quot;</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// use the function in SQL API</span></span><br><span class="line">tableEnv.sqlQuery(<span class="string">&quot;SELECT string, HASHCODE(string) FROM MyTable&quot;</span>);</span><br><span class="line"></span><br><span class="line">````</span><br><span class="line"></span><br><span class="line">默认情况下evaluation方法的返回值类型是由flink类型抽取工具决定。对于基础类型，简单的POJOS是足够的，但是更复杂的类型，自定义类型，组合类型，会报错。这种情况下，返回值类型的TypeInformation，需要手动指定，方法是重载</span><br><span class="line">ScalarFunction#getResultType()。</span><br><span class="line"></span><br><span class="line">下面给一个例子，通过复写ScalarFunction#getResultType()，将long型的返回值在代码生成的时候翻译成Types.TIMESTAMP。</span><br><span class="line"></span><br><span class="line">```java</span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">TimestampModifier</span> <span class="keyword">extends</span> <span class="title">ScalarFunction</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">long</span> <span class="title">eval</span><span class="params">(<span class="keyword">long</span> t)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> t % <span class="number">1000</span>;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">public</span> TypeInformation&lt;?&gt; getResultType(signature: Class&lt;?&gt;[]) &#123;</span><br><span class="line">    <span class="keyword">return</span> Types.TIMESTAMP;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="2-Table-Functions-表函数"><a href="#2-Table-Functions-表函数" class="headerlink" title="2. Table Functions 表函数"></a>2. Table Functions 表函数</h2><p>与标量函数相似之处是输入可以0，1，或者多个参数，但是不同之处可以输出任意数目的行数。返回的行也可以包含一个或者多个列。</p>
<p>为了自定义表函数，需要继承TableFunction，实现一个或者多个evaluation方法。表函数的行为定义在这些evaluation方法内部，函数名为eval并且必须是public。TableFunction可以重载多个eval方法。Evaluation方法的输入参数类型，决定着表函数的输入类型。Evaluation方法也支持变参，例如：eval(String… strs)。返回表的类型取决于TableFunction的基本类型。Evaluation方法使用collect(T)发射输出的rows。</p>
<p>在Table API中，表函数在scala语言中使用方法如下：.join(Expression) 或者 .leftOuterJoin(Expression)，在java语言中使用方法如下：.join(String) 或者.leftOuterJoin(String)。</p>
<p>Join操作算子会使用表值函数(操作算子右边的表)产生的所有行进行(cross) join 外部表(操作算子左边的表)的每一行。</p>
<p>leftOuterJoin操作算子会使用表值函数(操作算子右边的表)产生的所有行进行(cross) join 外部表(操作算子左边的表)的每一行，并且在表函数返回一个空表的情况下会保留所有的outer rows。</p>
<p>在sql语法中稍微有点区别：<br>cross join用法是LATERAL TABLE(<TableFunction>)。<br>LEFT JOIN用法是在join条件中加入ON TRUE。</p>
<p>下面的理智讲的是如何使用表值函数。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// The generic type &quot;Tuple2&lt;String, Integer&gt;&quot; determines the schema of the returned table as (String, Integer).</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Split</span> <span class="keyword">extends</span> <span class="title">TableFunction</span>&lt;<span class="title">Tuple2</span>&lt;<span class="title">String</span>, <span class="title">Integer</span>&gt;&gt; </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> String separator = <span class="string">&quot; &quot;</span>;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">Split</span><span class="params">(String separator)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.separator = separator;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">eval</span><span class="params">(String str)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">for</span> (String s : str.split(separator)) &#123;</span><br><span class="line">            <span class="comment">// use collect(...) to emit a row</span></span><br><span class="line">            collect(<span class="keyword">new</span> Tuple2&lt;String, Integer&gt;(s, s.length()));</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">BatchTableEnvironment tableEnv = TableEnvironment.getTableEnvironment(env);</span><br><span class="line">Table myTable = ...         <span class="comment">// table schema: [a: String]</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Register the function.</span></span><br><span class="line">tableEnv.registerFunction(<span class="string">&quot;split&quot;</span>, <span class="keyword">new</span> Split(<span class="string">&quot;#&quot;</span>));</span><br><span class="line"></span><br><span class="line"><span class="comment">// Use the table function in the Java Table API. &quot;as&quot; specifies the field names of the table.</span></span><br><span class="line">myTable.join(<span class="string">&quot;split(a) as (word, length)&quot;</span>).select(<span class="string">&quot;a, word, length&quot;</span>);</span><br><span class="line">myTable.leftOuterJoin(<span class="string">&quot;split(a) as (word, length)&quot;</span>).select(<span class="string">&quot;a, word, length&quot;</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// Use the table function in SQL with LATERAL and TABLE keywords.</span></span><br><span class="line">join.md</span><br><span class="line">tableEnv.sqlQuery(<span class="string">&quot;SELECT a, word, length FROM MyTable, LATERAL TABLE(split(a)) as T(word, length)&quot;</span>);</span><br><span class="line"><span class="comment">// LEFT JOIN a table function (equivalent to &quot;leftOuterJoin&quot; in Table API).</span></span><br><span class="line">tableEnv.sqlQuery(<span class="string">&quot;SELECT a, word, length FROM MyTable LEFT JOIN LATERAL TABLE(split(a)) as T(word, length) ON TRUE&quot;</span>);</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>需要注意的是PROJO类型不需要一个确定的字段顺序。意味着你不能使用as修改表函数返回的pojo的字段的名字。</p>
<p>默认情况下TableFunction返回值类型是由flink类型抽取工具决定。对于基础类型，简单的POJOS是足够的，但是更复杂的类型，自定义类型，组合类型，会报错。这种情况下，返回值类型的TypeInformation，需要手动指定，方法是重载<br>TableFunction#getResultType()。</p>
<p>下面的例子，我们通过复写TableFunction#getResultType()方法使得表返回类型是RowTypeInfo(String, Integer)。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CustomTypeSplit</span> <span class="keyword">extends</span> <span class="title">TableFunction</span>&lt;<span class="title">Row</span>&gt; </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">eval</span><span class="params">(String str)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">for</span> (String s : str.split(<span class="string">&quot; &quot;</span>)) &#123;</span><br><span class="line">            Row row = <span class="keyword">new</span> Row(<span class="number">2</span>);</span><br><span class="line">            row.setField(<span class="number">0</span>, s);</span><br><span class="line">            row.setField(<span class="number">1</span>, s.length);</span><br><span class="line">            collect(row);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> TypeInformation&lt;Row&gt; <span class="title">getResultType</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> Types.ROW(Types.STRING(), Types.INT());</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="3-Aggregation-Functions-聚合函数"><a href="#3-Aggregation-Functions-聚合函数" class="headerlink" title="3. Aggregation Functions 聚合函数"></a>3. Aggregation Functions 聚合函数</h2><p>用户自定义聚合函数聚合一张表(一行或者多行，一行有一个或者多个属性)为一个标量的值。</p>
<p>上图中是讲的一张饮料的表这个表有是那个字段五行数据，现在要做的事求出所有饮料的最高价。</p>
<p>聚合函数需要继承AggregateFunction。聚合函数工作方式如下：<br>首先，需要一个accumulator，这个是保存聚合中间结果的数据结构。调用AggregateFunction函数的createAccumulator()方法来创建一个空的accumulator.<br>随后，每个输入行都会调用accumulate()方法来更新accumulator。一旦所有的行被处理了，getValue()方法就会被调用，计算和返回最终的结果。</p>
<p>对于每个AggregateFunction，下面三个方法都是比不可少的：<br>createAccumulator()<br>accumulate()<br>getValue()</p>
<p>flink的类型抽取机制不能识别复杂的数据类型，比如，数据类型不是基础类型或者简单的pojos类型。所以，类似于ScalarFunction 和TableFunction，AggregateFunction提供了方法去指定返回结果类型的TypeInformation，用的是AggregateFunction#getResultType()。Accumulator类型用的是AggregateFunction#getAccumulatorType()。</p>
<p>除了上面的方法，这里有一些可选的方法。尽管有些方法是让系统更加高效的执行查询，另外的一些在特定的场景下是必须的。例如，merge()方法在会话组窗口上下文中是必须的。当一行数据是被视为跟两个回话窗口相关的时候，两个会话窗口的accumulators需要被join。</p>
<p>AggregateFunction的下面几个方法，根据使用场景的不同需要被实现：<br>retract()：在bounded OVER窗口的聚合方法中是需要实现的。<br>merge()：在很多batch 聚合和会话窗口聚合是必须的。<br>resetAccumulator(): 在大多数batch聚合是必须的。</p>
<p>AggregateFunction的所有方法都是需要被声明为public，而不是static。定义聚合函数需要实现org.apache.flink.table.functions.AggregateFunction同时需要实现一个或者多个accumulate方法。该方法可以被重载为不同的数据类型，并且支持变参。</p>
<p>在这里就不贴出来AggregateFunction的源码了。</p>
<p>下面举个求加权平均的栗子<br>为了计算加权平均值，累加器需要存储已累积的所有数据的加权和及计数。在栗子中定义一个WeightedAvgAccum类作为accumulator。尽管，retract(), merge(), 和resetAccumulator()方法在很多聚合类型是不需要的，这里也给出了栗子。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Accumulator for WeightedAvg.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">WeightedAvgAccum</span> </span>&#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">long</span> sum = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">int</span> count = <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Weighted Average user-defined aggregate function.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">WeightedAvg</span> <span class="keyword">extends</span> <span class="title">AggregateFunction</span>&lt;<span class="title">Long</span>, <span class="title">WeightedAvgAccum</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> WeightedAvgAccum <span class="title">createAccumulator</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> WeightedAvgAccum();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Long <span class="title">getValue</span><span class="params">(WeightedAvgAccum acc)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (acc.count == <span class="number">0</span>) &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="keyword">return</span> acc.sum / acc.count;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">accumulate</span><span class="params">(WeightedAvgAccum acc, <span class="keyword">long</span> iValue, <span class="keyword">int</span> iWeight)</span> </span>&#123;</span><br><span class="line">        acc.sum += iValue * iWeight;</span><br><span class="line">        acc.count += iWeight;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">retract</span><span class="params">(WeightedAvgAccum acc, <span class="keyword">long</span> iValue, <span class="keyword">int</span> iWeight)</span> </span>&#123;</span><br><span class="line">        acc.sum -= iValue * iWeight;</span><br><span class="line">        acc.count -= iWeight;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">merge</span><span class="params">(WeightedAvgAccum acc, Iterable&lt;WeightedAvgAccum&gt; it)</span> </span>&#123;</span><br><span class="line">        Iterator&lt;WeightedAvgAccum&gt; iter = it.iterator();</span><br><span class="line">        <span class="keyword">while</span> (iter.hasNext()) &#123;</span><br><span class="line">            WeightedAvgAccum a = iter.next();</span><br><span class="line">            acc.count += a.count;</span><br><span class="line">            acc.sum += a.sum;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">resetAccumulator</span><span class="params">(WeightedAvgAccum acc)</span> </span>&#123;</span><br><span class="line">        acc.count = <span class="number">0</span>;</span><br><span class="line">        acc.sum = <span class="number">0L</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// register function</span></span><br><span class="line">StreamTableEnvironment tEnv = ...</span><br><span class="line">tEnv.registerFunction(<span class="string">&quot;wAvg&quot;</span>, <span class="keyword">new</span> WeightedAvg());</span><br><span class="line"></span><br><span class="line"><span class="comment">// use function</span></span><br><span class="line">tEnv.sqlQuery(<span class="string">&quot;SELECT user, wAvg(points, level) AS avgPoints FROM userScores GROUP BY user&quot;</span>);</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="4-实现udf的最佳实践经验"><a href="#4-实现udf的最佳实践经验" class="headerlink" title="4. 实现udf的最佳实践经验"></a>4. 实现udf的最佳实践经验</h2><p>Table API和SQL 代码生成器内部会尽可能多的尝试使用原生值。用户定义的函数可能通过对象创建、强制转换(casting)和拆装箱((un)boxing)引入大量开销。因此，强烈推荐参数和返回值的类型定义为原生类型而不是他们包装类型(boxing class)。Types.DATE 和Types.TIME可以用int代替。Types.TIMESTAMP可以用long代替。</p>
<p>我们建议用户自定义函数使用java编写而不是scala编写，因为scala的类型可能会有不被flink类型抽取器兼容。</p>
<p>用Runtime集成UDFs</p>
<p>有时候udf需要获取全局runtime信息或者在进行实际工作之前做一些设置和清除工作。Udf提供了open()和close()方法，可以被复写，功能类似Dataset和DataStream API的RichFunction方法。</p>
<p>Open()方法是在evaluation方法调用前调用一次。Close()是在evaluation方法最后一次调用后调用。</p>
<p>Open()方法提共一个FunctionContext，FunctionContext包含了udf执行环境的上下文，比如，metric group，分布式缓存文件，全局的job参数。</p>
<p>通过调用FunctionContext的相关方法，可以获取到相关的信息：</p>
<p>方法描述</p>
<ul>
<li>getMetricGroup() - 并行子任务的指标组</li>
<li>getCachedFile(name) -分布式缓存文件的本地副本</li>
<li>getJobParameter(name, defaultValue) - 给定key全局job参数。</li>
</ul>
<p>下面，给出的例子就是通过FunctionContext在一个标量函数中获取全局job的参数。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">HashCode</span> <span class="keyword">extends</span> <span class="title">ScalarFunction</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">int</span> factor = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">open</span><span class="params">(FunctionContext context)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="comment">// access &quot;hashcode_factor&quot; parameter</span></span><br><span class="line">        <span class="comment">// &quot;12&quot; would be the default value if parameter does not exist</span></span><br><span class="line">        factor = Integer.valueOf(context.getJobParameter(<span class="string">&quot;hashcode_factor&quot;</span>, <span class="string">&quot;12&quot;</span>)); </span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">eval</span><span class="params">(String s)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> s.hashCode() * factor;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">BatchTableEnvironment tableEnv = TableEnvironment.getTableEnvironment(env);</span><br><span class="line"></span><br><span class="line"><span class="comment">// set job parameter</span></span><br><span class="line">Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">conf.setString(<span class="string">&quot;hashcode_factor&quot;</span>, <span class="string">&quot;31&quot;</span>);</span><br><span class="line">env.getConfig().setGlobalJobParameters(conf);</span><br><span class="line"></span><br><span class="line"><span class="comment">// register the function</span></span><br><span class="line">tableEnv.registerFunction(<span class="string">&quot;hashCode&quot;</span>, <span class="keyword">new</span> HashCode());</span><br><span class="line"></span><br><span class="line"><span class="comment">// use the function in Java Table API</span></span><br><span class="line">myTable.select(<span class="string">&quot;string, string.hashCode(), hashCode(string)&quot;</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// use the function in SQL</span></span><br><span class="line">tableEnv.sqlQuery(<span class="string">&quot;SELECT string, HASHCODE(string) FROM MyTable&quot;</span>);</span><br></pre></td></tr></table></figure>
<h1 id="内置函数"><a href="#内置函数" class="headerlink" title="内置函数"></a>内置函数</h1><h2 id="scala"><a href="#scala" class="headerlink" title="scala"></a>scala</h2><h3 id="三元运算符"><a href="#三元运算符" class="headerlink" title="三元运算符"></a>三元运算符</h3><p>sql或者table API筛选数据，必须保证每个字段不为空，<br>Flink内部，中间结果都是通过case class传递，而case class的字段必须保证不能为空</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">BOOLEAN</span>.?(<span class="type">VALUE1</span>, <span class="type">VALUE2</span>)</span><br><span class="line"><span class="symbol">&#x27;is_active_user</span>.isNull.?(<span class="string">&quot;0&quot;</span>, <span class="string">&quot;1&quot;</span>)</span><br></pre></td></tr></table></figure>
<h3 id="等值判断"><a href="#等值判断" class="headerlink" title="等值判断"></a>等值判断</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="symbol">&#x27;Fuin</span> === <span class="symbol">&#x27;active_user</span></span><br></pre></td></tr></table></figure>
<p>scala中的<code>===</code>是运算符重构</p>
<ol>
<li><a target="_blank" rel="noopener" href="https://www.cnblogs.com/Springmoon-venn/p/11826359.html">Flink Table Api &amp; SQL 翻译目录</a></li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://example.com/bigdata/Flink/StateManagement/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="aaronzhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Guadazi">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/bigdata/Flink/StateManagement/" class="post-title-link" itemprop="url">Flink状态管理</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2019-08-10 14:58:00" itemprop="dateCreated datePublished" datetime="2019-08-10T14:58:00+08:00">2019-08-10</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-04-19 15:02:11" itemprop="dateModified" datetime="2021-04-19T15:02:11+08:00">2021-04-19</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/bigdata/" itemprop="url" rel="index"><span itemprop="name">bigdata</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="state-management"><a href="#state-management" class="headerlink" title="state-management"></a>state-management</h1><h2 id="org-apache-flink-streaming-api-checkpoint-CheckpointedFunction"><a href="#org-apache-flink-streaming-api-checkpoint-CheckpointedFunction" class="headerlink" title="org.apache.flink.streaming.api.checkpoint.CheckpointedFunction"></a>org.apache.flink.streaming.api.checkpoint.CheckpointedFunction</h2><ul>
<li>CheckpointedFunction是stateful transformation functions的核心接口，用于跨stream维护state<ul>
<li>snapshotState 在checkpoint的时候会被调用，用于snapshot state，通常用于flush、commit、synchronize外部系统</li>
<li>initializeState 在parallel function初始化的时候(<strong>第一次初始化或者从前一次checkpoint recover的时候</strong>)被调用，通常用来初始化state，以及处理state recovery的逻辑</li>
</ul>
</li>
</ul>
<p>从checkpoint中恢复数据时，需要判断snapshot当前的情况，</p>
<p>FunctionSnapshotContext实现了ManagedSnapshotContext, 父类中的方法: <code>getCheckpointId</code>,<code>getCheckpointTimestamp</code><br>FunctionInitializationContext实现了ManagedInitializationContext接口, 实现了<code>isRestored</code>、<code>getOperatorStateStore</code>、<code>getKeyedStateStore</code>方法</p>
<p>在初始化容器之后，我们使用上下文的<code>isrestore()</code>方法检查失败后是否正在恢复。如果是true，即正在恢复，则应用恢复逻辑。</p>
<blockquote>
<p>样例: HBase写入OutPutFormat</p>
</blockquote>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br></pre></td><td class="code"><pre><span class="line"> <span class="class"><span class="keyword">class</span> <span class="title">PortraitOutputFormat</span> <span class="keyword">extends</span> <span class="title">RichOutputFormat</span>&lt;<span class="title">EventItem</span>&gt; <span class="keyword">implements</span> <span class="title">CheckpointedFunction</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 输出阈值，批量写入的条数</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">int</span> threshold;</span><br><span class="line">    <span class="comment">// 维护在状态中的数据</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">transient</span> ListState&lt;EventItem&gt; checkpointState;</span><br><span class="line">    <span class="comment">// 内存中的数据</span></span><br><span class="line">    <span class="keyword">private</span> List&lt;EventItem&gt; bufferedEventItem;</span><br><span class="line">    <span class="comment">// HBase客户端</span></span><br><span class="line">    <span class="keyword">private</span> HBaseClient hbaseClient;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">PortraitOutputFormat</span><span class="params">(HBaseClient hbaseClient)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.hbaseClient = hbaseClient;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">    * checkpoint时调用</span></span><br><span class="line"><span class="comment">    * 执行snapshot操作，将内存中的数据写入到内存</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">snapshotState</span><span class="params">(FunctionSnapshotContext functionSnapshotContext)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        checkpointState.clear();</span><br><span class="line">        <span class="keyword">for</span> (EventItem eventItem : bufferedEventItem) &#123;</span><br><span class="line">            checkpointState.add(eventItem);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">    * 创建state，判断是否存在需要恢复的状态，如果有则需要恢复到bufferedEventItem</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">initializeState</span><span class="params">(FunctionInitializationContext context)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        ListStateDescriptor&lt;EventItem&gt; descriptor = <span class="keyword">new</span> ListStateDescriptor&lt;&gt;(<span class="string">&quot;buf-p&quot;</span>, EventItem.class);</span><br><span class="line">        checkpointState = context.getOperatorStateStore().getListState(descriptor);</span><br><span class="line">        <span class="keyword">if</span> (context.isRestored()) &#123;</span><br><span class="line">            <span class="keyword">for</span> (EventItem eventItem : checkpointState.get()) &#123;</span><br><span class="line">                bufferedEventItem.add(eventItem);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">    *</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">configure</span><span class="params">(Configuration configuration)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">    *</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">open</span><span class="params">(<span class="keyword">int</span> taskNumber, <span class="keyword">int</span> numTasks)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">    * 将新消息写入到缓存bufferedEventItem，缓存个数大约threshold,则执行sink写入，然后清空bufferedEventItem</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">writeRecord</span><span class="params">(EventItem value)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (value.getAttachUserId() == <span class="keyword">null</span>) &#123;</span><br><span class="line">            <span class="keyword">return</span>;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        bufferedEventItem.add(value);</span><br><span class="line">        <span class="keyword">int</span> size = bufferedEventItem.size();</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (size &gt;= threshold) &#123;</span><br><span class="line">            List&lt;Put&gt; puts = bufferedEventItem</span><br><span class="line">                    .stream()</span><br><span class="line">                    .map(eventItem -&gt; &#123;</span><br><span class="line">                        String rowKey1 = portraitDataGenerator.rowKey(eventItem);</span><br><span class="line">                        Map&lt;String, String&gt; data = portraitDataGenerator.data(eventItem);</span><br><span class="line">                        Put put = <span class="keyword">new</span> Put(rowKey1.getBytes());</span><br><span class="line">                        <span class="keyword">for</span> (String cfc : data.keySet()) &#123;</span><br><span class="line">                            String[] cfcs = cfc.split(<span class="string">&quot;:&quot;</span>);</span><br><span class="line">                            String cf = cfcs[<span class="number">0</span>];</span><br><span class="line">                            String c = cfcs[<span class="number">1</span>];</span><br><span class="line">                            String dataOne = data.get(cfc);</span><br><span class="line">                            put.addColumn(cf.getBytes(), c.getBytes(), dataOne.getBytes());</span><br><span class="line">                        &#125;</span><br><span class="line">                        <span class="keyword">return</span> put;</span><br><span class="line">                    &#125;)</span><br><span class="line">                    .collect(Collectors.toList());</span><br><span class="line">            <span class="keyword">try</span> &#123;</span><br><span class="line">                hbaseClient.putAndFlush(puts);</span><br><span class="line">            &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">                e.printStackTrace();</span><br><span class="line">            &#125;</span><br><span class="line">            bufferedEventItem.clear();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">    *</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">close</span><span class="params">()</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (hTable != <span class="keyword">null</span>) &#123;</span><br><span class="line">            hTable.flushCommits();</span><br><span class="line">            hTable.close();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> (connection != <span class="keyword">null</span>) &#123;</span><br><span class="line">            connection.close();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>




<h2 id="org-apache-flink-runtime-state-CheckpointListener"><a href="#org-apache-flink-runtime-state-CheckpointListener" class="headerlink" title="org.apache.flink.runtime.state.CheckpointListener"></a>org.apache.flink.runtime.state.CheckpointListener</h2><p>一旦所有checkpoint参与者确认完全，该接口必须由想要接收提交通知的功能/操作来实现。</p>
<h1 id="TTL"><a href="#TTL" class="headerlink" title="TTL"></a>TTL</h1><p>1.8 自动清理原理</p>
<p>Apache Flink的1.6.0版本引入了State TTL功能。它使流处理应用程序的开发人员配置过期时间，并在定义时间超时（Time to Live）之后进行清理。在Flink 1.8.0中，该功能得到了扩展，包括对RocksDB和堆状态后端（FSStateBackend和MemoryStateBackend）的历史数据进行持续清理，从而实现旧条目的连续清理过程（根据TTL设置）。</p>
<p>RocksDB后台压缩可以过滤掉过期状态<br>如果你的Flink应用程序使用RocksDB作为状态后端存储，则可以启用另一个基于Flink特定压缩过滤器的清理策略。RocksDB定期运行异步压缩以合并状态更新并减少存储。Flink压缩过滤器使用TTL检查状态条目的到期时间戳，并丢弃所有过期值。</p>
<p>激活此功能的第一步是通过设置以下Flink配置选项来配置RocksDB状态后端：</p>
<p>state.backend.rocksdb.ttl.compaction.filter.enabled</p>
<p>配置RocksDB状态后端后，将为状态启用压缩清理策略，如以下代码示例所示：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">StateTtlConfig ttlConfig &#x3D; StateTtlConfig</span><br><span class="line">    .newBuilder(Time.days(7))</span><br><span class="line">    .cleanupInRocksdbCompactFilter()</span><br><span class="line">    .build();</span><br></pre></td></tr></table></figure>






<p>【参考文献】</p>
<ol>
<li><a target="_blank" rel="noopener" href="https://www.jianshu.com/p/6ed0ef5e2b74">Flink Streaming状态处理（Working with State）</a></li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://example.com/bigdata/Flink/Flink-distributed-snapshot/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="aaronzhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Guadazi">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/bigdata/Flink/Flink-distributed-snapshot/" class="post-title-link" itemprop="url">Flink-分布式快照机制</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2019-08-10 14:58:00" itemprop="dateCreated datePublished" datetime="2019-08-10T14:58:00+08:00">2019-08-10</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-04-19 15:02:11" itemprop="dateModified" datetime="2021-04-19T15:02:11+08:00">2021-04-19</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/bigdata/" itemprop="url" rel="index"><span itemprop="name">bigdata</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="Flink-distributed-snapshot"><a href="#Flink-distributed-snapshot" class="headerlink" title="Flink-distributed-snapshot"></a>Flink-distributed-snapshot</h1><p>Flink 的快照机制主要是为了保障作业 failover 时不丢失状态. Flink 提供了一种轻量级的快照机制，不需要停止作业就可以帮助用户持久化内存中的状态数据. </p>
<p><img src="vx_images/121103515291"> </p>
<p>上图中的 <code>markers</code>（与 <code>barrier</code> 语义相同）通过流动来触发快照的制作，每一个编号都代表了一次快照，比如编号为 n 的 <code>markers</code> 从最上游流动到最下游就代表了一次快照的制作过程. 简述如下：</p>
<ul>
<li>系统发送编号为 n 的 <code>markers</code> 到最上游的算子，<code>markers</code> 随着数据往下游流动；</li>
<li>当下游算子收到 <code>marker</code> 后，就开始将自身的状态保存到共享存储中；</li>
<li>当所有最下游的算子接收到 <code>marker</code> 并完成算子快照后，本次作业的快照制作完成. </li>
</ul>
<p>一旦作业失败，重启时就可以从快照恢复. </p>
<p>下面为一个简单的 demo 说明（<code>barrier</code> 等同于 <code>marker</code>）. </p>
<p><img src="vx_images/108294726199"></p>
<ul>
<li><code>barrier</code> 到达 Source，将状态 offset=7 存储到共享存储；</li>
<li><code>barrier</code> 到达 Task，将状态 sum=21 存储到共享存储；</li>
<li><code>barrier</code> 到达 Sink，commit 本次快照，标志着快照的成功制作. </li>
</ul>
<p><img src="vx_images/96412168676"></p>
<p>这时候突然间作业也挂掉， 重启时 Flink 会通过快照恢复各个状态. Source 会将自身的 offset 置为 7，Task 会将自身的 sum 置为 21.<br>现在我们可以认为 1、2、3、4、5、6 这 6 个数字的加和结果并没有丢失. 这个时候，offset 从 7 开始消费，跟作业失败前完全对接了起来，确保了 exactly-once</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://example.com/Java/metric/Java%E7%94%9F%E4%BA%A7%E7%8E%AF%E5%A2%83%E4%B8%8B%E6%80%A7%E8%83%BD%E7%9B%91%E6%8E%A7%E4%B8%8E%E8%B0%83%E4%BC%98%E8%AF%A6%E8%A7%A3%E7%AC%94%E8%AE%B0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="aaronzhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Guadazi">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/Java/metric/Java%E7%94%9F%E4%BA%A7%E7%8E%AF%E5%A2%83%E4%B8%8B%E6%80%A7%E8%83%BD%E7%9B%91%E6%8E%A7%E4%B8%8E%E8%B0%83%E4%BC%98%E8%AF%A6%E8%A7%A3%E7%AC%94%E8%AE%B0/" class="post-title-link" itemprop="url">【转载】Java生产环境下性能监控与调优详解笔记</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2019-08-10 14:58:00" itemprop="dateCreated datePublished" datetime="2019-08-10T14:58:00+08:00">2019-08-10</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-04-19 15:02:11" itemprop="dateModified" datetime="2021-04-19T15:02:11+08:00">2021-04-19</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/java/" itemprop="url" rel="index"><span itemprop="name">java</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="Java生产环境下性能监控与调优详解笔记"><a href="#Java生产环境下性能监控与调优详解笔记" class="headerlink" title="Java生产环境下性能监控与调优详解笔记"></a>Java生产环境下性能监控与调优详解笔记</h1><p>另一个整理<a target="_blank" rel="noopener" href="http://alanhou.org/java-optimization/">http://alanhou.org/java-optimization/</a></p>
<h2 id="1：JVM字节码指令与-javap"><a href="#1：JVM字节码指令与-javap" class="headerlink" title="1：JVM字节码指令与 javap"></a>1：JVM字节码指令与 javap</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">javap &lt;options&gt; &lt;classes&gt;</span><br><span class="line"><span class="built_in">cd</span> monitor\_tuning/target/classes/org/alanhou/monitor\_tuning/chapter8/</span><br><span class="line">javap -verbose Test1.class &gt; Test1.txt</span><br></pre></td></tr></table></figure>
<p>即可保存字节码文件<br>会有三个部分组成<br>操作数栈<br>LineNumberTable<br>LocalVariableTable</p>
<p>i++和++i 的执行效果完全相同 多了一个压入栈顶操作<br>for(int i=0;i&lt;10;i++) {}<br>for(int i=0;i&lt;10;++i) {} 执行效果一样</p>
<p>2：</p>
<p>public static void f1() {<br>String src = “”;<br>for(int i=0;i&lt;10;i++) {<br>//每一次循环都会new一个StringBuilder 然后在src.append(“A”);<br>src = src + “A”;<br>}<br>System.out.println(src);<br>}<br>public static void f2() {<br>//只要一个StringBuilder<br>StringBuilder src = new StringBuilder();<br>for(int i=0;i&lt;10;i++) {<br>src.append(“A”);<br>}<br>System.out.println(src);<br>}</p>
<p>3：</p>
<p>public static String f1() {<br>String str = “hello”;<br>try{<br>return str;<br>}<br>finally{<br>str = “imooc”;<br>}<br>} 返回 hello 但会执行finally 中的代码</p>
<p>4：字符串拼接都会在编译阶段转换成stringbuilder</p>
<p>5:字符串去重</p>
<p>字符串在任何应用中都占用了大量的内存。尤其数包含独立UTF-16字符的char[]数组对JVM内存的消耗贡献最多——因为每个字符占用2位。</p>
<p>内存的30%被字符串消耗其实是很常见的，不仅是因为字符串是与我们互动的最好的格式，而且是由于流行的HTTP API使用了大量的字符串。使用Java 8 Update 20，我们现在可以接触到一个新特性，叫做字符串去重，该特性需要G1垃圾回收器，该垃圾回收器默认是被关闭的。</p>
<p>字符串去重利用了字符串内部实际是char数组，并且是final的特性，所以JVM可以任意的操纵他们。</p>
<p>对于字符串去重，开发者考虑了大量的策略，但最终的实现采用了下面的方式：</p>
<p>无论何时垃圾回收器访问了String对象，它会对char数组进行一个标记。它获取char数组的hash value并把它和一个对数组的弱引用存在一起。只要垃圾回收器发现另一个字符串，而这个字符串和char数组具有相同的hash code，那么就会对两者进行一个字符一个字符的比对。</p>
<p>如果他们恰好匹配，那么一个字符串就会被修改，指向第二个字符串的char数组。第一个char数组就不再被引用，也就可以被回收了。</p>
<p>这整个过程当然带来了一些开销，但是被很紧实的上限控制了。例如，如果一个字符未发现有重复，那么一段时间之内，它会不再被检查。</p>
<p>那么该特性实际上是怎么工作的呢？首先，你需要刚刚发布的Java 8 Update 20，然后按照这个配置: -Xmx256m -XX:+UseG1GC 去运行下列的代码:</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">LotsOfStrings</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> LinkedList LOTS_OF_STRINGS = <span class="keyword">new</span> LinkedList&lt;&gt;();</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> iteration = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line">            <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">100</span>; i++) &#123;</span><br><span class="line">                <span class="keyword">for</span> (<span class="keyword">int</span> j = <span class="number">0</span>; j &lt; <span class="number">1000</span>; j++) &#123;</span><br><span class="line">                    LOTS_OF_STRINGS.add(<span class="keyword">new</span> String(<span class="string">&quot;String &quot;</span> + j));</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">            iteration++;</span><br><span class="line">            System.out.println(<span class="string">&quot;Survived Iteration: &quot;</span> + iteration);</span><br><span class="line">            Thread.sleep(<span class="number">100</span>);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这段代码会执行30个迭代之后报OutOfMemoryError。</p>
<p>现在，开启字符串去重，使用如下配置去跑上述代码：</p>
<p>-Xmx256m -XX:+UseG1GC -XX:+UseStringDeduplication -XX:+PrintStringDeduplicationStatistics</p>
<p>此时它已经可以运行更长的时间，而且在50个迭代之后才终止。</p>
<p>6:</p>
<p>ArrayLIst  底层是数组  扩容会拷贝<br>hashmap   底层也是数组+ 链表 扩容 重新计算key 负载因子是 0.75  </p>
<p>linklist底层是双向链表<br>1. 尽量重用对象，不要循环创建对象，比如:for 循环字符串拼接(不在 for中使用+拼接，先new 一个StringBuilder再在 for 里 append)  </p>
<p>2. 容器类初始化的地时候指定长度  </p>
<p>List<String> collection = new ArrayLIst<String>(5);  </p>
<p>Map&lt;String, String&gt; map = new HashMap&lt;String, String&gt;(32);  </p>
<p>3. ArrayList（底层数组）随机遍历快，LinkedList（底层双向链表）添加删除快  </p>
<p>4. 集合遍历尽量减少重复计算  </p>
<p>5. 使用 Entry 遍历 Map可以同时取出key和value  </p>
<p>6. 大数组复制使用System.arraycopy 底层是native实现的  </p>
<p>7. 尽量使用基本类型而不是包装类型  </p>
<p>public class Test03 {  </p>
<p>  public static void main(String[] args) {<br>  Integer f1 = 100, f2 = 100, f3 = 150, f4 = 150;  </p>
<p>  System.out.println(f1 == f2);<br>  System.out.println(f3 == f4);<br>}<br>}  </p>
<p>如果不明就里很容易认为两个输出要么都是true要么都是false。首先需要注意的是f1、f2、f3、f4四个变量都是Integer对象引用，所以下面的==运算比较的不是值而是引用。装箱的本质是什么呢？当我们给一个Integer对象赋一个int值的时候，会调用Integer类的静态方法valueOf，如果看看valueOf的源代码就知道发生了什么。<br>public static Integer valueOf(int i) {<br>  if (i &gt;= IntegerCache.low &amp;&amp; i &lt;= IntegerCache.high)<br>    return IntegerCache.cache[i + (-IntegerCache.low)];<br>  return new Integer(i);<br>}<br>简单的说，如果整型字面量的值在-128到127之间，那么不会new新的Integer对象，而是直接引用常量池中的Integer对象，所以上面的面试题中f1==f2的结果是true，而f3==f4的结果是false。<br>8. 不要手动调用 System.gc()  </p>
<p>9. 及时消除过期对象的引用，防止内存泄漏<br>public string pop()<br>{<br>  string currentValue=object[size];<br>  //object[size]=null;如果不添加这句话就会造成内存泄漏<br>  size–;<br>  return currentValue;<br>}  </p>
<p>10. 尽量使用局部变量，减小变量的作用域 方便出了作用域尽快垃圾回收  </p>
<p>11. 尽量使用非同步的容器ArraryList vs. Vector  </p>
<p>12. 尽量减小同步作用范围, synchronized 方法 vs. 代码块  </p>
<p>public class SynchronizedTest {<br>  public static void main(String[] args) {<br>}<br>public synchronized void f1() {//在this對象上加鎖<br>  System.out.println(“f1”);<br>}<br>public  void f2() {//在this對象上加鎖<br>  synchronized(this) {<br>    System.out.println(“f2”);<br>  }<br>}<br>public static synchronized void f3() {//在类上加鎖<br>  System.out.println(“f3”);<br>}<br>public static void f4() {//在类上加鎖<br>  synchronized(SynchronizedTest.class) {<br>    System.out.println(“f4”);<br>  }<br>}<br>}  </p>
<p>13. 用ThreadLocal 缓存线程不安全的对象，SimpleDateFormat 缓存重量的对象避免重新构造<br>@SuppressWarnings(“rawtypes”)<br>    private static ThreadLocal threadLocal = new ThreadLocal() {<br>        protected synchronized Object initialValue() {<br>            return new SimpleDateFormat(DATE_FORMAT);<br>        }<br>    };  </p>
<p>14. 尽量使用延迟加载  </p>
<p>15. 尽量减少使用反射，必须用加缓存，反射比较影响性能  </p>
<p>16. 尽量使用连接池、线程池、对象池、缓存  </p>
<p>17. 及时释放资源， I/O 流、Socket、数据库连接  </p>
<p>18. 慎用异常，不要用抛异常来表示正常的业务逻辑，异常也是比较重的对象要记录堆栈信息  </p>
<p>19. String 操作尽量少用正则表达式 比如replaceAll是用正则 比较耗费性能 replace就不是用正则  </p>
<p>20. 日志输出注意使用不同的级别  </p>
<p>21. 日志中参数拼接使用占位符<br>log.info(“orderId:” + orderId); 不推荐 会用字符串拼接<br>log.info(“orderId:{}”, orderId); 推荐 用占位符 不会进行字符串拼接  </p>
<p>7：JVM的参数类型</p>
<p>标准参数（各版本中保持稳定）</p>
<p>-help</p>
<p>-server -client</p>
<p>-version -showversion</p>
<p>-cp -classpath</p>
<p>X 参数（非标准化参数）</p>
<p>-Xint：解释执行</p>
<p>-Xcomp：第一次使用就编译成本地代码</p>
<p>-Xmixed：混合模式，JVM 自己决定是否编译成本地代码</p>
<p>示例：</p>
<p>java -version（默认是混合模式）</p>
<p>Java HotSpot(TM) 64-Bit Server VM (build 25.40-b25, mixed mode)</p>
<p>java -Xint -version</p>
<p>Java HotSpot(TM) 64-Bit Server VM (build 25.40-b25, interpreted mode)</p>
<p>XX 参数（非标准化参数）</p>
<p>主要用于 JVM调优和 debug</p>
<ul>
<li>Boolean类型</li>
</ul>
<p>格式：-XX:[+-]<name>表示启用或禁用 name 属性<br>如：-XX:+UseConcMarkSweepGC<br>-XX:+UseG1GC</p>
<ul>
<li>非Boolean类型</li>
</ul>
<p>格式：-XX:<name>=<value>表示 name 属性的值是 value<br>如：-XX:MaxGCPauseMillis=500<br>-xx:GCTimeRatio=19<br>-Xmx -Xms属于 XX 参数<br>-Xms 等价于-XX:InitialHeapSize<br>-Xmx 等价于-XX:MaxHeapSize<br>-xss 等价于-XX:ThreadStackSize</p>
<h3 id="查看"><a href="#查看" class="headerlink" title="查看"></a>查看</h3><p>-XX:+PrintFlagsInitial 查看jvm初始值</p>
<p>-XX:+PrintFlagsFinal 查看jvm最终值</p>
<p>-XX:+UnlockExperimentalVMOptions 解锁实验参数</p>
<p>-XX:+UnlockDiagnosticVMOptions 解锁诊断参数</p>
<p>-XX:+PrintCommandLineFlags 打印命令行参数</p>
<p>输出结果中=表示默认值，:=表示被用户或 JVM 修改后的值</p>
<p>示例：java -XX:+PrintFlagsFinal -version</p>
<p>补充：测试中需要用到 Tomcat，CentOS 7安装示例如下</p>
<p><code>sudo </code>yum -y ``install java-1.8.0-openjdk*<br>wget  <a target="_blank" rel="noopener" href="http://mirror.bit.edu.cn/apache/tomcat/tomcat-8/v8.5.32/bin/apache-tomcat-8.5.32.tar.gz">http://mirror.bit.edu.cn/apache/tomcat/tomcat-8/v8.5.32/bin/apache-tomcat-8.5.32.tar.gz</a><br>tar -zxvf apache-tomcat-8.5.32.tar.gz<br>mv apache-tomcat-8.5.32 tomcat<br>cd tomcat/bin/sh startup.sh</p>
<p>pid 可通过类似 ps -ef|grep tomcat或 jps来进行查看</p>
<h3 id="jps"><a href="#jps" class="headerlink" title="jps"></a>jps</h3><p>查看java进程 -l 可以知道完全类名</p>
<h3 id="jinfo"><a href="#jinfo" class="headerlink" title="jinfo"></a>jinfo</h3><p>jinfo -flag MaxHeapSize <pid></p>
<p>jinfo -flags <pid>  手动赋过值的参数</p>
<h3 id="jstat"><a href="#jstat" class="headerlink" title="jstat"></a>jstat</h3><p>可以查看jvm的统计信息 如类加载。垃圾回收信息，jit编译信息</p>
<p>详情参考 <a target="_blank" rel="noopener" href="https://docs.oracle.com/javase/8/docs/technotes/tools/unix/jstat.html">jstat 官方文档</a></p>
<p><img src="_v_images/20200117100110226_7723.jpg" alt="jstat 使用示例"></p>
<p>类加载</p>
<h1 id="以下1000表每隔1000ms-即1秒，共输出10次"><a href="#以下1000表每隔1000ms-即1秒，共输出10次" class="headerlink" title="以下1000表每隔1000ms 即1秒，共输出10次"></a>以下1000表每隔1000ms 即1秒，共输出10次</h1><p>jstat -class <pid> 1000 10</p>
<p>垃圾收集</p>
<p>-gc, -gcutil, -gccause, -gcnew, -gcold</p>
<p>jstat -gc <pid> 1000 10</p>
<p>以下大小的单位均为 KB</p>
<p>![](_v_images/20200117100110111_28215.png =800x600)</p>
<p>S0C, S1C, S0U, S1U: S0和 S1的总量和使用量</p>
<p>EC, EU: Eden区总量与使用量</p>
<p>OC, OU: Old区总量与使用量</p>
<p>MC, MU: Metacspace区(jdk1.8前为 PermGen)总量与使用量</p>
<p>CCSC, CCSU: 压缩类区总量与使用量</p>
<p>YGC, YGCT: YoungGC 的次数与时间</p>
<p>FGC, FGCT: FullGC 的次数与时间</p>
<p>GCT: 总的 GC 时间</p>
<p>JIT 编译</p>
<p>-compiler, -printcompilation</p>
<p>一个对象默认分配在堆上面 但是有个指针指向class默认是64位长指针，可以设置为用32位存储在压缩类空间</p>
<p>非堆区 即对应于虚拟机规范中的方法区 是操作系统本地内存 独立于jvm堆区之外 jdk8后面叫metaspace jdk8前面叫performancespace</p>
<p>codecache 存储的是jit即时编译的代码 以及native代码</p>
<h3 id="jmap-MAT"><a href="#jmap-MAT" class="headerlink" title="jmap+MAT"></a>jmap+MAT</h3><p>详情参考<a target="_blank" rel="noopener" href="https://docs.oracle.com/javase/8/docs/technotes/tools/unix/jmap.html">jmap 官方文档</a></p>
<p>内存溢出演示：</p>
<p><a target="_blank" rel="noopener" href="https://start.spring.io/%E7%94%9F%E6%88%90%E5%88%9D%E5%A7%8B%E4%BB%A3%E7%A0%81">https://start.spring.io/生成初始代码</a></p>
<p>最终代码：<a target="_blank" rel="noopener" href="https://github.com/alanhou7/java-codes/tree/master/monitor_tuning">monitor_tuning</a></p>
<p>为快速产生内存溢出，右击 Run As&gt;Run Configurations, Arguments 标签VM arguments 中填入</p>
<p>-Xmx32M -Xms32M</p>
<p><img src="_v_images/20200117100109881_9995.png"></p>
<p>访问 <a target="_blank" rel="noopener" href="http://localhost:8080/heap">http://localhost:8080/heap</a></p>
<p>Exception in thread “http-nio-8080-exec-2” Exception in thread “http-nio-8080-exec-1” java.lang.OutOfMemoryError: Java heap space<br>java.lang.OutOfMemoryError: Java heap space</p>
<p>-XX:MetaspaceSize=32M -XX:MaxMetaspaceSize=32M（同时在 pom.xml 中加入 asm 的依赖）</p>
<p><img src="_v_images/20200117100109670_1795.png"></p>
<p>访问 <a target="_blank" rel="noopener" href="http://localhost:8080/nonheap">http://localhost:8080/nonheap</a></p>
<p>Exception in thread “main” java.lang.OutOfMemoryError: Metaspace<br>Exception in thread “ContainerBackgroundProcessor[StandardEngine[Tomcat]]“ java.lang.OutOfMemoryError: Metaspace</p>
<p>内存溢出自动导出</p>
<p>-XX:+HeapDumpOnOutOfMemoryError</p>
<p>-XX:HeapDumpPath=./</p>
<p>右击 Run As&gt;Run Configurations, Arguments 标签VM arguments 中填入</p>
<p>-Xmx32M -Xms32M -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=./</p>
<p>可以看到自动在当前目录中生成了一个java_pid660.hprof文件</p>
<p>java.lang.OutOfMemoryError: GC overhead limit exceeded<br>Dumping heap to ./java_pid660.hprof …</p>
<p>另一种导出溢出也更推荐的方式是jmap</p>
<p>option: -heap, -clstats, -dump:<dump-options>, -F</p>
<p>jmap -dump:format=b,file=heap.hprof <pid></p>
<p><img src="_v_images/20200117100109462_23769.jpg" alt="jmap 导出溢出文件"></p>
<p>MAT下载地址：<a target="_blank" rel="noopener" href="http://www.eclipse.org/mat/">http://www.eclipse.org/mat/</a></p>
<p>找开上述导出的内存溢出文件即可进行分析，如下图的溢出源头分析：</p>
<p><img src="_v_images/20200117100109352_8610.jpg" alt="Memory Analyzer 内存溢出分析"></p>
<ol>
<li>Histogram可以列出内存中的对象，对象的个数以及大小。</li>
<li>Dominator Tree可以列出那个线程，以及线程下面的那些对象占用的空间。</li>
</ol>
<p>Histogram</p>
<pre><code>[![](_v_images/20200117100109236_4753.png)](http://static.oschina.net/uploads/space/2014/0702/120039_qSi5_1767531.png)</code></pre>
<ul>
<li><p>Class Name ： 类名称，java类名</p>
</li>
<li><p>Objects ： 类的对象的数量，这个对象被创建了多少个</p>
</li>
<li><p>Shallow Heap ：一个对象内存的消耗大小，不包含对其他对象的引用</p>
</li>
</ul>
<ul>
<li>Retained Heap ：是shallow Heap的总和，也就是该对象被GC之后所能回收到内存的总和</li>
</ul>
<p>Dominator Tree</p>
<p><a target="_blank" rel="noopener" href="http://static.oschina.net/uploads/space/2014/0702/145926_K3ET_1767531.png"><img src="_v_images/20200117100109129_22861.png"></a></p>
<p>我们可以看到ibatis占了较多内存</p>
<p>快速找出某个实例没被释放的原因，可以右健 Path to GC Roots–&gt;exclue all phantom/weak/soft etc. reference :</p>
<p> <img src="_v_images/20200117100108918_9987.png"></p>
<p>得到的结果是：</p>
<p><img src="_v_images/20200117100108703_17828.png"></p>
<p>从表中可以看出 PreferenceManager -&gt; … -&gt;HomePage这条线路就引用着这个 HomePage实例。用这个方法可以快速找到某个对象的 <strong>GC Root</strong>,一个存在 GC Root的对象是不会被 GC回收掉的.</p>
<h3 id="jstack"><a href="#jstack" class="headerlink" title="jstack"></a>jstack</h3><p>详情参考 <a target="_blank" rel="noopener" href="https://docs.oracle.com/javase/8/docs/technotes/tools/unix/jstack.html">jstack 官方文档</a></p>
<p>jstack <pid>  打印jvm内部所有的线程</p>
<p> <em>jstack 15672 &gt;15673.txt  导出当前进程文件</em></p>
<p>可查看其中包含java.lang.Thread.State: WAITING (parking)，JAVA 线程包含的状态有：</p>
<p>NEW：线程尚未启动</p>
<p>RUNNABLE：线程正在 JVM 中执行</p>
<p>BLOCKED：线程在等待监控锁(monitor lock)</p>
<p>WAITING：线程在等待另一个线程进行特定操作（时间不确定）</p>
<p>TIMED_WAITING：线程等待另一个线程进行限时操作</p>
<p>TERMINATED：线程已退出</p>
<p>此时会生成一个monitor_tuning-0.0.1-SNAPSHOT.jar的 jar包，为避免本地的 CPU 消耗过多导致死机，建议上传上传到虚拟机进行测试</p>
<p>nohup java -jar monitor_tuning-0.0.1-SNAPSHOT.jar &amp;</p>
<p>访问 <a target="_blank" rel="noopener" href="http://xx.xx.xx.xx:12345/loop(%E7%AB%AF%E5%8F%A312345%E5%9C%A8application.properties%E6%96%87%E4%BB%B6%E4%B8%AD%E5%AE%9A%E4%B9%89)">http://xx.xx.xx.xx:12345/loop(端口12345在application.properties文件中定义)</a></p>
<p>top 是查询所有进程的cpu 占用率<br>top还可以用来显示一个进程中各个线程CPU的占用率：top -p <pid> -H<br>top命令如下  </p>
<p><img src="_v_images/20200117100108489_10785.png"></p>
<p>top -p <pid>  -H 命令如下 看的是7930的进程</p>
<p><img src="_v_images/20200117100108169_20096.png"></p>
<p>使用 jstack <pid>可以导出追踪文件，文件中 PID 在 jstack 中显示的对应 nid 为十六进制(命令行可执行 print ‘%x’ <pid>可以进行转化，如1640对应的十六进制为668)</p>
<p>“http-nio-12345-exec-3” #18 daemon prio=5 os_prio=0 tid=0x00007f10003fb000 nid=0x668 runnable [0x00007f0fcf8f9000]<br>   java.lang.Thread.State: RUNNABLE<br>    at org.alanhou.monitor_tuning.chapter2.CpuController.getPartneridsFromJson(CpuController.java:77)<br>…</p>
<p>访问<a target="_blank" rel="noopener" href="http://xx.xx.xx.xx:12345/deadlock">http://xx.xx.xx.xx:12345/deadlock</a>(如上jstack <pid>导出追踪记录会发现如下这样的记录)</p>
<p> ![](_v_images/20200117100107951_17762.png =800x500)</p>
<h1 id="Java-stack-information-for-the-threads-listed-above"><a href="#Java-stack-information-for-the-threads-listed-above" class="headerlink" title="Java stack information for the threads listed above:"></a>Java stack information for the threads listed above:</h1><p>“Thread-5”:<br>    at org.alanhou.monitor_tuning.chapter2.CpuController.lambda$deadlock$1(CpuController.java:41)<br>    - waiting to lock &lt;0x00000000edcf3470&gt; (a java.lang.Object)<br>    - locked &lt;0x00000000edcf3480&gt; (a java.lang.Object)<br>    at org.alanhou.monitor_tuning.chapter2.CpuController$$Lambda$337/547045985.run(Unknown Source)<br>    at java.lang.Thread.run(Thread.java:748)<br>“Thread-4”:<br>    at org.alanhou.monitor_tuning.chapter2.CpuController.lambda$deadlock$0(CpuController.java:33)<br>    - waiting to lock &lt;0x00000000edcf3480&gt; (a java.lang.Object)<br>    - locked &lt;0x00000000edcf3470&gt; (a java.lang.Object)<br>    at org.alanhou.monitor_tuning.chapter2.CpuController$$Lambda$336/1704575158.run(Unknown Source)<br>    at java.lang.Thread.run(Thread.java:748)</p>
<p>Found 1 deadlock.</p>
<p>查看后台日志，都是使用tail -f catalina.out命令来查看  </p>
<p>jvisualvm 图形化工具<br>插件安装Tools&gt;Plugins&gt;Settings根据自身版本(java -version)更新插件中心地址，各版本查询地址：<br><a target="_blank" rel="noopener" href="http://visualvm.github.io/pluginscenters.html">http://visualvm.github.io/pluginscenters.html</a><br> 建议安装：Visual GC, BTrace Workbench<br>概述 监控可以堆dump 线程可以线程dump 抽样器可以对cpu和内存进行抽样调查</p>
<p>以上是本地的JAVA进程监控，还可以进行远程的监控，在上图左侧导航的 Applications 下的 Remote 处右击Add Remote Host…，输入主机 IP 即可添加，在 IP 上右击会发现有两种连接 JAVA 进程进行监控的方式:JMX, jstatd</p>
<p>bin/catalina.sh(以192.168.0.5为例)</p>
<p>JAVA_OPTS=”$JAVA_OPTS -Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.port=9004 -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false -Djava.net.preferIPv4Stack=true -Djava.rmi.server.hostname=192.168.0.5”</p>
<p>启动tomcat，</p>
<p>启动tomcat服务<br>方式一：直接启动 ./startup.sh<br>方式二：作为服务启动 nohup ./startup.sh &amp;<br>查看tomcat运行日志<br>tail -f catalina.out</p>
<p>tomcat设置jvm参数<br>修改文件 apache-tomcat-9.0.10/bin下catalina.bat文件</p>
<p>以 JMX 为例，在 IP 上右击点击Add JMX Connection…，输入 IP:PORT</p>
<p><img src="_v_images/20200117100107732_2132.jpg" alt="Add JMX Connection"></p>
<p>以上为 Tomcat，其它 JAVA 进程也是类似的，如：</p>
<p>nohup java -Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.port=9005 -Dcom.sun.management.jmxremote.local.only=false -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false -Djava.net.preferIPv4Stack=true -Djava.rmi.server.hostname=192.168.0.5 -jar monitor_tuning-0.0.1-SNAPSHOT.jar &amp;</p>
<h3 id="BTrace"><a href="#BTrace" class="headerlink" title="BTrace"></a>BTrace</h3><p><a target="_blank" rel="noopener" href="https://github.com/jbachorik/btrace/releases/latest">BTrace</a> 可以动态地向目标应用程序的字节码注入追踪代码，使用的技术有 JavaCompilerApi, JVMTI, Agent, Instrumentation+ASM</p>
<p>使用方法：JVisualVM中添加 BTrace 插件</p>
<p>方法二：btrace <pid> <trace_script></p>
<p>btrace只能调试本地进程<br>btrace修改后的字节码不能被还原</p>
<p>pom.xml 中添加 btrace-agent, btrace-boot, btrace-client的依赖</p>
<p><img src="_v_images/20200117100107618_23079.png"></p>
<p><img src="_v_images/20200117100107506_24487.png"></p>
<p>拦截构造方法</p>
<p><img src="_v_images/20200117100107296_11386.png"></p>
<p>拦截同名方法  </p>
<p><img src="_v_images/20200117100107084_31796.png"></p>
<p>拦截返回值  </p>
<p><img src="_v_images/20200117100106870_26027.png"></p>
<p>拦截行号</p>
<p><img src="_v_images/20200117100106659_4260.png"></p>
<p>拦截异常信息</p>
<p><img src="_v_images/20200117100106449_4493.png"></p>
<p>拦截复杂类型</p>
<p><img src="_v_images/20200117100106216_21011.png"></p>
<p>拦截正则表达式</p>
<p><img src="_v_images/20200117100105902_16999.png"></p>
<p>拦截环境参数信息  </p>
<p><img src="_v_images/20200117100105692_8879.png">  </p>
<p>常用参数：  </p>
<p>-Xms -Xmx  </p>
<p>-XX:NewSize -XX:MaxNewSize  </p>
<p>-XX:NewRatio -XX:SurvivorRatio  </p>
<p>-XX:MetaspaceSize -XX:MaxMetaspaceSize 以下几个参数通常这样只设置这个值即可  </p>
<p>-XX:+UseCompressedClassPointers  </p>
<p>-XX:CompressedClassSpaceSize  </p>
<p>-XX:InitialCodeCacheSize  </p>
<p>-XX:ReservedCodeCacheSize</p>
<p>Tomcat 远程 Debug</p>
<p>JDWP</p>
<p>bin/startup.sh 修改最后一行(添加 jpda)</p>
<p>exec “$PRGDIR”/“$EXECUTABLE” jpda start “$@”</p>
<p>bin/catalina.sh 为便于远程调试进行如下修改</p>
<p>JPDA_ADDRESS=”localhost:8000”</p>
<h1 id="修改为"><a href="#修改为" class="headerlink" title="修改为"></a>修改为</h1><p>JPDA_ADDRESS=”54321”</p>
<p>若发现54321端口启动存在问题可尝试bin/catalina.sh jpda start</p>
<p>使用 Eclipse 远程调试，右击 Debug As &gt; Debug Configurations… &gt; Remote Java Application &gt; 右击 New 新建</p>
<p>普通java进程可以这样配置<br>java -jar -Xdebug -Xrunjdwp:transport=dt_socket,server=y,suspend=n,address=10001 access-10000.jar</p>
<p>tomcat-manager 监控</p>
<p>1.conf/tomcat-users.xml添加用户</p>
  <role rolename="tomcat"/>
  <role rolename="manager-status"/>
  <role rolename="manager-gui"/>
  <user username="tomcat" password="123456" roles="tomcat,manager-gui,manager-status"/>

<p>2.conf/Catalina/localhost/manager.xml配置允许的远程连接</p>
<?xml version="1.0" encoding="UTF-8"?>
<p><Context privileged="true" antiResourceLocking="false"
        docBase="$(catalina.home)/webapps/manager"><br>  <Valve className="org.apache.catalina.valves.RemoteAddrValve"
        allow="127\\.0\\.0\\.1" /><br></Context></p>
<p>远程连接将allow=”127\.0\.0\.1”修改为allow=”^.*$”，浏览器中输入<a target="_blank" rel="noopener" href="http://127.0.0.1:8080/manage%E6%88%96%E5%AF%B9%E5%BA%94%E7%9A%84">http://127.0.0.1:8080/manage或对应的</a> IP，用户名密码为tomcat-users.xml中所设置的</p>
<p>3.重启 Tomcat 服务</p>
<p><img src="_v_images/20200117100105380_4818.jpg" alt="Tomcat Manager"></p>
<p>psi-probe 监控</p>
<p>下载地址：<a target="_blank" rel="noopener" href="https://github.com/psi-probe/psi-probe%EF%BC%8C">https://github.com/psi-probe/psi-probe，</a></p>
<p>下载后进入psi-probe-master目录，执行：</p>
<p>mvn clean package -Dmaven.test.skip</p>
<p>将 web/target/probe.war放到 Tomcat 的 webapps 目录下，同样需要conf/tomcat-users.xml和conf/Catalina/localhost/manager.xml中的配置（可保持不变），启动 Tomcat 服务</p>
<p>浏览器中输入<a target="_blank" rel="noopener" href="http://127.0.0.1:8080/probe%E6%88%96%E5%AF%B9%E5%BA%94%E7%9A%84">http://127.0.0.1:8080/probe或对应的</a> IP，用户名密码为tomcat-users.xml中所设置的</p>
<p><img src="_v_images/20200117100105268_9524.jpg" alt="PSI Probe演示"></p>
<p>Tomcat 调优</p>
<p>线程优化（webapps/docs/config/http.html）：</p>
<p>maxConnections</p>
<p>acceptCount</p>
<p>maxThreads</p>
<p>minSpareThreads</p>
<p>配置优化（webapps/docs/config/host.html）：</p>
<p>autoDeploy</p>
<p>enableLookups（http.html）</p>
<p>reloadable（context.html）</p>
<p>protocol=”org.apache.coyote.http11.Http11AprProtocol”</p>
<p>Session 优化：</p>
<p>如果是 JSP, 可以禁用 Session</p>
<h3 id="Nginx-性能监控与调优"><a href="#Nginx-性能监控与调优" class="headerlink" title="Nginx 性能监控与调优"></a>Nginx 性能监控与调优</h3><p>Nginx 安装</p>
<p>添加 yum 源（/etc/yum.repos.d/nginx.repo）</p>
<p>[nginx]<br>name=nginx repo<br>baseurl=<a target="_blank" rel="noopener" href="http://nginx.org/packages/centos/7/$basesearch/">http://nginx.org/packages/centos/7/$basesearch/</a><br>gpgcheck=0<br>enabled=1</p>
<p>安装及常用命令</p>
<p>yum install -y nginx</p>
<p>systemctl status|start|stop|reload|restart nginx<br>nginx -s stop|reload|quit|reopen  nginx  启动nginx<br>cat default.conf | grep -v “#’ &gt; default2.conf  移除配置文件中的注释 并生成新的配置文件<br>nginx -V<br>nginx -t</p>
<p>配置反向代理 setenforce 0</p>
<p>ngx_http_stub_status 监控连接信息</p>
<p>location = /nginx_status {<br>    stub_status on;<br>    access_log off;<br>    allow 127.0.0.1;<br>    deny all;<br>}</p>
<p>可通过curl <a target="_blank" rel="noopener" href="http://127.0.0.1/nginx_status">http://127.0.0.1/nginx_status</a> 进行查看或注释掉 allow 和 deny 两行使用 IP 进行访问</p>
<p>ngxtop监控请求信息</p>
<p>查看官方使用方法：<a target="_blank" rel="noopener" href="https://github.com/lebinh/ngxtop">https://github.com/lebinh/ngxtop</a></p>
<h1 id="安装-python-pip"><a href="#安装-python-pip" class="headerlink" title="安装 python-pip"></a>安装 python-pip</h1><p>yum install epel-release<br>yum install python-pip</p>
<h1 id="安装-ngxtop"><a href="#安装-ngxtop" class="headerlink" title="安装 ngxtop"></a>安装 ngxtop</h1><p>pip install ngxtop</p>
<p>使用示例</p>
<p>指定配置文件：ngxtop -c /etc/nginx/nginx.conf</p>
<p>查询状态是200：ngxtop -c /etc/nginx/nginx.conf -i ‘status == 200’</p>
<p>查询访问最多 ip：ngxtop -c /etc/nginx/nginx.conf -g remote_addr</p>
<p><img src="_v_images/20200117100103032_6684.jpg" alt="ngxtop查询访问最多 ip"></p>
<p>Nginx 优化</p>
<p>增加工作线程数和并发连接数</p>
<p>worker_processes  4; # 一般CPU 是几核就设置为几<br>events {<br>    worker_connections  1024; # 每个进程打开的最大连接数，包含了 Nginx 与客户端和 Nginx 与 upstream 之间的连接<br>    multi_accept on; # 可以一次建立多个连接<br>    use epoll;<br>}</p>
<p>启用长连接</p>
<p>upstream server_pool{<br>    server localhost:8080 weight=1 max_fails=2 fail_timeout=30s;<br>    server localhost:8081 weight=1 max_fails=2 fail_timeout=30s;<br>    keepalive 300; # 300个长连接<br>}<br>location / {<br>    proxy_http_version 1.1;<br>    proxy_set_header Upgrade $http_upgrade;<br>    proxy_set_header Connection “upgrade”;<br>    proxy_pass <a target="_blank" rel="noopener" href="http://server/_pool">http://server\_pool</a>;<br>}</p>
<p>启用缓存压缩</p>
<p>gzip on;<br>gzip_http_version 1.1;<br>gzip_disable “MSIE [1-6]\.(?!.*SV1)”;<br>gzip_proxied any;<br>gzip_types text/plain text/css application/javascript application/x-javascript application/json application/xml application/vnd.ms-fontobject application/x-font-ttf application/svg+xml application/x-icon;<br>gzip_vary on;<br>gzip_static on;</p>
<p>操作系统优化</p>
<h1 id="配置文件-etc-sysctl-conf"><a href="#配置文件-etc-sysctl-conf" class="headerlink" title="配置文件/etc/sysctl.conf"></a>配置文件/etc/sysctl.conf</h1><p>sysctl -w net.ipv4.tcp_syncookies=1 # 防止一个套接字在有过多试图连接到时引起过载<br>sysctl -w net.core.somaxconn=1024 # 默认128，连接队列<br>sysctl -w net.ipv4.tcp_fin_timeout=10 # timewait 的超时时间<br>sysctl -w net.ipv4.tcp_tw_reuse=1 # os 直接使用 timewait的连接<br>sysctl -w net.ipv4.tcp_tw_recycle=0 # 回收禁用</p>
<h1 id="etc-security-limits-conf"><a href="#etc-security-limits-conf" class="headerlink" title="/etc/security/limits.conf"></a>/etc/security/limits.conf</h1><ul>
<li><pre><code>          hard    nofile            204800</code></pre>
</li>
<li><pre><code>          soft    nofile             204800</code></pre>
</li>
<li><pre><code>          soft    core             unlimited</code></pre>
</li>
<li><pre><code>          soft    stack             204800</code></pre>
</li>
</ul>
<p>其它优化</p>
<p>sendfile    on; # 减少文件在应用和内核之间拷贝<br>tcp_nopush    on; # 当数据包达到一定大小再发送<br>tcp_nodelay    off; # 有数据随时发送</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://example.com/bigdata/Flink/StreamingAPI/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="aaronzhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Guadazi">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/bigdata/Flink/StreamingAPI/" class="post-title-link" itemprop="url">Flink-streaming API</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2019-08-10 14:58:00" itemprop="dateCreated datePublished" datetime="2019-08-10T14:58:00+08:00">2019-08-10</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-04-19 15:02:11" itemprop="dateModified" datetime="2021-04-19T15:02:11+08:00">2021-04-19</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/bigdata/" itemprop="url" rel="index"><span itemprop="name">bigdata</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="Flink-Streaming-API"><a href="#Flink-Streaming-API" class="headerlink" title="Flink Streaming API"></a>Flink Streaming API</h1><h2 id="org-apache-flink-streaming-api-functions-source-SourceFunction"><a href="#org-apache-flink-streaming-api-functions-source-SourceFunction" class="headerlink" title="org.apache.flink.streaming.api.functions.source.SourceFunction"></a>org.apache.flink.streaming.api.functions.source.SourceFunction</h2>
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://example.com/Java/metric/02_CMS_GC/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="aaronzhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Guadazi">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/Java/metric/02_CMS_GC/" class="post-title-link" itemprop="url">CMS GC</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2019-08-10 14:58:00" itemprop="dateCreated datePublished" datetime="2019-08-10T14:58:00+08:00">2019-08-10</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-04-19 15:02:11" itemprop="dateModified" datetime="2021-04-19T15:02:11+08:00">2021-04-19</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/java/" itemprop="url" rel="index"><span itemprop="name">java</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="CMS-GC"><a href="#CMS-GC" class="headerlink" title="CMS GC"></a>CMS GC</h1><h2 id="垃圾回收器组合"><a href="#垃圾回收器组合" class="headerlink" title="垃圾回收器组合"></a>垃圾回收器组合</h2><table>
<thead>
<tr>
<th>Young 年轻代</th>
<th>Tenured 老生代</th>
<th>JVM options</th>
<th>备注</th>
</tr>
</thead>
<tbody><tr>
<td>Serial</td>
<td>Serial</td>
<td>-XX:+UseSerialGC</td>
<td>单线程回收，全程STW</td>
</tr>
<tr>
<td>Parallel Scavenge</td>
<td>Serial</td>
<td>-XX:+UseParallelGC -XX:-UseParallelOldGC</td>
<td>年轻代并行，老年代串行，全程STW</td>
</tr>
<tr>
<td>Parallel Scavenge</td>
<td>Parallel Old</td>
<td>-XX:+UseParallelGC -XX:+UseParallelOldGC</td>
<td>多线程回收，全程STW</td>
</tr>
<tr>
<td>Parallel New或Serial</td>
<td>CMS</td>
<td>-XX:+UseParNewGC -XX:+UseConcMarkSweepGC</td>
<td>年轻代并行或串行，老年代并发，只有某个阶段会STW</td>
</tr>
<tr>
<td>G1</td>
<td>G1</td>
<td>-XX:+UseG1GC</td>
<td>并发回收， 某个阶段会STW</td>
</tr>
</tbody></table>
<p>垃圾回收器从线程运行情况分类有三种：</p>
<ul>
<li><code>串行回收</code>： Serial回收器，单线程回收，全程STW；</li>
<li><code>并行回收</code>： 名称以Parallel开头的回收器，多线程回收，全程STW;</li>
<li><code>并发回收</code>： CMS与G1，多线程分阶段回收，只有某阶段会STW；</li>
</ul>
<p><img src="_v_images/20200716125314502_95470953.png"></p>
<h2 id="Minor-GC、Major-GC与Full-GC"><a href="#Minor-GC、Major-GC与Full-GC" class="headerlink" title="Minor GC、Major GC与Full GC"></a>Minor GC、Major GC与Full GC</h2><p>分代回收中:</p>
<p>Minor GC清理年轻代(Young GC)，除了G1 GC外，都会STW<br>Major GC清理老年代(Tenured GC)<br>Full GC清理整个堆</p>
<p>Minor GC触发条件:</p>
<p>Major GC触发条件:</p>
<p>Full GC触发条件:</p>
<ul>
<li>调用<code>System.gc</code>时，系统建议执行Full GC，不是必然执行</li>
<li>老年代空间不足</li>
<li>方法区空间不足</li>
<li>通过Minor GC后，进入老年代的平均大小 &gt; 老年代的可用内存</li>
<li>由Eden区、From Space区向To Space区复制时，对象大小大于To Space可用内存，则把该对象转存到老年代，且老年代的可用内存小于该对象大小。即老年代无法存放新年代过度到老年代的对象的时候，会触发Full GC</li>
<li>手动触发Full GC: jmap -histo:live <pid> 或者 jmap -dump:live,file=dump_001.bin PID,然后删掉dump_001.bin文件</li>
</ul>
<h2 id="CMS垃圾收集器-Concurrent-Mark-Sweep-CMS-Collector"><a href="#CMS垃圾收集器-Concurrent-Mark-Sweep-CMS-Collector" class="headerlink" title="CMS垃圾收集器 Concurrent Mark Sweep(CMS) Collector"></a>CMS垃圾收集器 Concurrent Mark Sweep(CMS) Collector</h2><p>并发，低停顿<br>特别是拥有大量长期数据（大老年代），多核心，低停顿</p>
<p>启用<code>-XX:+UseConcMarkSweepGC</code></p>
<p>CMS收集器是分代的。 因此，minor GC和major GC都会发生。 CMS收集器尝试通过使用单独的垃圾收集器线程在执行应用程序线程的同时跟踪可访问对象，来减少由于major GC而导致的暂停时间。 在每个major收集周期中，CMS收集器会在收集开始时暂停所有应用程序线程一小段时间，然后收集中间再暂停一次。 第二次停顿往往是两个停顿中较长的一个。 在两个暂停期间都使用多个线程来执行收集工作。 收集的其余部分（包括大部分活动对象的跟踪和无法访问对象的清除）是通过与应用程序同时运行的一个或多个垃圾收集器线程来完成的。minor GC可以与正在进行的主要周期交错，并在一个 类似于并行收集器的方式（特别是在次要收集期间停止了应用程序线程）。</p>
<p><strong>并发模式失效Concurrent Mode Failure</strong></p>
<ol>
<li>如果CMS收集器在老年代填满之前无法完成回收无法访问的对象，</li>
<li>如果老年代的可用空闲空间块(出现了碎片)无法满足分配，则暂停应用程序，并使所有应用程序线程已停止。 无法同时完成收集的情况称为并发模式失败，代表需要调整CMS收集器参数。</li>
<li>如果并发收集被显式垃圾收集（<code>System.gc()</code>）中断</li>
<li>为提供诊断工具信息所需的垃圾收集中断了，则将报告并发模式中断。</li>
</ol>
<blockquote>
<p>if the CMS collector is unable to finish reclaiming the unreachable objects before the tenured generation fills up, or if an allocation cannot be satisfied with the available free space blocks in the tenured generation, then the application is paused and the collection is completed with all the application threads stopped. The inability to complete a collection concurrently is referred to as concurrent mode failure and indicates the need to adjust the CMS collector parameters. If a concurrent collection is interrupted by an explicit garbage collection (System.gc()) or for a garbage collection needed to provide information for diagnostic tools, then a concurrent mode interruption is reported.</p>
</blockquote>
<p><strong>Excessive GC Time and OutOfMemoryError</strong></p>
<p>太多的时间花在gc上: 如果总时间的98%花在GC上，并且回收不到2%的堆空间，将抛出<code>OutOfMemoryError</code></p>
<p>禁用命令行: <code>-XX:-UseGCOverheadLimit</code></p>
<p><strong>浮动垃圾Floating Garbage</strong></p>
<p>边收集边运行，出现浮动垃圾</p>
<h2 id="CMS垃圾回收特点"><a href="#CMS垃圾回收特点" class="headerlink" title="CMS垃圾回收特点"></a>CMS垃圾回收特点</h2><p>CMS只会回收老年代和永久代（1.8开始为元数据区，需要设置CMSClassUnloadingEnabled），不会收集年轻代；</p>
<p>CMS是一种预处理垃圾回收器，它不能等到老年代内存用尽时回收，需要在内存用尽前，完成回收操作，否则会导致并发回收失败(并发回收降级)；<br>所以CMS垃圾回收器开始执行回收操作，有一个触发阈值(<code>参数名称</code>)，默认是老年代或永久代达到92%；</p>
<h2 id="CMS垃圾收集器步骤"><a href="#CMS垃圾收集器步骤" class="headerlink" title="CMS垃圾收集器步骤"></a>CMS垃圾收集器步骤</h2><p>CMS 处理过程有七个步骤：</p>
<table>
<thead>
<tr>
<th>步骤</th>
<th>是否STW</th>
<th>详情</th>
</tr>
</thead>
<tbody><tr>
<td>初始标记(CMS-initial-mark)</td>
<td>会导致STW</td>
<td>标记GCRoot和被年轻代引用的老年代对象</td>
</tr>
<tr>
<td>并发标记(CMS-concurrent-mark)</td>
<td>与用户线程同时运行；</td>
<td>扫描整个老年代，将引用关系变化的对象置为dirty</td>
</tr>
<tr>
<td>预清理（CMS-concurrent-preclean）</td>
<td>与用户线程同时运行；</td>
<td></td>
</tr>
<tr>
<td>可被终止的预清理（CMS-concurrent-abortable-preclean）</td>
<td>与用户线程同时运行；</td>
<td></td>
</tr>
<tr>
<td>重新标记(CMS-remark)</td>
<td>会导致STW</td>
<td></td>
</tr>
<tr>
<td>并发清除(CMS-concurrent-sweep)</td>
<td>与用户线程同时运行；</td>
<td></td>
</tr>
<tr>
<td>并发重置状态等待下次CMS的触发(CMS-concurrent-reset)</td>
<td>与用户线程同时运行；</td>
<td></td>
</tr>
</tbody></table>
<p>CMS运行流程图如下所示：</p>
<p><img src="_v_images/20200208214054081_1122407096.png"></p>
<h3 id="Phase-1-Initial-Mark（初始化标记）"><a href="#Phase-1-Initial-Mark（初始化标记）" class="headerlink" title="Phase 1: Initial Mark（初始化标记）"></a>Phase 1: Initial Mark（初始化标记）</h3><p>这是CMS中两次stop-the-world事件中的一次。这一步的作用是标记存活的对象，有两部分：</p>
<ol>
<li>从GC Roots遍历可直达的老年代对象，下图中1；</li>
<li>遍历被新生代存活对象所引用的老年代对象，如下图节点2、3；</li>
</ol>
<ul>
<li>支持单线程或并发标记</li>
<li>发生STW</li>
</ul>
<p><img src="_v_images/20200208214053938_762753439.png"></p>
<p>在Java语言里，可作为GC Roots对象的包括如下几种：</p>
<ol>
<li>虚拟机栈(栈桢中的本地变量表)中的引用的对象 ；</li>
<li>方法区中的类静态属性引用的对象 ；</li>
<li>方法区中的常量引用的对象 ；</li>
<li>本地方法栈中JNI的引用的对象；</li>
</ol>
<blockquote>
<p>ps：为了加快此阶段处理速度，减少停顿时间:</p>
<ul>
<li>开启并行化初始标记: <code>-XX:+CMSParallelInitialMarkEnabled</code></li>
<li>同时调大并行标记的线程数，线程数不要超过cpu的核数: <code>-XX:ConcGCThreads=4</code></li>
</ul>
</blockquote>
<h3 id="Phase-2-Concurrent-Mark（并发标记）"><a href="#Phase-2-Concurrent-Mark（并发标记）" class="headerlink" title="Phase 2: Concurrent Mark（并发标记）"></a>Phase 2: Concurrent Mark（并发标记）</h3><p>通过遍历第一个阶段（Initial Mark）标记出来的存活对象，继续递归遍历老年代，并标记可直接或间接到达的所有老年代存活对象。</p>
<p>由于应用线程和GC线程是并发执行的，因此可能产生新的对象或对象关系发生变化，例如：</p>
<ul>
<li>新生代的对象晋升到老年代；</li>
<li>直接在老年代分配对象；</li>
<li>老年代对象的引用关系发生变更；</li>
<li>等等。</li>
</ul>
<p>对于这些对象，需要重新标记以防止被遗漏。为了提高重新标记的效率，本阶段只会把发生变化的对象所在的Card标识为Dirty，这样后续就只需要扫描这些Dirty Card的对象，从而避免扫描整个老年代。</p>
<p>并发标记阶段只负责将引用发生改变的Card标记为Dirty状态，不负责处理；</p>
<p>如下图所示，也就是节点1、2、3，最终找到了节点4和5。并不是老年代的所有存活对象都会被标记，因为标记的同时应用程序会改变一些对象的引用等。</p>
<p><img src="_v_images/20200208214053815_1244593749.png"></p>
<p>这个阶段因为是并发的, 容易导致concurrent mode failure</p>
<h3 id="Phase-3-Concurrent-Preclean（并发预清理）"><a href="#Phase-3-Concurrent-Preclean（并发预清理）" class="headerlink" title="Phase 3: Concurrent Preclean（并发预清理）"></a>Phase 3: Concurrent Preclean（并发预清理）</h3><p>在并发预清洗阶段，将会重新扫描前一个阶段标记的Dirty对象，并标记被Dirty对象直接或间接引用的对象，然后清除Card标识。</p>
<p>前一个阶段已经说明，不能标记出老年代全部的存活对象，是因为标记的同时应用程序会改变一些对象引用，这个阶段就是用来处理前一个阶段因为引用关系改变导致没有标记到的存活对象的，它会扫描所有标记为Direty的Card</p>
<p>如下图所示，在并发清理阶段，节点3的引用指向了6；则会把节点3的card标记为Dirty；</p>
<p><img src="_v_images/20200208214053691_1851185723.png"></p>
<p>最后将6标记为存活,如下图所示：</p>
<p><img src="_v_images/20200208214053551_1708409185.png"></p>
<h3 id="Phase-4-Concurrent-Abortable-Preclean（可中止的并发预清理）"><a href="#Phase-4-Concurrent-Abortable-Preclean（可中止的并发预清理）" class="headerlink" title="Phase 4: Concurrent Abortable Preclean（可中止的并发预清理）"></a>Phase 4: Concurrent Abortable Preclean（可中止的并发预清理）</h3><p>本阶段尽可能承担更多的并发预处理工作，从而减轻在Final Remark阶段的stop-the-world。</p>
<p>这个阶段尝试着去承担下一个阶段Final Remark阶段足够多的工作。这个阶段持续的时间依赖好多的因素，由于这个阶段是重复的做相同的事情直到发生abort的条件（比如：重复的次数、多少量的工作、持续的时间等等）之一才会停止。</p>
<p>ps:此阶段最大持续时间为5秒，之所以可以持续5秒，另外一个原因也是为了期待这5秒内能够发生一次ygc，清理年轻代的引用，是的下个阶段的重新标记阶段，扫描年轻代指向老年代的引用的时间减少；</p>
<p>在该阶段，主要循环的做两件事：</p>
<ul>
<li>处理 From 和 To 区的对象，标记可达的老年代对象；</li>
<li>和上一个阶段一样，扫描处理Dirty Card中的对象。</li>
</ul>
<p>具体执行多久，取决于许多因素，满足其中一个条件将会中止运行：</p>
<ul>
<li>执行循环次数达到了阈值；</li>
<li>执行时间达到了阈值；</li>
<li>新生代Eden区的内存使用率达到了阈值。</li>
</ul>
<h3 id="Phase-5-Final-Remark（重新标记）"><a href="#Phase-5-Final-Remark（重新标记）" class="headerlink" title="Phase 5: Final Remark（重新标记）"></a>Phase 5: Final Remark（重新标记）</h3><p>预清理阶段也是并发执行的，并不一定是所有存活对象都会被标记，因为在并发标记的过程中对象及其引用关系还在不断变化中。</p>
<p>因此，需要有一个stop-the-world的阶段来完成最后的标记工作，这就是重新标记阶段（CMS标记阶段的最后一个阶段）。主要目的是重新扫描之前并发处理阶段的所有残留更新对象。</p>
<p>主要工作：</p>
<p>遍历新生代对象，重新标记；（新生代会被分块，多线程扫描）<br>根据GC Roots，重新标记；<br>遍历老年代的Dirty Card，重新标记。这里的Dirty Card，大部分已经在Preclean阶段被处理过了。</p>
<p>这个阶段会导致第二次stop the world，该阶段的任务是完成标记整个年老代的所有的存活对象。</p>
<p>这个阶段，重新标记的内存范围是整个堆，包含young_gen和old_gen。为什么要扫描新生代呢，因为对于老年代中的对象，如果被新生代中的对象引用，那么就会被视为存活对象，即使新生代的对象已经不可达了，也会使用这些不可达的对象当做CMS的“gc root”，来扫描老年代； 因此对于老年代来说，引用了老年代中对象的新生代的对象，也会被老年代视作“GC ROOTS”:<br>当此阶段耗时较长的时候，可以加入参数<code>-XX:+CMSScavengeBeforeRemark</code>，在重新标记之前，先执行一次ygc，回收掉年轻代的对象无用的对象，并将对象放入survivor区或晋升到老年代，这样再进行年轻代扫描时，只需要扫描幸存区的对象即可，一般survivor区非常小，这大大减少了扫描时间</p>
<p>由于之前的预处理阶段是与用户线程并发执行的，这时候可能年轻代的对象对老年代的引用已经发生了很多改变，这个时候，remark阶段要花很多时间处理这些改变，会导致很长stop the word，所以通常CMS尽量运行Final Remark阶段在年轻代是足够干净的时候。</p>
<p>另外，还可以开启并行收集：<code>-XX:+CMSParallelRemarkEnabled</code></p>
<h3 id="Phase-6-Concurrent-Sweep（并发清理"><a href="#Phase-6-Concurrent-Sweep（并发清理" class="headerlink" title="Phase 6: Concurrent Sweep（并发清理"></a>Phase 6: Concurrent Sweep（并发清理</h3><p>并发清理阶段，主要工作是清理所有未被标记的死亡对象，回收被占用的空间。</p>
<p><img src="_v_images/20200209233712414_1450669107.png"></p>
<p>通过以上5个阶段的标记，老年代所有存活的对象已经被标记并且现在要通过Garbage Collector采用清扫的方式回收那些不能用的对象了。</p>
<p>这个阶段主要是清除那些没有标记的对象并且回收空间；</p>
<p>由于CMS并发清理阶段用户线程还在运行着，伴随程序运行自然就还会有新的垃圾不断产生，这一部分垃圾出现在标记过程之后，CMS无法在当次收集中处理掉它们，只好留待下一次GC时再清理掉。这一部分垃圾就称为“浮动垃圾”。</p>
<h3 id="步骤7-并发重置"><a href="#步骤7-并发重置" class="headerlink" title="步骤7: 并发重置"></a>步骤7: 并发重置</h3><p>并发重置阶段，将清理并恢复在CMS GC过程中的各种状态，重新初始化CMS相关数据结构，为下一个垃圾收集周期做好准备。</p>
<p>这个阶段并发执行，重新设置CMS算法内部的数据结构，准备下一个CMS生命周期的使用。</p>
<h2 id="CMS日志分析"><a href="#CMS日志分析" class="headerlink" title="CMS日志分析"></a>CMS日志分析</h2><p>下面就是该参数设置打印出来的gc信息，一些非关键的信息已经去掉，如时间：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//第一步 初始标记 这一步会停顿*</span></span><br><span class="line">[GC (CMS Initial Mark) [<span class="number">1</span> CMS-initial-mark: 299570K(307200K)] 323315K(491520K), <span class="number">0.0026208</span> secs] [Times: user=<span class="number">0.00</span> sys=<span class="number">0.00</span>, real=<span class="number">0.00</span> secs]</span><br><span class="line">vmop [threads: total initially_running wait_to_block] [time: spin block sync cleanup vmop] page_trap_count</span><br><span class="line"><span class="number">0.345</span>: CMS_Initial_Mark [ <span class="number">10</span> <span class="number">0</span> <span class="number">1</span> ] [ <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">2</span> ] <span class="number">0</span></span><br><span class="line">Total time <span class="keyword">for</span> which application threads were stopped: <span class="number">0.0028494</span> seconds</span><br><span class="line"></span><br><span class="line"><span class="comment">//第二步 并发标记</span></span><br><span class="line">[CMS-concurrent-mark-start]</span><br><span class="line">[CMS-concurrent-mark: <span class="number">0.012</span>/<span class="number">0.012</span> secs] [Times: user=<span class="number">0.00</span> sys=<span class="number">0.00</span>, real=<span class="number">0.01</span> secs]</span><br><span class="line"></span><br><span class="line"><span class="comment">//第三步 并发预清理</span></span><br><span class="line">[CMS-concurrent-preclean-start]</span><br><span class="line">[CMS-concurrent-preclean: <span class="number">0.001</span>/<span class="number">0.001</span> secs] [Times: user=<span class="number">0.00</span> sys=<span class="number">0.00</span>, real=<span class="number">0.00</span> secs]</span><br><span class="line"></span><br><span class="line"><span class="comment">//第四步 可被终止的并发预清理</span></span><br><span class="line">[CMS-concurrent-abortable-preclean-start]</span><br><span class="line">[CMS-concurrent-abortable-preclean: <span class="number">0.000</span>/<span class="number">0.000</span> secs] [Times: user=<span class="number">0.00</span> sys=<span class="number">0.00</span>, real=<span class="number">0.00</span> secs]</span><br><span class="line"></span><br><span class="line"><span class="comment">//第五步 最终重新标记</span></span><br><span class="line">[GC (CMS Final Remark) [YG occupancy: 72704 K (184320 K)][Rescan (parallel) , 0.0009069 secs][weak refs processing, 0.0000083 secs][class unloading, 0.0002626 secs][scrub symbol table, 0.0003789 secs][scrub string table, 0.0001326 secs][1 CMS-remark: 299570K(307200K)] 372275K(491520K), 0.0017842 secs] [Times: user=0.05 sys=0.00, real=0.00 secs]</span><br><span class="line">vmop [threads: total initially_running wait_to_block] [time: spin block sync cleanup vmop] page_trap_count</span><br><span class="line"><span class="number">0.360</span>: CMS_Final_Remark [ <span class="number">10</span> <span class="number">0</span> <span class="number">1</span> ] [ <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> ] <span class="number">0</span></span><br><span class="line">Total time <span class="keyword">for</span> which application threads were stopped: <span class="number">0.0018800</span> seconds</span><br><span class="line"></span><br><span class="line"><span class="comment">//第六步 并发清理</span></span><br><span class="line">[CMS-concurrent-sweep-start]</span><br><span class="line">[CMS-concurrent-sweep: <span class="number">0.007</span>/<span class="number">0.007</span> secs] [Times: user=<span class="number">0.00</span> sys=<span class="number">0.00</span>, real=<span class="number">0.01</span> secs]</span><br><span class="line"></span><br><span class="line"><span class="comment">//第七步 并发重置</span></span><br><span class="line">[CMS-concurrent-reset-start]</span><br><span class="line">[CMS-concurrent-reset: <span class="number">0.002</span>/<span class="number">0.002</span> secs] [Times: user=<span class="number">0.00</span> sys=<span class="number">0.00</span>, real=<span class="number">0.00</span> secs]</span><br></pre></td></tr></table></figure>
<p>输出GC详情，需要添加 <code>-verbose:gc</code> 和 <code>-XX:+PrintGCDetails</code> 参数</p>
<p><code>CMS-initial-mark</code>标示着并发收集周期的开始<br><code>CMS-concurrent-mark</code>标示着并发标记阶段的结束<br><code>CMS-concurrent-sweep</code>标志着并发清理阶段的结束<br><code>CMS-concurrent-preclean</code>标志着预清理阶段，预清理代表着在准备CMS-remark阶段可以并发处理的工作<br><code>CMS-concurrent-reset</code>是最后阶段，为下一次并发收集做准备</p>
<blockquote>
<p>CMS-initial-mark indicates the start of the concurrent collection cycle,<br>CMS-concurrent-mark indicates the end of the concurrent marking phase,<br>and CMS-concurrent-sweep marks the end of the concurrent sweeping phase.<br>Not discussed previously is the precleaning phase indicated by CMS-concurrent-preclean.<br>Precleaning represents work that can be done concurrently in preparation for the remark phase CMS-remark.<br>The final phase is indicated by CMS-concurrent-reset and is in preparation for the next concurrent collection.</p>
</blockquote>
<h2 id="调优参数与启用参数"><a href="#调优参数与启用参数" class="headerlink" title="调优参数与启用参数"></a>调优参数与启用参数</h2><p>下面抓取一下gc信息，来进行详细分析，首先将jvm中加入以下运行参数：</p>
<ul>
<li>-XX:+PrintCommandLineFlags [0]</li>
<li>-XX:+UseConcMarkSweepGC [1]</li>
<li>-XX:+UseCMSInitiatingOccupancyOnly [2]</li>
<li>-XX:CMSInitiatingOccupancyFraction=80 [3]</li>
<li>-XX:+CMSClassUnloadingEnabled [4]</li>
<li>-XX:+UseParNewGC [5]</li>
<li>-XX:+CMSParallelRemarkEnabled [6]</li>
<li>-XX:+CMSScavengeBeforeRemark [7]</li>
<li>-XX:+UseCMSCompactAtFullCollection [8]</li>
<li>-XX:CMSFullGCsBeforeCompaction=0 [9]</li>
<li>-XX:+CMSConcurrentMTEnabled [10]</li>
<li>-XX:ConcGCThreads=4 [11]</li>
<li>-XX:+ExplicitGCInvokesConcurrent [12]</li>
<li>-XX:+ExplicitGCInvokesConcurrentAndUnloadsClasses [13]</li>
<li>-XX:+CMSParallelInitialMarkEnabled [14]</li>
<li>-XX:+PrintGCDetails [15]</li>
<li>-XX:+PrintGCCause [16]</li>
<li>-XX:+PrintGCTimeStamps [17]</li>
<li>-XX:+PrintGCDateStamps [18]</li>
<li>-Xloggc:../logs/gc.log [19]</li>
<li>-XX:+HeapDumpOnOutOfMemoryError [20]</li>
<li>-XX:HeapDumpPath=../dump [21]</li>
</ul>
<p>先来介绍下下面几个参数的作用：</p>
<p>[0] 打印出启动参数行</p>
<p>[1] 参数指定使用CMS垃圾回收器；</p>
<p>[2]、[3] 参数指定CMS垃圾回收器在老年代达到80%的时候开始工作，如果不指定那么默认的值为92%；</p>
<p>[4] 开启永久代（jdk1.8以下版本）或元数据区（jdk1.8及其以上版本）收集，如果没有设置这个标志，一旦永久代或元数据区间也会尝试进行垃圾回收，但是收集不会是并行的，而再一次进行Full GC；</p>
<p>[5] 使用CMS时默认这个参数就是打开的，不需要配置，CMS只回收老年代，年轻代只能配合Parallel New或Serial回收器；</p>
<p>[6] 减少Remark阶段暂停的时间，启用并行Remark，如果Remark阶段暂停时间长，可以启用这个参数</p>
<p>[7] 如果Remark阶段暂停时间太长，可以启用这个参数，在Remark执行之前，先做一次ygc。因为这个阶段，年轻代也是CMS的gcroot，CMS会扫描年轻代指向老年代对象的引用，如果年轻代有大量引用需要被扫描，会让Remark阶段耗时增加；</p>
<p>[8]、[9]两个参数是针对CMS垃圾回收器碎片做优化的，CMS是不会移动内存的， 运行时间长了，会产生很多内存碎片， 导致没有一段连续区域可以存放大对象，出现”promotion failed”、”concurrent mode failure”, 导致fullgc，启用UseCMSCompactAtFullCollection 在FULL GC的时候， 对年老代的内存进行压缩。-XX:CMSFullGCsBeforeCompaction=0 则是代表多少次FGC后对老年代做压缩操作，默认值为0，代表每次都压缩, 把对象移动到内存的最左边，可能会影响性能,但是可以消除碎片；</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">106.641</span>: [GC <span class="number">106.641</span>: [ParNew (promotion failed): 14784K-&gt;14784K(14784K), <span class="number">0.0370328</span> secs]<span class="number">106.678</span>: [CMS106<span class="number">.715</span>: [CMS-concurrent-mark: <span class="number">0.065</span>/<span class="number">0.103</span> secs] [Times: user=<span class="number">0.17</span> sys=<span class="number">0.00</span>, real=<span class="number">0.11</span> secs]</span><br><span class="line"></span><br><span class="line">(concurrent mode failure): 41568K-&gt;27787K(49152K), <span class="number">0.2128504</span> secs] 52402K-&gt;27787K(63936K), [CMS Perm : 2086K-&gt;2086K(12288K)], <span class="number">0.2499776</span> secs] [Times: user=<span class="number">0.28</span> sys=<span class="number">0.00</span>, real=<span class="number">0.25</span> secs]</span><br></pre></td></tr></table></figure>
<p>[11] 定义并发CMS过程运行时的线程数。比如value=4意味着CMS周期的所有阶段都以4个线程来执行。尽管更多的线程会加快并发CMS过程，但其也会带来额外的同步开销。因此，对于特定的应用程序，应该通过测试来判断增加CMS线程数是否真的能够带来性能的提升。如果未设置这个参数，JVM会根据并行收集器中的-XX:ParallelGCThreads参数的值来计算出默认的并行CMS线程数：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ParallelGCThreads = (ncpus &lt;=<span class="number">8</span> ? ncpus : <span class="number">8</span>+(ncpus-<span class="number">8</span>)*<span class="number">5</span>/<span class="number">8</span>) ，ncpus为cpu个数，</span><br><span class="line">ConcGCThreads =(ParallelGCThreads + <span class="number">3</span>)/<span class="number">4</span></span><br></pre></td></tr></table></figure>
<p>这个参数一般不要自己设置，使用默认就好，除非发现默认的参数有调整的必要；<br>[12]、[13]开启foreground CMS GC，CMS gc 有两种模式，background和foreground，正常的CMS gc使用background模式，就是我们平时说的CMS gc；当并发收集失败或者调用了System.gc()的时候，就会导致一次full gc，这个fullgc是不是CMS回收，而是Serial单线程回收器，加入了参数[12]后，执行full gc的时候，就变成了CMS foreground gc，它是并行full gc，只会执行CMS中stop the world阶段的操作，效率比单线程Serial full GC要高；需要注意的是它只会回收old，因为CMS收集器是老年代收集器；而正常的Serial收集是包含整个堆的，加入了参数[13],代表永久代也会被CMS收集；</p>
<p>[14] 开启初始标记过程中的并行化，进一步提升初始化标记效率;</p>
<p>[15]、[16]、[17]、[18] 、[19]是打印gc日志，其中[16]在jdk1.8之后无需设置</p>
<p>[20]、[21]则是内存溢出时dump堆</p>
<h2 id="CMS需要注意的问题"><a href="#CMS需要注意的问题" class="headerlink" title="CMS需要注意的问题"></a>CMS需要注意的问题</h2><h3 id="CMS不是full-GC"><a href="#CMS不是full-GC" class="headerlink" title="CMS不是full GC"></a>CMS不是full GC</h3><p>有一点需要注意的是：CMS并发GC不是“full GC”。HotSpot VM里对concurrent collection和full collection有明确的区分。所有带有“FullCollection”字样的VM参数都是跟真正的full GC相关，而跟CMS并发GC无关的，CMS收集算法只是清理老年代。</p>
<h3 id="减少remark阶段停顿"><a href="#减少remark阶段停顿" class="headerlink" title="减少remark阶段停顿"></a>减少remark阶段停顿</h3><p>一般CMS的GC耗时 80%都在remark阶段，如果发现remark阶段停顿时间很长，可以尝试添加该参数：</p>
<p>-XX:+CMSScavengeBeforeRemark</p>
<p>在执行remark操作之前先做一次ygc，目的在于减少ygen对oldgen的无效引用，降低remark时的开销，如果添加该参数后 ”ygc停顿时间+remark时间&lt;添加该参数之前的remark时间“,说明该参数是有效的；</p>
<h3 id="内存碎片"><a href="#内存碎片" class="headerlink" title="内存碎片"></a>内存碎片</h3><p>CMS是基于标记-清除算法的，只会将标记为为存活的对象删除，并不会移动对象整理内存空间，会造成内存碎片，这时候我们需要用到这个参数;</p>
<p>-XX:CMSFullGCsBeforeCompaction=n</p>
<p>这个参数大部分人的使用方式都是错误的，往往会导致设置后问题更大。</p>
<p>CMSFullGCsBeforeCompaction这个参数在HotSpot VM里是这样声明的：</p>
<p>product(bool, UseCMSCompactAtFullCollection, true, \</p>
<p>“Use mark sweep compact at full collections”) \</p>
<p>\</p>
<p>product(uintx, CMSFullGCsBeforeCompaction, 0, \</p>
<p>“Number of CMS full collection done before compaction if &gt; 0”) \</p>
<p>然后这样使用的：</p>
<p>*should_compact =</p>
<p>UseCMSCompactAtFullCollection &amp;&amp;</p>
<p>((_full_gcs_since_conc_gc &gt;= CMSFullGCsBeforeCompaction) ||</p>
<p>GCCause::is_user_requested_gc(gch-&gt;gc_cause()) ||</p>
<p>gch-&gt;incremental_collection_will_fail(true <em>/\</em> consult_young <em>/</em>));</p>
<p>CMS GC要决定是否在full GC时做压缩，会依赖几个条件。其中，</p>
<ol>
<li><p>UseCMSCompactAtFullCollection 与 CMSFullGCsBeforeCompaction 是搭配使用的；前者目前默认就是true了，也就是关键在后者上。</p>
</li>
<li><p>用户调用了System.gc()，而且DisableExplicitGC没有开启。</p>
</li>
<li><p>young gen报告接下来如果做增量收集会失败；简单来说也就是young gen预计old gen没有足够空间来容纳下次young GC晋升的对象。</p>
</li>
</ol>
<p>上述三种条件的任意一种成立都会让CMS决定这次做full GC时要做压缩。</p>
<p>CMSFullGCsBeforeCompaction 说的是，在上一次CMS并发GC执行过后，到底还要再执行多少次full GC才会做压缩。默认是0，也就是在默认配置下每次CMS GC顶不住了而要转入full GC的时候都会做压缩。 如果把CMSFullGCsBeforeCompaction配置为10，就会让上面说的第一个条件变成每隔10次真正的full GC才做一次压缩（而不是每10次CMS并发GC就做一次压缩，目前VM里没有这样的参数）。这会使full GC更少做压缩，也就更容易使CMS的old gen受碎片化问题的困扰。 本来这个参数就是用来配置降低full GC压缩的频率，以期减少某些full GC的暂停时间。CMS回退到full GC时用的算法是mark-sweep-compact，但compaction是可选的，不做的话碎片化会严重些但这次full GC的暂停时间会短些；这是个取舍。</p>
<h3 id="concurrent-mode-failure"><a href="#concurrent-mode-failure" class="headerlink" title="concurrent mode failure"></a>concurrent mode failure</h3><p>这个异常发生在CMS正在回收的时候。执行CMS GC的过程中，同时业务线程也在运行，当年轻代空间满了，执行ygc时，需要将存活的对象放入到老年代，而此时老年代空间不足，这时CMS还没有机会回收老年带产生的，或者在做Minor GC的时候，新生代救助空间放不下，需要放入老年代，而老年代也放不下而产生的。</p>
<p>设置CMS触发时机有两个参数：</p>
<p>-XX:+UseCMSInitiatingOccupancyOnly</p>
<p>-XX:CMSInitiatingOccupancyFraction=70</p>
<p>-XX:CMSInitiatingOccupancyFraction=70 是指设定CMS在对内存占用率达到70%的时候开始GC。</p>
<p>-XX:+UseCMSInitiatingOccupancyOnly如果不指定, 只是用设定的回收阈值CMSInitiatingOccupancyFraction,则JVM仅在第一次使用设定值,后续则自动调整会导致上面的那个参数不起作用。</p>
<p>为什么要有这两个参数？</p>
<p>由于在垃圾收集阶段用户线程还需要运行，那也就还需要预留有足够的内存空间给用户线程使用，因此CMS收集器不能像其他收集器那样等到老年代几乎完全被填满了再进行收集，需要预留一部分空间提供并发收集时的程序运作使用。</p>
<p>CMS前五个阶段都是标记存活对象的，除了”初始标记”和”重新标记”阶段会stop the word ，其它三个阶段都是与用户线程一起跑的，就会出现这样的情况gc线程正在标记存活对象，用户线程同时向老年代提升新的对象，清理工作还没有开始，old gen已经没有空间容纳更多对象了，这时候就会导致concurrent mode failure， 然后就会使用串行收集器回收老年代的垃圾，导致停顿的时间非常长。</p>
<p>CMSInitiatingOccupancyFraction参数要设置一个合理的值，设置大了，会增加concurrent mode failure发生的频率，设置的小了，又会增加CMS频率，所以要根据应用的运行情况来选取一个合理的值。</p>
<p>如果发现这两个参数设置大了会导致fullgc，设置小了会导致频繁的CMSgc，说明你的老年代空间过小，应该增加老年代空间的大小了；</p>
<h3 id="promotion-failed"><a href="#promotion-failed" class="headerlink" title="promotion failed"></a>promotion failed</h3><p>这个异常发生在年轻代回收的时候；</p>
<p>在进行Minor GC时，Survivor Space放不下，对象只能放入老年代，而此时老年代也放不下造成的，多数是由于老年带有足够的空闲空间，但是由于碎片较多，新生代要转移到老年带的对象比较大,找不到一段连续区域存放这个对象导致的，以下是一段promotion failed的日志：</p>
<p>106.641: [GC 106.641: [ParNew (promotion failed): 14784K-&gt;14784K(14784K), 0.0370328 secs]106.678: [CMS106.715: [CMS-concurrent-mark: 0.065/0.103 secs] [Times: user=0.17 sys=0.00, real=0.11 secs]</p>
<p>(concurrent mode failure): 41568K-&gt;27787K(49152K), 0.2128504 secs] 52402K-&gt;27787K(63936K), [CMS Perm : 2086K-&gt;2086K(12288K)], 0.2499776 secs] [Times: user=0.28 sys=0.00, real=0.25 secs]</p>
<p><strong><em>过早提升与提升失败</em></strong></p>
<p>在 Minor GC 过程中，Survivor Unused 可能不足以容纳 Eden 和另一个 Survivor 中的存活对象， 那么多余的将被移到老年代， 称为过早提升（Premature Promotion）,这会导致老年代中短期存活对象的增长， 可能会引发严重的性能问题。 再进一步， 如果老年代满了， Minor GC 后会进行 Full GC， 这将导致遍历整个堆， 称为提升失败（Promotion Failure）。</p>
<p><strong><em>早提升的原因</em></strong></p>
<ol>
<li><p>Survivor空间太小，容纳不下全部的运行时短生命周期的对象，如果是这个原因，可以尝试将Survivor调大，否则端生命周期的对象提升过快，导致老年代很快就被占满，从而引起频繁的full gc；</p>
</li>
<li><p>对象太大，Survivor和Eden没有足够大的空间来存放这些大象；</p>
</li>
</ol>
<p><strong><em>提升失败原因</em></strong></p>
<p>当提升的时候，发现老年代也没有足够的连续空间来容纳该对象。</p>
<p>为什么是没有足够的连续空间而不是空闲空间呢？</p>
<p>老年代容纳不下提升的对象有两种情况：</p>
<ol>
<li><p>老年代空闲空间不够用了；</p>
</li>
<li><p>老年代虽然空闲空间很多，但是碎片太多，没有连续的空闲空间存放该对象；</p>
</li>
</ol>
<p><strong><em>解决方法</em></strong></p>
<ol>
<li><p>如果是因为内存碎片导致的大对象提升失败，CMS需要进行空间整理压缩；</p>
</li>
<li><p>如果是因为提升过快导致的，说明Survivor 空闲空间不足，那么可以尝试调大 Survivor；</p>
</li>
<li><p>如果是因为老年代空间不够导致的，尝试将CMS触发的阈值调低；</p>
</li>
</ol>
<h2 id="其它导致回收停顿时间变长原因"><a href="#其它导致回收停顿时间变长原因" class="headerlink" title="其它导致回收停顿时间变长原因"></a>其它导致回收停顿时间变长原因</h2><p>linux使用了swap，内存换入换出（vmstat），尤其是开启了大内存页的时候，因为swap只支持4k的内存页，大内存页的大小为2M，大内存页在swap的交换的时候需要先将swap中4k内存页合并成一个大内存页再放入内存或将大内存页切分为4k的内存页放入swap，合并和切分的操作会导致操作系统占用cup飙高，用户cpu占用反而很低；</p>
<p>除了swap交换外，网络io（netstat）、磁盘I/O （iostat）在 GC 过程中发生会使 GC 时间变长。</p>
<p>如果是以上原因，就要去查看gc日志中的Times耗时：</p>
<p>[Times: user=0.00 sys=0.00, real=0.00 secs]</p>
<p>user是用户线程占用的时间，sys是系统线程占用的时间，如果是io导致的问题，会有两种情况</p>
<ol>
<li>user与sys时间都非常小，但是real却很长，如下：</li>
</ol>
<p>[ Times: user=0.51 sys=0.10, real=5.00 secs ]</p>
<p>user+sys的时间远远小于real的值，这种情况说明停顿的时间并不是消耗在cup执行上了，不是cup肯定就是io导致的了，所以这时候要去检查系统的io情况。</p>
<p>sys时间很长，user时间很短，real几乎等于sys的时间，如下：</p>
<p>[ Times: user=0.11 sys=31.10, real=33.12 secs ]</p>
<p>这时候其中一种原因是开启了大内存页，还开启了swap，大内存进行swap交换时会有这种现象；</p>
<h2 id="增加线程数"><a href="#增加线程数" class="headerlink" title="增加线程数"></a>增加线程数</h2><p>CMS默认启动的回收线程数目是 (ParallelGCThreads + 3)/4) ，这里的ParallelGCThreads是年轻代的并行收集线程数，感觉有点怪怪的；</p>
<p>年轻代的并行收集线程数默认是(ncpus &lt;= 8) ? ncpus : 3 + ((ncpus * 5) / 8)，可以通过-XX:ParallelGCThreads= N 来调整；</p>
<p>如果要直接设定CMS回收线程数，可以通过-XX:ParallelCMSThreads=n，注意这个n不能超过cpu线程数，需要注意的是增加gc线程数，就会和应用争抢资源；</p>
<p>参考</p>
<p><a target="_blank" rel="noopener" href="https://plumbr.eu/handbook/garbage-collection-algorithms-implementations#concurrent-mark-and-sweep">https://plumbr.eu/handbook/garbage-collection-algorithms-implementations#concurrent-mark-and-sweep</a></p>
<p><a target="_blank" rel="noopener" href="http://www.infoq.com/cn/presentations/a-long-period-of-atypical-jvm-gc-caused-by-os/">http://www.infoq.com/cn/presentations/a-long-period-of-atypical-jvm-gc-caused-by-os/</a></p>
<p>GC Cause</p>
<p>Heap Inspection Initiated GC</p>
<p>因为执行了jmap -histo:live 触发的gc</p>
<p>![](_v_images/20200621105321510_162455977.png =546x)</p>
<p>![](_v_images/20200621105825309_1900989220.png =541x)</p>
<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><ul>
<li><a target="_blank" rel="noopener" href="https://docs.oracle.com/javase/8/">Java 8 document</a></li>
<li><a target="_blank" rel="noopener" href="https://docs.oracle.com/javase/8/docs/technotes/guides/vm/gctuning/cms.html">Java 8 CMS collector</a></li>
<li><a target="_blank" rel="noopener" href="https://www.iteye.com/blog/zhanjia-2435266">Java之CMS GC的7个阶段</a></li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://example.com/bigdata/Flink/flink-on-yarn/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="aaronzhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Guadazi">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/bigdata/Flink/flink-on-yarn/" class="post-title-link" itemprop="url">Flink on yarn</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2019-08-10 14:58:00" itemprop="dateCreated datePublished" datetime="2019-08-10T14:58:00+08:00">2019-08-10</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-04-19 15:02:11" itemprop="dateModified" datetime="2021-04-19T15:02:11+08:00">2021-04-19</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/bigdata/" itemprop="url" rel="index"><span itemprop="name">bigdata</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="flink-on-yarn部署"><a href="#flink-on-yarn部署" class="headerlink" title="flink on yarn部署"></a>flink on yarn部署</h2><p>flink on yarn需要的组件与版本如下</p>
<ol>
<li>Zookeeper 3.4.9 用于做Flink的JobManager的HA服务</li>
<li>hadoop 2.7.2 搭建HDFS和Yarn</li>
<li>flink 1.3.2 或者 1.4.1版本（scala 2.11）</li>
</ol>
<p>Zookeeper, HDFS 和 Yarn 的组件的安装可以参照网上的教程。</p>
<p>在zookeeper，HDFS 和Yarn的组件的安装好的前提下，在客户机上提交Flink任务，具体流程如下：</p>
<ul>
<li>在启动Yarn-Session 之前， 设置好HADOOP_HOME,YARN_CONF_DIR ， HADOOP_CONF_DIR环境变量中三者的一个。如下所示， 根据具体的hadoop 路径来设置<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">export</span> HADOOP_HOME=/usr/<span class="built_in">local</span>/hadoop-current</span></span><br></pre></td></tr></table></figure></li>
<li>配置flink 目录下的flink-conf.yaml, 如下所示<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">jobmanager.rpc.address:</span> <span class="string">localhost</span></span><br><span class="line"><span class="attr">jobmanager.rpc.port:</span> <span class="number">6123</span></span><br><span class="line"><span class="attr">jobmanager.heap.mb:</span> <span class="number">256</span></span><br><span class="line"><span class="attr">taskmanager.heap.mb:</span> <span class="number">512</span></span><br><span class="line"><span class="attr">taskmanager.numberOfTaskSlots:</span> <span class="number">1</span></span><br><span class="line"><span class="attr">taskmanager.memory.preallocate:</span> <span class="literal">false</span></span><br><span class="line"><span class="attr">parallelism.default:</span> <span class="number">1</span></span><br><span class="line"><span class="attr">jobmanager.web.port:</span> <span class="number">8081</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># yarn</span></span><br><span class="line"><span class="attr">yarn.maximum-failed-containers:</span> <span class="number">99999</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#akka config</span></span><br><span class="line"><span class="attr">akka.watch.heartbeat.interval:</span> <span class="number">5</span> <span class="string">s</span></span><br><span class="line"><span class="attr">akka.watch.heartbeat.pause:</span> <span class="number">20</span> <span class="string">s</span></span><br><span class="line"><span class="attr">akka.ask.timeout:</span> <span class="number">60</span> <span class="string">s</span></span><br><span class="line"><span class="attr">akka.framesize:</span> <span class="string">20971520b</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#high-avaliability</span></span><br><span class="line"><span class="attr">high-availability:</span> <span class="string">zookeeper</span></span><br><span class="line"><span class="comment">## 根据安装的zookeeper信息填写</span></span><br><span class="line"><span class="attr">high-availability.zookeeper.quorum:</span> <span class="number">10.141</span><span class="number">.61</span><span class="number">.226</span><span class="string">:2181,10.141.53.244:2181,10.141.18.219:2181</span></span><br><span class="line"><span class="attr">high-availability.zookeeper.path.root:</span> <span class="string">/flink</span></span><br><span class="line"><span class="comment">## HA 信息存储到HDFS的目录，根据各自的Hdfs情况修改</span></span><br><span class="line"><span class="attr">high-availability.zookeeper.storageDir:</span> <span class="string">hdfs://hdcluster/flink/recovery/</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#checkpoint config</span></span><br><span class="line"><span class="attr">state.backend:</span> <span class="string">rocksdb</span></span><br><span class="line"><span class="comment">## checkpoint到HDFS的目录 根据各自安装的HDFS情况修改</span></span><br><span class="line"><span class="attr">state.backend.fs.checkpointdir:</span> <span class="string">hdfs://hdcluster/flink/checkpoint</span></span><br><span class="line"><span class="comment">## 对外checkpoint到HDFS的目录</span></span><br><span class="line"><span class="attr">state.checkpoints.dir:</span> <span class="string">hdfs://hdcluster/flink/savepoint</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#memory config</span></span><br><span class="line"><span class="attr">env.java.opts:</span> <span class="string">-XX:+UseConcMarkSweepGC</span> <span class="string">-XX:CMSInitiatingOccupancyFraction=75</span> <span class="string">-XX:+UseCMSInitiatingOccupancyOnly</span> <span class="string">-XX:+AlwaysPreTouch</span> <span class="string">-server</span> <span class="string">-XX:+HeapDumpOnOutOfMemoryError</span></span><br><span class="line"><span class="attr">yarn.heap-cutoff-ratio:</span> <span class="number">0.2</span></span><br><span class="line"><span class="attr">taskmanager.memory.off-heap:</span> <span class="literal">true</span></span><br><span class="line"></span><br></pre></td></tr></table></figure></li>
<li>提交Yarn-Session，切换到flink的bin 目录下,提交命令如下<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> ./yarn-session.sh -n 2 -s 6 -jm 3072 -tm 6144 -nm <span class="built_in">test</span> -d</span></span><br></pre></td></tr></table></figure>
启动yarn-session的参数解释如下</li>
</ul>
<table>
<thead>
<tr>
<th>参数</th>
<th>参数解释</th>
<th>设置推荐</th>
</tr>
</thead>
<tbody><tr>
<td>-n(–container)</td>
<td>taskmanager的数量</td>
<td></td>
</tr>
<tr>
<td>-s(–slots)</td>
<td>用启动应用所需的slot数量/ -s 的值向上取整，有时可以多一些taskmanager，做冗余 每个taskmanager的slot数量，默认一个slot一个core，默认每个taskmanager的slot的个数为1</td>
<td>6～10</td>
</tr>
<tr>
<td>-jm</td>
<td>jobmanager的内存（单位MB)</td>
<td>3072</td>
</tr>
<tr>
<td>-tm</td>
<td>每个taskmanager的内存（单位MB)</td>
<td>根据core 与内存的比例来设置，-s的值＊ （core与内存的比）来算</td>
</tr>
<tr>
<td>-nm</td>
<td>yarn 的appName(现在yarn的ui上的名字)｜</td>
<td></td>
</tr>
<tr>
<td>-d</td>
<td>后台执行</td>
<td></td>
</tr>
</tbody></table>
<ul>
<li>提交yarn－session 后，可以在yarn的ui上看到一个应用（应用有一个appId）, 切换到flink的bin目录下，提交flink 应用。命令如下<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> ./flink -run file:///home/yarn/test.jar -a 1 -p 12 -yid appId -nm flink-test -d</span></span><br></pre></td></tr></table></figure>
启动flink 应用的参数解释如下</li>
</ul>
<table>
<thead>
<tr>
<th>参数</th>
<th>参数解释</th>
</tr>
</thead>
<tbody><tr>
<td>-j</td>
<td>运行flink 应用的jar所在的目录</td>
</tr>
<tr>
<td>-a</td>
<td>运行flink 应用的主方法的参数</td>
</tr>
<tr>
<td>-p</td>
<td>运行flink应用的并行度</td>
</tr>
<tr>
<td>-c</td>
<td>运行flink应用的主类, 可以通过在打包设置主类</td>
</tr>
<tr>
<td>-nm</td>
<td>flink 应用名字，在flink-ui 上面展示</td>
</tr>
<tr>
<td>-d</td>
<td>后台执行</td>
</tr>
<tr>
<td>–fromsavepoint</td>
<td>flink 应用启动的状态恢复点</td>
</tr>
</tbody></table>
<ul>
<li>启动flink应用成功，即可在yarn ui 点击对应应用的ApplicationMaster链接,既可以查看flink-ui ，并查看flink 应用运行情况。</li>
</ul>
<p>注：在安装部署遇到任何问题，可以在小象问答，微信群以及私聊提出，我们一般会在晚上作答（由于白天要上班，作答不及时请谅解。）</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://example.com/database/MySQL/01.MySQL/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="aaronzhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Guadazi">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/database/MySQL/01.MySQL/" class="post-title-link" itemprop="url">MacOS中MySQL密码丢失处理</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2019-08-10 14:58:00" itemprop="dateCreated datePublished" datetime="2019-08-10T14:58:00+08:00">2019-08-10</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-04-19 15:02:11" itemprop="dateModified" datetime="2021-04-19T15:02:11+08:00">2021-04-19</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/bigdata/" itemprop="url" rel="index"><span itemprop="name">bigdata</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="01-MySQL"><a href="#01-MySQL" class="headerlink" title="01.MySQL"></a>01.MySQL</h1><h2 id="修改root密码-不成功"><a href="#修改root密码-不成功" class="headerlink" title="修改root密码 不成功"></a>修改root密码 不成功</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">Open &amp; Edit /etc/my.cnf or /etc/mysql/my.cnf, depending on your distro.</span><br><span class="line">Add skip-grant-tables under [mysqld]</span><br><span class="line">Restart Mysql</span><br><span class="line">You should be able to login to mysql now using the below command mysql -u root -p</span><br><span class="line">Run mysql&gt; flush privileges;</span><br><span class="line">Set new password by ALTER USER &#x27;root&#x27;@&#x27;localhost&#x27; IDENTIFIED BY &#x27;NewPassword&#x27;;</span><br><span class="line">Go back to /etc/my.cnf and remove/comment skip-grant-tables</span><br><span class="line">Restart Mysql</span><br><span class="line">Now you will be able to login with the new password mysql -u root -p</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">mysql&gt;</span><span class="bash"> drop user root@localhost;</span></span><br><span class="line">Query OK, 0 rows affected (0.00 sec)</span><br><span class="line"></span><br><span class="line"><span class="meta">mysql&gt;</span><span class="bash"> flush privileges;</span></span><br><span class="line">Query OK, 0 rows affected (0.00 sec)</span><br><span class="line"></span><br><span class="line"><span class="meta">mysql&gt;</span><span class="bash"> create user root@localhost identified by <span class="string">&#x27;abc.123&#x27;</span>;</span></span><br><span class="line">Query OK, 0 rows affected (0.02 sec)</span><br><span class="line"></span><br><span class="line">grant all privileges on *.* to root@localhost;</span><br><span class="line"></span><br><span class="line">flush privileges;</span><br></pre></td></tr></table></figure>
<h2 id="mac-brew重装-不成功"><a href="#mac-brew重装-不成功" class="headerlink" title="mac brew重装 不成功"></a>mac brew重装 不成功</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">brew uninstall mysql --ignore-dependencies</span><br><span class="line">sudo rm -rf /usr/local/Cellar/mysql</span><br><span class="line">brew cleanup</span><br><span class="line">sudo rm -rf /usr/local/var/mysql</span><br><span class="line">brew install mysql</span><br></pre></td></tr></table></figure>
<h2 id="索引"><a href="#索引" class="headerlink" title="索引"></a>索引</h2><h3 id="创建索引"><a href="#创建索引" class="headerlink" title="创建索引"></a>创建索引</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> t_zxg_hot_news_result <span class="keyword">add</span> index</span><br><span class="line"> idx_hot_news_cnt (ftype,fcnt,ftime,fnews_id,fmsg_type);</span><br></pre></td></tr></table></figure>
<h3 id="删除索引"><a href="#删除索引" class="headerlink" title="删除索引"></a>删除索引</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> t_zxg_hot_news_result <span class="keyword">drop</span> index idx_hot_news_cnt ;</span><br></pre></td></tr></table></figure>
<h3 id="查询索引"><a href="#查询索引" class="headerlink" title="查询索引"></a>查询索引</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">show</span> index <span class="keyword">from</span> t_zxg_hot_news_result;</span><br><span class="line"><span class="operator">+</span><span class="comment">-----------------------+------------+------------------+--------------+-------------+-----------+-------------+----------+--------+------+------------+---------+---------------+</span></span><br><span class="line"><span class="operator">|</span> <span class="keyword">Table</span>                 <span class="operator">|</span> Non_unique <span class="operator">|</span> Key_name         <span class="operator">|</span> Seq_in_index <span class="operator">|</span> Column_name <span class="operator">|</span> <span class="keyword">Collation</span> <span class="operator">|</span> <span class="keyword">Cardinality</span> <span class="operator">|</span> Sub_part <span class="operator">|</span> Packed <span class="operator">|</span> <span class="keyword">Null</span> <span class="operator">|</span> Index_type <span class="operator">|</span> Comment <span class="operator">|</span> Index_comment <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-----------------------+------------+------------------+--------------+-------------+-----------+-------------+----------+--------+------+------------+---------+---------------+</span></span><br><span class="line"><span class="operator">|</span> t_zxg_hot_news_result <span class="operator">|</span> <span class="number">0</span>          <span class="operator">|</span> <span class="keyword">PRIMARY</span>          <span class="operator">|</span> <span class="number">1</span>            <span class="operator">|</span> fid         <span class="operator">|</span> A         <span class="operator">|</span> <span class="number">765715</span>      <span class="operator">|</span> <span class="operator">&lt;</span><span class="keyword">null</span><span class="operator">&gt;</span>   <span class="operator">|</span> <span class="operator">&lt;</span><span class="keyword">null</span><span class="operator">&gt;</span> <span class="operator">|</span>      <span class="operator">|</span> BTREE      <span class="operator">|</span>         <span class="operator">|</span>               <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> t_zxg_hot_news_result <span class="operator">|</span> <span class="number">1</span>          <span class="operator">|</span> idx_hot_news_cnt <span class="operator">|</span> <span class="number">1</span>            <span class="operator">|</span> ftype       <span class="operator">|</span> A         <span class="operator">|</span> <span class="number">5</span>           <span class="operator">|</span> <span class="operator">&lt;</span><span class="keyword">null</span><span class="operator">&gt;</span>   <span class="operator">|</span> <span class="operator">&lt;</span><span class="keyword">null</span><span class="operator">&gt;</span> <span class="operator">|</span>      <span class="operator">|</span> BTREE      <span class="operator">|</span>         <span class="operator">|</span>               <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> t_zxg_hot_news_result <span class="operator">|</span> <span class="number">1</span>          <span class="operator">|</span> idx_hot_news_cnt <span class="operator">|</span> <span class="number">2</span>            <span class="operator">|</span> fcnt        <span class="operator">|</span> A         <span class="operator">|</span> <span class="number">4072</span>        <span class="operator">|</span> <span class="operator">&lt;</span><span class="keyword">null</span><span class="operator">&gt;</span>   <span class="operator">|</span> <span class="operator">&lt;</span><span class="keyword">null</span><span class="operator">&gt;</span> <span class="operator">|</span>      <span class="operator">|</span> BTREE      <span class="operator">|</span>         <span class="operator">|</span>               <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> t_zxg_hot_news_result <span class="operator">|</span> <span class="number">1</span>          <span class="operator">|</span> idx_hot_news_cnt <span class="operator">|</span> <span class="number">3</span>            <span class="operator">|</span> ftime       <span class="operator">|</span> A         <span class="operator">|</span> <span class="number">49236</span>       <span class="operator">|</span> <span class="operator">&lt;</span><span class="keyword">null</span><span class="operator">&gt;</span>   <span class="operator">|</span> <span class="operator">&lt;</span><span class="keyword">null</span><span class="operator">&gt;</span> <span class="operator">|</span> YES  <span class="operator">|</span> BTREE      <span class="operator">|</span>         <span class="operator">|</span>               <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> t_zxg_hot_news_result <span class="operator">|</span> <span class="number">1</span>          <span class="operator">|</span> idx_hot_news_cnt <span class="operator">|</span> <span class="number">4</span>            <span class="operator">|</span> fnews_id    <span class="operator">|</span> A         <span class="operator">|</span> <span class="number">800020</span>      <span class="operator">|</span> <span class="operator">&lt;</span><span class="keyword">null</span><span class="operator">&gt;</span>   <span class="operator">|</span> <span class="operator">&lt;</span><span class="keyword">null</span><span class="operator">&gt;</span> <span class="operator">|</span>      <span class="operator">|</span> BTREE      <span class="operator">|</span>         <span class="operator">|</span>               <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> t_zxg_hot_news_result <span class="operator">|</span> <span class="number">1</span>          <span class="operator">|</span> idx_hot_news_cnt <span class="operator">|</span> <span class="number">5</span>            <span class="operator">|</span> fmsg_type   <span class="operator">|</span> A         <span class="operator">|</span> <span class="number">792176</span>      <span class="operator">|</span> <span class="operator">&lt;</span><span class="keyword">null</span><span class="operator">&gt;</span>   <span class="operator">|</span> <span class="operator">&lt;</span><span class="keyword">null</span><span class="operator">&gt;</span> <span class="operator">|</span>      <span class="operator">|</span> BTREE      <span class="operator">|</span>         <span class="operator">|</span>               <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-----------------------+------------+------------------+--------------+-------------+-----------+-------------+----------+--------+------+------------+---------+---------------+</span></span><br></pre></td></tr></table></figure>





      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/7/"><i class="fa fa-angle-left" aria-label="Previous page"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/7/">7</a><span class="page-number current">8</span><a class="page-number" href="/page/9/">9</a><span class="space">&hellip;</span><a class="page-number" href="/page/24/">24</a><a class="extend next" rel="next" href="/page/9/"><i class="fa fa-angle-right" aria-label="Next page"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">aaronzhang</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">237</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">16</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">127</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">aaronzhang</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  

</body>
</html>
