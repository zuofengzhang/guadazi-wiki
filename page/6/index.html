<!DOCTYPE html>
<html lang="zh-Hans">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta property="og:type" content="website">
<meta property="og:title" content="Guadazi">
<meta property="og:url" content="http://example.com/page/6/index.html">
<meta property="og:site_name" content="Guadazi">
<meta property="og:locale">
<meta property="article:author" content="aaronzhang">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://example.com/page/6/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'zh-Hans'
  };
</script>

  <title>Guadazi</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Guadazi</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://example.com/bigdata/Spark/Spark-SQL/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="aaronzhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Guadazi">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/bigdata/Spark/Spark-SQL/" class="post-title-link" itemprop="url">Spark SQL</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-01-15 19:47:00" itemprop="dateCreated datePublished" datetime="2021-01-15T19:47:00+08:00">2021-01-15</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-06-01 20:52:40" itemprop="dateModified" datetime="2021-06-01T20:52:40+08:00">2021-06-01</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/bigdata/" itemprop="url" rel="index"><span itemprop="name">bigdata</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="Spark-SQL"><a href="#Spark-SQL" class="headerlink" title="Spark-SQL"></a>Spark-SQL</h1><h2 id="join"><a href="#join" class="headerlink" title="join"></a>join</h2><p>Spark 中支持多种连接类型：</p>
<ul>
<li>Inner Join : 内连接；</li>
<li>Full Outer Join : 全外连接；</li>
<li>Left Outer Join : 左外连接；</li>
<li>Right Outer Join : 右外连接；</li>
<li>Left Semi Join : 左半连接；</li>
<li>Left Anti Join : 左反连接；</li>
<li>Natural Join : 自然连接；</li>
<li>Cross (or Cartesian) Join : 交叉 (或笛卡尔) 连接</li>
</ul>
<p><img src="vx_images/289801248595.png" alt="SQL JOINS"></p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">emp 员工表</span><br><span class="line"> <span class="operator">|</span><span class="comment">-- ENAME: 员工姓名</span></span><br><span class="line"> <span class="operator">|</span><span class="comment">-- DEPTNO: 部门编号</span></span><br><span class="line"> <span class="operator">|</span><span class="comment">-- EMPNO: 员工编号</span></span><br><span class="line"> <span class="operator">|</span><span class="comment">-- HIREDATE: 入职时间</span></span><br><span class="line"> <span class="operator">|</span><span class="comment">-- JOB: 职务</span></span><br><span class="line"> <span class="operator">|</span><span class="comment">-- MGR: 上级编号</span></span><br><span class="line"> <span class="operator">|</span><span class="comment">-- SAL: 薪资</span></span><br><span class="line"> <span class="operator">|</span><span class="comment">-- COMM: 奖金  </span></span><br><span class="line"></span><br><span class="line">dept 部门表</span><br><span class="line"> <span class="operator">|</span><span class="comment">-- DEPTNO: 部门编号</span></span><br><span class="line"> <span class="operator">|</span><span class="comment">-- DNAME:  部门名称</span></span><br><span class="line"> <span class="operator">|</span><span class="comment">-- LOC:    部门所在城市</span></span><br><span class="line"></span><br><span class="line"><span class="comment">-- LEFT SEMI JOIN</span></span><br><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> emp <span class="keyword">LEFT</span> SEMI <span class="keyword">JOIN</span> dept <span class="keyword">ON</span> emp.deptno <span class="operator">=</span> dept.deptno</span><br><span class="line"><span class="comment">-- 等价于如下的 IN 语句</span></span><br><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> emp <span class="keyword">WHERE</span> deptno <span class="keyword">IN</span> (<span class="keyword">SELECT</span> deptno <span class="keyword">FROM</span> dept)</span><br><span class="line"></span><br><span class="line"><span class="comment">-- LEFT ANTI JOIN</span></span><br><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> emp <span class="keyword">LEFT</span> ANTI <span class="keyword">JOIN</span> dept <span class="keyword">ON</span> emp.deptno <span class="operator">=</span> dept.deptno</span><br><span class="line"><span class="comment">-- 等价于如下的 IN 语句</span></span><br><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> emp <span class="keyword">WHERE</span> deptno <span class="keyword">NOT</span> <span class="keyword">IN</span> (<span class="keyword">SELECT</span> deptno <span class="keyword">FROM</span> dept)</span><br><span class="line"></span><br><span class="line"><span class="comment">--CROSS JOIN</span></span><br><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> emp <span class="keyword">CROSS</span> <span class="keyword">JOIN</span> dept <span class="keyword">ON</span> emp.deptno <span class="operator">=</span> dept.deptno</span><br><span class="line"></span><br><span class="line"><span class="comment">--自然连接是在两张表中寻找那些数据类型和列名都相同的字段，然后自动地将他们连接起来，并返回所有符合条件的结果。</span></span><br><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> emp <span class="keyword">NATURAL</span> <span class="keyword">JOIN</span> dept</span><br><span class="line"></span><br><span class="line"><span class="comment">--程序自动推断出使用两张表都存在的 dept 列进行连接</span></span><br><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> emp <span class="keyword">JOIN</span> dept <span class="keyword">ON</span> emp.deptno <span class="operator">=</span> dept.deptno</span><br></pre></td></tr></table></figure>
<h3 id="内部实现"><a href="#内部实现" class="headerlink" title="内部实现"></a>内部实现</h3><p>broadcast join –&gt; hash join  –&gt; sort-merge join</p>
<p>在对大表与大表之间进行连接操作时，通常都会触发 <code>Shuffle Join</code>，两表的所有分区节点会进行 <code>All-to-All</code> 的通讯，这种查询通常比较昂贵，会对网络 IO 会造成比较大的负担。</p>
<p><img src="vx_images/4246497595210.png" alt="https://github.com/heibaiying"></p>
<p>而对于大表和小表的连接操作，Spark 会在一定程度上进行优化，如果小表的数据量小于 Worker Node 的内存空间，Spark 会考虑将小表的数据广播到每一个 Worker Node，在每个工作节点内部执行连接计算，这可以降低网络的 IO，但会加大每个 Worker Node 的 CPU 负担。</p>
<p><img src="vx_images/4195188806118"></p>
<p>是否采用广播方式进行 <code>Join</code> 取决于程序内部对小表的判断，如果想明确使用广播方式进行 <code>Join</code>，则可以在 DataFrame API 中使用 <code>broadcast</code> 方法指定需要广播的小表：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">empDF.join(broadcast(deptDF), joinExpression).show()</span><br></pre></td></tr></table></figure>

<h2 id="优化"><a href="#优化" class="headerlink" title="优化"></a>优化</h2><table>
<thead>
<tr>
<th align="left"><strong>优化规则</strong></th>
<th align="left"><strong>规则名称</strong></th>
<th align="left"><strong>简介</strong></th>
</tr>
</thead>
<tbody><tr>
<td align="left">列裁剪</td>
<td align="left">column_prune</td>
<td align="left">对于上层算子不需要的列，不在下层算子输出该列，减少计算</td>
</tr>
<tr>
<td align="left">子查询去关联</td>
<td align="left">decorrelate</td>
<td align="left">尝试对相关子查询进行改写，将其转换为普通 join 或 aggregation 计算</td>
</tr>
<tr>
<td align="left">聚合消除</td>
<td align="left">aggregation_eliminate</td>
<td align="left">尝试消除执行计划中的某些不必要的聚合算子</td>
</tr>
<tr>
<td align="left">投影消除</td>
<td align="left">projection_eliminate</td>
<td align="left">消除执行计划中不必要的投影算子</td>
</tr>
<tr>
<td align="left">最大最小消除</td>
<td align="left">max_min_eliminate</td>
<td align="left">改写聚合中的 max/min 计算，转化为 <code>order by</code> + <code>limit 1</code></td>
</tr>
<tr>
<td align="left">谓词下推</td>
<td align="left">predicate_push_down</td>
<td align="left">尝试将执行计划中过滤条件下推到离数据源更近的算子上</td>
</tr>
<tr>
<td align="left">外连接消除</td>
<td align="left">outer_join_eliminate</td>
<td align="left">尝试消除执行计划中不必要的 left join 或者 right join</td>
</tr>
<tr>
<td align="left">分区裁剪</td>
<td align="left">partition_processor</td>
<td align="left">将分区表查询改成为用 union all，并裁剪掉不满足过滤条件的分区</td>
</tr>
<tr>
<td align="left">聚合下推</td>
<td align="left">aggregation_push_down</td>
<td align="left">尝试将执行计划中的聚合算子下推到更底层的计算节点</td>
</tr>
<tr>
<td align="left">TopN 下推</td>
<td align="left">topn_push_down</td>
<td align="left">尝试将执行计划中的 TopN 算子下推到离数据源更近的算子上</td>
</tr>
<tr>
<td align="left">Join 重排序</td>
<td align="left">join_reorder</td>
<td align="left">对多表 join 确定连接顺序</td>
</tr>
</tbody></table>
<h2 id="逻辑优化"><a href="#逻辑优化" class="headerlink" title="逻辑优化"></a>逻辑优化</h2><h3 id="子查询相关的优化"><a href="#子查询相关的优化" class="headerlink" title="子查询相关的优化"></a>子查询相关的优化</h3><p>关联子查询去关联</p>
<h3 id="列裁剪"><a href="#列裁剪" class="headerlink" title="列裁剪"></a>列裁剪</h3><h3 id="关联子查询去关联"><a href="#关联子查询去关联" class="headerlink" title="关联子查询去关联"></a>关联子查询去关联</h3><h3 id="Max-Min-消除"><a href="#Max-Min-消除" class="headerlink" title="Max/Min 消除"></a>Max/Min 消除</h3><h3 id="谓词下推"><a href="#谓词下推" class="headerlink" title="谓词下推"></a>谓词下推</h3><h3 id="分区裁剪"><a href="#分区裁剪" class="headerlink" title="分区裁剪"></a>分区裁剪</h3><h3 id="TopN-和-Limit-下推"><a href="#TopN-和-Limit-下推" class="headerlink" title="TopN 和 Limit 下推"></a>TopN 和 Limit 下推</h3><h3 id="Join-Reorder"><a href="#Join-Reorder" class="headerlink" title="Join Reorder"></a>Join Reorder</h3><h2 id="物理优化"><a href="#物理优化" class="headerlink" title="物理优化"></a>物理优化</h2><h3 id="选择最优的索引进行表的访问"><a href="#选择最优的索引进行表的访问" class="headerlink" title="选择最优的索引进行表的访问"></a>选择最优的索引进行表的访问</h3><h3 id="收集统计信息来获得表的数据分布情况"><a href="#收集统计信息来获得表的数据分布情况" class="headerlink" title="收集统计信息来获得表的数据分布情况"></a>收集统计信息来获得表的数据分布情况</h3><h3 id="在错误索引的解决方案中会介绍当发现-TiDB-索引选错时，你应该使用那些手段来让它使用正确的索引"><a href="#在错误索引的解决方案中会介绍当发现-TiDB-索引选错时，你应该使用那些手段来让它使用正确的索引" class="headerlink" title="在错误索引的解决方案中会介绍当发现 TiDB 索引选错时，你应该使用那些手段来让它使用正确的索引"></a>在错误索引的解决方案中会介绍当发现 TiDB 索引选错时，你应该使用那些手段来让它使用正确的索引</h3><h3 id="在-Distinct-优化中会介绍在物理优化中会做的一个有关-DISTINCT-关键字的优化，在这一小节中会介绍它的优缺点以及如何使用它。"><a href="#在-Distinct-优化中会介绍在物理优化中会做的一个有关-DISTINCT-关键字的优化，在这一小节中会介绍它的优缺点以及如何使用它。" class="headerlink" title="在 Distinct 优化中会介绍在物理优化中会做的一个有关 DISTINCT 关键字的优化，在这一小节中会介绍它的优缺点以及如何使用它。"></a>在 Distinct 优化中会介绍在物理优化中会做的一个有关 DISTINCT 关键字的优化，在这一小节中会介绍它的优缺点以及如何使用它。</h3><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="[参考文献]"></a>[参考文献]</h2><ol>
<li><a target="_blank" rel="noopener" href="https://www.infoq.cn/article/an-article-mastering-sql-on-hadoop-core-technology">The Business Intelligence for Hadoop Benchmark</a></li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://example.com/bigdata/Flink/Flink-DataStream-join/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="aaronzhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Guadazi">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/bigdata/Flink/Flink-DataStream-join/" class="post-title-link" itemprop="url">Flink join</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-01-15 19:47:00" itemprop="dateCreated datePublished" datetime="2021-01-15T19:47:00+08:00">2021-01-15</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-06-01 20:52:39" itemprop="dateModified" datetime="2021-06-01T20:52:39+08:00">2021-06-01</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/bigdata/" itemprop="url" rel="index"><span itemprop="name">bigdata</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="flink-join"><a href="#flink-join" class="headerlink" title="flink join"></a>flink join</h1><h2 id="Cogroup"><a href="#Cogroup" class="headerlink" title="Cogroup"></a>Cogroup</h2><p>CoGroupFunction中会返回所有数据，不管有没有匹配上</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line">DataStream&lt;Tuple3&lt;Long, String, String&gt;&gt; input1 = ...;</span><br><span class="line">input1 = input1.assignTimestampsAndWatermarks(<span class="keyword">new</span> AscendingTimestampExtractor&lt;Tuple3&lt;Long, String, String&gt;&gt;() &#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">long</span> <span class="title">extractAscendingTimestamp</span><span class="params">(Tuple3&lt;Long, String, String&gt; arg0)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> arg0.f0;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line">DataStream&lt;Tuple2&lt;Long, String&gt;&gt; input2 = ...;</span><br><span class="line">input2 = input2.assignTimestampsAndWatermarks(<span class="keyword">new</span> AscendingTimestampExtractor&lt;Tuple2&lt;Long, String&gt;&gt;() &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">long</span> <span class="title">extractAscendingTimestamp</span><span class="params">(Tuple2&lt;Long, String&gt; stringStringTuple2)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> stringStringTuple2.f0;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line">input1.coGroup(input2).where(<span class="keyword">new</span> KeySelector&lt;Tuple3&lt;Long, String, String&gt;, String&gt;() &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">getKey</span><span class="params">(Tuple3&lt;Long, String, String&gt; itemEntity)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> itemEntity.f1;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;)</span><br><span class="line">.equalTo(<span class="keyword">new</span> KeySelector&lt;Tuple2&lt;Long, String&gt;, String&gt;() &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">getKey</span><span class="params">(Tuple2&lt;Long, String&gt; value)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> value.f1;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;)</span><br><span class="line">.window(TumblingEventTimeWindows.of(Time.minutes(<span class="number">1</span>)))</span><br><span class="line">.apply(<span class="keyword">new</span> CoGroupFunction&lt;Tuple3&lt;Long, String, String&gt;, Tuple2&lt;Long, String&gt;, String&gt;() &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">coGroup</span><span class="params">(Iterable&lt;Tuple3&lt;Long, String, String&gt;&gt; first,</span></span></span><br><span class="line"><span class="function"><span class="params">            Iterable&lt;Tuple2&lt;Long, String&gt;&gt; second, Collector&lt;String&gt; collector)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        StringBuilder buffer = <span class="keyword">new</span> StringBuilder();</span><br><span class="line">        buffer.append(<span class="string">&quot;DataStream first:\n&quot;</span>);</span><br><span class="line">        <span class="keyword">for</span> (Tuple3&lt;Long, String, String&gt; value : first) &#123;</span><br><span class="line">            buffer.append(value).append(<span class="string">&quot;\n&quot;</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        buffer.append(<span class="string">&quot;DataStream second:\n&quot;</span>);</span><br><span class="line">        <span class="keyword">for</span> (Tuple2&lt;Long, String&gt; value : second) &#123;</span><br><span class="line">            buffer.append(value.f0).append(<span class="string">&quot;=&gt;&quot;</span>).append(value.f1).append(<span class="string">&quot;\n&quot;</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        collector.collect(buffer.toString());</span><br><span class="line">    &#125;</span><br><span class="line">&#125;)</span><br><span class="line">.print();</span><br></pre></td></tr></table></figure>
<p>上面的例子，左流有三个元素 <code>Tuple3&lt;String,String,String&gt;</code>，右流有两个元素<code>Tuple2&lt;String,String&gt;</code><br>两个流第一个元素相互关联。分别指定两个流的事件时间字段。<br>两个流关联后，按照EventTime划分窗口。与单流类似。<br>不管元素是否可以关联上，都会输出</p>
<p>用户可以定义CoGroupFunction函数, 可以实现在窗口内，任意组合，如笛卡尔积</p>
<p>举例说明:</p>
<h2 id="window-Join"><a href="#window-Join" class="headerlink" title="window Join"></a>window Join</h2><h2 id="interval-join"><a href="#interval-join" class="headerlink" title="interval join"></a>interval join</h2><h2 id="broadcast-join"><a href="#broadcast-join" class="headerlink" title="broadcast join"></a>broadcast join</h2>
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://example.com/bigdata/Flink/Flink-window/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="aaronzhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Guadazi">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/bigdata/Flink/Flink-window/" class="post-title-link" itemprop="url">Flink-window</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-01-15 19:47:00" itemprop="dateCreated datePublished" datetime="2021-01-15T19:47:00+08:00">2021-01-15</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-06-01 20:52:39" itemprop="dateModified" datetime="2021-06-01T20:52:39+08:00">2021-06-01</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/bigdata/" itemprop="url" rel="index"><span itemprop="name">bigdata</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>窗口会自动管理状态和触发计算，Flink 提供了丰富的窗口函数来进行计算。主要包括以下两种：</p>
<ul>
<li>ProcessWindowFunction，全量计算会把所有数据缓存到状态里，一直到窗口结束时统一计算。相对来说，状态会比较大，计算效率也会低一些；</li>
<li>AggregateFunction，增量计算就是来一条数据就算一条，可能我们的状态就会特别的小，计算效率也会比 ProcessWindowFunction 高很多，但是如果状态存储在磁盘频繁访问状态可能会影响性能。</li>
</ul>
<p><img src="https://gitee.com/averyzhang/pic-go/raw/master/img/20210425094934"></p>
<h2 id="窗口的触发"><a href="#窗口的触发" class="headerlink" title="窗口的触发"></a>窗口的触发</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime);</span><br><span class="line">SingleOutputStreamOperator&lt;ItemEntity&gt; streamOperator = env</span><br><span class="line">        .socketTextStream(<span class="string">&quot;127.0.0.1&quot;</span>, <span class="number">9091</span>)</span><br><span class="line">        .map(<span class="keyword">new</span> MapFunction&lt;String, ItemEntity&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> ItemEntity <span class="title">map</span><span class="params">(String s)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                String[] split = <span class="keyword">null</span>;</span><br><span class="line">                <span class="keyword">if</span> (s.isEmpty() || (split = s.split(<span class="string">&quot;,&quot;</span>)).length != <span class="number">2</span>) &#123;</span><br><span class="line">                    <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">                &#125;</span><br><span class="line">                ItemEntity itemEntity = ItemEntity.builder().timestamp(split[<span class="number">0</span>])</span><br><span class="line">                        .eventId(split[<span class="number">1</span>])</span><br><span class="line">                        .build();</span><br><span class="line">                <span class="keyword">return</span> itemEntity;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;)</span><br><span class="line">        .filter(Objects::nonNull)</span><br><span class="line">        .assignTimestampsAndWatermarks(<span class="keyword">new</span> AscendingTimestampExtractor&lt;ItemEntity&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">long</span> <span class="title">extractAscendingTimestamp</span><span class="params">(ItemEntity itemEntity)</span> </span>&#123;</span><br><span class="line">                <span class="keyword">long</span> timestamp = itemEntity.getTimestamp();</span><br><span class="line">                <span class="keyword">return</span> timestamp;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line">streamOperator</span><br><span class="line">        .keyBy(<span class="keyword">new</span> KeySelector&lt;ItemEntity, String&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> String <span class="title">getKey</span><span class="params">(ItemEntity itemEntity)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                String eventId = itemEntity.getEventId();</span><br><span class="line">                <span class="keyword">return</span> eventId;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;)</span><br><span class="line">        .window(TumblingEventTimeWindows.of(Time.seconds(<span class="number">5</span>)))</span><br><span class="line">        .process(</span><br><span class="line">                <span class="keyword">new</span> ProcessWindowFunction&lt;ItemEntity, Tuple2&lt;String, String&gt;, String, TimeWindow&gt;() &#123;</span><br><span class="line">                    <span class="meta">@Override</span></span><br><span class="line">                    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">process</span><span class="params">(String s, Context context,</span></span></span><br><span class="line"><span class="function"><span class="params">                            Iterable&lt;ItemEntity&gt; iterable,</span></span></span><br><span class="line"><span class="function"><span class="params">                            Collector&lt;Tuple2&lt;String, String&gt;&gt; collector)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                        <span class="keyword">for</span> (ItemEntity itemEntity : iterable) &#123;</span><br><span class="line">                            <span class="keyword">long</span> timestamp = itemEntity.getTimestamp();</span><br><span class="line">                            Date date = <span class="keyword">new</span> Date(timestamp);</span><br><span class="line">                            SimpleDateFormat simpleDateFormat = <span class="keyword">new</span> SimpleDateFormat(</span><br><span class="line">                                    <span class="string">&quot;yyyy-MM-dd HH:mm:ss&quot;</span>);</span><br><span class="line">                            collector.collect(Tuple2.of(itemEntity.getEventId(),</span><br><span class="line">                                    timestamp + <span class="string">&quot;-&gt;&quot;</span> + simpleDateFormat.format(date)));</span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;)</span><br><span class="line">        .print();</span><br><span class="line">env.execute(<span class="string">&quot;test-window&quot;</span>);</span><br></pre></td></tr></table></figure>
<p>5秒的窗口</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">2020-04-01 00:01:00,1</span><br><span class="line">2020-04-01 00:01:00,1</span><br><span class="line">2020-04-01 00:01:06,1 # 触发计算</span><br><span class="line">(1,1585670460000-&gt;2020-04-01 00:01:00)</span><br><span class="line">(1,1585670460000-&gt;2020-04-01 00:01:00)</span><br><span class="line">2020-04-01 00:01:00,1 # 窗口已经关闭，旧数据</span><br><span class="line"><span class="meta">#</span><span class="bash"> WARN  org.apache.flink.streaming.api.functions.timestamps.AscendingTimestampExtractor [] - Timestamp monotony violated: 1585670460000 &lt; 1585670466000</span></span><br><span class="line">2020-04-01 00:01:06,1</span><br><span class="line">2020-04-01 00:01:07,1</span><br><span class="line">2020-04-01 00:01:11,1 # 触发计算</span><br><span class="line">(1,1585670466000-&gt;2020-04-01 00:01:06)</span><br><span class="line">(1,1585670466000-&gt;2020-04-01 00:01:06)</span><br><span class="line">(1,1585670467000-&gt;2020-04-01 00:01:07)</span><br></pre></td></tr></table></figure>
<h2 id="window的抽象概念"><a href="#window的抽象概念" class="headerlink" title="window的抽象概念"></a>window的抽象概念</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">Keyed Windows</span><br><span class="line"></span><br><span class="line">stream</span><br><span class="line">       .keyBy(...)               &lt;-  keyed versus non-keyed windows</span><br><span class="line">       .window(...)              &lt;-  required: <span class="string">&quot;assigner&quot;</span></span><br><span class="line">      [.trigger(...)]            &lt;-  optional: <span class="string">&quot;trigger&quot;</span> (<span class="keyword">else</span> <span class="keyword">default</span> trigger)</span><br><span class="line">      [.evictor(...)]            &lt;-  optional: <span class="string">&quot;evictor&quot;</span> (<span class="keyword">else</span> no evictor)</span><br><span class="line">      [.allowedLateness(...)]    &lt;-  optional: <span class="string">&quot;lateness&quot;</span> (<span class="keyword">else</span> zero)</span><br><span class="line">      [.sideOutputLateData(...)] &lt;-  optional: <span class="string">&quot;output tag&quot;</span> (<span class="keyword">else</span> no side output <span class="keyword">for</span> late data)</span><br><span class="line">       .reduce/aggregate/fold/apply()      &lt;-  required: <span class="string">&quot;function&quot;</span></span><br><span class="line">      [.getSideOutput(...)]      &lt;-  optional: <span class="string">&quot;output tag&quot;</span></span><br><span class="line">Non-Keyed Windows</span><br><span class="line"></span><br><span class="line">stream</span><br><span class="line">       .windowAll(...)           &lt;-  required: <span class="string">&quot;assigner&quot;</span></span><br><span class="line">      [.trigger(...)]            &lt;-  optional: <span class="string">&quot;trigger&quot;</span> (<span class="keyword">else</span> <span class="keyword">default</span> trigger)</span><br><span class="line">      [.evictor(...)]            &lt;-  optional: <span class="string">&quot;evictor&quot;</span> (<span class="keyword">else</span> no evictor)</span><br><span class="line">      [.allowedLateness(...)]    &lt;-  optional: <span class="string">&quot;lateness&quot;</span> (<span class="keyword">else</span> zero)</span><br><span class="line">      [.sideOutputLateData(...)] &lt;-  optional: <span class="string">&quot;output tag&quot;</span> (<span class="keyword">else</span> no side output <span class="keyword">for</span> late data)</span><br><span class="line">       .reduce/aggregate/fold/apply()      &lt;-  required: <span class="string">&quot;function&quot;</span></span><br><span class="line">      [.getSideOutput(...)]      &lt;-  optional: <span class="string">&quot;output tag&quot;</span></span><br></pre></td></tr></table></figure>
<h3 id="window-assigner"><a href="#window-assigner" class="headerlink" title="window assigner"></a>window assigner</h3><h3 id="window-trigger"><a href="#window-trigger" class="headerlink" title="window trigger"></a>window trigger</h3><h3 id="window-evictor"><a href="#window-evictor" class="headerlink" title="window evictor"></a>window evictor</h3><h2 id="windowOperator工作流程"><a href="#windowOperator工作流程" class="headerlink" title="windowOperator工作流程"></a>windowOperator工作流程</h2><p><img src="_v_images/20201206173044901_44534749.png"></p>
<h3 id="window-state"><a href="#window-state" class="headerlink" title="window state"></a>window state</h3><p><img src="_v_images/20201206173354630_1171217287.png"></p>
<h2 id="Session-window"><a href="#Session-window" class="headerlink" title="Session window"></a>Session window</h2><p><a target="_blank" rel="noopener" href="http://wuchong.me/blog/2016/06/06/flink-internals-session-window/">Flink 原理与实现：Session Window</a></p>
<p><code>SESSION(time_attr, interval)</code>定义一个会话时间窗口。<br>会话时间窗口没有一个固定的持续时间，但是它们的边界会根据 <code>interval</code> 所定义的不活跃时间所确定；即一个会话时间窗口在定义的间隔时间内没有时间出现，该窗口会被关闭。例如时间窗口的间隔时间是 30 分钟，当其不活跃的时间达到30分钟后，若观测到新的记录，则会启动一个新的会话时间窗口（否则该行数据会被添加到当前的窗口），且若在 30 分钟内没有观测到新纪录，这个窗口将会被关闭。会话时间窗口可以使用事件时间（批处理、流处理）或处理时间（流处理）。</p>
<p>流式数据处理中，很多操作要依赖于时间属性进行，因此时间属性也是流式引擎能够保证准确处理数据的基石。在这篇文章中，我们将对 Flink 中时间属性和窗口的实现逻辑进行分析。</p>
<h2 id="all-window-operator’s-parallelism-is-1"><a href="#all-window-operator’s-parallelism-is-1" class="headerlink" title="all window operator’s parallelism is 1"></a>all window operator’s parallelism is 1</h2><p>If the parallelism of the environment is set to 3 and you are using a WindowAll operator, only the window operator runs in parallelism 1. The sink will still be running with parallelism 3. Hence, the plan looks as follows:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">In_1 -\               /- Out_1</span><br><span class="line">In_2 --- WindowAll_1 --- Out_2</span><br><span class="line">In_3 -/               \- Out_3</span><br></pre></td></tr></table></figure>
<p>The WindowAll operator emits its output to its subsequent tasks using a round-robin strategy. That’s the reason for the different threads emitting the result records of program.</p>
<p>When you set the environment parallelism to 1, all operators run with a single task.</p>
<p>keyed-windown —分布式计算—&gt; all-windown</p>
<p>【参考文献】</p>
<ol>
<li><a target="_blank" rel="noopener" href="http://wuchong.me/blog/2016/05/25/flink-internals-window-mechanism/">flink原理与实现: window机制</a></li>
<li><a target="_blank" rel="noopener" href="https://xie.infoq.cn/article/0d5038bc42862b3db79c571bd">flink窗口应用与实现</a></li>
<li><a target="_blank" rel="noopener" href="https://blog.csdn.net/a1240466196/article/details/108334436">Flink原理: 窗口原理详解</a></li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://example.com/bigdata/iceberg/Apache%20iceberg%EF%BC%9ANetflix%20%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E7%9A%84%E5%9F%BA%E7%9F%B3/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="aaronzhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Guadazi">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/bigdata/iceberg/Apache%20iceberg%EF%BC%9ANetflix%20%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E7%9A%84%E5%9F%BA%E7%9F%B3/" class="post-title-link" itemprop="url">Apache iceberg：Netflix 数据仓库的基石</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-01-15 19:47:00" itemprop="dateCreated datePublished" datetime="2021-01-15T19:47:00+08:00">2021-01-15</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-06-01 20:52:39" itemprop="dateModified" datetime="2021-06-01T20:52:39+08:00">2021-06-01</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/bigdata/" itemprop="url" rel="index"><span itemprop="name">bigdata</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="Apache-iceberg：Netflix-数据仓库的基石"><a href="#Apache-iceberg：Netflix-数据仓库的基石" class="headerlink" title="Apache iceberg：Netflix 数据仓库的基石"></a>Apache iceberg：Netflix 数据仓库的基石</h1><p><a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s/acWcoZ25zDXetA3ewypG2g?spm=a2c6h.12873639.0.0.7e4b13839s5rpH">Apache iceberg：Netflix 数据仓库的基石</a></p>
<h2 id="5-year-challenges"><a href="#5-year-challenges" class="headerlink" title="5-year challenges"></a>5-year challenges</h2><p>智能处理引擎</p>
<ul>
<li>CBO，更好的join实现</li>
<li>缓存结果集，物化视图</li>
</ul>
<p>减少人工维护数据</p>
<ul>
<li>data librarian services 数据图书馆服务</li>
<li>declarative instead of imperative 陈述式而不是命令式</li>
</ul>
<h2 id="Problem-Whack-a-mole"><a href="#Problem-Whack-a-mole" class="headerlink" title="Problem Whack-a-mole"></a>Problem Whack-a-mole</h2><p>1、不安全的操作随处可见: 同时写多个分区，列重命名<br>2、和对象存储交互有时候会出现很大的问题: eventual consistency to performance problems(最终一致性的性能问题)、output committees can’t fix it<br>3、无休止的可扩展性挑战。</p>
<h2 id="iceberg"><a href="#iceberg" class="headerlink" title="iceberg"></a>iceberg</h2><ol>
<li>在单个文件中修改或跳过数据</li>
<li>当然多个文件也支持这些操作</li>
</ol>
<p><img src="_v_images/20201014205326408_1460756353.png"></p>
<p><img src="_v_images/20201014205344160_898972367.png"></p>
<p>Hive 表的核心思想是把数据组织成目录树，如上所述。</p>
<p>如果我们需要过滤数据，可以在 where 里面添加分区相关的信息。</p>
<p>带来的问题是如果一张表有很多分区，我们需要使用 HMS（Hive MetaStore）来记录这些分区，同时底层的文件系统（比如 HDFS）仍然需要在每个分区里面记录这些分区数据。</p>
<p>这就导致我们需要在 HMS 和 文件系统里面同时保存一些状态信息；因为缺乏锁机制，所以对上面两个系统进行修改也不能保证原子性。</p>
<p>当然 Hive 这样维护表也不是没有好处。这种设计使得很多引擎（Hive、Spark、Presto、Flink、Pig）都支持读写 Hive 表，同时支持很多第三方工具。简单和透明使得 Hive 表变得不可或缺的。</p>
<p>Iceberg 的目标包括：</p>
<p>1、成为静态数据交换的开放规范，维护一个清晰的格式规范，支持多语言，支持跨项目的需求等。<br>2、提升扩展性和可靠性。能够在一个节点上运行，也能在集群上运行。所有的修改都是原子性的，串行化隔离。原生支持云对象存储，支持多并发写。<br>3、修复持续的可用性问题，比如模式演进，分区隐藏，支持时间旅行、回滚等。</p>
<p>Iceberg 主要设计思想：</p>
<p>记录表在所有时间的所有文件，和 Delta Lake 或 Apache Hudi 一样，支持 snapshot，其是表在某个时刻的完整文件列表。每一次写操作都会生成一个新的快照。</p>
<p>读取数据的时候使用当前的快照，Iceberg 使用乐观锁机制来创建新的快照，然后提交。</p>
<p>Iceberg 这么设计的好处是：</p>
<ul>
<li>所有的修改都是原子性的；</li>
<li>没有耗时的文件系统操作；</li>
<li>快照是索引好的，以便加速读取；</li>
<li>CBO metrics 信息是可靠的；</li>
<li>更新支持版本，支持物化视图。</li>
</ul>
<p>Iceberg 在 Netflix 生产环境维护着数十 PB 的数据，数百万个分区。对大表进行查询能够提供低延迟的响应。</p>
<p>未来工作：1、支持 Spark 向量化以便实现快速的 bulk read，Presto 向量化已经支持。2、行级别的删除，支持 MERGE INTO 等</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://example.com/bigdata/Flink/Flink-SQL-syntactic-sugar/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="aaronzhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Guadazi">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/bigdata/Flink/Flink-SQL-syntactic-sugar/" class="post-title-link" itemprop="url">Flink-语法糖</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-01-15 19:47:00" itemprop="dateCreated datePublished" datetime="2021-01-15T19:47:00+08:00">2021-01-15</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-06-01 20:52:39" itemprop="dateModified" datetime="2021-06-01T20:52:39+08:00">2021-06-01</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/bigdata/" itemprop="url" rel="index"><span itemprop="name">bigdata</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="Flink-SQL语法"><a href="#Flink-SQL语法" class="headerlink" title="Flink-SQL语法"></a>Flink-SQL语法</h1><p><a target="_blank" rel="noopener" href="https://github.com/ververica/sql-training/wiki">Apache Flink SQL training</a></p>
<h2 id="group-window"><a href="#group-window" class="headerlink" title="group window"></a>group window</h2><p>groupByWindow会直接生成回撤流</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">insert</span>       </span><br><span class="line"><span class="keyword">into</span></span><br><span class="line">    dim_result_lct_activy_config</span><br><span class="line">    <span class="keyword">select</span></span><br><span class="line">        Fact_id,</span><br><span class="line">        <span class="built_in">LAST_VALUE</span>(Fact_name),</span><br><span class="line">        regexp_Replace( <span class="built_in">LAST_VALUE</span>(Fact_start_time), <span class="string">&#x27;-|:|\s&#x27;</span>,<span class="string">&#x27;&#x27;</span>) <span class="keyword">as</span> startTime,</span><br><span class="line">        regexp_Replace(<span class="built_in">LAST_VALUE</span>(Fact_end_time),<span class="string">&#x27;-|:|\s&#x27;</span>,<span class="string">&#x27;&#x27;</span>) <span class="keyword">as</span> endTime,</span><br><span class="line">        <span class="built_in">LAST_VALUE</span>(Fstate)                               </span><br><span class="line">    <span class="keyword">from</span></span><br><span class="line">        db_act_config_t_act_logic_config         </span><br><span class="line">    <span class="keyword">group</span> <span class="keyword">by</span></span><br><span class="line">        Fact_id</span><br></pre></td></tr></table></figure>
<p>这是一个同步数据的demo，db_act_config_t_act_logic_config 是kafka数据源，来自源MySQL的变更数据；dim_result_lct_activy_config是目的表，Fact_id为主键。</p>
<h4 id="group-window生成retract-stream"><a href="#group-window生成retract-stream" class="headerlink" title="group window生成retract stream"></a>group window生成retract stream</h4><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> mysql_sink <span class="keyword">select</span> fkey,<span class="built_in">count</span>(<span class="number">1</span>) <span class="keyword">as</span> cnt <span class="keyword">from</span> kafka_source <span class="keyword">group</span> <span class="keyword">by</span> fkey</span><br></pre></td></tr></table></figure>
<p>上述语句是一个group window, 每从kafka中过来一条数据，都会产生两条记录(<code>Tuple2&lt;Row,Boolean&gt;</code>), 删除旧记录，添加新记录。</p>
<p>group window会产生 <code>retract stream</code>, 下游系统必须支持<code>retract stream</code>，(目前共有三种sink: <code>AppendStreamSink</code>, <code>UpsertStreamSink</code>, <code>RetractStreamSink</code> )</p>
<p>Flink-connector-JDBC 使用的是<code>JDBCUpsertTableSink.java</code>写入MySQL, 支持Retract</p>
<p> <a target="_blank" rel="noopener" href="https://github.com/apache/flink/tree/master/flink-connectors/flink-jdbc/src/main/java/org/apache/flink/api/java/io/jdbc">https://github.com/apache/flink/tree/master/flink-connectors/flink-jdbc/src/main/java/org/apache/flink/api/java/io/jdbc</a> </p>
<p>Flink-connector-kafka 实现的是 <code>AppendStreamSink</code>，只支持insert，不支持retract. 会报错</p>
<p><code>AppendStreamTableSink requires that Table has only insert changes</code></p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> mysql_sink <span class="keyword">select</span> fkey,<span class="built_in">count</span>(<span class="number">1</span>) <span class="keyword">as</span> cnt <span class="keyword">from</span> kafka_source</span><br></pre></td></tr></table></figure>
<p>如果不带<code>group by</code>, 无法推导出unique key, 无法按照unique key更新</p>
<p><a target="_blank" rel="noopener" href="http://apache-flink.147419.n8.nabble.com/FlinkSQL-Upsert-Retraction-MySQL-td2785.html">http://apache-flink.147419.n8.nabble.com/FlinkSQL-Upsert-Retraction-MySQL-td2785.html</a></p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/** </span></span><br><span class="line"><span class="comment"> * Get dialect upsert statement, the database has its own upsert syntax, such as Mysql </span></span><br><span class="line"><span class="comment"> * using DUPLICATE KEY UPDATE, and PostgresSQL using ON CONFLICT... DO UPDATE SET.. </span></span><br><span class="line"><span class="comment"> * </span></span><br><span class="line"><span class="comment"> * <span class="doctag">@return</span> None if dialect does not support upsert statement, the writer will degrade to </span></span><br><span class="line"><span class="comment"> * the use of select + update/insert, this performance is poor. </span></span><br><span class="line"><span class="comment"> */</span> </span><br><span class="line"><span class="function"><span class="keyword">default</span> Optional&lt;String&gt; <span class="title">getUpsertStatement</span><span class="params">( </span></span></span><br><span class="line"><span class="function"><span class="params">   String tableName, String[] fieldNames, String[] uniqueKeyFields)</span> </span>&#123; </span><br><span class="line"><span class="keyword">return</span> Optional.empty(); </span><br><span class="line">&#125; </span><br></pre></td></tr></table></figure>

<p>不同的数据库产品有不同的语句，所以默认实现是delete +insert </p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Override</span> </span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">executeBatch</span><span class="params">()</span> <span class="keyword">throws</span> SQLException </span>&#123; </span><br><span class="line">  <span class="keyword">if</span> (keyToRows.size() &gt; <span class="number">0</span>) &#123; </span><br><span class="line">  <span class="keyword">for</span> (Map.Entry&lt;Row, Tuple2&lt;Boolean, Row&gt;&gt; entry : keyToRows.entrySet()) &#123; </span><br><span class="line">       Row pk = entry.getKey(); </span><br><span class="line">       Tuple2&lt;Boolean, Row&gt; tuple = entry.getValue(); </span><br><span class="line">       <span class="keyword">if</span> (tuple.f0) &#123; </span><br><span class="line">          processOneRowInBatch(pk, tuple.f1); </span><br><span class="line">       &#125; <span class="keyword">else</span> &#123; </span><br><span class="line">          setRecordToStatement(deleteStatement, pkTypes, pk); </span><br><span class="line">          deleteStatement.addBatch(); </span><br><span class="line">       &#125; </span><br><span class="line">     &#125; </span><br><span class="line">     internalExecuteBatch(); </span><br><span class="line">     deleteStatement.executeBatch(); </span><br><span class="line">     keyToRows.clear(); </span><br><span class="line">  &#125; </span><br><span class="line">&#125; </span><br></pre></td></tr></table></figure>
<h2 id="Over-window"><a href="#Over-window" class="headerlink" title="Over window"></a>Over window</h2><p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/92654574">SQL窗口函数</a> 传统SQL窗口函数的介绍</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> </span><br><span class="line">	to_char(SYSTIMESTAMP(),<span class="string">&#x27;yyyymmddhh24miss&#x27;</span>) fetl_time,</span><br><span class="line">	<span class="operator">*</span> </span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">(</span><br><span class="line">  <span class="keyword">select</span></span><br><span class="line">      <span class="operator">*</span>,</span><br><span class="line">  		<span class="built_in">row_number</span>() <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> fid <span class="keyword">order</span> <span class="keyword">by</span> fmodify_time <span class="keyword">desc</span>,exp_time_stample_order <span class="keyword">desc</span>) rn </span><br><span class="line">  <span class="keyword">from</span> </span><br><span class="line">  		db.table1 </span><br><span class="line">  <span class="keyword">where</span> </span><br><span class="line">  		fdate<span class="operator">=</span><span class="number">20210101</span></span><br><span class="line">) t</span><br><span class="line"><span class="keyword">where</span> rn<span class="operator">=</span><span class="number">1</span> </span><br></pre></td></tr></table></figure>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="built_in">COUNT</span>(amount) <span class="keyword">OVER</span> (</span><br><span class="line">  <span class="keyword">PARTITION</span> <span class="keyword">BY</span> <span class="keyword">user</span></span><br><span class="line">  <span class="keyword">ORDER</span> <span class="keyword">BY</span> proctime</span><br><span class="line">  <span class="keyword">ROWS</span> <span class="keyword">BETWEEN</span> <span class="number">2</span> PRECEDING <span class="keyword">AND</span> <span class="keyword">CURRENT</span> <span class="type">ROW</span>)</span><br><span class="line"><span class="keyword">FROM</span> Orders;</span><br><span class="line"><span class="keyword">SELECT</span> <span class="built_in">COUNT</span>(amount) <span class="keyword">OVER</span> w, <span class="built_in">SUM</span>(amount) <span class="keyword">OVER</span> w</span><br><span class="line"><span class="keyword">FROM</span> Orders </span><br><span class="line"><span class="keyword">WINDOW</span> w <span class="keyword">AS</span> (</span><br><span class="line">  <span class="keyword">PARTITION</span> <span class="keyword">BY</span> <span class="keyword">user</span></span><br><span class="line">  <span class="keyword">ORDER</span> <span class="keyword">BY</span> proctime</span><br><span class="line">  <span class="keyword">ROWS</span> <span class="keyword">BETWEEN</span> <span class="number">2</span> PRECEDING <span class="keyword">AND</span> <span class="keyword">CURRENT</span> <span class="type">ROW</span>)  ;</span><br></pre></td></tr></table></figure>

<h3 id="OVER-窗口应用示例"><a href="#OVER-窗口应用示例" class="headerlink" title="OVER 窗口应用示例"></a>OVER 窗口应用示例</h3><p>首先通过 DDL 定义源数据表和结果表，如下输入是用户行为消息，输出到计算结果消息。</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> `user_action` (</span><br><span class="line">    `user_id` <span class="type">VARCHAR</span>,</span><br><span class="line">    `page_id` <span class="type">VARCHAR</span>,</span><br><span class="line">    `action_type` <span class="type">VARCHAR</span>,</span><br><span class="line">    `event_time` <span class="type">TIMESTAMP</span>,</span><br><span class="line">    WATERMARK <span class="keyword">FOR</span> event_time <span class="keyword">AS</span> event_time <span class="operator">-</span> <span class="type">INTERVAL</span> <span class="string">&#x27;5&#x27;</span> <span class="keyword">SECOND</span></span><br><span class="line">) <span class="keyword">WITH</span> (</span><br><span class="line">    <span class="string">&#x27;connector.type&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;kafka&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;connector.topic&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;user_action&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;connector.version&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;0.11&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;connector.properties.0.key&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;bootstrap.servers&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;connector.properties.0.value&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;xxx:9092&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;connector.startup-mode&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;latest-offset&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;update-mode&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;append&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;...&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;...&#x27;</span></span><br><span class="line">);</span><br><span class="line"></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> `agg_result` (</span><br><span class="line">    `user_id` <span class="type">VARCHAR</span>,</span><br><span class="line">    `page_id` <span class="type">VARCHAR</span>,</span><br><span class="line">    `result_type` <span class="type">VARCHAR</span>,</span><br><span class="line">    `result_value` <span class="type">BIGINT</span></span><br><span class="line">) <span class="keyword">WITH</span> (</span><br><span class="line">    <span class="string">&#x27;connector.type&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;kafka&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;connector.topic&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;agg_result&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;...&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;...&#x27;</span></span><br><span class="line">);</span><br></pre></td></tr></table></figure>
<p>场景一，实时触发的最近2小时用户+页面维度的点击量，注意窗口是向前2小时，类似于实时触发的滑动窗口。</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span></span><br><span class="line">    agg_result</span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">    user_id,</span><br><span class="line">    page_id,</span><br><span class="line">    <span class="string">&#x27;click-type1&#x27;</span> <span class="keyword">as</span> result_type</span><br><span class="line">    <span class="built_in">count</span>(<span class="number">1</span>) <span class="keyword">OVER</span> (</span><br><span class="line">        <span class="keyword">PARTITION</span> <span class="keyword">BY</span> user_id, page_id</span><br><span class="line">        <span class="keyword">ORDER</span> <span class="keyword">BY</span> event_time </span><br><span class="line">        <span class="keyword">RANGE</span> <span class="keyword">BETWEEN</span> <span class="type">INTERVAL</span> <span class="string">&#x27;2&#x27;</span> <span class="keyword">HOUR</span> PRECEDING <span class="keyword">AND</span> <span class="keyword">CURRENT</span> <span class="type">ROW</span></span><br><span class="line">    ) <span class="keyword">as</span> result_value</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">    user_action</span><br><span class="line"><span class="keyword">where</span></span><br><span class="line">    action_type <span class="operator">=</span> <span class="string">&#x27;click&#x27;</span></span><br></pre></td></tr></table></figure>
<p>场景二，实时触发的当天用户+页面维度的浏览量，这就是开篇问题解法，其中多了一个日期维度分组条件，这样就做到输出结果从滑动时间转为固定时间（根据时间区间分组），因为 WATERMARK 机制，今天并不会有昨天数据到来（如果有都被自动抛弃），因此只会输出今天的分组结果。</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span></span><br><span class="line">    agg_result</span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">    user_id,</span><br><span class="line">    page_id,</span><br><span class="line">    <span class="string">&#x27;view-type1&#x27;</span> <span class="keyword">as</span> result_type</span><br><span class="line">    <span class="built_in">count</span>(<span class="number">1</span>) <span class="keyword">OVER</span> (</span><br><span class="line">        <span class="keyword">PARTITION</span> <span class="keyword">BY</span> user_id, page_id, DATE_FORMAT(event_time, <span class="string">&#x27;yyyyMMdd&#x27;</span>)</span><br><span class="line">        <span class="keyword">ORDER</span> <span class="keyword">BY</span> event_time </span><br><span class="line">        <span class="keyword">RANGE</span> <span class="keyword">BETWEEN</span> <span class="type">INTERVAL</span> <span class="string">&#x27;1&#x27;</span> <span class="keyword">DAY</span> PRECEDING <span class="keyword">AND</span> <span class="keyword">CURRENT</span> <span class="type">ROW</span></span><br><span class="line">    ) <span class="keyword">as</span> result_value</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">    user_action</span><br><span class="line"><span class="keyword">where</span></span><br><span class="line">    action_type <span class="operator">=</span> <span class="string">&#x27;view&#x27;</span></span><br></pre></td></tr></table></figure>
<p>场景三，实时触发的当天用户+页面点击率 CTR（Click-Through-Rate），这相比前面增加了多个 OVER 聚合计算，可以将窗口定义写在最后。注意示例中缺少了类型转换，因为除法结果是 decimal，也缺少精度处理函数 ROUND。</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span></span><br><span class="line">    agg_result</span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">    user_id,</span><br><span class="line">    page_id,</span><br><span class="line">    <span class="string">&#x27;ctr-type1&#x27;</span> <span class="keyword">as</span> result_type,</span><br><span class="line">    <span class="built_in">sum</span>(</span><br><span class="line">        <span class="keyword">case</span></span><br><span class="line">            <span class="keyword">when</span> action_type <span class="operator">=</span> <span class="string">&#x27;click&#x27;</span> <span class="keyword">then</span> <span class="number">1</span> <span class="keyword">else</span> <span class="number">0</span></span><br><span class="line">        <span class="keyword">end</span></span><br><span class="line">    ) <span class="keyword">OVER</span> w </span><br><span class="line">    <span class="operator">/</span> </span><br><span class="line">    if(</span><br><span class="line">        <span class="built_in">sum</span>(</span><br><span class="line">            <span class="keyword">case</span></span><br><span class="line">                <span class="keyword">when</span> action_type <span class="operator">=</span> <span class="string">&#x27;view&#x27;</span> <span class="keyword">then</span> <span class="number">1</span> <span class="keyword">else</span> <span class="number">0</span></span><br><span class="line">            <span class="keyword">end</span></span><br><span class="line">        ) <span class="keyword">OVER</span> w <span class="operator">=</span> <span class="number">0</span>,</span><br><span class="line">        <span class="number">1</span>,</span><br><span class="line">        <span class="built_in">sum</span>(</span><br><span class="line">            <span class="keyword">case</span></span><br><span class="line">                <span class="keyword">when</span> action_type <span class="operator">=</span> <span class="string">&#x27;view&#x27;</span> <span class="keyword">then</span> <span class="number">1</span> <span class="keyword">else</span> <span class="number">0</span></span><br><span class="line">            <span class="keyword">end</span></span><br><span class="line">        ) <span class="keyword">OVER</span> w</span><br><span class="line">    )</span><br><span class="line">    <span class="keyword">as</span> result_value</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">    user_action</span><br><span class="line"><span class="keyword">where</span></span><br><span class="line">    <span class="number">1</span> <span class="operator">=</span> <span class="number">1</span> </span><br><span class="line"><span class="keyword">WINDOW</span> w <span class="keyword">AS</span> (</span><br><span class="line">        <span class="keyword">PARTITION</span> <span class="keyword">BY</span> user_id, page_id, DATE_FORMAT(event_time,<span class="string">&#x27;yyyyMMdd&#x27;</span>)</span><br><span class="line">        <span class="keyword">ORDER</span> <span class="keyword">BY</span> event_time </span><br><span class="line">        <span class="keyword">RANGE</span> <span class="keyword">BETWEEN</span> <span class="type">INTERVAL</span> <span class="string">&#x27;1&#x27;</span> <span class="keyword">DAY</span> PRECEDING <span class="keyword">AND</span> <span class="keyword">CURRENT</span> <span class="type">ROW</span></span><br><span class="line">    )</span><br></pre></td></tr></table></figure>
<p>此外，OVER 窗口聚合还可以支持查询子句、关联查询、UNION ALL 等组合，并可以实现对关联出来的列进行聚合等复杂情况。</p>
<h3 id="实时TopN"><a href="#实时TopN" class="headerlink" title="实时TopN"></a>实时TopN</h3><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/wangpei1949/article/details/105471974">SQL实时TopN</a></p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> source_kafka </span><br><span class="line">( </span><br><span class="line">    userID String, </span><br><span class="line">    eventType String, </span><br><span class="line">    eventTime String, </span><br><span class="line">    productID String </span><br><span class="line">) <span class="keyword">with</span> ( </span><br><span class="line">    <span class="string">&#x27;connector.type&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;kafka&#x27;</span>, </span><br><span class="line">    <span class="string">&#x27;connector.version&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;0.10&#x27;</span>, </span><br><span class="line">    <span class="string">&#x27;connector.properties.bootstrap.servers&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;kafka01:9092&#x27;</span>, </span><br><span class="line">    <span class="string">&#x27;connector.properties.zookeeper.connect&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;kafka01:2181&#x27;</span>, </span><br><span class="line">    <span class="string">&#x27;connector.topic&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;test_1&#x27;</span>, </span><br><span class="line">    <span class="string">&#x27;connector.properties.group.id&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;c1_test_1&#x27;</span>, </span><br><span class="line">    <span class="string">&#x27;connector.startup-mode&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;latest-offset&#x27;</span>, </span><br><span class="line">    <span class="string">&#x27;format.type&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;json&#x27;</span> </span><br><span class="line">);</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> sink_mysql</span><br><span class="line">(</span><br><span class="line">    datetime STRING,</span><br><span class="line">    productID STRING,</span><br><span class="line">    userID STRING,</span><br><span class="line">    clickPV <span class="type">BIGINT</span></span><br><span class="line">) <span class="keyword">with</span> (</span><br><span class="line"><span class="string">&#x27;connector.type&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;jdbc&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;connector.url&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;jdbc:mysql://localhost:3306/bigdata&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;connector.table&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;t_product_click_topn&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;connector.username&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;root&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;connector.password&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;bigdata&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;connector.write.flush.max-rows&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;50&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;connector.write.flush.interval&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;2s&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;connector.write.max-retries&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;3&#x27;</span></span><br><span class="line">);</span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> sink_mysql </span><br><span class="line"><span class="keyword">SELECT</span> datetime, productID, userID, clickPV </span><br><span class="line"><span class="keyword">FROM</span> ( </span><br><span class="line">  <span class="keyword">SELECT</span> <span class="operator">*</span>, </span><br><span class="line">     <span class="built_in">ROW_NUMBER</span>() <span class="keyword">OVER</span> (<span class="keyword">PARTITION</span> <span class="keyword">BY</span> datetime, productID <span class="keyword">ORDER</span> <span class="keyword">BY</span> clickPV <span class="keyword">desc</span>) <span class="keyword">AS</span> rownum </span><br><span class="line">  <span class="keyword">FROM</span> ( </span><br><span class="line">        <span class="keyword">SELECT</span> <span class="built_in">SUBSTRING</span>(eventTime,<span class="number">1</span>,<span class="number">13</span>) <span class="keyword">AS</span> datetime, </span><br><span class="line">            productID, </span><br><span class="line">            userID, </span><br><span class="line">            <span class="built_in">count</span>(<span class="number">1</span>) <span class="keyword">AS</span> clickPV </span><br><span class="line">        <span class="keyword">FROM</span> source_kafka </span><br><span class="line">        <span class="keyword">GROUP</span> <span class="keyword">BY</span> <span class="built_in">SUBSTRING</span>(eventTime,<span class="number">1</span>,<span class="number">13</span>), productID, userID </span><br><span class="line">    ) a </span><br><span class="line">) t </span><br><span class="line"><span class="keyword">WHERE</span> rownum <span class="operator">&lt;=</span> <span class="number">3</span>&quot;;               </span><br></pre></td></tr></table></figure>


<h3 id="OVER-窗口问题和优化"><a href="#OVER-窗口问题和优化" class="headerlink" title="OVER 窗口问题和优化"></a>OVER 窗口问题和优化</h3><p>在底层实现中，所有细分 OVER 窗口的数据都是共享的，只存一份，这点不像滑动窗口会保存多份窗口数据。但是 OVER  窗口会把所有数据明细存在状态后端中（内存、RocksDB 或  HDFS），每一次窗口计算后会清除过期数据。因此如果向前窗口时间较大，或数据明细过多，可能会占用大量内存，即使通过 RocksDB  存在磁盘上，也有因为磁盘访问慢导致性能下降进而产生反压问题。在实现源码 <code>RowTimeRangeBoundedPrecedingFunction</code> 可以看到虽然每次窗口计算时新增聚合值和减少过期聚合值是增量式的，不用遍历全部窗口明细，但是为了计算过期数据，即超过 PRECEDING  的数据，仍然需要把存储的那些时间戳全部拿出来遍历，判断是否过期，以及是否要减少聚合值。我们尝试了通过数据有序性减少查询操作，但是效果并不明显，目前主要是配置调优和加大任务分片数进行优化。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://example.com/bigdata/iceberg/DataLake%E4%B8%89%E5%89%91%E5%AE%A2/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="aaronzhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Guadazi">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/bigdata/iceberg/DataLake%E4%B8%89%E5%89%91%E5%AE%A2/" class="post-title-link" itemprop="url">DataLake三剑客</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-01-15 19:47:00" itemprop="dateCreated datePublished" datetime="2021-01-15T19:47:00+08:00">2021-01-15</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-06-01 20:52:39" itemprop="dateModified" datetime="2021-06-01T20:52:39+08:00">2021-06-01</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/bigdata/" itemprop="url" rel="index"><span itemprop="name">bigdata</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="DataLake三剑客"><a href="#DataLake三剑客" class="headerlink" title="DataLake三剑客"></a>DataLake三剑客</h1><p><strong>作者</strong>：辛庸，阿里巴巴计算平台事业部 EMR 技术专家。Apache Hadoop，Apache Spark contributor。对 Hadoop、Spark、Hive、Druid 等大数据组件有深入研究。目前从事大数据云化相关工作，专注于计算引擎、存储结构、数据库事务等内容。</p>
<h3 id="共同点"><a href="#共同点" class="headerlink" title="共同点"></a>共同点</h3><p>定性上讲，三者均为 Data Lake 的数据存储中间层，其数据管理的功能均是基于一系列的 meta 文件。meta 文件的角色类似于数据库的 catalog/wal，起到 schema 管理、事务管理和数据管理的功能。与数据库不同的是，这些 meta 文件是与数据文件一起存放在存储引擎中的，用户可以直接看到。这种做法直接继承了大数据分析中数据对用户可见的传统，但是无形中也增加了数据被不小心破坏的风险。一旦某个用户不小心删了 meta 目录，表就被破坏了，想要恢复难度非常大。</p>
<p>Meta 文件包含有表的 schema 信息。因此系统可以自己掌握 Schema 的变动，提供 Schema 演化的支持。Meta 文件也有 transaction log 的功能（需要文件系统有原子性和一致性的支持）。所有对表的变更都会生成一份新的 meta 文件，于是系统就有了 ACID 和多版本的支持，同时可以提供访问历史的功能。在这些方面，三者是相同的。</p>
<p>下面来谈一下三者的不同。</p>
<h3 id="Hudi"><a href="#Hudi" class="headerlink" title="Hudi"></a>Hudi</h3><p>先说 Hudi。Hudi 的设计目标正如其名，Hadoop Upserts Deletes and Incrementals（原为 Hadoop Upserts anD Incrementals），强调了其主要支持 Upserts、Deletes 和 Incremental 数据处理，其主要提供的写入工具是 Spark HudiDataSource API 和自身提供的 DeltaStreamer，均支持三种数据写入方式：UPSERT，INSERT 和 BULK_INSERT。其对 Delete 的支持也是通过写入时指定一定的选项支持的，并不支持纯粹的 delete 接口。</p>
<p>其典型用法是将上游数据通过 Kafka 或者 Sqoop，经由 DeltaStreamer 写入 Hudi。DeltaStreamer 是一个常驻服务，不断地从上游拉取数据，并写入 hudi。写入是分批次的，并且可以设置批次之间的调度间隔。默认间隔为 0，类似于 Spark Streaming 的 As-soon-as-possible 策略。随着数据不断写入，会有小文件产生。对于这些小文件，DeltaStreamer 可以自动地触发小文件合并的任务。</p>
<p>在查询方面，Hudi 支持 Hive、Spark、Presto。</p>
<p>在性能方面，Hudi 设计了 <code>`</code><br>HoodieKey<br><code>，一个类似于主键的东西。</code><br>HoodieKey<br><code>有 Min/Max 统计，BloomFilter，用于快速定位 Record 所在的文件。在具体做 Upserts 时，如果 </code>HoodieKey<br><code>不存在于 BloomFilter，则执行插入，否则，确认 </code>HoodieKey<br>是否真正存在，如果真正存在，则执行 update。这种基于 HoodieKey + BloomFilter 的 upserts 方法是比较高效的，否则，需要做全表的 Join 才能实现 upserts。对于查询性能，一般需求是根据查询谓词生成过滤条件下推至 datasource。Hudi 这方面没怎么做工作，其性能完全基于引擎自带的谓词下推和 partition prune 功能。</p>
<p>Hudi 的另一大特色是支持 Copy On Write 和 Merge On Read。前者在写入时做数据的 merge，写入性能略差，但是读性能更高一些。后者读的时候做 merge，读性能查，但是写入数据会比较及时，因而后者可以提供近实时的数据分析能力。</p>
<p>最后，Hudi 提供了一个名为 run_sync_tool 的脚本同步数据的 schema 到 Hive 表。Hudi 还提供了一个命令行工具用于管理 Hudi 表。</p>
<p><strong>hudi</strong><br><img src="_v_images/20201014213247817_752642290.png" alt="image.png" title="image.png"></p>
<hr>
<h3 id="Iceberg"><a href="#Iceberg" class="headerlink" title="Iceberg"></a>Iceberg</h3><p>Iceberg 没有类似的 HoodieKey 设计，其不强调主键。上文已经说到，没有主键，做 update/delete/merge 等操作就要通过 Join 来实现，而 Join 需要有一个 类似 SQL 的执行引擎。Iceberg 并不绑定某个引擎，也没有自己的引擎，所以 Iceberg 并不支持 update/delete/merge。如果用户需要 update 数据，最好的方法就是找出哪些 partition 需要更新，然后通过 overwrite 的方式重写数据。Iceberg 官网提供的 quickstart 以及 Spark 的接口均只是提到了使用 Spark dataframe API 向 Iceberg 写数据的方式，没有提及别的数据摄入方法。至于使用 Spark Streaming 写入，代码中是实现了相应的 StreamWriteSupport，应该是支持流式写入，但是貌似官网并未明确提及这一点。支持流式写入意味着有小文件问题，对于怎么合并小文件，官网也未提及。我怀疑对于流式写入和小文件合并，可能 Iceberg 还没有很好的生产 ready，因而没有提及（纯属个人猜测）。</p>
<p>在查询方面，Iceberg 支持 Spark、Presto。</p>
<p>Iceberg 在查询性能方面做了大量的工作。值得一提的是它的 hidden partition 功能。Hidden partition 意思是说，对于用户输入的数据，用户可以选取其中某些列做适当的变换（Transform）形成一个新的列作为 partition 列。这个 partition 列仅仅为了将数据进行分区，并不直接体现在表的 schema 中。例如，用户有 timestamp 列，那么可以通过 hour(timestamp) 生成一个 timestamp_hour 的新分区列。timestamp_hour 对用户不可见，仅仅用于组织数据。Partition 列有 partition 列的统计，如该 partition 包含的数据范围。当用户查询时，可以根据 partition 的统计信息做 partition prune。</p>
<p>除了 hidden partition，Iceberg 也对普通的 column 列做了信息收集。这些统计信息非常全，包括列的 size，列的 value count，null value count，以及列的最大最小值等等。这些信息都可以用来在查询时过滤数据。</p>
<p>Iceberg 提供了建表的 API，用户可以使用该 API 指定表明、schema、partition 信息等，然后在 Hive catalog 中完成建表。</p>
<hr>
<h3 id="Delta"><a href="#Delta" class="headerlink" title="Delta"></a>Delta</h3><p>我们最后来说 Delta。Delta 的定位是流批一体的 Data Lake 存储层，支持 update/delete/merge。由于出自 Databricks，spark 的所有数据写入方式，包括基于 dataframe 的批式、流式，以及 SQL 的 Insert、Insert Overwrite 等都是支持的（开源的 SQL 写暂不支持，EMR 做了支持）。与 Iceberg 类似，Delta 不强调主键，因此其 update/delete/merge 的实现均是基于 spark 的 join 功能。在数据写入方面，Delta 与 Spark 是强绑定的，这一点 Hudi 是不同的：Hudi 的数据写入不绑定 Spark（可以用 Spark，也可以使用 Hudi 自己的写入工具写入）。</p>
<p>在查询方面，开源 Delta 目前支持 Spark 与 Presto，但是，Spark 是不可或缺的，因为 delta log 的处理需要用到 Spark。这意味着如果要用 Presto 查询 Delta，查询时还要跑一个 Spark 作业。更为蛋疼的是，Presto 查询是基于 SymlinkTextInputFormat。在查询之前，要运行 Spark 作业生成这么个 Symlink 文件。如果表数据是实时更新的，意味着每次在查询之前先要跑一个 SparkSQL，再跑 Presto。这样的话为何不都在 SparkSQL 里搞定呢？这是一个非常蛋疼的设计。为此，EMR 在这方面做了改进，支持了 DeltaInputFormat，用户可以直接使用 Presto 查询 Delta 数据，而不必事先启动一个 Spark 任务。</p>
<p>在查询性能方面，开源的 Delta 几乎没有任何优化。Iceberg 的 hidden partition 且不说，普通的 column 的统计信息也没有。Databricks 对他们引以为傲的 Data Skipping 技术做了保留。不得不说这对于推广 Delta 来说不是件好事。EMR 团队在这方面正在做一些工作，希望能弥补这方面能力的缺失。</p>
<p>Delta 在数据 merge 方面性能不如 Hudi，在查询方面性能不如 Iceberg，是不是意味着 Delta 一无是处了呢？其实不然。Delta 的一大优点就是与 Spark 的整合能力（虽然目前仍不是很完善，但 Spark-3.0 之后会好很多），尤其是其流批一体的设计，配合 multi-hop 的 data pipeline，可以支持分析、Machine learning、CDC 等多种场景。使用灵活、场景支持完善是它相比 Hudi 和 Iceberg 的最大优点。另外，Delta 号称是 Lambda 架构、Kappa 架构的改进版，无需关心流批，无需关心架构。这一点上 Hudi 和 Iceberg 是力所不及的。</p>
<p><strong>delta</strong><br><img src="_v_images/20201014213246690_850183439.png" alt="image.png" title="image.png"></p>
<hr>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>通过上面的分析能够看到，三个引擎的初衷场景并不完全相同，Hudi 为了 incremental 的 upserts，Iceberg 定位于高性能的分析与可靠的数据管理，Delta 定位于流批一体的数据处理。这种场景的不同也造成了三者在设计上的差别。尤其是 Hudi，其设计与另外两个相比差别更为明显。随着时间的发展，三者都在不断补齐自己缺失的能力，可能在将来会彼此趋同，互相侵入对方的领地。当然也有可能各自关注自己专长的场景，筑起自己的优势壁垒，因此最终谁赢谁输还是未知之数。</p>
<p>下表从多个维度对三者进行了总结，需要注意的是此表所列的能力仅代表至 2019 年底。</p>
<table>
<thead>
<tr>
<th>·</th>
<th>Delta</th>
<th>Hudi</th>
<th>Iceberg</th>
</tr>
</thead>
<tbody><tr>
<td>Incremental Ingestion</td>
<td>Spark</td>
<td>Spark</td>
<td>Spark</td>
</tr>
<tr>
<td>ACID updates</td>
<td>HDFS, S3 (Databricks), OSS</td>
<td>HDFS</td>
<td>HDFS, S3</td>
</tr>
<tr>
<td>Upserts/Delete/Merge/Update</td>
<td>Delete/Merge/Update</td>
<td>Upserts/Delete</td>
<td>No</td>
</tr>
<tr>
<td>Streaming sink</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes(not ready?)</td>
</tr>
<tr>
<td>Streaming source</td>
<td>Yes</td>
<td>No</td>
<td>No</td>
</tr>
<tr>
<td>FileFormats</td>
<td>Parquet</td>
<td>Avro,Parquet</td>
<td>Parquet, ORC</td>
</tr>
<tr>
<td>Data Skipping</td>
<td>File-Level Max-Min stats + Z-Ordering (Databricks)</td>
<td>File-Level Max-Min stats + Bloom Filter</td>
<td>File-Level Max-Min Filtering</td>
</tr>
<tr>
<td>Concurrency control</td>
<td>Optimistic</td>
<td>Optimistic</td>
<td>Optimistic</td>
</tr>
<tr>
<td>Data Validation</td>
<td>Yes (Databricks)</td>
<td>No</td>
<td>Yes</td>
</tr>
<tr>
<td>Merge on read</td>
<td>No</td>
<td>Yes</td>
<td>No</td>
</tr>
<tr>
<td>Schema Evolution</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
</tr>
<tr>
<td>File I/O Cache</td>
<td>Yes (Databricks)</td>
<td>No</td>
<td>No</td>
</tr>
<tr>
<td>Cleanup</td>
<td>Manual</td>
<td>Automatic</td>
<td>No</td>
</tr>
<tr>
<td>Compaction</td>
<td>Manual</td>
<td>Automatic</td>
<td>No</td>
</tr>
</tbody></table>
<p>注：限于本人水平，文中内容可能有误，也欢迎读者批评指正！</p>
<hr>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://example.com/bigdata/iceberg/%E6%95%B0%E6%8D%AE%E6%B9%96/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="aaronzhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Guadazi">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/bigdata/iceberg/%E6%95%B0%E6%8D%AE%E6%B9%96/" class="post-title-link" itemprop="url">数据湖</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-01-15 19:47:00" itemprop="dateCreated datePublished" datetime="2021-01-15T19:47:00+08:00">2021-01-15</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-06-01 20:52:39" itemprop="dateModified" datetime="2021-06-01T20:52:39+08:00">2021-06-01</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/bigdata/" itemprop="url" rel="index"><span itemprop="name">bigdata</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="数据湖"><a href="#数据湖" class="headerlink" title="数据湖"></a>数据湖</h1><p><img src="vx_images/4964231239501" alt="图片"></p>
<h2 id="数据仓库-VS-数据湖"><a href="#数据仓库-VS-数据湖" class="headerlink" title="数据仓库 VS 数据湖"></a>数据仓库 VS 数据湖</h2><p>相较而言，数据湖是较新的技术，拥有不断演变的架构。数据湖存储任何形式（包括结构化和非结构化）和任何格式（包括文本、音频、视频和图像）的原始数据。根据定义，<code>数据湖不会接受数据治理</code>，但专家们一致认为<code>良好的数据管理对预防数据湖转变为数据沼泽不可或缺</code>。数据湖在数据读取期间创建模式。与数据仓库相比，数据湖缺乏结构性，而且更灵活，并且提供了更高的敏捷性。值得一提的是，数据湖非常适合使用机器学习和深度学习来执行各种任务，比如数据挖掘和数据分析，以及提取非结构化数据等。<br><img src="vx_images/3593617797024" alt="图片"></p>
<p><img src="vx_images/5612546586116.png"></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://example.com/bigdata/storm/Storm-runtime/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="aaronzhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Guadazi">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/bigdata/storm/Storm-runtime/" class="post-title-link" itemprop="url">Storm runtime</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-01-15 19:47:00" itemprop="dateCreated datePublished" datetime="2021-01-15T19:47:00+08:00">2021-01-15</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-06-01 20:52:39" itemprop="dateModified" datetime="2021-06-01T20:52:39+08:00">2021-06-01</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/bigdata/" itemprop="url" rel="index"><span itemprop="name">bigdata</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="Storm-runtime"><a href="#Storm-runtime" class="headerlink" title="Storm-runtime"></a>Storm-runtime</h1><p><img src="_v_images/20210116161223695_155417172.jpg"></p>
<p>master-slave结构:</p>
<ul>
<li>Nimbus是主节点，负责分发用户代码，指派Supervisor上的worker进程，运行topology的(Spout/Bolt)Task</li>
<li>Supervisor是从节点，守护进程. 负责启动和终止worker进程. 通过Storm的配置文件中的 supervisor.slots.ports配置项，可以指定在一个Supervisor上最大允许多少个Slot，每个Slot通过端口号来唯一标识，一个端口号 对应一个Worker进程（如果该Worker进程被启动）。</li>
</ul>
<p><img src="_v_images/20210116162304428_695142382.jpg"></p>
<p>运行流程</p>
<p>1）户端提交拓扑到nimbus。</p>
<p>2） Nimbus针对该拓扑建立本地的目录根据topology的配置计算task，分配task，在zookeeper上建立assignments节点存储 task和supervisor机器节点中woker的对应关系；</p>
<p>在zookeeper上创建taskbeats节点来监控task的心跳；启动topology。</p>
<p>3） Supervisor去zookeeper上获取分配的tasks，启动多个woker进行，每个woker生成task，一个task一个线程；根据topology 信息初始化建立task之间的连接;Task和Task之间是通过zeroMQ管理的；后整个拓扑运行起来。</p>
<h2 id="内核-队列"><a href="#内核-队列" class="headerlink" title="内核-队列"></a>内核-队列</h2><p>在Storm中大量使用Disrupt Queue来解耦Storm内部的消息处理过程。分析这些Queue的分布，是分析Storm运行时态的基础。</p>
<p><img src="_v_images/20210116174558832_184704273.png"></p>
<p>在Storm的worker中，最小的执行单元是executor,一个executor只会有一个component。目前一个component只会有一个task。而一个executor会有两个Disruptor Queue, 一个用于接受数据的receive Disrupor Queue和一个用于发送send Disrupor Queue。然后在worker中有一个全局的send Disrupor Queue。分析完队列的分布，在分析topology的运行时状况。</p>
<p>当一个Topology提交到Storm集群后，task被分配到各个wrker中开始执行后，task是怎么执行的。Storm的Task分为两类，一类是消息的源头Spout,它负责在源头产生消息；然后就是Bolt，它是执行单元。但是这两者各自的工作逻辑如下。</p>
<p>我们来分析Spout在运行时的工作状况。Spout的nextTuple用于发送数据，接口注释上说明它不能阻塞，因为它与active， deactive，ack, fail在一个处理线程里面被处理。但是nextTuple被阻塞会有什么副作用列？当nextTuple被阻塞，应用代码中另起线程调用SpoutCollector会有什么后果？下图是SpoutExecutor的执行逻辑。</p>
<p><img src="_v_images/20210116174558730_1417384061.png"></p>
<p>在SpoutExecutor处理循环里面，第一步做的事情是从receive Disrupor Queue里面消费里面的消息。这里面的就是SpoutExecutor所收到的消息。处理逻辑如下图所示</p>
<p><img src="_v_images/20210116174558627_554147077.png"></p>
<p>然后SpoutExecutor会将overflow中的数据再次发送。overflow是用于接受SpoutCollector.emit()无发及时发送的数据的，具体SpoutCollector发送数据的逻辑见下文分析。但是这里在发送overflow的数据时，与SpoutCollector.emit()有个区别，就是数据还是无法被正常发送时，数据会丢弃，也就是不会被再次写入overflow中。当overflow中没有数据，以及pending中的数据量小于TOPOLOGY_MAX_SPOUT_PENDING时，判断topology的状态。当topology不是deactive状态时，如果topology有触发active命令，会调用spout的active接口，然后调用spout的nextTuple接口。否则调用deactive接口。所以这里当Spout的nextTuple被阻塞时，spout没办法处理acker回报的消息，回报的消息会阻塞在executor的receive DisruptorQueue中，当receive DisruptorQueue塞满后，是会阻塞在对应的网络处理模块中，storm中经典的是zeroMQ,老版本的zeroMQ是没有设置水位，这样会大量堆积到内存中，因为zeroMQ是C++的，占用的是堆外内存，JVM无法管理，最坏就是把机器的内存耗光。如果这个是另起线程调用SpoutCollector的emit，当emit数据速度过快，会导致overflow中堆积数据，导致worker内存消耗。</p>
<p>上面讲到SpoutCollector在emit数据会写入overflow,那什么情况下会写入overflow。SpoutCollector.emit是否真的就把数据发送到了网络。下面是SpoutCollector的处理逻辑。</p>
<p><img src="_v_images/20210116174558522_1112615304.png"></p>
<p>Spout当调用SpoutCollect.emit()发送时，首先的逻辑是根据根据消息的STREAM ID,和对应的values,得到目的端task id。当topology开启acker机制时，会生成一个随机的rootId，否则使用一个默认值作为rootId。然后为消息生成一个随机的messageId，由rootId和messageId组成对应的tuple Id。这里tuple Id是有rootId为key,messageId为value的一个HashMap。这里采用HashMap的作用会在下面Spout的ack机制是做说明。在将生成的tuple Id和用会的values组成一个tuple。并判断overflower是否有数据，让overflow中有数据时，数据会直接放入overflow中，而不会放入executor  的send Disraptor Queue中。当overflow为空时，会将tuple放入send Disraptor Queue中。当捕获Disraptor Queue的InsufficientCapacityException时，数据就放入了overflow中。然后处理ack。当开启了ack机制，会在pending中加入对应的tuple数据，然后项acker发送ack init消息。</p>
<p>根据上面的发送过程，数据emit只是被写入了executor的send Disraptor Queue。而数据在Spout端的丢失多是数据在overflow中被SpoutExecutor在次处理时，send Disraptor Queue满导致。</p>
<p>关于pending，首先它是一个RotatingMap。它通过定时旋转，以达到定时器的目的。在SpoutExecutor的RotatingMap中有两个桶，然后executor有个定时线程，会按照用户设定的message timeout second，定期向executor的receive DisruptorQueue写入SYSTEM_TICK_STREAM_ID消息，然后SpoutExecutor处理SYSTEM_TICK_STREAM_ID消息时就旋转RotatingMap，当被清除的桶中有数据时，被清除的桶中的数据会调用fail接口，通知业务逻辑fail。这就是当超时间设置比业务逻辑短时，导致数据重复的原因。还有一种是acker消息丢失，导致数据重复。由于RotatingMap的底层是HashMap，中间没有锁，overflow是个LinkedList，所以SpoutCollector和SpoutExecutor不能在两个线程中并发 处理。</p>
<p>BoltExecutor的处理逻辑非常简单，就是消费receive Disrupor Queue中数据，然后调用bolt的excue。但是这里也是单线程处理的，阻塞或者处理速度不匹配，就会导致数据在Disrupor Queue或者网络模块中堆积，其中使用zeroMQ的副作用最大。</p>
<p>BoltCollector的emit与SpoutCollector的emit处理相比，首先是少了overflow承接无法发送的数据，会直接丢弃。其次是没有pending和acker消息的发送。</p>
<p>接下来分析一下Storm的Acker机制。Storm的Ack机制在Storm刚刚开源时被大书特书，处理原理也确实非常的精彩。由于这里ID都是随机数，所以这里不会在讨论随机数的唯一问题。</p>
<p><img src="_v_images/20210116174558419_714266215.png"></p>
<p>如上图所示，spout发送t1给bolt a, bolt a 在t1的基础上生成t2,t3,t4给bolt b，bolt b ack所收到的数据。下面来追踪整个id变化的过程。</p>
<p>Spout 发送t1， message id为&lt;r_1, m_1&gt;,发送ack</p>
<p>Acker收到ack init, map中缓存&lt;r_1,m_1&gt;</p>
<p>Bolt a 收到t1, messageId为&lt;r_1,m_1&gt;,生成t2，t3, t4</p>
<p>Bolt a 发送t2, anchor t1, t2,messageId为&lt;r_1,m_2&gt;, 更新t1的ackVal为m_2</p>
<p>Bolt a 发送t3, anchor t1, t3,messageId为&lt;r_1,m_3&gt;, 更新t1的ackVal为m_2^m_3</p>
<p>Bolt a 发送t4, anchor t1, t4,messageId为&lt;r_1,m_4&gt;, 更新t1的ackVal为m_2^m_3^m_4</p>
<p>Bolt a ack t1, 向acker发送ack消息&lt;r_1, m_1^ m_2^m_3^m_4&gt;</p>
<p>Acker 收到bolt a的ack消息，更新缓存为&lt;r_1, m_1^ m_1^ m_2^m_3^m_4&gt;即&lt;r_1,  m_2^m_3^m_4&gt;</p>
<p>Bolt b ack t2, 向acker发送ack消息&lt;r_1, m_2&gt;</p>
<p>Acker 收到bolt b的ack消息，更新缓存为&lt;r_1, m_2^ m_2^m_3^m_4&gt;即&lt;r_1, m_3^m_4&gt;</p>
<p>Bolt b ack t3, 向acker发送ack消息&lt;r_1, m_3&gt;</p>
<p>Acker 收到bolt b的ack消息，更新缓存为&lt;r_1, m_3^m_3^m_4&gt;即&lt;r_1, m_4&gt;</p>
<p>Bolt b ack t4, 向acker发送ack消息&lt;r_1, m_4&gt;</p>
<p>Acker 收到bolt b的ack消息，更新缓存为&lt;r_1, m_4^m_4&gt;即&lt;r_1, 0&gt;</p>
<p>messageId确认完毕，向Spout发送ack消息。当消息没有被ack,会一直在spout的pending队列中，知道被ack或者超时。</p>
<p>它基本上使用两个long值就跟踪了一个消息在整个流中的处理过程。</p>
<p>【参考文献】</p>
<hr>
<ol>
<li><a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s/PRDSu-qOxb17qjdfpO1IYA">Apache storm内核原理</a></li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://example.com/bigdata/Flink/Flink-SQL-Join-temporal-table/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="aaronzhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Guadazi">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/bigdata/Flink/Flink-SQL-Join-temporal-table/" class="post-title-link" itemprop="url">Flink-temporal-table-join</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-01-15 19:47:00" itemprop="dateCreated datePublished" datetime="2021-01-15T19:47:00+08:00">2021-01-15</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-06-01 20:52:39" itemprop="dateModified" datetime="2021-06-01T20:52:39+08:00">2021-06-01</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/bigdata/" itemprop="url" rel="index"><span itemprop="name">bigdata</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="Flink-temporal-table-join"><a href="#Flink-temporal-table-join" class="headerlink" title="Flink-temporal-table-join"></a>Flink-temporal-table-join</h1><p>Temporal Table记录了表历史上任何时间点所有的数据改动</p>
<h2 id="ANSI-SQL-2011-Temporal-Table示例"><a href="#ANSI-SQL-2011-Temporal-Table示例" class="headerlink" title="ANSI-SQL 2011 Temporal Table示例"></a>ANSI-SQL 2011 Temporal Table示例</h2><p>我们以一个DDL和一套DML示例说明Temporal Table的原理，DDL定义PK是可选的，下面的示例我们以不定义PK的为例进行说明：</p>
<ul>
<li>  DDL 示例</li>
</ul>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> Emp</span><br><span class="line">ENo <span class="type">INTEGER</span>,</span><br><span class="line">Sys_start <span class="type">TIMESTAMP</span>(<span class="number">12</span>) GENERATED</span><br><span class="line">ALWAYS <span class="keyword">AS</span> <span class="type">ROW</span> <span class="keyword">START</span>,</span><br><span class="line">Sys_end <span class="type">TIMESTAMP</span>(<span class="number">12</span>) GENERATED</span><br><span class="line">ALWAYS <span class="keyword">AS</span> <span class="type">ROW</span> <span class="keyword">END</span>,</span><br><span class="line">EName <span class="type">VARCHAR</span>(<span class="number">30</span>),</span><br><span class="line"><span class="keyword">PERIOD</span> <span class="keyword">FOR</span> <span class="built_in">SYSTEM_TIME</span> (Sys_start,Sys_end)</span><br><span class="line">) <span class="keyword">WITH</span> <span class="keyword">SYSTEM</span> <span class="keyword">VERSIONING</span></span><br></pre></td></tr></table></figure>
<ul>
<li>DML 示例<ul>
<li>  INSERT</li>
</ul>
</li>
</ul>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> Emp (ENo, EName) <span class="keyword">VALUES</span> (<span class="number">22217</span>, <span class="string">&#x27;Joe&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p><a target="_blank" rel="noopener" href="https://img.colabug.com/2018/06/5ed5fa9fbdc39f3a26b3dec9816bf691.png" title="Blink 漫谈系列 - Temporal Table JOIN"><img src="vx_images/3323646810742.png"></a></p>
<p>说明: 其中Sys_Start和Sys_End是数据库系统默认填充的。</p>
<ul>
<li><ul>
<li>  UPDATE</li>
</ul>
</li>
</ul>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">UPDATE</span> Emp <span class="keyword">SET</span> EName <span class="operator">=</span> <span class="string">&#x27;Tom&#x27;</span> <span class="keyword">WHERE</span> ENo <span class="operator">=</span> <span class="number">22217</span></span><br></pre></td></tr></table></figure>
<p><a target="_blank" rel="noopener" href="https://img.colabug.com/2018/06/a7750b4fe6d6d7a9aa6f826a2e2c91e9.png" title="Blink 漫谈系列 - Temporal Table JOIN"><img src="vx_images/3312923484991.png"></a></p>
<p>说明: 假设是在 <code>2012-02-03 10:00:00</code> 执行的UPDATE，执行之后上一个值 <code>&quot;Joe&quot;</code> 的Sys_End值由 <code>9999-12-31 23:59:59</code> 变成了 <code>2012-02-03 10:00:00</code> , 也就是下一个值 <code>&quot;Tom&quot;</code> 生效的开始时间。可见我们执行的是UPDATE但是数据库里面会存在两条数据，数据值和有效期不同，也就是版本不同 。</p>
<ul>
<li>  DELETE (假设执行DELETE之前的表内容如下)</li>
</ul>
<p><a target="_blank" rel="noopener" href="https://img.colabug.com/2018/06/d4524993a5a38ee8e41362845e9cab04.png" title="Blink 漫谈系列 - Temporal Table JOIN"><img src="vx_images/3301614695899.png"></a></p>
<p>DELETE FROM Emp WHERE ENo = 22217</p>
<p><a target="_blank" rel="noopener" href="https://img.colabug.com/2018/06/f8cd8a5d23000609cbc13cbb17508e84.png" title="Blink 漫谈系列 - Temporal Table JOIN"><img src="vx_images/3291132138376.png"></a></p>
<p>说明: 假设我们是在 <code>2012-06-01 00:00:00</code> 执行的DELETE，则Sys_End值由 <code>9999-12-31 23:59:59</code> 变成了 <code>2012-06-01 00:00:00</code> , 也就是在执行DELETE时候没有真正的删除符合条件的行，而是系统将符合条件的行的Sys_end修改为执行DELETE的事物时间。标识数据的有效期到DELETE执行那一刻为止。</p>
<ul>
<li>  SELECT</li>
</ul>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> ENo,EName,Sys_Start,Sys_End <span class="keyword">FROM</span> Emp</span><br><span class="line"><span class="keyword">FOR</span> <span class="built_in">SYSTEM_TIME</span> <span class="keyword">AS</span> <span class="keyword">OF</span> <span class="type">TIMESTAMP</span> <span class="string">&#x27;2011-01-02 00:00:00&#x27;</span></span><br></pre></td></tr></table></figure>
<p>说明: 这个查询会返回所有 <code>Sys_start &lt;= 2011-01-02 00:00:00</code> 并且 <code>Sys_end &gt; 2011-01-02 00:00:00</code> 的记录。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://example.com/bigdata/Flink/Flink-SQL-inside/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="aaronzhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Guadazi">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/bigdata/Flink/Flink-SQL-inside/" class="post-title-link" itemprop="url">Flink SQL inside</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-01-15 19:47:00" itemprop="dateCreated datePublished" datetime="2021-01-15T19:47:00+08:00">2021-01-15</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-06-01 20:52:39" itemprop="dateModified" datetime="2021-06-01T20:52:39+08:00">2021-06-01</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/bigdata/" itemprop="url" rel="index"><span itemprop="name">bigdata</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p><a target="_blank" rel="noopener" href="https://github.com/ververica/sql-training/wiki">Apache Flink® SQL Training</a></p>
<h1 id="Flink-SQL"><a href="#Flink-SQL" class="headerlink" title="Flink SQL"></a>Flink SQL</h1><ul>
<li>Flink SQL/Table 如何转化为Flink graph?</li>
<li>Blink 进行了什么优化?</li>
</ul>
<p>本节将主要从 SQL/Table API 如何转化为真正的 Job Graph 的流程开始，让大家对 Blink Planner 有一个比较清晰的认识，希望对大家阅读 Blink 代码，或者使用 Blink 方面有所帮助。然后介绍 Blink Planner 的改进及优化。</p>
<p><img src="vx_images/5763696502773"></p>
<p>从上图可以很清楚的看到，解析的过程涉及到了三层：Table API/SQL，Blink Planner，Runtime，下面将对主要的步骤进行讲解：</p>
<p>Table API&amp;SQL 解析验证：在 Flink 1.9 中，Table API 进行了大量的重构，引入了一套新的 Operation，这套 Operation 主要是用来描述任务的 Logic Tree。</p>
<p>当 SQL 传输进来后，首先会去做 SQL 解析，SQL 解析完成之后，会得到 SqlNode Tree(抽象语法树)，然后会紧接着去做 Validator（验证），验证时会去访问 FunctionManger 和 CatalogManger，FunctionManger 主要是查询用户定义的 UDF，以及检查 UDF 是否合法，CatalogManger 主要是检查这个 Table 或者 Database 是否存在，如果验证都通过，就会生成一个 Operation DAG（有向无环图）。</p>
<p>从这一步可以看出，Table API 和 SQL 在 Flink 中最终都会转化为统一的结构，即 Operation DAG。</p>
<p>生成RelNode：Operation DAG 会被转化为 RelNode(关系表达式) DAG。</p>
<p>优化：优化器会对 RelNode 做各种优化，优化器的输入是各种优化的规则，以及各种统计信息。当前，在 Blink Planner 里面，绝大部分的优化规则，Stream 和 Batch 是共享的。差异在于，对 Batch 而言，它没有 state 的概念，而对于 Stream 而言，它是不支持 sort 的，所以目前 Blink Planner 中，还是运行了两套独立的规则集（Rule Set），然后定义了两套独立的 Physical Rel：BatchPhysical Rel 和 StreamPhysical Rel。优化器优化的结果，就是具体的 Physical Rel DAG。</p>
<p>转化：得到 Physical Rel Dag 后，继续会转化为 ExecNode，通过名字可以看出，ExecNode 已经属于执行层的概念了，但是这个执行层是 Blink 的执行层，在 ExecNode 中，会进行大量的 CodeGen 的操作，还有非 Code 的 Operator 操作，最后，将 ExecNode 转化为 Transformation DAG。</p>
<p>**生成可执行 Job Graph：**得到 Transformation DAG 后，最终会被转化成 Job Graph，完成 SQL 或者 Table API 的解析。</p>
<h3 id="Blink-Planner-改进及优化"><a href="#Blink-Planner-改进及优化" class="headerlink" title="Blink Planner 改进及优化"></a>Blink Planner 改进及优化</h3><p>Blink Planner 功能方面改进主要包含如下几个方面：</p>
<ul>
<li>  更完整的 SQL 语法支持：例如，IN，EXISTS，NOT EXISTS，子查询，完整的 Over 语句，Group Sets 等。而且已经跑通了所有的 TPCH，TPCDS 这两个测试集，性能还非常不错。</li>
<li>  提供了更丰富，高效的算子。</li>
<li>  提供了非常完善的 cost 模型，同时能够对接 Catalog 中的统计信息，使 cost 根据统计信息得到更优的执行计划。</li>
<li>  支持 join reorder。</li>
<li>  shuffle service：对 Batch 而言，Blink Planner 还支持 shuffle service，这对 Batch 作业的稳定性有非常大的帮助，如果遇到 Batch 作业失败，通过 shuffle service 能够很快的进行恢复。</li>
</ul>
<p>性能方面，主要包括以下部分：</p>
<ul>
<li><p>  分段优化。</p>
</li>
<li><p>  Sub-Plan Reuse。</p>
</li>
<li><p>  更丰富的优化 Rule：共一百多个 Rule ，并且绝大多数 Rule 是 Stream 和 Batch 共享的。</p>
</li>
<li><p>  更高效的数据结构 BinaryRow：能够节省序列化和反序列化的操作。</p>
</li>
<li><p>  mini-batch 支持（仅 Stream）：节省 state 的访问的操作。</p>
</li>
<li><p>  节省多余的 Shuffle 和 Sort（Batch 模式）：两个算子之间，如果已经按 A 做 Shuffle，紧接着他下的下游也是需要按 A Shuffle 的数据，那中间的这一层 Shuffle，就可以省略，这样就可以省很多网络的开销，Sort 的情况也是类似。Sort 和 Shuffle 如果在整个计算里面是占大头，对整个性能是有很大的提升的。</p>
</li>
</ul>
<h3 id="深入性能优化及实践"><a href="#深入性能优化及实践" class="headerlink" title="深入性能优化及实践"></a>深入性能优化及实践</h3><p>本节中，将使用具体的示例进行讲解，让你深入理解 Blink Planner 性能优化的设计。</p>
<h4 id="分段优化"><a href="#分段优化" class="headerlink" title="分段优化"></a>分段优化</h4><p>示例 5</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">view</span> MyView <span class="keyword">as</span> <span class="keyword">select</span> word, <span class="built_in">count</span>(<span class="number">1</span>) <span class="keyword">as</span> freq <span class="keyword">from</span> SourceTable <span class="keyword">group</span> <span class="keyword">by</span> word; <span class="keyword">insert</span> <span class="keyword">into</span> SinkTable1 <span class="keyword">select</span> \<span class="operator">*</span> <span class="keyword">from</span> MyView <span class="keyword">where</span> freq <span class="operator">&gt;</span><span class="number">10</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> SinkTable2 <span class="keyword">select</span> <span class="built_in">count</span>(word) <span class="keyword">as</span> freq2, freq <span class="keyword">from</span> MyView <span class="keyword">group</span> <span class="keyword">by</span> freq; </span><br></pre></td></tr></table></figure>
<p>复制代码</p>
<p>上面的这几个 SQL，转化为 RelNode DAG，大致图形如下：</p>
<p><img src="vx_images/5754200881233">  </p>
<p>图5 示例5 RelNode DAG</p>
<p>如果是使用 Flink Planner，经过优化层后，会生成如下执行层的 DAG：</p>
<p><img src="vx_images/5742768787093">  </p>
<p>图6 示例 5 Flink Planner DAG</p>
<p>可以看到，Flink Planner 只是简单的从 Sink 出发，反向的遍历到 Source，从而形成两个独立的执行链路，从上图也可以清楚的看到，Scan 和第一层 Aggregate 是有重复计算的。</p>
<p>在 Blink Planner 中，经过优化层之后，会生成如下执行层的 DAG：</p>
<p><img src="vx_images/5732658916907">  </p>
<p>图7 示例 5 Blink Planner DAG</p>
<p>Blink Planner 不是在每次调用 insert into 的时候就开始优化，而是先将所有的 insert into 操作缓存起来，等到执行前才进行优化，这样就可以看到完整的执行图，可以知道哪些部分是重复计算的。Blink Planner 通过寻找可以优化的最大公共子图，找到这些重复计算的部分。经过优化后，Blink Planner 会将最大公共子图的部分当做一个临时表，供其他部分直接使用。</p>
<p>这样，上面的图可以分为三部分，最大公共子图部分（临时表），临时表与 Filter 和 SinkTable1 优化，临时表与第二个 Aggregate 和 SinkTable 2 优化。</p>
<p>Blink Planner 其实是通过声明的 View 找到最大公共子图的，因此在开发过程中，如果需要复用某段逻辑，就将其定义为 View，这样就可以充分利用 Blink Planner 的分段优化功能，减少重复计算。</p>
<p>当然，当前的优化也不是最完美的，因为提前对图进行了切割，可能会导致一些优化丢失，今后会持续地对这部分算法进行改进。</p>
<p>总结一下，Blink Planner 的分段优化，其实解的是多 Sink 优化问题（DAG 优化），单 Sink 不是分段优化关心的问题，单 Sink 可以在所有节点上优化，不需要分段。</p>
<h4 id="Sub-Plan-Reuse"><a href="#Sub-Plan-Reuse" class="headerlink" title="Sub-Plan Reuse"></a>Sub-Plan Reuse</h4><p>示例 6</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> SinkTabl</span><br><span class="line"></span><br><span class="line"><span class="keyword">select</span> freq <span class="keyword">from</span> (<span class="keyword">select</span> word, <span class="built_in">count</span>(<span class="number">1</span>) <span class="keyword">as</span> freq <span class="keyword">from</span> SourceTable <span class="keyword">group</span> <span class="keyword">by</span> word) t <span class="keyword">where</span> word <span class="keyword">like</span> <span class="string">&#x27;T%&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">union</span> <span class="keyword">all</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">select</span> <span class="built_in">count</span>(word) <span class="keyword">as</span> freq2 <span class="keyword">from</span> (<span class="keyword">select</span> word, <span class="built_in">count</span>(<span class="number">1</span>) <span class="keyword">as</span> freq <span class="keyword">from</span> SourceTable <span class="keyword">group</span> <span class="keyword">by</span> word) t <span class="keyword">group</span> <span class="keyword">by</span> freq; </span><br></pre></td></tr></table></figure>
<p>复制代码</p>
<p>这个示例的 SQL 和分段优化的 SQL 其实是类似的，不同的是，没有将结果 Sink 到两个 Table 里面，而是将结果 Union 起来，Sink 到一个结果表里面。</p>
<p>下面看一下转化为 RelNode 的 DAG 图：</p>
<p><img src="vx_images/5722510102563">  </p>
<p>图 8 示例 6 RelNode DAG</p>
<p>从上图可以看出，Scan 和第一层的 Aggregate 也是有重复计算的，Blink Planner 其实也会将其找出来，变成下面的图：</p>
<p><img src="vx_images/5711511556085">  </p>
<p>图9 示例 6 Blink Planner DAG</p>
<p>Sub-Plan 优化的启用，有两个相关的配置：</p>
<ul>
<li><p>  table.optimizer.reuse-sub-plan-enabled （默认开启）</p>
</li>
<li><p>  table.optimizer.reuse-source-enabled（默认开启）</p>
</li>
</ul>
<p>这两个配置，默认都是开启的，用户可以根据自己的需求进行关闭。这里主要说明一下 table.optimizer.reuse-source-enabled 这个参数。在 Batch 模式下，join 操作可能会导致死锁，具体场景是在执行 hash-join 或者 nested-loop-join 时一定是先读 build 端，然后再读 probe 端，如果启用 reuse-source-enabled，当数据源是同一个 Source 的时候，Source 的数据会同时发送给 build 和 probe 端。这时候，build 端的数据将不会被消费，导致 join 操作无法完成，整个 join 就被卡住了。</p>
<p>为了解决死锁问题，Blink Planner 会先将 probe 端的数据落盘，这样 build 端读数据的操作才会正常，等 build 端的数据全部读完之后，再从磁盘中拉取 probe 端的数据，从而解决死锁问题。但是，落盘会有额外的开销，会多一次写的操作；有时候，读两次 Source 的开销，可能比一次写的操作更快，这时候，可以关闭 reuse-source，性能会更好。当然，如果读两次 Source 的开销，远大于一次落盘的开销，可以保持 reuse-source 开启。需要说明的是，Stream 模式是不存在死锁问题的，因为 Stream 模式 join 不会有选边的问题。</p>
<p>总结而言，sub-plan reuse 解的问题是优化结果的子图复用问题，它和分段优化类似，但他们是一个互补的过程。</p>
<p>注：Hash Join：对于两张待 join 的表 t1, t2。选取其中的一张表按照 join 条件给的列建立hash 表。然后扫描另外一张表，一行一行去建好的 hash 表判断是否有对应相等的行来完成 join 操作，这个操作称之为 probe (探测)。前一张表叫做 build 表，后一张表的叫做 probe 表。</p>
<h4 id="Agg-分类优化"><a href="#Agg-分类优化" class="headerlink" title="Agg 分类优化"></a>Agg 分类优化</h4><p>Blink 中的 Aggregate 操作是非常丰富的：</p>
<ul>
<li><p>  group agg，例如：select count(a) from t group by b</p>
</li>
<li><p>  over agg，例如：select count(a) over (partition by b order by c) from t</p>
</li>
<li><p>  window agg，例如：select count(a) from t group by tumble(ts, interval ‘10’ second), b</p>
</li>
<li><p>  table agg ，例如：tEnv.scan(‘t’).groupBy(‘a’).flatAggregate(flatAggFunc(‘b’ as (‘c’, ‘d’)))</p>
</li>
</ul>
<p>下面主要对 Group Agg 优化进行讲解，主要是两类优化。</p>
<p>1. Local/Global Agg 优化</p>
<p>Local/Global Agg 主要是为了减少网络 Shuffle。要运用 Local/Global 的优化，必要条件如下：</p>
<ul>
<li><p>  Aggregate 的所有 Agg Function 都是 mergeable 的，每个 Aggregate 需要实现 merge 方法，例如 SUM，COUNT，AVG，这些都是可以分多阶段完成，最终将结果合并；但是求中位数，计算 95% 这种类似的问题，无法拆分为多阶段，因此，无法运用 Local/Global 的优化。</p>
</li>
<li><p>  table.optimizer.agg-phase-strategy 设置为 AUTO 或者 TWO_PHASE。</p>
</li>
<li><p>  Stream 模式下，mini-batch 开启 ；Batch 模式下 AUTO 会根据 cost 模型加上统计数据，选择是否进行 Local/Global 优化。</p>
</li>
</ul>
<p>示例 7</p>
<p>select count(*) from t group by color</p>
<p>复制代码</p>
<p>没有优化的情况下，下面的这个 Aggregate 会产生 10 次的 Shuffle 操作。</p>
<p><img src="vx_images/5702882869443">  </p>
<p>图 10 示例 7 未做优化的 Count 操作</p>
<p>使用 Local/Global 优化后，会转化为下面的操作，会在本地先进行聚合，然后再进行 Shuffle 操作，整个 Shuffle 的数据剩下 6 条。在 Stream 模式下，Blink 其实会以 mini-batch 的维度对结果进行预聚合，然后将结果发送给 Global Agg 进行汇总。</p>
<p><img src="vx_images/5692318921039">  </p>
<p>图 11 示例 7 经过 Local/Global 优化的 Count 操作</p>
<p>2. Distinct Agg 优化</p>
<p>Distinct Agg 进行优化，主要是对 SQL 语句进行改写，达到优化的目的。但 Batch 模式和 Stream 模式解决的问题是不同的：</p>
<ul>
<li><p>  Batch 模式下的 Distinct Agg，需要先做 Distinct，再做 Agg，逻辑上需要两步才能实现，直接实现 Distinct Agg 开销太大。</p>
</li>
<li><p>  Stream 模式下，主要是解决热点问题，因为 Stream 需要将所有的输入数据放在 State 里面，如果数据有热点，State 操作会很频繁，这将影响性能。</p>
</li>
</ul>
<p>Batch 模式</p>
<p>第一层，求 distinct 的值和非 distinct agg function 的值，第二层求 distinct agg function 的值</p>
<p>示例 8</p>
<p>select color, count(distinct id), count(*) from t group by color </p>
<p>复制代码</p>
<p>手工改写成：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> color, <span class="built_in">count</span>(id), <span class="built_in">min</span>(cnt) <span class="keyword">from</span> (</span><br><span class="line"></span><br><span class="line"><span class="keyword">select</span> color, id, <span class="built_in">count</span>(\<span class="operator">*</span>) <span class="keyword">filter</span> (<span class="keyword">where</span> $e<span class="operator">=</span><span class="number">2</span>) <span class="keyword">as</span> cnt <span class="keyword">from</span> (</span><br><span class="line"></span><br><span class="line"><span class="keyword">select</span> color, id, <span class="number">1</span> <span class="keyword">as</span> $e <span class="keyword">from</span> t <span class="comment">--for distinct id</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">union</span> <span class="keyword">all</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">select</span> color, <span class="keyword">null</span> <span class="keyword">as</span> id, <span class="number">2</span> <span class="keyword">as</span> $e <span class="keyword">from</span> t <span class="comment">-- for count(\*)</span></span><br><span class="line"></span><br><span class="line">) <span class="keyword">group</span> <span class="keyword">by</span> color, id, $e</span><br><span class="line"></span><br><span class="line">) <span class="keyword">group</span> <span class="keyword">by</span> color </span><br></pre></td></tr></table></figure>
<p>复制代码</p>
<p>转化的逻辑过程，如下图所示：</p>
<p><img src="vx_images/5681295595288">  </p>
<p>图 12 示例 8 Batch 模式 Distinct 改写逻辑</p>
<p>Stream 模式</p>
<p>Stream 模式的启用有一些必要条件：</p>
<ul>
<li><p>  必须是支持的 agg function：avg/count/min/max/sum/first_value/concat_agg/single_value；</p>
</li>
<li><p>  table.optimizer.distinct-agg.split.enabled（默认关闭）</p>
</li>
</ul>
<p>示例 9</p>
<p>select color, count(distinct id), count(*) from t group by color </p>
<p>复制代码</p>
<p>手工改写成：</p>
<p>select color, sum(dcnt), sum(cnt) from (</p>
<p>select color, count(distinct id) as dcnt, count(*) as cnt from t</p>
<p>group by color, mod(hash_code(id), 1024)</p>
<p>) group by color</p>
<p>复制代码</p>
<p>改写前，逻辑图大概如下：</p>
<p><img src="vx_images/5669686806196">  </p>
<p>图 13 示例 9 Stream 模式未优化 Distinct</p>
<p>改写后，逻辑图就会变为下面这样，热点数据被打散到多个中间节点上。</p>
<p><img src="vx_images/5659004248673">  </p>
<p>图14 示例 9 Stream 模式优化 Distinct</p>
<p>需要注意的是，示例 5 的 SQL 中 mod(hash_code(id),1024)中的这个 1024 为打散的维度，这个值建议设置大一些，设置太小产生的效果可能不好。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本文首先对新的 TableEnvironment 的整体设计进行了介绍，并且列举了各种模式下TableEnvironment 的选择，然后通过具体的示例，展示了各种模式下代码的写法，以及需要注意的事项。</p>
<p>在新的 Catalog 和 DDL 部分，对 Catalog 的整体设计、DDL 的使用部分也都以实例进行拆分讲解。最后，对 Blink Planner 解析 SQL/Table API 的流程、Blink Planner 的改进以及优化的原理进行了讲解，希望对大家探索和使用 Flink SQL 有所帮助。</p>
<h2 id="SQL解析工具"><a href="#SQL解析工具" class="headerlink" title="SQL解析工具"></a>SQL解析工具</h2><p>hive使用了antlr3实现了自己的HQL,<br>Flink使用Apache Calcite,<br>而Calcite的解析器是使用JavaCC实现的,<br>Spark2.x以后采用了antlr4实现自己的解析器,<br>Presto也是使用antlr4。</p>
<h2 id="Window"><a href="#Window" class="headerlink" title="Window"></a>Window</h2><p>在Apache Flink中有2种类型的Window，一种是OverWindow，即传统数据库的标准开窗，每一个元素都对应一个窗口。一种是GroupWindow，目前在SQL中GroupWindow都是基于时间进行窗口划分的。</p>
<h3 id="Over-Window"><a href="#Over-Window" class="headerlink" title="Over Window"></a>Over Window</h3><p>Apache Flink中对OVER Window的定义遵循标准SQL的定义语法。<br>按ROWS和RANGE分类是传统数据库的标准分类方法，在Apache Flink中还可以根据时间类型(ProcTime/EventTime)和窗口的有限和无限(Bounded/UnBounded)进行分类，共计8种类型。为了避免大家对过细分类造成困扰，我们按照确定当前行的不同方式将OVER Window分成两大类进行介绍，如下:</p>
<ul>
<li>  ROWS OVER Window - 每一行元素都视为新的计算行，即，每一行都是一个新的窗口。</li>
<li>  RANGE OVER Window - 具有相同时间值的所有元素行视为同一计算行，即，具有相同时间值的所有行都是同一个窗口。</li>
</ul>
<h4 id="Bounded-ROWS-OVER-Window"><a href="#Bounded-ROWS-OVER-Window" class="headerlink" title="Bounded ROWS OVER Window"></a>Bounded ROWS OVER Window</h4><p>Bounded ROWS OVER Window 每一行元素都视为新的计算行，即，每一行都是一个新的窗口。</p>
<h5 id="语义"><a href="#语义" class="headerlink" title="语义"></a>语义</h5><p>我们以3个元素(2 PRECEDING)的窗口为例，如下图:<br><img src="vx_images/5456982869145.png" alt="image" title="image"></p>
<p>上图所示窗口 user 1 的 w5和w6， user 2的 窗口 w2 和 w3，虽然有元素都是同一时刻到达，但是他们仍然是在不同的窗口，这一点有别于RANGE OVER Window。</p>
<h5 id="语法"><a href="#语法" class="headerlink" title="语法"></a>语法</h5><p>Bounded ROWS OVER Window 语法如下：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> </span><br><span class="line">    agg1(col1) <span class="keyword">OVER</span>(</span><br><span class="line">     [<span class="keyword">PARTITION</span> <span class="keyword">BY</span> (value_expression1,..., value_expressionN)] </span><br><span class="line">     <span class="keyword">ORDER</span> <span class="keyword">BY</span> timeCol</span><br><span class="line">     <span class="keyword">ROWS</span> </span><br><span class="line">     <span class="keyword">BETWEEN</span> (UNBOUNDED <span class="operator">|</span> rowCount) PRECEDING <span class="keyword">AND</span> <span class="keyword">CURRENT</span> <span class="type">ROW</span>) <span class="keyword">AS</span> colName, </span><br><span class="line">... </span><br><span class="line"><span class="keyword">FROM</span> Tab1</span><br></pre></td></tr></table></figure>
<ul>
<li>  value_expression - 进行分区的字表达式；</li>
<li>  timeCol - 用于元素排序的时间字段；</li>
<li>  rowCount - 是定义根据当前行开始向前追溯几行元素。</li>
</ul>
<h5 id="SQL-示例"><a href="#SQL-示例" class="headerlink" title="SQL 示例"></a>SQL 示例</h5><p>利用<code>item_tab</code>测试数据，我们统计同类商品中当前和当前商品之前2个商品中的最高价格。</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span>  </span><br><span class="line">    itemID,</span><br><span class="line">    itemType, </span><br><span class="line">    onSellTime, </span><br><span class="line">    price,  </span><br><span class="line">    <span class="built_in">MAX</span>(price) <span class="keyword">OVER</span> (</span><br><span class="line">        <span class="keyword">PARTITION</span> <span class="keyword">BY</span> itemType </span><br><span class="line">        <span class="keyword">ORDER</span> <span class="keyword">BY</span> onSellTime </span><br><span class="line">        <span class="keyword">ROWS</span> <span class="keyword">BETWEEN</span> <span class="number">2</span> preceding <span class="keyword">AND</span> <span class="keyword">CURRENT</span> <span class="type">ROW</span>) <span class="keyword">AS</span> maxPrice</span><br><span class="line">  <span class="keyword">FROM</span> item_tab</span><br></pre></td></tr></table></figure>
<h4 id="Result"><a href="#Result" class="headerlink" title="Result"></a>Result</h4><table>
<thead>
<tr>
<th>itemID</th>
<th>itemType</th>
<th>onSellTime</th>
<th>price</th>
<th>maxPrice</th>
</tr>
</thead>
<tbody><tr>
<td>ITEM001</td>
<td>Electronic</td>
<td>2017-11-11 10:01:00</td>
<td>20</td>
<td>20</td>
</tr>
<tr>
<td>ITEM002</td>
<td>Electronic</td>
<td>2017-11-11 10:02:00</td>
<td>50</td>
<td>50</td>
</tr>
<tr>
<td>ITEM003</td>
<td>Electronic</td>
<td>2017-11-11 10:03:00</td>
<td>30</td>
<td>50</td>
</tr>
<tr>
<td>ITEM004</td>
<td>Electronic</td>
<td>2017-11-11 10:03:00</td>
<td>60</td>
<td>60</td>
</tr>
<tr>
<td>ITEM005</td>
<td>Electronic</td>
<td>2017-11-11 10:05:00</td>
<td>40</td>
<td>60</td>
</tr>
<tr>
<td>ITEM006</td>
<td>Electronic</td>
<td>2017-11-11 10:06:00</td>
<td>20</td>
<td>60</td>
</tr>
<tr>
<td>ITEM007</td>
<td>Electronic</td>
<td>2017-11-11 10:07:00</td>
<td>70</td>
<td>70</td>
</tr>
<tr>
<td>ITEM008</td>
<td>Clothes</td>
<td>2017-11-11 10:08:00</td>
<td>20</td>
<td>20</td>
</tr>
</tbody></table>
<h4 id="Bounded-RANGE-OVER-Window"><a href="#Bounded-RANGE-OVER-Window" class="headerlink" title="Bounded RANGE OVER Window"></a>Bounded RANGE OVER Window</h4><p>Bounded RANGE OVER Window 具有相同时间值的所有元素行视为同一计算行，即，具有相同时间值的所有行都是同一个窗口。</p>
<h5 id="语义-1"><a href="#语义-1" class="headerlink" title="语义"></a>语义</h5><p>我们以3秒中数据(INTERVAL ‘2’ SECOND)的窗口为例，如下图：<br><img src="vx_images/5426718920741.png" alt="image" title="image"></p>
<p>注意: 上图所示窗口 user 1 的 w6， user 2的 窗口 w3，元素都是同一时刻到达,他们是在同一个窗口，这一点有别于ROWS OVER Window。</p>
<h5 id="语法-1"><a href="#语法-1" class="headerlink" title="语法"></a>语法</h5><p>Bounded RANGE OVER Window的语法如下：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> </span><br><span class="line">    agg1(col1) <span class="keyword">OVER</span>(</span><br><span class="line">     [<span class="keyword">PARTITION</span> <span class="keyword">BY</span> (value_expression1,..., value_expressionN)] </span><br><span class="line">     <span class="keyword">ORDER</span> <span class="keyword">BY</span> timeCol</span><br><span class="line">     <span class="keyword">RANGE</span> </span><br><span class="line">     <span class="keyword">BETWEEN</span> (UNBOUNDED <span class="operator">|</span> timeInterval) PRECEDING <span class="keyword">AND</span> <span class="keyword">CURRENT</span> <span class="type">ROW</span>) <span class="keyword">AS</span> colName, </span><br><span class="line">... </span><br><span class="line"><span class="keyword">FROM</span> Tab1</span><br></pre></td></tr></table></figure>
<ul>
<li>  value_expression - 进行分区的字表达式；</li>
<li>  timeCol - 用于元素排序的时间字段；</li>
<li>  timeInterval - 是定义根据当前行开始向前追溯指定时间的元素行；</li>
</ul>
<h5 id="SQL-示例-1"><a href="#SQL-示例-1" class="headerlink" title="SQL 示例"></a>SQL 示例</h5><p>我们统计同类商品中当前和当前商品之前2分钟商品中的最高价格。</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span>  </span><br><span class="line">    itemID,</span><br><span class="line">    itemType, </span><br><span class="line">    onSellTime, </span><br><span class="line">    price,  </span><br><span class="line">    <span class="built_in">MAX</span>(price) <span class="keyword">OVER</span> (</span><br><span class="line">        <span class="keyword">PARTITION</span> <span class="keyword">BY</span> itemType </span><br><span class="line">        <span class="keyword">ORDER</span> <span class="keyword">BY</span> rowtime </span><br><span class="line">        <span class="keyword">RANGE</span> <span class="keyword">BETWEEN</span> <span class="type">INTERVAL</span> <span class="string">&#x27;2&#x27;</span> <span class="keyword">MINUTE</span> preceding <span class="keyword">AND</span> <span class="keyword">CURRENT</span> <span class="type">ROW</span>) <span class="keyword">AS</span> maxPrice</span><br><span class="line">  <span class="keyword">FROM</span> item_tab</span><br></pre></td></tr></table></figure>
<h5 id="Result（Bounded-RANGE-OVER-Window）"><a href="#Result（Bounded-RANGE-OVER-Window）" class="headerlink" title="Result（Bounded RANGE OVER Window）"></a>Result（Bounded RANGE OVER Window）</h5><table>
<thead>
<tr>
<th>itemID</th>
<th>itemType</th>
<th>onSellTime</th>
<th>price</th>
<th>maxPrice</th>
</tr>
</thead>
<tbody><tr>
<td>ITEM001</td>
<td>Electronic</td>
<td>2017-11-11 10:01:00</td>
<td>20</td>
<td>20</td>
</tr>
<tr>
<td>ITEM002</td>
<td>Electronic</td>
<td>2017-11-11 10:02:00</td>
<td>50</td>
<td>50</td>
</tr>
<tr>
<td>ITEM003</td>
<td>Electronic</td>
<td><strong><em>2017-11-11 10:03:00</em></strong></td>
<td>30</td>
<td>60</td>
</tr>
<tr>
<td>ITEM004</td>
<td>Electronic</td>
<td><strong><em>2017-11-11 10:03:00</em></strong></td>
<td>60</td>
<td>60</td>
</tr>
<tr>
<td>ITEM005</td>
<td>Electronic</td>
<td>2017-11-11 10:05:00</td>
<td>40</td>
<td>60</td>
</tr>
<tr>
<td>ITEM006</td>
<td>Electronic</td>
<td>2017-11-11 10:06:00</td>
<td>20</td>
<td>40</td>
</tr>
<tr>
<td>ITEM007</td>
<td>Electronic</td>
<td>2017-11-11 10:07:00</td>
<td>70</td>
<td>70</td>
</tr>
<tr>
<td>ITEM008</td>
<td>Clothes</td>
<td>2017-11-11 10:08:00</td>
<td>20</td>
<td>20</td>
</tr>
</tbody></table>
<h4 id="特别说明"><a href="#特别说明" class="headerlink" title="特别说明"></a>特别说明</h4><p>OverWindow最重要是要理解每一行数据都确定一个窗口，同时目前在Apache Flink中只支持按时间字段排序。并且OverWindow开窗与GroupBy方式数据分组最大的不同在于，GroupBy数据分组统计时候，在<code>SELECT</code>中除了GROUP BY的key，不能直接选择其他非key的字段，但是OverWindow没有这个限制，<code>SELECT</code>可以选择任何字段。比如一张表table(a,b,c,d)4个字段，如果按d分组求c的最大值，两种写完如下:</p>
<ul>
<li>  GROUP BY - <code>SELECT d, MAX(c) FROM table GROUP BY d</code></li>
<li>OVER Window = <code>SELECT a, b, c, d, MAX(c) OVER(PARTITION BY d, ORDER BY ProcTime())</code><br>  如上 OVER Window 虽然PARTITION BY d,但SELECT 中仍然可以选择 a,b,c字段。但在GROUPBY中，SELECT 只能选择 d 字段。</li>
</ul>
<h3 id="Group-Window"><a href="#Group-Window" class="headerlink" title="Group Window"></a>Group Window</h3><p>根据窗口数据划分的不同，目前Apache Flink有如下3种Bounded Winodw:</p>
<ul>
<li>  Tumble - 滚动窗口，窗口数据有固定的大小，窗口数据无叠加；</li>
<li>  Hop - 滑动窗口，窗口数据有固定大小，并且有固定的窗口重建频率，窗口数据有叠加；</li>
<li>  Session - 会话窗口，窗口数据没有固定的大小，根据窗口数据活跃程度划分窗口，窗口数据无叠加。</li>
</ul>
<p><strong>说明：</strong> Aapche Flink 还支持UnBounded的 Group Window，也就是全局Window，流上所有数据都在一个窗口里面，语义非常简单，这里不做详细介绍了。</p>
<h4 id="Tumble"><a href="#Tumble" class="headerlink" title="Tumble"></a>Tumble</h4><h5 id="语义-2"><a href="#语义-2" class="headerlink" title="语义"></a>语义</h5><p>Tumble 滚动窗口有固定size，窗口数据不重叠,具体语义如下：<br><img src="vx_images/5385895594990.png" alt="image" title="image"></p>
<h5 id="语法-2"><a href="#语法-2" class="headerlink" title="语法"></a>语法</h5><p>Tumble 滚动窗口对应的语法如下：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> </span><br><span class="line">    [gk],</span><br><span class="line">    [TUMBLE_START(timeCol, size)], </span><br><span class="line">    [TUMBLE_END(timeCol, size)], </span><br><span class="line">    agg1(col1), </span><br><span class="line">    ... </span><br><span class="line">    aggn(colN)</span><br><span class="line"><span class="keyword">FROM</span> Tab1</span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> [gk], TUMBLE(timeCol, size)</span><br></pre></td></tr></table></figure>
<ul>
<li>  [gk] - 决定了流是Keyed还是/Non-Keyed;</li>
<li>  TUMBLE_START - 窗口开始时间;</li>
<li>  TUMBLE_END - 窗口结束时间;</li>
<li>  timeCol - 是流表中表示时间字段；</li>
<li>  size - 表示窗口的大小，如 秒，分钟，小时，天。</li>
</ul>
<h5 id="SQL-示例-2"><a href="#SQL-示例-2" class="headerlink" title="SQL 示例"></a>SQL 示例</h5><p>利用<code>pageAccess_tab</code>测试数据，我们需要按不同地域统计每2分钟的淘宝首页的访问量(PV)。</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span>  </span><br><span class="line">    region,</span><br><span class="line">    TUMBLE_START(rowtime, <span class="type">INTERVAL</span> <span class="string">&#x27;2&#x27;</span> <span class="keyword">MINUTE</span>) <span class="keyword">AS</span> winStart,  </span><br><span class="line">    TUMBLE_END(rowtime, <span class="type">INTERVAL</span> <span class="string">&#x27;2&#x27;</span> <span class="keyword">MINUTE</span>) <span class="keyword">AS</span> winEnd,  </span><br><span class="line">    <span class="built_in">COUNT</span>(region) <span class="keyword">AS</span> pv</span><br><span class="line"><span class="keyword">FROM</span> pageAccess_tab </span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> region, TUMBLE(rowtime, <span class="type">INTERVAL</span> <span class="string">&#x27;2&#x27;</span> <span class="keyword">MINUTE</span>)</span><br></pre></td></tr></table></figure>
<h5 id="Result-1"><a href="#Result-1" class="headerlink" title="Result"></a>Result</h5><table>
<thead>
<tr>
<th>region</th>
<th>winStart</th>
<th>winEnd</th>
<th>pv</th>
</tr>
</thead>
<tbody><tr>
<td>BeiJing</td>
<td>2017-11-11 02:00:00.0</td>
<td>2017-11-11 02:02:00.0</td>
<td>1</td>
</tr>
<tr>
<td>BeiJing</td>
<td>2017-11-11 02:10:00.0</td>
<td>2017-11-11 02:12:00.0</td>
<td>2</td>
</tr>
<tr>
<td>ShangHai</td>
<td>2017-11-11 02:00:00.0</td>
<td>2017-11-11 02:02:00.0</td>
<td>1</td>
</tr>
<tr>
<td>ShangHai</td>
<td>2017-11-11 04:10:00.0</td>
<td>2017-11-11 04:12:00.0</td>
<td>1</td>
</tr>
</tbody></table>
<h4 id="Hop"><a href="#Hop" class="headerlink" title="Hop"></a>Hop</h4><p>Hop 滑动窗口和滚动窗口类似，窗口有固定的size，与滚动窗口不同的是滑动窗口可以通过slide参数控制滑动窗口的新建频率。因此当slide值小于窗口size的值的时候多个滑动窗口会重叠。</p>
<h5 id="语义-3"><a href="#语义-3" class="headerlink" title="语义"></a>语义</h5><p>Hop 滑动窗口语义如下所示：<br><img src="vx_images/5354486805898.png" alt="image" title="image"></p>
<h5 id="语法-3"><a href="#语法-3" class="headerlink" title="语法"></a>语法</h5><p>Hop 滑动窗口对应语法如下：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> </span><br><span class="line">    [gk], </span><br><span class="line">    [HOP_START(timeCol, slide, size)] ,  </span><br><span class="line">    [HOP_END(timeCol, slide, size)],</span><br><span class="line">    agg1(col1), </span><br><span class="line">    ... </span><br><span class="line">    aggN(colN) </span><br><span class="line"><span class="keyword">FROM</span> Tab1</span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> [gk], HOP(timeCol, slide, size)</span><br></pre></td></tr></table></figure>
<ul>
<li>  [gk] 决定了流是Keyed还是/Non-Keyed;</li>
<li>  HOP_START - 窗口开始时间;</li>
<li>  HOP_END - 窗口结束时间;</li>
<li>  timeCol - 是流表中表示时间字段；</li>
<li>  slide - 是滑动步伐的大小；</li>
<li>  size - 是窗口的大小，如 秒，分钟，小时，天；</li>
</ul>
<h5 id="SQL-示例-3"><a href="#SQL-示例-3" class="headerlink" title="SQL 示例"></a>SQL 示例</h5><p>利用<code>pageAccessCount_tab</code>测试数据，我们需要每5分钟统计近10分钟的页面访问量(PV).</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span>  </span><br><span class="line">  HOP_START(rowtime, <span class="type">INTERVAL</span> <span class="string">&#x27;5&#x27;</span> <span class="keyword">MINUTE</span>, <span class="type">INTERVAL</span> <span class="string">&#x27;10&#x27;</span> <span class="keyword">MINUTE</span>) <span class="keyword">AS</span> winStart,  </span><br><span class="line">  HOP_END(rowtime, <span class="type">INTERVAL</span> <span class="string">&#x27;5&#x27;</span> <span class="keyword">MINUTE</span>, <span class="type">INTERVAL</span> <span class="string">&#x27;10&#x27;</span> <span class="keyword">MINUTE</span>) <span class="keyword">AS</span> winEnd,  </span><br><span class="line">  <span class="built_in">SUM</span>(accessCount) <span class="keyword">AS</span> accessCount  </span><br><span class="line"><span class="keyword">FROM</span> pageAccessCount_tab </span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> HOP(rowtime, <span class="type">INTERVAL</span> <span class="string">&#x27;5&#x27;</span> <span class="keyword">MINUTE</span>, <span class="type">INTERVAL</span> <span class="string">&#x27;10&#x27;</span> <span class="keyword">MINUTE</span>)</span><br></pre></td></tr></table></figure>
<h5 id="Result-2"><a href="#Result-2" class="headerlink" title="Result"></a>Result</h5><table>
<thead>
<tr>
<th>winStart</th>
<th>winEnd</th>
<th>accessCount</th>
</tr>
</thead>
<tbody><tr>
<td>2017-11-11 01:55:00.0</td>
<td>2017-11-11 02:05:00.0</td>
<td>186</td>
</tr>
<tr>
<td>2017-11-11 02:00:00.0</td>
<td>2017-11-11 02:10:00.0</td>
<td>396</td>
</tr>
<tr>
<td>2017-11-11 02:05:00.0</td>
<td>2017-11-11 02:15:00.0</td>
<td>243</td>
</tr>
<tr>
<td>2017-11-11 02:10:00.0</td>
<td>2017-11-11 02:20:00.0</td>
<td>33</td>
</tr>
<tr>
<td>2017-11-11 04:05:00.0</td>
<td>2017-11-11 04:15:00.0</td>
<td>129</td>
</tr>
<tr>
<td>2017-11-11 04:10:00.0</td>
<td>2017-11-11 04:20:00.0</td>
<td>129</td>
</tr>
</tbody></table>
<h4 id="Session"><a href="#Session" class="headerlink" title="Session"></a>Session</h4><p>Seeeion 会话窗口 是没有固定大小的窗口，通过session的活跃度分组元素。不同于滚动窗口和滑动窗口，会话窗口不重叠,也没有固定的起止时间。一个会话窗口在一段时间内没有接收到元素时，即当出现非活跃间隙时关闭。一个会话窗口 分配器通过配置session gap来指定非活跃周期的时长.</p>
<h5 id="语义-4"><a href="#语义-4" class="headerlink" title="语义"></a>语义</h5><p>Session 会话窗口语义如下所示：</p>
<p><img src="vx_images/5324204248375.png" alt="image" title="image"></p>
<h5 id="语法-4"><a href="#语法-4" class="headerlink" title="语法"></a>语法</h5><p>Seeeion 会话窗口对应语法如下：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> </span><br><span class="line">    [gk], </span><br><span class="line">    SESSION_START(timeCol, gap) <span class="keyword">AS</span> winStart,  </span><br><span class="line">    SESSION_END(timeCol, gap) <span class="keyword">AS</span> winEnd,</span><br><span class="line">    agg1(col1),</span><br><span class="line">     ... </span><br><span class="line">    aggn(colN)</span><br><span class="line"><span class="keyword">FROM</span> Tab1</span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> [gk], SESSION(timeCol, gap)</span><br></pre></td></tr></table></figure>
<ul>
<li>  [gk] 决定了流是Keyed还是/Non-Keyed;</li>
<li>  SESSION_START - 窗口开始时间；</li>
<li>  SESSION_END - 窗口结束时间；</li>
<li>  timeCol - 是流表中表示时间字段；</li>
<li>  gap - 是窗口数据非活跃周期的时长；</li>
</ul>
<h5 id="SQL-示例-4"><a href="#SQL-示例-4" class="headerlink" title="SQL 示例"></a>SQL 示例</h5><p>利用<code>pageAccessSession_tab</code>测试数据，我们按地域统计连续的两个访问用户之间的访问时间间隔不超过3分钟的的页面访问量(PV).</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span>  </span><br><span class="line">    region, </span><br><span class="line">    SESSION_START(rowtime, <span class="type">INTERVAL</span> <span class="string">&#x27;3&#x27;</span> <span class="keyword">MINUTE</span>) <span class="keyword">AS</span> winStart,  </span><br><span class="line">    SESSION_END(rowtime, <span class="type">INTERVAL</span> <span class="string">&#x27;3&#x27;</span> <span class="keyword">MINUTE</span>) <span class="keyword">AS</span> winEnd, </span><br><span class="line">    <span class="built_in">COUNT</span>(region) <span class="keyword">AS</span> pv  </span><br><span class="line"><span class="keyword">FROM</span> pageAccessSession_tab</span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> region, SESSION(rowtime, <span class="type">INTERVAL</span> <span class="string">&#x27;3&#x27;</span> <span class="keyword">MINUTE</span>)</span><br></pre></td></tr></table></figure>
<h5 id="Result-3"><a href="#Result-3" class="headerlink" title="Result"></a>Result</h5><table>
<thead>
<tr>
<th>region</th>
<th>winStart</th>
<th>winEnd</th>
<th>pv</th>
</tr>
</thead>
<tbody><tr>
<td>BeiJing</td>
<td>2017-11-11 02:10:00.0</td>
<td>2017-11-11 02:13:00.0</td>
<td>1</td>
</tr>
<tr>
<td>ShangHai</td>
<td>2017-11-11 02:01:00.0</td>
<td>2017-11-11 02:08:00.0</td>
<td>4</td>
</tr>
<tr>
<td>ShangHai</td>
<td>2017-11-11 02:10:00.0</td>
<td>2017-11-11 02:14:00.0</td>
<td>2</td>
</tr>
<tr>
<td>ShangHai</td>
<td>2017-11-11 04:16:00.0</td>
<td>2017-11-11 04:19:00.0</td>
<td>1</td>
</tr>
</tbody></table>
<h2 id="UDX"><a href="#UDX" class="headerlink" title="UDX"></a>UDX</h2><p>Apache Flink 除了提供了大部分ANSI-SQL的核心算子，也为用户提供了自己编写业务代码的机会，那就是User-Defined Function,目前支持如下三种 User-Defined Function：</p>
<ul>
<li>  UDF - User-Defined Scalar Function</li>
<li>  UDTF - User-Defined Table Function</li>
<li>  UDAF - User-Defined Aggregate Funciton</li>
</ul>
<p>UDX都是用户自定义的函数，那么Apache Flink框架为啥将自定义的函数分成三类呢？是根据什么划分的呢？Apache Flink对自定义函数进行分类的依据是根据函数语义的不同，函数的输入和输出不同来分类的，具体如下：</p>
<table>
<thead>
<tr>
<th>UDX</th>
<th>INPUT</th>
<th>OUTPUT</th>
<th>INPUT:OUTPUT</th>
</tr>
</thead>
<tbody><tr>
<td>UDF</td>
<td>单行中的N(N&gt;=0)列</td>
<td>单行中的1列</td>
<td>1:1</td>
</tr>
<tr>
<td>UDTF</td>
<td>单行中的N(N&gt;=0)列</td>
<td>M(M&gt;=0)行</td>
<td>1:N(N&gt;=0)</td>
</tr>
<tr>
<td>UDAF</td>
<td>M(M&gt;=0)行中的每行的N(N&gt;=0)列</td>
<td>单行中的1列</td>
<td>M：1(M&gt;=0)</td>
</tr>
</tbody></table>
<h3 id="UDF"><a href="#UDF" class="headerlink" title="UDF"></a>UDF</h3><ul>
<li>定义<br>  用户想自己编写一个字符串联接的UDF，我们只需要实现<code>ScalarFunction#eval()</code>方法即可，简单实现如下：</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">MyConnect</span> <span class="keyword">extends</span> <span class="title">ScalarFunction</span> </span>&#123;</span><br><span class="line">  <span class="meta">@varargs</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">eval</span></span>(args: <span class="type">String</span>*): <span class="type">String</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> sb = <span class="keyword">new</span> <span class="type">StringBuilder</span></span><br><span class="line">    <span class="keyword">var</span> i = <span class="number">0</span></span><br><span class="line">    <span class="keyword">while</span> (i &lt; args.length) &#123;</span><br><span class="line">      <span class="keyword">if</span> (args(i) == <span class="literal">null</span>) &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">null</span></span><br><span class="line">      &#125;</span><br><span class="line">      sb.append(args(i))</span><br><span class="line">      i += <span class="number">1</span></span><br><span class="line">    &#125;</span><br><span class="line">    sb.toString</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ul>
<li>  使用</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> fun = <span class="type">MyConnect</span></span><br><span class="line">tEnv.registerFunction(<span class="string">&quot;myConnect&quot;</span>, fun)</span><br><span class="line"><span class="keyword">val</span> sql = <span class="string">&quot;SELECT myConnect(a, b) as str FROM tab&quot;</span></span><br></pre></td></tr></table></figure>
<h3 id="UDTF"><a href="#UDTF" class="headerlink" title="UDTF"></a>UDTF</h3><ul>
<li>定义<br>  用户想自己编写一个字符串切分的UDTF，我们只需要实现<code>TableFunction#eval()</code>方法即可，简单实现如下：</li>
</ul>
<p>ScalarFunction#eval()`</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MySplit</span> <span class="keyword">extends</span> <span class="title">TableFunction</span>[<span class="type">String</span>] </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">eval</span></span>(str: <span class="type">String</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">if</span> (str.contains(<span class="string">&quot;#&quot;</span>))&#123;</span><br><span class="line">      str.split(<span class="string">&quot;#&quot;</span>).foreach(collect)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">eval</span></span>(str: <span class="type">String</span>, prefix: <span class="type">String</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">if</span> (str.contains(<span class="string">&quot;#&quot;</span>)) &#123;</span><br><span class="line">      str.split(<span class="string">&quot;#&quot;</span>).foreach(s =&gt; collect(prefix + s))</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ul>
<li>  使用</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> fun = <span class="keyword">new</span> <span class="type">MySplit</span>()</span><br><span class="line">tEnv.registerFunction(<span class="string">&quot;mySplit&quot;</span>, fun)</span><br><span class="line"><span class="keyword">val</span> sql = <span class="string">&quot;SELECT c, s FROM MyTable, LATERAL TABLE(mySplit(c)) AS T(s)&quot;</span></span><br></pre></td></tr></table></figure>
<h3 id="UDAF"><a href="#UDAF" class="headerlink" title="UDAF"></a>UDAF</h3><ul>
<li>定义<br>  UDAF 要实现的接口比较多，我们以一个简单的CountAGG为例，做简单实现如下：</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CountAccumulator</span> <span class="keyword">extends</span> <span class="title">JTuple1</span>[<span class="type">Long</span>] </span>&#123;</span><br><span class="line">  f0 = <span class="number">0</span>L </span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyCount</span></span></span><br><span class="line"><span class="class">  <span class="keyword">extends</span> <span class="title">AggregateFunction</span>[<span class="type">JLong</span>, <span class="type">CountAccumulator</span>] </span>&#123;</span><br><span class="line"></span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">accumulate</span></span>(acc: <span class="type">CountAccumulator</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    acc.f0 += <span class="number">1</span>L</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">retract</span></span>(acc: <span class="type">CountAccumulator</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    acc.f0 -= <span class="number">1</span>L</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">accumulate</span></span>(acc: <span class="type">CountAccumulator</span>, value: <span class="type">Any</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">if</span> (value != <span class="literal">null</span>) &#123;</span><br><span class="line">      acc.f0 += <span class="number">1</span>L</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">retract</span></span>(acc: <span class="type">CountAccumulator</span>, value: <span class="type">Any</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">if</span> (value != <span class="literal">null</span>) &#123;</span><br><span class="line">      acc.f0 -= <span class="number">1</span>L</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">getValue</span></span>(acc: <span class="type">CountAccumulator</span>): <span class="type">JLong</span> = &#123;</span><br><span class="line">    acc.f0</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">merge</span></span>(acc: <span class="type">CountAccumulator</span>, its: <span class="type">JIterable</span>[<span class="type">CountAccumulator</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> iter = its.iterator()</span><br><span class="line">    <span class="keyword">while</span> (iter.hasNext) &#123;</span><br><span class="line">      acc.f0 += iter.next().f0</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">createAccumulator</span></span>(): <span class="type">CountAccumulator</span> = &#123;</span><br><span class="line">    <span class="keyword">new</span> <span class="type">CountAccumulator</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">resetAccumulator</span></span>(acc: <span class="type">CountAccumulator</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    acc.f0 = <span class="number">0</span>L</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">getAccumulatorType</span></span>: <span class="type">TypeInformation</span>[<span class="type">CountAccumulator</span>] = &#123;</span><br><span class="line">    <span class="keyword">new</span> <span class="type">TupleTypeInfo</span>(classOf[<span class="type">CountAccumulator</span>], <span class="type">BasicTypeInfo</span>.<span class="type">LONG_TYPE_INFO</span>)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">getResultType</span></span>: <span class="type">TypeInformation</span>[<span class="type">JLong</span>] =</span><br><span class="line">    <span class="type">BasicTypeInfo</span>.<span class="type">LONG_TYPE_INFO</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ul>
<li>  使用</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> fun = <span class="keyword">new</span> <span class="type">MyCount</span>()</span><br><span class="line">tEnv.registerFunction(<span class="string">&quot;myCount&quot;</span>, fun)</span><br><span class="line"><span class="keyword">val</span> sql = <span class="string">&quot;SELECT myCount(c) FROM MyTable GROUP BY  a&quot;</span></span><br></pre></td></tr></table></figure>
<p>上面我们介绍了Apache Flink SQL核心算子的语法及语义，这部分将选取Bounded EventTime Tumble Window为例为大家编写一个完整的包括Source和Sink定义的Apache Flink SQL Job。假设有一张淘宝页面访问表(PageAccess_tab)，有地域，用户ID和访问时间。我们需要按不同地域统计每2分钟的淘宝首页的访问量(PV). 具体数据如下：</p>
<table>
<thead>
<tr>
<th>region</th>
<th>userId</th>
<th>accessTime</th>
</tr>
</thead>
<tbody><tr>
<td>ShangHai</td>
<td>U0010</td>
<td>2017-11-11 10:01:00</td>
</tr>
<tr>
<td>BeiJing</td>
<td>U1001</td>
<td>2017-11-11 10:01:00</td>
</tr>
<tr>
<td>BeiJing</td>
<td>U2032</td>
<td>2017-11-11 10:10:00</td>
</tr>
<tr>
<td>BeiJing</td>
<td>U1100</td>
<td>2017-11-11 10:11:00</td>
</tr>
<tr>
<td>ShangHai</td>
<td>U0011</td>
<td>2017-11-11 12:10:00</td>
</tr>
</tbody></table>
<p>大家都知道，在 Flink 中，通过 Table API 和 SQL 实现的流处理逻辑，最终会翻译为基于 DataStreamAPI 实现的 DataStream 作业，返回这个作业输出的 DataStream (writeToSink 本质上也是先得到 DataStream 作业，再为其输出 DataStream 加上一个DataStreamSink) 。</p>
<p>从一段 SQL 到 DataStream 作业，其过程简单描述如下：</p>
<ol>
<li><p> 在 TableEnvironment，即“表环境”，将数据源注册为动态表。例如，通过表环境的接口`registerDataStream`, 作为源的DataStream，即数据流, 在表环境注册为动态表</p>
</li>
<li><p> 通过表环境的接口 `sqlQuery`，将 SQL 构造为 Table 对象</p>
</li>
<li><p> 通过toAppendStream/toRetractedStream接口，即翻译接口，将 Table 对象表达的作业逻辑，翻译为 DataStream 作业。</p>
</li>
</ol>
<p><img src="vx_images/4467647168580" alt="图片"></p>
<p>在调用翻译接口，将 Table 对象翻译为 DataStream 作业时，通过翻译接口传入的 TTL 配置，递归传递到各个计算节点的翻译、构造逻辑里，使得翻译出来的 DataStream 算子的内部状态按照该 TTL 配置及时清理。</p>
<p>【参考文献】</p>
<ol>
<li><p><a target="_blank" rel="noopener" href="http://www.10tiao.com/html/157/201707/2653162664/1.html">在数据流中使用SQL查询：Apache Flink中的动态表的持续查询</a></p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://jiamaoxiang.top/2020/05/25/Flink-Table-API-SQL%E7%BC%96%E7%A8%8B%E6%8C%87%E5%8D%97/">Flink Table API &amp; SQL编程指南</a></p>
</li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/5/"><i class="fa fa-angle-left" aria-label="Previous page"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/5/">5</a><span class="page-number current">6</span><a class="page-number" href="/page/7/">7</a><span class="space">&hellip;</span><a class="page-number" href="/page/25/">25</a><a class="extend next" rel="next" href="/page/7/"><i class="fa fa-angle-right" aria-label="Next page"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">aaronzhang</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">242</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">16</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">128</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">aaronzhang</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  

</body>
</html>
