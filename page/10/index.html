<!DOCTYPE html>
<html lang="zh-Hans">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta property="og:type" content="website">
<meta property="og:title" content="Guadazi">
<meta property="og:url" content="http://example.com/page/10/index.html">
<meta property="og:site_name" content="Guadazi">
<meta property="og:locale">
<meta property="article:author" content="aaronzhang">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://example.com/page/10/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'zh-Hans'
  };
</script>

  <title>Guadazi</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Guadazi</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://example.com/bigdata/Flink/Flink-SQL/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="aaronzhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Guadazi">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/bigdata/Flink/Flink-SQL/" class="post-title-link" itemprop="url">FlinkSQL与动态表</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2019-08-10 14:58:00" itemprop="dateCreated datePublished" datetime="2019-08-10T14:58:00+08:00">2019-08-10</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-06-01 20:52:39" itemprop="dateModified" datetime="2021-06-01T20:52:39+08:00">2021-06-01</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/bigdata/" itemprop="url" rel="index"><span itemprop="name">bigdata</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="Flink-SQL"><a href="#Flink-SQL" class="headerlink" title="Flink SQL"></a>Flink SQL</h1><h2 id="Flink-sql可插拔架构"><a href="#Flink-sql可插拔架构" class="headerlink" title="Flink sql可插拔架构"></a>Flink sql可插拔架构</h2><h3 id="Connector"><a href="#Connector" class="headerlink" title="Connector"></a>Connector</h3><h4 id="Format"><a href="#Format" class="headerlink" title="Format"></a>Format</h4><h4 id="Schema"><a href="#Schema" class="headerlink" title="Schema"></a>Schema</h4><h2 id="Retract-mode-和-Append-mode"><a href="#Retract-mode-和-Append-mode" class="headerlink" title="Retract mode 和 Append mode"></a>Retract mode 和 Append mode</h2><p>toAppendStream  只支持insert<br>toRetractStream  其余模式都可以</p>
<p>如果动态表仅只有Insert操作，即之前输出的结果不会被更新，则使用该模式。如果更新或删除操作使用追加模式会失败报错，始终可以使用此模式。返回值是boolean类型。它用true或false来标记数据的插入和撤回，返回true代表数据插入，false代表数据的撤回。</p>
<p>使用flinkSQL处理实时数据当我们把表转化成流的时候，需要用toAppendStream与toRetractStream这两个方法。稍不注意可能直接选择了toAppendStream。</p>
<p>始终可以使用此模式。返回值是boolean类型。它用true或false来标记数据的插入和撤回，返回true代表数据插入，false代表数据的撤回。</p>
<p>当我们使用的sql语句包含：count() group by时，必须使用缩进模式</p>
<p><img src="_v_images/20201111111954370_1165111473.png"><br><img src="_v_images/20201111112036578_1272633365.png"></p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 获取StreamTableEnvironment.</span></span><br><span class="line">StreamTableEnvironment tableEnv = ...;</span><br><span class="line"><span class="comment">// 包含两个字段的表(String name, Integer age)</span></span><br><span class="line">Table table = ...</span><br><span class="line"><span class="comment">// 将表转为DataStream，使用Append Mode追加模式，数据类型为Row</span></span><br><span class="line">DataStream&lt;Row&gt; dsRow = tableEnv.toAppendStream(table, Row.class);</span><br><span class="line"><span class="comment">// 将表转为DataStream，使用Append Mode追加模式，数据类型为定义好的TypeInformation</span></span><br><span class="line">TupleTypeInfo&lt;Tuple2&lt;String, Integer&gt;&gt; tupleType = <span class="keyword">new</span> TupleTypeInfo&lt;&gt;(</span><br><span class="line">  Types.STRING(),</span><br><span class="line">  Types.INT());</span><br><span class="line">  DataStream&lt;Tuple2&lt;String, Integer&gt;&gt; dsTuple =</span><br><span class="line">  tableEnv.toAppendStream(table, tupleType);</span><br><span class="line"><span class="comment">// 将表转为DataStream，使用的模式为Retract Mode撤回模式，类型为Row</span></span><br><span class="line"><span class="comment">// 对于转换后的DataStream&lt;Tuple2&lt;Boolean, X&gt;&gt;，X表示流的数据类型，</span></span><br><span class="line"><span class="comment">// boolean值表示数据改变的类型，其中INSERT返回true，DELETE返回的是false</span></span><br><span class="line">DataStream&lt;Tuple2&lt;Boolean, Row&gt;&gt; retractStream</span><br><span class="line">  tableEnv.toRetractStream(table, Row.class);</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h3 id="如何实现回退更新"><a href="#如何实现回退更新" class="headerlink" title="如何实现回退更新?"></a>如何实现回退更新?</h3><p>flink-connector-jdbc 最终使用的是SQL引擎的upsert语法:</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> tbl() <span class="keyword">values</span> ( ),( ) <span class="keyword">on</span> duplicate key <span class="keyword">update</span></span><br></pre></td></tr></table></figure>
<p>可以看下 MySQL/upsert 一节</p>
<p>对应flink 源码</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">org.apache.flink.connector.jdbc.dialect.MySQLDialect#getUpsertStatement</span><br></pre></td></tr></table></figure>



<h2 id="keyedBy与group-by区别"><a href="#keyedBy与group-by区别" class="headerlink" title="keyedBy与group by区别"></a>keyedBy与group by区别</h2><h2 id="SQL解析工具"><a href="#SQL解析工具" class="headerlink" title="SQL解析工具"></a>SQL解析工具</h2><p>hive使用了antlr3实现了自己的HQL,<br>Flink使用Apache Calcite,<br>而Calcite的解析器是使用JavaCC实现的,<br>Spark2.x以后采用了antlr4实现自己的解析器,<br>Presto也是使用antlr4。</p>
<h2 id="通过Table-api创建表"><a href="#通过Table-api创建表" class="headerlink" title="通过Table api创建表"></a>通过Table api创建表</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># create a <span class="type">Table</span> from a <span class="type">Table</span> <span class="type">API</span> query</span><br><span class="line">tapi_result = table_env.from_path(<span class="string">&quot;table1&quot;</span>).select(...)</span><br></pre></td></tr></table></figure>
<h2 id="视图-view"><a href="#视图-view" class="headerlink" title="视图 view"></a>视图 view</h2><h2 id="window-aggregate与group-aggregate区别"><a href="#window-aggregate与group-aggregate区别" class="headerlink" title="window aggregate与group aggregate区别"></a>window aggregate与group aggregate区别</h2><p>参考自<a target="_blank" rel="noopener" href="https://ververica.cn/developers/flink-sql-programming-practice/">Apache Flink 零基础入门（九）：Flink SQL 编程实践</a></p>
<h3 id="Group-Aggregate的例子"><a href="#Group-Aggregate的例子" class="headerlink" title="Group Aggregate的例子"></a>Group Aggregate的例子</h3><p>这是一个group aggregate, 内存中累计每个分类的数据，有变化时，update sink</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> psgCnt, <span class="built_in">COUNT</span>(<span class="operator">*</span>) <span class="keyword">AS</span> cnt </span><br><span class="line"><span class="keyword">FROM</span> Rides </span><br><span class="line"><span class="keyword">WHERE</span> isInNYC(lon, lat)</span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> psgCnt;</span><br></pre></td></tr></table></figure>
<h3 id="Window-Aggregate"><a href="#Window-Aggregate" class="headerlink" title="Window Aggregate"></a>Window Aggregate</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> </span><br><span class="line">  toAreaId(lon, lat) <span class="keyword">AS</span> area, <span class="comment">-- toAreaId为UDF</span></span><br><span class="line">  TUMBLE_END(rideTime, <span class="type">INTERVAL</span> <span class="string">&#x27;5&#x27;</span> <span class="keyword">MINUTE</span>) <span class="keyword">AS</span> window_end, </span><br><span class="line">  <span class="comment">--滚动窗口的结束时间: </span></span><br><span class="line">  <span class="comment">-- ① 可以时间不同吗?</span></span><br><span class="line">  <span class="comment">-- ② 窗口可以别名吗?</span></span><br><span class="line">  <span class="built_in">COUNT</span>(<span class="operator">*</span>) <span class="keyword">AS</span> cnt </span><br><span class="line"><span class="keyword">FROM</span> Rides </span><br><span class="line"><span class="keyword">WHERE</span> isInNYC(lon, lat) <span class="keyword">and</span> isStart</span><br><span class="line"><span class="comment">-- isStart 为boolean</span></span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> </span><br><span class="line">  toAreaId(lon, lat), </span><br><span class="line">  TUMBLE(rideTime, <span class="type">INTERVAL</span> <span class="string">&#x27;5&#x27;</span> <span class="keyword">MINUTE</span>) </span><br><span class="line">  <span class="comment">-- 定义窗口</span></span><br><span class="line"><span class="keyword">HAVING</span> <span class="built_in">COUNT</span>(<span class="operator">*</span>) <span class="operator">&gt;=</span> <span class="number">5</span>;</span><br><span class="line"><span class="comment">-- 5分钟超过5次才输出</span></span><br></pre></td></tr></table></figure>
<h3 id="Window-Aggregate-与-Group-Aggregate-的区别"><a href="#Window-Aggregate-与-Group-Aggregate-的区别" class="headerlink" title="Window Aggregate 与 Group Aggregate 的区别"></a>Window Aggregate 与 Group Aggregate 的区别</h3><p>Window Aggregate 是当window结束时才输出，其输出的结果是最终值，不会再进行修改，其输出流是一个 Append 流。而 Group Aggregate 是每处理一条数据，就输出最新的结果，其结果是在不断更新的，就好像数据库中的数据一样，其输出流是一个 Update 流。</p>
<p>window 由于有 watermark ，可以精确知道哪些窗口已经过期了，所以可以及时清理过期状态，保证状态维持在稳定的大小。而 Group Aggregate 因为不知道哪些数据是过期的，所以状态会无限增长，这对于生产作业来说不是很稳定，所以建议对 Group Aggregate 的作业配上 State TTL 的配置。</p>
<p><img src="https://gitee.com/averyzhang/pic-go/raw/master/img/20210427094359.png"></p>
<p>例如统计每个店铺每天的实时PV，那么就可以将 TTL 配置成 24+ 小时，因为一天前的状态一般来说就用不到了。</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span>  DATE_FORMAT(ts, <span class="string">&#x27;yyyy-MM-dd&#x27;</span>), shop_id, <span class="built_in">COUNT</span>(<span class="operator">*</span>) <span class="keyword">as</span> pv</span><br><span class="line"><span class="keyword">FROM</span> T</span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> DATE_FORMAT(ts, <span class="string">&#x27;yyyy-MM-dd&#x27;</span>), shop_id</span><br></pre></td></tr></table></figure>
<p>当然，如果 TTL 配置地太小，可能会清除掉一些有用的状态和数据，从而导致数据精确性地问题。这也是用户需要权衡地一个参数。</p>
<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="[参考文献]"></a>[参考文献]</h2><ol>
<li><p><a target="_blank" rel="noopener" href="http://www.10tiao.com/html/157/201707/2653162664/1.html">在数据流中使用SQL查询：Apache Flink中的动态表的持续查询</a></p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://jiamaoxiang.top/2020/05/25/Flink-Table-API-SQL%E7%BC%96%E7%A8%8B%E6%8C%87%E5%8D%97/">Flink Table API &amp; SQL编程指南(1)</a></p>
</li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://example.com/bigdata/Spark/Spark-%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="aaronzhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Guadazi">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/bigdata/Spark/Spark-%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5/" class="post-title-link" itemprop="url">Spark开发集锦</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2019-08-10 14:58:00" itemprop="dateCreated datePublished" datetime="2019-08-10T14:58:00+08:00">2019-08-10</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-06-01 20:52:40" itemprop="dateModified" datetime="2021-06-01T20:52:40+08:00">2021-06-01</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/bigdata/" itemprop="url" rel="index"><span itemprop="name">bigdata</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="Spark"><a href="#Spark" class="headerlink" title="Spark"></a>Spark</h1><p><a target="_blank" rel="noopener" href="https://spark-reference-doc-cn.readthedocs.io/zh_CN/latest/programming-guide/rdd-guide.html">Spark 2.2.x中文文档</a><br>spark两个重要概念</p>
<ul>
<li>RDD</li>
<li>共享变量</li>
</ul>
<h2 id="共享变量"><a href="#共享变量" class="headerlink" title="共享变量"></a>共享变量</h2><h3 id="广播变量"><a href="#广播变量" class="headerlink" title="广播变量"></a>广播变量</h3><h3 id="累加器"><a href="#累加器" class="headerlink" title="累加器"></a>累加器</h3><h2 id="dev"><a href="#dev" class="headerlink" title="dev"></a>dev</h2><h3 id="依赖"><a href="#依赖" class="headerlink" title="依赖"></a>依赖</h3><p>spark core依赖</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-core_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.3.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>hdfs client</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hadoop-hdfs-client<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.2.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">scope</span>&gt;</span>provided<span class="tag">&lt;/<span class="name">scope</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>导包</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkContext</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkConf</span></span><br></pre></td></tr></table></figure>
<h3 id="初始化"><a href="#初始化" class="headerlink" title="初始化"></a>初始化</h3><p>每个JVM进程中，只能有一个活跃（active）的 SparkContext 对象。如果你非要再新建一个，那首先必须将之前那个活跃的 SparkContext 对象stop()掉。</p>
<blockquote>
<p>如何保证SparkContext是单例</p>
</blockquote>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(appName).setMaster(master)</span><br><span class="line"><span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spark-shell –help 可以查看完整的选项列表</span><br></pre></td></tr></table></figure>
<h2 id="RDD-弹性分布式数据集"><a href="#RDD-弹性分布式数据集" class="headerlink" title="RDD: 弹性分布式数据集"></a>RDD: 弹性分布式数据集</h2><p>可容错、可并行操作的分布式元素集合</p>
<p>有两种方法可以创建 RDD 对象：由驱动程序中的集合对象通过并行化操作创建，或者从外部存储系统中数据集加载（如：共享文件系统、HDFS、HBase或者其他Hadoop支持的数据源）。</p>
<h3 id="并行集合"><a href="#并行集合" class="headerlink" title="并行集合"></a>并行集合</h3><h2 id="Spark-SQL-partition个数与宽窄依赖"><a href="#Spark-SQL-partition个数与宽窄依赖" class="headerlink" title="Spark SQL partition个数与宽窄依赖"></a>Spark SQL partition个数与宽窄依赖</h2><h3 id="groupByKey"><a href="#groupByKey" class="headerlink" title="groupByKey"></a>groupByKey</h3><h4 id="默认值"><a href="#默认值" class="headerlink" title="默认值"></a>默认值</h4><ol>
<li>不指定partition大小</li>
</ol>
<p>默认的 Spark SQL 会使用 <code>spark.sql.shuffle.partitions</code> 的数量来进行 <code>aggregation</code> 和 <code>join</code>，默认值为 200。这会导致 partition 膨胀的问题，200个 partition 都需要执行，无论大小，尽管有些 partition 是没有数据的。</p>
<ol start="2">
<li>通过repartition设定大小后，再groupByKey</li>
</ol>
<p>仍然是200</p>
<ol start="3">
<li><p>Using repartition Operator With Explicit Number of Partitions</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">repartition(numPartitions: <span class="type">Int</span>, partitionExprs: <span class="type">Column</span>*): <span class="type">Dataset</span>[<span class="type">T</span>]</span><br></pre></td></tr></table></figure>
<p>指定partitionExprs，则可以实现指定的分区数</p>
</li>
</ol>
<h2 id="RDD"><a href="#RDD" class="headerlink" title="RDD"></a>RDD</h2><p>rdd的几个基本方法:</p>
<p>getPartitions()</p>
<p>compute()</p>
<h2 id="Hadoop-split-Hadoop分片"><a href="#Hadoop-split-Hadoop分片" class="headerlink" title="Hadoop split: Hadoop分片"></a>Hadoop split: Hadoop分片</h2><p>hdfs dfs blockSize</p>
<p>hdfs-site.xml中修改dfs.blockSize, spark 2.7.x 默认值为128M</p>
<p><img src="_v_images/20200728185752151_2000769439.png"></p>
<h2 id="spark-3-0-读取-orcfile"><a href="#spark-3-0-读取-orcfile" class="headerlink" title="spark 3.0 读取 orcfile"></a>spark 3.0 读取 orcfile</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> path=<span class="string">&quot;/data/sample.orc&quot;</span></span><br><span class="line"><span class="keyword">val</span> orcRdd: <span class="type">RDD</span>[(<span class="type">NullWritable</span>, <span class="type">OrcStruct</span>)] =</span><br><span class="line">  sc.hadoopFile(</span><br><span class="line">    path,</span><br><span class="line">    classOf[<span class="type">OrcInputFormat</span>],</span><br><span class="line">    classOf[<span class="type">NullWritable</span>],</span><br><span class="line">    classOf[<span class="type">OrcStruct</span>],</span><br><span class="line">    <span class="number">10</span>)</span><br><span class="line"><span class="keyword">val</span> result= orcRdd.map((line: (<span class="type">NullWritable</span>, <span class="type">OrcStruct</span>)) =&gt; &#123;</span><br><span class="line">  line._2.getNumFields</span><br><span class="line">&#125;).collect()</span><br></pre></td></tr></table></figure>



<h2 id="TODO"><a href="#TODO" class="headerlink" title="TODO"></a>TODO</h2><p>toDebugString</p>
<p>参考文献:</p>
<ol>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/52328286">Number of Partitions for groupBy Aggregation</a></li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://example.com/bigdata/Spark/Spark-usage/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="aaronzhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Guadazi">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/bigdata/Spark/Spark-usage/" class="post-title-link" itemprop="url">Spark 使用</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2019-08-10 14:58:00" itemprop="dateCreated datePublished" datetime="2019-08-10T14:58:00+08:00">2019-08-10</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-06-01 20:52:40" itemprop="dateModified" datetime="2021-06-01T20:52:40+08:00">2021-06-01</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/bigdata/" itemprop="url" rel="index"><span itemprop="name">bigdata</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="Spark-使用"><a href="#Spark-使用" class="headerlink" title="Spark 使用"></a>Spark 使用</h1><h2 id="copy-file"><a href="#copy-file" class="headerlink" title="copy file"></a>copy file</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.&#123;<span class="type">FileAlreadyExistsException</span>, <span class="type">FileSystem</span>, <span class="type">FileUtil</span>, <span class="type">Path</span>&#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> srcFileSystem: <span class="type">FileSystem</span> = <span class="type">FileSystemUtil</span></span><br><span class="line">      .apply(spark.sparkContext.hadoopConfiguration)</span><br><span class="line">      .getFileSystem(sourceFile)</span><br><span class="line">    <span class="keyword">val</span> dstFileSystem: <span class="type">FileSystem</span> = <span class="type">FileSystemUtil</span></span><br><span class="line">      .apply(spark.sparkContext.hadoopConfiguration)</span><br><span class="line">      .getFileSystem(sourceFile)</span><br><span class="line"></span><br><span class="line">    <span class="type">FileUtil</span>.copy(</span><br><span class="line">      srcFileSystem,</span><br><span class="line">      <span class="keyword">new</span> <span class="type">Path</span>(<span class="keyword">new</span> <span class="type">URI</span>(sourceFile)),</span><br><span class="line">      dstFileSystem,</span><br><span class="line">      <span class="keyword">new</span> <span class="type">Path</span>(<span class="keyword">new</span> <span class="type">URI</span>(targetFile)),</span><br><span class="line">      <span class="literal">true</span>,</span><br><span class="line">      spark.sparkContext.hadoopConfiguration)</span><br></pre></td></tr></table></figure>
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://example.com/bigdata/kudu/kudu/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="aaronzhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Guadazi">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/bigdata/kudu/kudu/" class="post-title-link" itemprop="url">kudu</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2019-08-10 14:58:00" itemprop="dateCreated datePublished" datetime="2019-08-10T14:58:00+08:00">2019-08-10</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-06-01 20:52:39" itemprop="dateModified" datetime="2021-06-01T20:52:39+08:00">2021-06-01</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/bigdata/" itemprop="url" rel="index"><span itemprop="name">bigdata</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="kudu"><a href="#kudu" class="headerlink" title="kudu"></a>kudu</h1><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">ALTER</span> <span class="keyword">TABLE</span> `db_name`.`table_name`</span><br><span class="line">        ADD RANGE PARTITION VALUE = $&#123;partition&#125;</span><br></pre></td></tr></table></figure>
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://example.com/Java/metric/01_tuner/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="aaronzhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Guadazi">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/Java/metric/01_tuner/" class="post-title-link" itemprop="url">Java性能调优</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2019-08-10 14:58:00" itemprop="dateCreated datePublished" datetime="2019-08-10T14:58:00+08:00">2019-08-10</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-06-01 20:52:39" itemprop="dateModified" datetime="2021-06-01T20:52:39+08:00">2021-06-01</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/java/" itemprop="url" rel="index"><span itemprop="name">java</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="调优"><a href="#调优" class="headerlink" title="调优"></a>调优</h1><h2 id="TODO"><a href="#TODO" class="headerlink" title="TODO"></a>TODO</h2><p>jhat<br>vmstat</p>
<h2 id="测试样例"><a href="#测试样例" class="headerlink" title="测试样例"></a>测试样例</h2><p>新docker container，全新的系统，以纯净的系统Centos为例</p>
<h3 id="安装java"><a href="#安装java" class="headerlink" title="安装java"></a>安装java</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 安装java8</span></span><br><span class="line">[root@centos ~]# yum install -y java-1.8.0-openjdk*</span><br><span class="line"><span class="meta">#</span><span class="bash"> 创建deploy用户，用于折腾</span></span><br><span class="line">[root@centos ~]# useradd deploy</span><br><span class="line">[root@centos ~]# su deploy</span><br><span class="line">[root@centos ~]# cd</span><br></pre></td></tr></table></figure>


<h3 id="创建工作目录"><a href="#创建工作目录" class="headerlink" title="创建工作目录"></a>创建工作目录</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 创建并切换到工作目录</span></span><br><span class="line">[deploy@centos ~]$ mkdir -p lk-optimization</span><br><span class="line">[deploy@centos ~]$ cd !$</span><br></pre></td></tr></table></figure>
<p>创建package，用于存放java源文件和编译后的class文件</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[deploy@centos ~]$ mkdir -p src/com/lk/optimization/demo target</span><br><span class="line">[deploy@centos ~]$ vim !$/DemoTest.java</span><br></pre></td></tr></table></figure>
<h3 id="新建Java文件"><a href="#新建Java文件" class="headerlink" title="新建Java文件"></a>新建Java文件</h3><p>//        Thread thread = Thread.currentThread();</p>
<p>//        System.out.println(thread.getName());<br>//        thread.join();<br>        System.out.println(“join over”);</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.lk.optimization.demo;</span><br><span class="line"><span class="keyword">import</span> java.util.List;</span><br><span class="line"><span class="keyword">import</span> java.util.ArrayList;</span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">DemoTest</span> </span>&#123;</span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] atgs)</span> <span class="keyword">throws</span> InterruptedException </span>&#123;</span><br><span class="line">		<span class="keyword">new</span> Handler().doHandle();</span><br><span class="line">		Runtime.getRuntime().addShutdownHook(<span class="keyword">new</span> Thread() &#123;</span><br><span class="line">				<span class="meta">@Override</span></span><br><span class="line">				<span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>&#123;</span><br><span class="line">				System.out.println(<span class="keyword">this</span>.getName() + <span class="string">&quot; is shutting down ....&quot;</span>);</span><br><span class="line">				&#125;</span><br><span class="line">				&#125;);</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Handler</span> </span>&#123;</span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">doHandle</span><span class="params">()</span> </span>&#123;</span><br><span class="line">		<span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">10</span>; i++) &#123;</span><br><span class="line">			String workerName=<span class="string">&quot;worker-&quot;</span>+(i+<span class="number">1</span>);</span><br><span class="line">			<span class="keyword">new</span> Thread(<span class="keyword">new</span> Worker(workerName), <span class="string">&quot;handler-&quot;</span> + i).start();</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Worker</span> <span class="keyword">implements</span> <span class="title">Runnable</span> </span>&#123;</span><br><span class="line">	<span class="keyword">private</span> String workerName;</span><br><span class="line">	<span class="keyword">private</span> List&lt;String&gt; list;</span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="title">Worker</span><span class="params">(String workerName)</span></span>&#123;</span><br><span class="line">		<span class="keyword">this</span>.workerName=workerName;</span><br><span class="line">		<span class="keyword">this</span>.list=<span class="keyword">new</span> ArrayList&lt;&gt;(<span class="number">1000</span>);</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="meta">@Override</span></span><br><span class="line">		<span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>&#123;</span><br><span class="line">			System.out.println(workerName+<span class="string">&quot; start!&quot;</span>);</span><br><span class="line">			<span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line">				<span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;<span class="number">100</span>;i++)&#123;</span><br><span class="line">					list.add(<span class="string">&quot;&quot;</span>+i);</span><br><span class="line">				&#125;</span><br><span class="line">				<span class="keyword">if</span>(list.size()&gt;<span class="number">1000</span>)&#123;</span><br><span class="line">					list.clear();</span><br><span class="line">				&#125;</span><br><span class="line">				<span class="keyword">try</span> &#123;</span><br><span class="line">					Thread.sleep(<span class="number">100</span>);</span><br><span class="line">					System.out.println(workerName+<span class="string">&quot;: sleep over&quot;</span>);</span><br><span class="line">				&#125; <span class="keyword">catch</span> (InterruptedException e) &#123;</span><br><span class="line">					e.printStackTrace();</span><br><span class="line">				&#125;</span><br><span class="line">			&#125;</span><br><span class="line">		&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="编译"><a href="#编译" class="headerlink" title="编译"></a>编译</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">[deploy@centos ~]$ find src -name &quot;*.java&quot; | xargs javac -d target</span><br><span class="line"></span><br><span class="line">[deploy@centos ~]$ tree</span><br><span class="line">.</span><br><span class="line">├── src</span><br><span class="line">│   └── com</span><br><span class="line">│       └── lk</span><br><span class="line">│           └── optimization</span><br><span class="line">│               └── demo</span><br><span class="line">│                   └── DemoTest.java</span><br><span class="line">└── target</span><br><span class="line">    └── com</span><br><span class="line">        └── lk</span><br><span class="line">            └── optimization</span><br><span class="line">                └── demo</span><br><span class="line">                    ├── DemoTest$1.class</span><br><span class="line">                    ├── DemoTest.class</span><br><span class="line">                    ├── Handler.class</span><br><span class="line">                    └── Worker.class</span><br><span class="line"></span><br><span class="line">10 directories, 5 files</span><br></pre></td></tr></table></figure>
<h3 id="运行"><a href="#运行" class="headerlink" title="运行"></a>运行</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[deploy@centos ~]$ nohup java -cp target  -Xcomp -XX:MaxNewSize=1000000  -XX:MaxHeapSize=2000000 com.lk.optimization.demo.DemoTest &amp;</span><br><span class="line">main</span><br></pre></td></tr></table></figure>


<h2 id="ps-查看进程和线程信息"><a href="#ps-查看进程和线程信息" class="headerlink" title="ps 查看进程和线程信息"></a>ps 查看进程和线程信息</h2><p>ps命令选项:</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">********* simple selection *********  ********* selection by list *********</span><br><span class="line">-A all processes                      -C by command name</span><br><span class="line">-N negate selection                   -G by real group ID (supports names)</span><br><span class="line">-a all w/ tty except session leaders  -U by real user ID (supports names)</span><br><span class="line">-d all except session leaders         -g by session OR by effective group name</span><br><span class="line">-e all processes                      -p by process ID</span><br><span class="line">T  all processes on this terminal     -s processes in the sessions given</span><br><span class="line">a  all w/ tty, including other users  -t by tty</span><br><span class="line">g  OBSOLETE -- DO NOT USE             -u by effective user ID (supports names)</span><br><span class="line">r  only running processes             U  processes for specified users</span><br><span class="line">x  processes w/o controlling ttys     t  by tty</span><br><span class="line">*********** output format **********  *********** long options ***********</span><br><span class="line">-o,o user-defined  -f full            --Group --User --pid --cols --ppid</span><br><span class="line">-j,j job control   s  signal          --group --user --sid --rows --info</span><br><span class="line">-O,O preloaded -o  v  virtual memory  --cumulative --format --deselect</span><br><span class="line">-l,l long          u  user-oriented   --sort --tty --forest --version</span><br><span class="line">-F   extra full    X  registers       --heading --no-heading --context</span><br><span class="line">                         ********* misc options *********</span><br><span class="line">-V,V  show version      L  list format codes  f  ASCII art forest</span><br><span class="line">-m,m,-L,-T,H  threads   S  children in sum    -y change -l format</span><br><span class="line">-M,Z  security data     c  true command name  -c scheduling class</span><br><span class="line">-w,w  wide output       n  numeric WCHAN,UID  -H process hierarchy</span><br></pre></td></tr></table></figure>


<p>root用户查看进程信息</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">[root@centos ~]# ps -ef  | grep -v ps</span><br><span class="line">UID        PID  PPID  C STIME TTY          TIME CMD</span><br><span class="line">root         1     0  0 13:16 ?        00:00:00 /usr/sbin/sshd -D</span><br><span class="line">root        14     0  0 13:23 pts/0    00:00:00 /bin/bash</span><br><span class="line">root       421     1  0 13:58 ?        00:00:00 sshd: root@pts/1</span><br><span class="line">root       423   421  0 13:58 pts/1    00:00:00 -bash</span><br><span class="line">root       461     1  0 14:15 ?        00:00:00 sshd: root@pts/2</span><br><span class="line">root       463   461  0 14:15 pts/2    00:00:00 -bash</span><br><span class="line">root       519     1  0 14:16 ?        00:00:00 sshd: root@pts/3</span><br><span class="line">root       521   519  0 14:16 pts/3    00:00:00 -bash</span><br><span class="line">root       785   423  0 14:59 pts/1    00:00:00 su deploy</span><br><span class="line">deploy     786   785  0 14:59 pts/1    00:00:00 bash</span><br><span class="line">deploy    1129   786  0 15:32 pts/1    00:00:02 java -cp target com.lk.optimization.demo.DemoTest</span><br></pre></td></tr></table></figure>
<p>ps 命令</p>
<ul>
<li>-e 显示所有进程信息</li>
<li>-f 显示所有的字段</li>
<li>-l long format</li>
<li>-L 显示NLWP (number of threads) and LWP (thread ID) 显示线程信息</li>
<li>-p 只显示指定的PID</li>
</ul>
<p>字段的解释可以在man页的<code>STANDARD FORMAT SPECIFIERS</code>中查找</p>
<p>默认情况下，ps显示与当前用户相同EUID以及调用了同一个终端的进程。</p>
<ul>
<li>PID是进程ID</li>
<li>TTY为与进程关联的终端名称</li>
<li>TIME为以<code>[dd-]hh:mm:ss</code>格式显示的CPU时间</li>
<li>CMD为执行的名称, 默认不排序</li>
<li>PPID为父进程ID，根据ID可以找到进程的调用关系</li>
</ul>
<p>样例中的进程信息:</p>
<table>
<thead>
<tr>
<th>进程id</th>
<th>解释</th>
</tr>
</thead>
<tbody><tr>
<td>0</td>
<td>0号进程是根进程</td>
</tr>
<tr>
<td>1</td>
<td>通过sshd发起的ssh连接</td>
</tr>
<tr>
<td>421</td>
<td>ssh连接，用户为root，终端为pts/1，子进程全部这个终端</td>
</tr>
<tr>
<td>423</td>
<td>bash shell</td>
</tr>
<tr>
<td>785</td>
<td>切换deploy账户</td>
</tr>
<tr>
<td>786</td>
<td>deploy账户的bash shell</td>
</tr>
<tr>
<td>1129</td>
<td>java启动进程</td>
</tr>
</tbody></table>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">[root@centos ~]# ps -flL -p 1129</span><br><span class="line">F S UID        PID  PPID   LWP  C NLWP PRI  NI ADDR SZ WCHAN  STIME TTY          TIME CMD</span><br><span class="line">0 S deploy    1129   786  1129  0   27  80   0 - 934507 -     15:32 pts/1    00:00:00 java -cp target com.lk.optimization.demo.DemoTest</span><br><span class="line">5 S deploy    1129   786  1130  0   27  80   0 - 934507 -     15:32 pts/1    00:00:00 java -cp target com.lk.optimization.demo.DemoTest</span><br><span class="line">1 S deploy    1129   786  1131  0   27  80   0 - 934507 -     15:32 pts/1    00:00:00 java -cp target com.lk.optimization.demo.DemoTest</span><br><span class="line">1 S deploy    1129   786  1132  0   27  80   0 - 934507 -     15:32 pts/1    00:00:00 java -cp target com.lk.optimization.demo.DemoTest</span><br><span class="line">1 S deploy    1129   786  1133  0   27  80   0 - 934507 -     15:32 pts/1    00:00:00 java -cp target com.lk.optimization.demo.DemoTest</span><br><span class="line">1 S deploy    1129   786  1134  0   27  80   0 - 934507 -     15:32 pts/1    00:00:00 java -cp target com.lk.optimization.demo.DemoTest</span><br><span class="line">1 S deploy    1129   786  1135  0   27  80   0 - 934507 -     15:32 pts/1    00:00:00 java -cp target com.lk.optimization.demo.DemoTest</span><br><span class="line">1 S deploy    1129   786  1136  0   27  80   0 - 934507 -     15:32 pts/1    00:00:00 java -cp target com.lk.optimization.demo.DemoTest</span><br><span class="line">1 S deploy    1129   786  1137  0   27  80   0 - 934507 -     15:32 pts/1    00:00:00 java -cp target com.lk.optimization.demo.DemoTest</span><br><span class="line">1 S deploy    1129   786  1138  0   27  80   0 - 934507 -     15:32 pts/1    00:00:00 java -cp target com.lk.optimization.demo.DemoTest</span><br><span class="line">1 S deploy    1129   786  1139  0   27  80   0 - 934507 -     15:32 pts/1    00:00:00 java -cp target com.lk.optimization.demo.DemoTest</span><br><span class="line">1 S deploy    1129   786  1140  0   27  80   0 - 934507 -     15:32 pts/1    00:00:00 java -cp target com.lk.optimization.demo.DemoTest</span><br></pre></td></tr></table></figure>
<h3 id="Process-Format"><a href="#Process-Format" class="headerlink" title="Process Format"></a>Process Format</h3><ol>
<li>F PROCESS FLAGS   1 forked但没有执行 5超级用户权限</li>
<li>S PROCESS STATE CODES<pre><code>D    Uninterruptible sleep (usually IO)
R    Running or runnable (on run queue)
S    Interruptible sleep (waiting for an event to complete)
T    Stopped, either by a job control signal or because it is being traced.
W    paging (not valid since the 2.6.xx kernel)
X    dead (should never be seen)
Z    Defunct (&quot;zombie&quot;) process, terminated but not reaped by its parent.</code></pre>
</li>
<li>lwp (light weight process, or thread) ID 线程ID</li>
<li>C  processor utilization. Currently, this is the integer value of the percent usage over the lifetime of the process.</li>
<li>NLWP 轻量级进程的个数</li>
<li>ni         NI       nice value. This ranges from 19 (nicest) to -20 (not nice to others)</li>
<li>size       SZ       approximate amount of swap space that would be required if the process were to dirty all writable pages and then be swapped out. This number is very rough!</li>
<li>wchan      WCHAN    name of the kernel function in which the process is sleeping, a “-“ if the process is running, or a “*” if the process is multi-threaded and ps is not displaying threads.</li>
</ol>
<h3 id="线程信息"><a href="#线程信息" class="headerlink" title="线程信息"></a>线程信息</h3><p>线程线程的选项:</p>
<ul>
<li>H     Show threads as if they were processes 显示正在处理的线程</li>
<li>-L     Show threads, possibly with LWP and NLWP columns 显示线程</li>
<li>-T     Show threads, possibly with SPID column 显示线程</li>
<li>m     Show threads after processes</li>
<li>-m    Show threads after processes</li>
</ul>
<p>通过上面的例子，可以看出</p>
<p>线程id是1129~1154, 共27个线程<br>实际handler线程和main线程加起来是11个，为什么是27个线程呢</p>
<h2 id="top-查看资源"><a href="#top-查看资源" class="headerlink" title="top 查看资源"></a>top 查看资源</h2><p>以线程模式查看下进程31951的所有线程情况</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">top -Hp 31951</span><br></pre></td></tr></table></figure>
<p>![top-thread](images/20190728100827299_123372753.png =634x)</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">-H : Threads toggle</span><br><span class="line">            Starts  top with the last remembered ’H’ state reversed.  When this toggle is On, all individual threads will be displayed.  Otherwise, top displays a summation of all threads in a process. 当此开关打开时，将显示所有单个线程。否则，top显示进程中所有线程的总和。</span><br><span class="line"></span><br><span class="line">-p : Monitor PIDs as:  -pN1 -pN2 ...  or  -pN1, N2 [,...]</span><br><span class="line">            Monitor only processes with specified process IDs.  This option can be given up to 20 times, or you can provide a comma delimited list with up  to  20 pids.  Co-mingling both approaches is permitted. This  is  a  command-line  option  only.  And should you wish to return to normal operation, it is not necessary to quit and and restart top  --  just issue the ’&#x3D;’ interactive command.</span><br><span class="line">            打开top后，&#x3D; 可以切换回全部进程</span><br></pre></td></tr></table></figure>


<h2 id="jstack-Java线程栈信息"><a href="#jstack-Java线程栈信息" class="headerlink" title="jstack Java线程栈信息"></a>jstack Java线程栈信息</h2><p>jstack中的线程id为16进制，需要将从top或ps获取的线程id转换为16进制,</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">printf %x &lt;tid&gt;</span><br></pre></td></tr></table></figure>
<blockquote>
<p>注意:</p>
<ol>
<li>jstack必须和运行的JVM进程是同一个用户</li>
</ol>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Options:</span><br><span class="line">    -F  to force a thread dump. Use when jstack &lt;pid&gt; does not respond (process is hung)</span><br><span class="line">    -m  to print both java and native frames (mixed mode)</span><br><span class="line">    -l  long listing. Prints additional information about locks</span><br><span class="line">    -h or -help to print this help message</span><br></pre></td></tr></table></figure>
<h2 id="虚拟机信息flag"><a href="#虚拟机信息flag" class="headerlink" title="虚拟机信息flag"></a>虚拟机信息flag</h2><p>打印虚拟机所有的参数</p>
<p><a target="_blank" rel="noopener" href="https://docs.oracle.com/javase/8/docs/technotes/tools/unix/java.html">java 命令文档</a></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">java -XX:+PrintFlagsFinal -version | grep :</span><br><span class="line">java -XX:+PrintFlagsInitial -version</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">java -XX:+PrintFlagsFinal -version | grep -v :&#x3D;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">The -XX:+PrintFlagsFinal (emphasis on &quot;Final&quot;) option displays what options HotSpot ended up using for running Java code while </span><br><span class="line">-XX:+PrintFlagsInitial (emphasis on &quot;Initial&quot;) displays what options were provided to HotSpot initially, before HotSpot has made its own tweaks.</span><br></pre></td></tr></table></figure>

<h2 id="jinfo"><a href="#jinfo" class="headerlink" title="jinfo"></a>jinfo</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">jinfo -flags <span class="variable">$PID</span></span><br><span class="line"></span><br><span class="line">Attaching to process ID 29893, please <span class="built_in">wait</span>...</span><br><span class="line">Debugger attached successfully.</span><br><span class="line">Server compiler detected.  <span class="comment"># 服务器版本的编译器</span></span><br><span class="line">JVM version is 25.232-b09 <span class="comment"># jvm版本</span></span><br><span class="line">Non-default VM flags:</span><br><span class="line">-XX:CICompilerCount=2</span><br><span class="line">-XX:InitialHeapSize=33554432 <span class="comment"># 初始堆大小 33M</span></span><br><span class="line">-XX:MaxHeapSize=524288000 <span class="comment"># 最大堆大小 524M</span></span><br><span class="line">-XX:MaxNewSize=174718976  <span class="comment">#年轻代最大大小 174M</span></span><br><span class="line">-XX:MinHeapDeltaBytes=196608 <span class="comment">#堆自动增加步长最小196K</span></span><br><span class="line">-XX:NewSize=11141120  <span class="comment">#年轻代大小11M</span></span><br><span class="line">-XX:OldSize=22413312 <span class="comment">#老年代大小22M</span></span><br><span class="line">-XX:+UseCompressedClassPointers  <span class="comment">#压缩类指针</span></span><br><span class="line">-XX:+UseCompressedOops <span class="comment">#压缩</span></span><br><span class="line">-XX:+UseParallelGC <span class="comment"># 使用并行回收器</span></span><br><span class="line"></span><br><span class="line">Command line:</span><br></pre></td></tr></table></figure>
<h2 id="jstat查看jvm统计信息和垃圾回收信息"><a href="#jstat查看jvm统计信息和垃圾回收信息" class="headerlink" title="jstat查看jvm统计信息和垃圾回收信息"></a>jstat查看jvm统计信息和垃圾回收信息</h2><p>JVM statistics</p>
<p><a target="_blank" rel="noopener" href="https://docs.oracle.com/javase/8/docs/technotes/tools/windows/jstat.html#BEHHGFAE">jstat docs</a></p>
<p><a target="_blank" rel="noopener" href="https://app.yinxiang.com/shard/s30/nl/6747836/1db66cb0-6a66-4766-84ba-5d5b51df1fd2">jstat 命令使用样例</a></p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">➜ jstat -options</span><br><span class="line">-<span class="class"><span class="keyword">class</span> # <span class="title">classLoader</span>行为的统计信息</span></span><br><span class="line"><span class="class">-<span class="title">compiler</span> #  <span class="title">Java</span> <span class="title">HotSpot</span> <span class="title">VM</span> <span class="title">Just</span>-<span class="title">in</span>-<span class="title">Time</span> <span class="title">compiler</span>行为的统计信息</span></span><br><span class="line"><span class="class">-<span class="title">gc</span> # 垃圾回收堆的统计信息</span></span><br><span class="line"><span class="class">-<span class="title">gccapacity</span> # 每代的大小和容量</span></span><br><span class="line"><span class="class">-<span class="title">gccause</span> # 垃圾收集统计及回收原因</span></span><br><span class="line"><span class="class">-<span class="title">gcmetacapacity</span></span></span><br><span class="line"><span class="class">-<span class="title">gcnew</span></span></span><br><span class="line"><span class="class">-<span class="title">gcnewcapacity</span></span></span><br><span class="line"><span class="class">-<span class="title">gcold</span></span></span><br><span class="line"><span class="class">-<span class="title">gcoldcapacity</span></span></span><br><span class="line"><span class="class">-<span class="title">gcutil</span> # <span class="title">gc</span>统计信息汇总，展示<span class="title">gc</span>次数、耗时和各个分区的大小</span></span><br><span class="line"><span class="class">-<span class="title">printcompilation</span></span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"></span></span><br><span class="line">class: Displays statistics about the behavior of the class loader.</span><br><span class="line">compiler: Displays statistics about the behavior of the Java HotSpot VM Just-in-Time compiler.</span><br><span class="line">gc: Displays statistics about the behavior of the garbage collected heap.</span><br><span class="line">gccapacity: Displays statistics about the capacities of the generations and their corresponding spaces.</span><br><span class="line">gccause: <span class="function">Displays a summary about garbage collection <span class="title">statistics</span> <span class="params">(same as -gcutil)</span>, with the cause of the last and <span class="title">current</span> <span class="params">(when applicable)</span> garbage collection events.</span></span><br><span class="line"><span class="function">gcnew: Displays statistics of the behavior of the new generation.</span></span><br><span class="line"><span class="function">gcnewcapacity: Displays statistics about the sizes of the new generations and its corresponding spaces.</span></span><br><span class="line"><span class="function">gcold: Displays statistics about the behavior of the old generation and metaspace statistics.</span></span><br><span class="line"><span class="function">gcoldcapacity: Displays statistics about the sizes of the old generation.</span></span><br><span class="line"><span class="function">gcmetacapacity: Displays statistics about the sizes of the metaspace.</span></span><br><span class="line"><span class="function">gcutil: Displays a summary about garbage collection statistics.</span></span><br><span class="line"><span class="function">printcompilation: Displays Java HotSpot VM compilation method statistics.</span></span><br></pre></td></tr></table></figure>
<p>GC堆统计</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">jstat -gc <span class="number">12196</span></span><br><span class="line"> S0C    S1C    S0U    S1U      EC       EU        OC         OU       MC     MU    CCSC   CCSU   YGC     YGCT    FGC    FGCT     GCT   </span><br><span class="line"><span class="number">512.0</span>  <span class="number">1024.0</span> <span class="number">496.0</span>   <span class="number">0.0</span>   <span class="number">67584.0</span>  <span class="number">38337.9</span>   <span class="number">437248.0</span>   <span class="number">75268.8</span>   <span class="number">83992.0</span> <span class="number">80411.0</span> <span class="number">9496.0</span> <span class="number">8854.4</span>    <span class="number">388</span>    <span class="number">5.643</span>   <span class="number">3</span>      <span class="number">0.390</span>    <span class="number">6.033</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">-gc option</span><br><span class="line">Garbage-collected heap statistics.</span><br><span class="line"></span><br><span class="line">S0C: Current survivor space 0 capacity (kB).   #s0区当前容量</span><br><span class="line">S1C: Current survivor space 1 capacity (kB). #s1区当前容量</span><br><span class="line">S0U: Survivor space 0 utilization (kB). #s0区使用量</span><br><span class="line">S1U: Survivor space 1 utilization (kB). #s1区使用量</span><br><span class="line">EC: <span class="function">Current eden space <span class="title">capacity</span> <span class="params">(kB)</span>. #eden区容量</span></span><br><span class="line"><span class="function">EU: Eden space <span class="title">utilization</span> <span class="params">(kB)</span>. #eden区使用量</span></span><br><span class="line"><span class="function">OC: Current old space <span class="title">capacity</span> <span class="params">(kB)</span>. #old区容量</span></span><br><span class="line"><span class="function">OU: Old space <span class="title">utilization</span> <span class="params">(kB)</span>. #old区使用量</span></span><br><span class="line"><span class="function">MC: Metaspace <span class="title">capacity</span> <span class="params">(kB)</span>. #metaspace容量</span></span><br><span class="line"><span class="function">MU: Metacspace <span class="title">utilization</span> <span class="params">(kB)</span>. #metaspace使用量</span></span><br><span class="line"><span class="function">CCSC: Compressed class space <span class="title">capacity</span> <span class="params">(kB)</span>. #压缩类空间容量</span></span><br><span class="line"><span class="function">CCSU: Compressed class space <span class="title">used</span> <span class="params">(kB)</span>. #压缩类空间使用量</span></span><br><span class="line"><span class="function">YGC: Number of young generation garbage collection events.  # YGC次数</span></span><br><span class="line"><span class="function">YGCT: Young generation garbage collection time. #YGC耗时</span></span><br><span class="line"><span class="function">FGC: Number of full GC events. # Full GC次数</span></span><br><span class="line"><span class="function">FGCT: Full garbage collection time. # Full GC耗时</span></span><br><span class="line"><span class="function">GCT: Total garbage collection time. # GC总耗时</span></span><br></pre></td></tr></table></figure>
<p>GC统计</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">jstat -gcutil <span class="number">12196</span></span><br><span class="line">  S0     S1     E      O      M     CCS    YGC     YGCT    FGC    FGCT     GCT</span><br><span class="line"> <span class="number">96.88</span>   <span class="number">0.00</span>  <span class="number">89.96</span>  <span class="number">17.21</span>  <span class="number">95.74</span>  <span class="number">93.24</span>    <span class="number">388</span>    <span class="number">5.643</span>     <span class="number">3</span>    <span class="number">0.390</span>    <span class="number">6.033</span></span><br></pre></td></tr></table></figure>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">-gcutil option</span><br><span class="line">Summary of garbage collection statistics.</span><br><span class="line"></span><br><span class="line">S0: Survivor space <span class="number">0</span> utilization as a percentage of the space<span class="string">&#x27;s current capacity. # s0区使用率</span></span><br><span class="line">S1: Survivor space 1 utilization as a percentage of the space&#x27;s current capacity. # S1区使用率</span><br><span class="line">E: Eden space utilization as a percentage of the space<span class="string">&#x27;s current capacity. # eden区使用率</span></span><br><span class="line">O: Old space utilization as a percentage of the space&#x27;s current capacity. # old区使用率</span><br><span class="line">M: Metaspace utilization as a percentage of the space<span class="string">&#x27;s current capacity. # Metaspace使用率</span></span><br><span class="line"><span class="string">CCS: Compressed class space utilization as a percentage. # 压缩类空间使用率</span></span><br><span class="line"><span class="string">YGC: Number of young generation GC events. # YGC次数</span></span><br><span class="line"><span class="string">YGCT: Young generation garbage collection time. # YGC耗时</span></span><br><span class="line"><span class="string">FGC: Number of full GC events. # Full GC次数</span></span><br><span class="line"><span class="string">FGCT: Full garbage collection time. # Full GC耗时</span></span><br><span class="line"><span class="string">GCT: Total garbage collection time. # GC总耗时</span></span><br></pre></td></tr></table></figure>

<h2 id="堆dump"><a href="#堆dump" class="headerlink" title="堆dump"></a>堆dump</h2><p>full gc前后dump</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">java -XX:HeapDumpPath=./heap/ -XX:+HeapDumpOnOutOfMemoryError -XX:+HeapDumpAfterFullGC</span><br></pre></td></tr></table></figure>
<p><code>HeapDumpPath</code>: dump path<br><code>HeapDumpOnOutOfMemoryError</code> 内存溢出dump<br><code>HeapDumpAfterFullGC</code>、<code>HeapDumpBeforeFullGC</code> ：Full GC前后dump</p>
<h3 id="jmap"><a href="#jmap" class="headerlink" title="jmap"></a>jmap</h3><p>Prints shared object memory maps or heap memory details for a process, core file, or remote debug server. This command is experimental and unsupported.<br>打印进程、核心文件或远程调试服务器的共享对象内存映射或堆内存详细信息</p>
<p><a target="_blank" rel="noopener" href="https://docs.oracle.com/javase/8/docs/technotes/tools/windows/jmap.html#BEHHGFAE">jmap docs</a></p>
<p>jmap -heap <pid></p>
<p>堆的各个分区大小</p>
<p><a target="_blank" rel="noopener" href="https://segmentfault.com/a/1190000010648021">堆内存分析-GC日志解读</a></p>
<p>展示堆满的情况下的各个分区大小</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line">&gt; jmap -heap <span class="number">16186</span></span><br><span class="line">Attaching to process ID <span class="number">16186</span>, please wait...</span><br><span class="line">Debugger attached successfully.</span><br><span class="line">Server compiler detected.</span><br><span class="line">JVM version is <span class="number">25.242</span>-b08</span><br><span class="line"></span><br><span class="line">using thread-local object allocation.</span><br><span class="line">Parallel GC with <span class="number">6</span> thread(s)</span><br><span class="line"></span><br><span class="line">Heap Configuration:</span><br><span class="line">   MinHeapFreeRatio         = <span class="number">0</span></span><br><span class="line">   MaxHeapFreeRatio         = <span class="number">100</span></span><br><span class="line">   MaxHeapSize              = <span class="number">522190848</span> (<span class="number">498.</span>0MB)</span><br><span class="line">   NewSize                  = <span class="number">11010048</span> (<span class="number">10.</span>5MB)</span><br><span class="line">   MaxNewSize               = <span class="number">174063616</span> (<span class="number">166.</span>0MB)</span><br><span class="line">   OldSize                  = <span class="number">22544384</span> (<span class="number">21.</span>5MB)</span><br><span class="line">   NewRatio                 = <span class="number">2</span></span><br><span class="line">   SurvivorRatio            = <span class="number">8</span></span><br><span class="line">   MetaspaceSize            = <span class="number">21807104</span> (<span class="number">20.</span>796875MB)</span><br><span class="line">   CompressedClassSpaceSize = <span class="number">1073741824</span> (<span class="number">1024.</span>0MB)</span><br><span class="line">   MaxMetaspaceSize         = <span class="number">17592186044415</span> MB</span><br><span class="line">   G1HeapRegionSize         = <span class="number">0</span> (<span class="number">0.</span>0MB)</span><br><span class="line"></span><br><span class="line">Heap Usage:</span><br><span class="line">PS Young Generation</span><br><span class="line">Eden Space:</span><br><span class="line">   capacity = <span class="number">58720256</span> (<span class="number">56.</span>0MB)</span><br><span class="line">   used     = <span class="number">49193976</span> (<span class="number">46.</span>91503143310547MB)</span><br><span class="line">   free     = <span class="number">9526280</span> (<span class="number">9.</span>084968566894531MB)</span><br><span class="line">   <span class="number">83.7768418448312</span>% used</span><br><span class="line">From Space:</span><br><span class="line">   capacity = <span class="number">57147392</span> (<span class="number">54.</span>5MB)</span><br><span class="line">   used     = <span class="number">0</span> (<span class="number">0.</span>0MB)</span><br><span class="line">   free     = <span class="number">57147392</span> (<span class="number">54.</span>5MB)</span><br><span class="line">   <span class="number">0.0</span>% used</span><br><span class="line">To Space:</span><br><span class="line">   capacity = <span class="number">57671680</span> (<span class="number">55.</span>0MB)</span><br><span class="line">   used     = <span class="number">0</span> (<span class="number">0.</span>0MB)</span><br><span class="line">   free     = <span class="number">57671680</span> (<span class="number">55.</span>0MB)</span><br><span class="line">   <span class="number">0.0</span>% used</span><br><span class="line">PS Old Generation</span><br><span class="line">   capacity = <span class="number">348127232</span> (<span class="number">332.</span>0MB)</span><br><span class="line">   used     = <span class="number">347644232</span> (<span class="number">331.</span>5393753051758MB)</span><br><span class="line">   free     = <span class="number">483000</span> (<span class="number">0.</span>46062469482421875MB)</span><br><span class="line">   <span class="number">99.8612576220409</span>% used</span><br><span class="line"></span><br><span class="line"><span class="number">10381</span> interned Strings occupying <span class="number">935984</span> bytes.</span><br></pre></td></tr></table></figure>

<h2 id="远程监控"><a href="#远程监控" class="headerlink" title="远程监控"></a>远程监控</h2><h3 id="JVM开启远程JMX"><a href="#JVM开启远程JMX" class="headerlink" title="JVM开启远程JMX"></a>JVM开启远程JMX</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 开启远程调试端口</span></span><br><span class="line">-Dcom.sun.management.jmxremote.port=8777</span><br><span class="line"><span class="comment"># 开启本地调试端口</span></span><br><span class="line">-Dcom.sun.management.jmxremote.rmi.port=8777</span><br><span class="line"><span class="comment"># jmx远程服务默认是开启ssl和认证功能功能的，也可以通过jvm选项把这两个功能关闭</span></span><br><span class="line">-Dcom.sun.management.jmxremote.authenticate=<span class="literal">false</span></span><br><span class="line">-Dcom.sun.management.jmxremote.ssl=<span class="literal">false</span></span><br><span class="line"><span class="comment"># jmx默认是通过localhost的ip地址提供RMI服务的，如果要明确指定RMI服务地址或主机名（比如主机有多个接口，想使用非hostname关联的接口）</span></span><br><span class="line">-Djava.rmi.server.hostname=10.242.93.40</span><br></pre></td></tr></table></figure>
<p>例子:</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">java \</span><br><span class="line">-Djava.rmi.server.hostname=0.0.0.0 \</span><br><span class="line">-Dcom.sun.management.jmxremote \</span><br><span class="line">-Dcom.sun.management.jmxremote.rmi.port=8000 \</span><br><span class="line">-Dcom.sun.management.jmxremote.port=8001 \</span><br><span class="line">-Dcom.sun.management.jmxremote.authenticate=false \</span><br><span class="line">-Dcom.sun.management.jmxremote.ssl=false \</span><br><span class="line">-jar demo-0.0.1-SNAPSHOT.jar</span><br></pre></td></tr></table></figure>
<h3 id="Tomcat-开启远程"><a href="#Tomcat-开启远程" class="headerlink" title="Tomcat 开启远程"></a>Tomcat 开启远程</h3><p>修改<code>catalina.sh</code>,</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">JAVA_OPTS&#x3D;&quot;$JAVA_OPTS -Djava.rmi.server.hostname&#x3D;0.0.0.0 </span><br><span class="line">-Dcom.sun.management.jmxremote </span><br><span class="line">-Dcom.sun.management.jmxremote.rmi.port&#x3D;8000 </span><br><span class="line">-Dcom.sun.management.jmxremote.port&#x3D;8001 </span><br><span class="line">-Dcom.sun.management.jmxremote.authenticate&#x3D;false </span><br><span class="line">-Dcom.sun.management.jmxremote.ssl&#x3D;false &quot;</span><br></pre></td></tr></table></figure>
<h3 id="jvisualvm"><a href="#jvisualvm" class="headerlink" title="jvisualvm"></a>jvisualvm</h3><p><a target="_blank" rel="noopener" href="https://visualvm.github.io/pluginscenters.html">插件中心</a></p>
<ul>
<li>新建远程主机0.0.0.0</li>
<li>新建JMX链接: 端口是8001</li>
</ul>
<p><a target="_blank" rel="noopener" href="https://visualvm.github.io/documentation.html">中文文档</a></p>
<h2 id="开启jstatd连接"><a href="#开启jstatd连接" class="headerlink" title="开启jstatd连接"></a>开启jstatd连接</h2><h2 id="BTrace"><a href="#BTrace" class="headerlink" title="BTrace"></a>BTrace</h2><p><a target="_blank" rel="noopener" href="https://github.com/btraceio/btrace">官网</a></p>
<h2 id="JDWP与tomcat远程调试"><a href="#JDWP与tomcat远程调试" class="headerlink" title="JDWP与tomcat远程调试"></a>JDWP与tomcat远程调试</h2><h2 id="Tomcat-监控"><a href="#Tomcat-监控" class="headerlink" title="Tomcat 监控"></a>Tomcat 监控</h2><h3 id="JDWP"><a href="#JDWP" class="headerlink" title="JDWP"></a>JDWP</h3><h3 id="Tomcat-manager"><a href="#Tomcat-manager" class="headerlink" title="Tomcat-manager"></a>Tomcat-manager</h3><h3 id="psi-probe监控"><a href="#psi-probe监控" class="headerlink" title="psi-probe监控"></a>psi-probe监控</h3><h2 id="Nginx"><a href="#Nginx" class="headerlink" title="Nginx"></a>Nginx</h2><h3 id="ngx-http-stub-status监控连接信息"><a href="#ngx-http-stub-status监控连接信息" class="headerlink" title="ngx_http_stub_status监控连接信息"></a>ngx_http_stub_status监控连接信息</h3><h3 id="ngxtop监控请求信息"><a href="#ngxtop监控请求信息" class="headerlink" title="ngxtop监控请求信息"></a>ngxtop监控请求信息</h3><h2 id="字节码"><a href="#字节码" class="headerlink" title="字节码"></a>字节码</h2><p><a target="_blank" rel="noopener" href="https://docs.oracle.com/javase/specs/jvms/se8/html/jvms-4.html#jvms-4.4">常量池官方文档</a></p>
<h3 id="深入解析String-intern"><a href="#深入解析String-intern" class="headerlink" title="深入解析String#intern"></a>深入解析String#intern</h3><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/goldenfish1919/article/details/80410349">深入解析String.intern</a><br><a target="_blank" rel="noopener" href="https://tech.meituan.com/2014/03/06/in-depth-understanding-string-intern.html">原文出处: 深入解析String.intern</a></p>
<p>字符串的常量池</p>
<p>常量池在哪里？</p>
<ul>
<li>Jdk1.6及之前： 有永久代, 常量池在方法区</li>
<li>Jdk1.7：       有永久代，但已经逐步“去永久代”，常量池在堆</li>
<li>Jdk1.8及之后： 无永久代，常量池在元空间</li>
</ul>
<h3 id="String去重"><a href="#String去重" class="headerlink" title="String去重"></a>String去重</h3><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/goldenfish1919/article/details/20233263">String Deduplication in G1</a></p>
<p>-XX:+UseStringDeduplication   开启String去重<br>-XX:+PrintStringDeduplicationStatistics 打印详细的去重统计<br>-XX:StringDeduplicationAgeThreshold=15 达到这个年龄的String对象才会去重</p>
<h2 id="重用代码优化方法"><a href="#重用代码优化方法" class="headerlink" title="重用代码优化方法"></a>重用代码优化方法</h2><ul>
<li>尽量重用对象，不要循环创建对象，比如：for循环字符串拼接</li>
<li>容器类初始化的时候指定长度</li>
<li>ArrayList随机遍历快，LinkedList添加删除快</li>
<li>集合遍历尽量减少重复计算  <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;collection.size();i++) <span class="comment">// collection.size() 提取成常量</span></span><br></pre></td></tr></table></figure></li>
<li>使用Entry遍历Map  <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span>(Map.Entry&lt;String,String&gt; entry:map.entrySet())&#123;</span><br><span class="line">    String key=entry.getKey();</span><br><span class="line">    String value=entry.getValue();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li>大数组复制用<code>System.arrayCopy()</code></li>
<li>尽量使用基本类型而不是包装类型  <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Integer i=<span class="number">100</span>;</span><br><span class="line">System.out.println(i);</span><br></pre></td></tr></table></figure>
  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">Code:</span><br><span class="line">  stack&#x3D;2, locals&#x3D;2, args_size&#x3D;1</span><br><span class="line">     0: bipush        100                 &#x2F;&#x2F; 将100压入栈</span><br><span class="line">     2: invokestatic  #2                  &#x2F;&#x2F; Method java&#x2F;lang&#x2F;Integer.valueOf:(I)Ljava&#x2F;lang&#x2F;Integer;</span><br><span class="line">     5: astore_1</span><br><span class="line">     6: getstatic     #3                  &#x2F;&#x2F; Field java&#x2F;lang&#x2F;System.out:Ljava&#x2F;io&#x2F;PrintStream;</span><br><span class="line">     9: aload_1</span><br><span class="line">    10: invokevirtual #4                  &#x2F;&#x2F; Method java&#x2F;io&#x2F;PrintStream.println:(Ljava&#x2F;lang&#x2F;Object;)V</span><br><span class="line">    13: return</span><br></pre></td></tr></table></figure>
  <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Integer i1=<span class="number">100</span>;</span><br><span class="line">Integer i2=<span class="number">100</span>;</span><br><span class="line">System.out.println(i1==i2); <span class="comment">// true</span></span><br></pre></td></tr></table></figure>
  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">Code:</span><br><span class="line">  stack&#x3D;3, locals&#x3D;3, args_size&#x3D;1</span><br><span class="line">     0: bipush        100</span><br><span class="line">     2: invokestatic  #2                  &#x2F;&#x2F; Method java&#x2F;lang&#x2F;Integer.valueOf:(I)Ljava&#x2F;lang&#x2F;Integer;</span><br><span class="line">     5: astore_1</span><br><span class="line">     6: bipush        100</span><br><span class="line">     8: invokestatic  #2                  &#x2F;&#x2F; Method java&#x2F;lang&#x2F;Integer.valueOf:(I)Ljava&#x2F;lang&#x2F;Integer;</span><br><span class="line">    11: astore_2</span><br><span class="line">    12: getstatic     #3                  &#x2F;&#x2F; Field java&#x2F;lang&#x2F;System.out:Ljava&#x2F;io&#x2F;PrintStream;</span><br><span class="line">    15: aload_1</span><br><span class="line">    16: aload_2</span><br><span class="line">    17: if_acmpne     24</span><br><span class="line">    20: iconst_1</span><br><span class="line">    21: goto          25</span><br><span class="line">    24: iconst_0</span><br><span class="line">    25: invokevirtual #5                  &#x2F;&#x2F; Method java&#x2F;io&#x2F;PrintStream.println:(Z)V</span><br><span class="line">    28: return</span><br></pre></td></tr></table></figure>
  <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Integer i1=<span class="number">1000</span>;</span><br><span class="line">Integer i2=<span class="number">1000</span>;</span><br><span class="line">System.out.println(i1==i2);  <span class="comment">// false</span></span><br></pre></td></tr></table></figure>
  <figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">Code:</span><br><span class="line">  stack=3, locals=3, args_size=1</span><br><span class="line">     0: sipush        1000</span><br><span class="line">     3: invokestatic  #2                  // Method java/lang/Integer.valueOf:(I)Ljava/lang/Integer;</span><br><span class="line">     6: astore_1</span><br><span class="line">     7: sipush        1000</span><br><span class="line">    10: invokestatic  #2                  // Method java/lang/Integer.valueOf:(I)Ljava/lang/Integer;</span><br><span class="line">    13: astore_2</span><br><span class="line">    14: getstatic     #3                  // Field java/lang/System.out:Ljava/io/PrintStream;</span><br><span class="line">    17: aload_1</span><br><span class="line">    18: aload_2</span><br><span class="line">    19: if_acmpne     26</span><br><span class="line">    22: iconst_1</span><br><span class="line">    23: goto          27</span><br><span class="line">    26: iconst_0</span><br><span class="line">    27: invokevirtual #5                  // Method java/io/PrintStream.println:(Z)V</span><br><span class="line">    30: return</span><br></pre></td></tr></table></figure>
  Integer.valueOf 内部有缓存，如果大于-128且小于<code>java.lang.Integer.IntegerCache.high</code>(默认值是127)，则返回缓存</li>
<li>尽量使用非同步的容器，vector和ArrayList</li>
<li>尽量减少同步作用范围，synchronized方法vs代码块  <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">synchronized</span> <span class="keyword">void</span> <span class="title">f1</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    System.out.println(<span class="string">&quot;f1&quot;</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">f2</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">synchronized</span> (<span class="keyword">this</span>) &#123;</span><br><span class="line">        System.out.println(<span class="string">&quot;f4&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">synchronized</span> <span class="keyword">void</span> <span class="title">f3</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    System.out.println(<span class="string">&quot;f3&quot;</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">f4</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">synchronized</span> (TestPrimaryType.class) &#123;</span><br><span class="line">        System.out.println(<span class="string">&quot;f4&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li>使用ThreadLocal缓存线程不安全对象，simpleDateFormat</li>
<li>尽量使用延迟加载，懒惰单例模式  <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TestPrimaryType</span></span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="title">TestPrimaryType</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">TestPrimaryTypeHolder</span> </span>&#123;</span><br><span class="line">        <span class="keyword">private</span> <span class="keyword">static</span> TestPrimaryType instance = <span class="keyword">new</span> TestPrimaryType();</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">    ** 调用这个方法时，发生对TestPrimaryTypeHolder的引用，才创建instance对象</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> TestPrimaryType <span class="title">getInstance</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> TestPrimaryTypeHolder.instance;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li>尽量减少使用反射，加缓存</li>
<li>尽量使用连接池、线程池、对象池、缓存</li>
<li>及时释放资源，I/O流、socket、数据库连接</li>
<li>慎用异常，不要用抛异常来标示正常的业务逻辑</li>
<li>String操作尽量少用正则表达式  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">replace VS replaceAll: 尽量用replace</span><br><span class="line">split</span><br></pre></td></tr></table></figure></li>
<li>日志输出注意使用不同的级别</li>
<li>日志中参数拼接使用占位符  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">log.info(“orderId:”+orgerId); &#x2F;&#x2F; 不推荐</span><br><span class="line">log.info(“orderId:&#123;&#125;”,orgerId); &#x2F;&#x2F;推荐</span><br></pre></td></tr></table></figure></li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://example.com/bigdata/Flink/Flink-time/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="aaronzhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Guadazi">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/bigdata/Flink/Flink-time/" class="post-title-link" itemprop="url">Flink:时间</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2019-08-10 14:58:00" itemprop="dateCreated datePublished" datetime="2019-08-10T14:58:00+08:00">2019-08-10</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-06-01 20:52:39" itemprop="dateModified" datetime="2021-06-01T20:52:39+08:00">2021-06-01</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/bigdata/" itemprop="url" rel="index"><span itemprop="name">bigdata</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="Flink-Time"><a href="#Flink-Time" class="headerlink" title="Flink Time"></a>Flink Time</h1><h2 id="Event-Time与Process-Time"><a href="#Event-Time与Process-Time" class="headerlink" title="Event Time与Process Time"></a>Event Time与Process Time</h2><p>如果你对数据的准确性要求比较高的话，采用 <code>Event time</code> 能保障 exactly-once。Processing Time 一般用于实时消费、精准性要求略低的场景，主要是因为时间生成不是 deterministic。</p>
<p>我们可以看下面的关系图， X 轴是 Event time，Y 轴是 Processing time。理想情况下 <code>Event time</code> 和 <code>Processing Time</code> 是相同的，就是说只要有一个事件发生，就可以立刻处理。但是实际场景中，事件发生后往往会经过一定延时才会被处理，这样就会导致我们系统的时间往往会滞后于事件时间。这里它们两个的差 <code>Processing-time lag</code> 表示我们处理事件的延时。</p>
<p><img src="vx_images/1726029789446"></p>
<p>事件时间常用在窗口中，使用 watermark 来确保数据完备性，比如说 watermarker 值大于 window 末尾时间时，我们就可以认为 window 窗口所有数据都已经到达了，就可以触发计算了。</p>
<p><img src="vx_images/1714865841042"></p>
<p>比如上面 <code>[0-10]</code> 的窗口，现在 watermark 走到了 10，已经到达了窗口的结束，触发计算 SUM=21。如果要是想对迟到的数据再进行触发，可以再定义一下后面 late data 的触发，比如说后面来了个 9，我们的 SUM 就等于 30。</p>
<h2 id="北京时间与格林尼治时间"><a href="#北京时间与格林尼治时间" class="headerlink" title="北京时间与格林尼治时间"></a>北京时间与格林尼治时间</h2><p>![北京时间](_v_images/20190731222505723_213254839.png =968x)</p>
<blockquote>
<p>划定时间窗口和时间戳以毫秒为单位</p>
</blockquote>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">eventStream.assignTimestampsAndWatermarks(assigner = <span class="keyword">new</span> <span class="type">AscendingTimestampExtractor</span>[<span class="type">Event</span>]() &#123;</span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">extractAscendingTimestamp</span></span>(t: <span class="type">Event</span>): <span class="type">Long</span> = &#123;</span><br><span class="line">    t.getTime * <span class="number">1000</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;)</span><br></pre></td></tr></table></figure>
<h2 id="processFunction中的时间"><a href="#processFunction中的时间" class="headerlink" title="processFunction中的时间"></a>processFunction中的时间</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">new</span> <span class="type">ProcessFunction</span>[<span class="type">Event</span>, (<span class="type">String</span>, <span class="type">Long</span>)] &#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">processElement</span></span>(event: <span class="type">Event</span>, context: <span class="type">ProcessFunction</span>[<span class="type">Event</span>,</span><br><span class="line">          (<span class="type">String</span>, <span class="type">Long</span>)]#<span class="type">Context</span>, collector: <span class="type">Collector</span>[(<span class="type">String</span>, <span class="type">Long</span>)]): <span class="type">Unit</span> = &#123;</span><br></pre></td></tr></table></figure>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">context.timestamp=<span class="number">1565064599999</span></span><br><span class="line">context.timerService().currentWatermark()=-<span class="number">9223372036854775808</span></span><br><span class="line">context.timerService().currentProcessingTime()=<span class="number">1565064607343</span></span><br><span class="line"></span><br><span class="line">Event(time=<span class="number">1565064350</span>, count=<span class="number">9</span>, url=/apply/main, channelId=<span class="number">1057</span>)</span><br></pre></td></tr></table></figure>
<p>以事件事件，窗口大小5min</p>
<p>一条消息，事件的时间是 1565064350(2019-08-06 12:05:50)<br><code>Context.timestamp</code>是1565064599999(2019-08-06 12:09:59), 可以理解为窗口结束的时间<br><code>context.timerService().currentWatermark()</code> 莫名其妙，这么大<br><code>context.timerService().currentProcessingTime()</code>是当前处理事件 2019-08-06 12:10:07</p>
<h3 id="Context-timestamp"><a href="#Context-timestamp" class="headerlink" title="Context.timestamp"></a><code>Context.timestamp</code></h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Timestamp of the element currently being processed or timestamp of a firing timer.</span><br><span class="line">This might be &#123;<span class="meta">@code</span> <span class="keyword">null</span>&#125;, <span class="keyword">for</span> example <span class="keyword">if</span> the time characteristic of your program</span><br></pre></td></tr></table></figure>
<h2 id="时间戳timestamp与水印watermark"><a href="#时间戳timestamp与水印watermark" class="headerlink" title="时间戳timestamp与水印watermark"></a>时间戳timestamp与水印watermark</h2><p>提取Timestamp与生成Watermark一般步骤</p>
<ol>
<li>设置时间特性为Event Time。<code>StreamExecutionEnvironment#setStreamTimeCharacteristic(TimeCharacteristic.EventTime)</code>。</li>
<li>在Source后Window前用<code>DataStream#assignTimestampsAndWatermarks</code>方法(<code>AssignerWithPeriodicWatermarks</code>或<code>AssignerWithPunctuatedWatermarks</code>)提取时间戳并生成水印。</li>
<li>重写<code>extractTimestamp</code>方法提取Timestamp，重写<code>getCurrentWatermark</code>方法或<code>checkAndGetNextWatermark</code>方法生成水印。</li>
</ol>
<h3 id="数据源中指定"><a href="#数据源中指定" class="headerlink" title="数据源中指定"></a>数据源中指定</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">ExampleSourceFunction</span> <span class="keyword">implements</span> <span class="title">SourceFunction</span>&lt;<span class="title">Tuple4</span>&lt;<span class="title">String</span>,<span class="title">Long</span>,<span class="title">String</span>,<span class="title">Integer</span>&gt;&gt;</span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">(SourceContext&lt;Tuple4&lt;String,Long,String,Integer&gt;&gt; ctx)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">            <span class="keyword">while</span> (isRunning)&#123;</span><br><span class="line">                <span class="comment">// 构造测试数据</span></span><br><span class="line">                ....</span><br><span class="line"></span><br><span class="line">                <span class="comment">// 发出一条数据以及数据对应的Timestamp</span></span><br><span class="line">                ctx.collectWithTimestamp(record,eventTime);</span><br><span class="line"></span><br><span class="line">                <span class="comment">// 发出一条Watermark</span></span><br><span class="line">                ctx.emitWatermark(<span class="keyword">new</span> Watermark(eventTime - maxOutOfOrderness));</span><br><span class="line"></span><br><span class="line">                Thread.sleep(<span class="number">1000</span>);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">cancel</span><span class="params">()</span> </span>&#123;</span><br><span class="line">            isRunning = <span class="keyword">false</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>时间戳和水位线分别通过两个方法实现的</p>
<h3 id="时间戳的抽取方法"><a href="#时间戳的抽取方法" class="headerlink" title="时间戳的抽取方法"></a>时间戳的抽取方法</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">* 传入了两个参数，第二个参数是上一个时间戳，元素的当前内部时间戳，如果尚未分配时间戳，则为负值。</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="function"><span class="keyword">long</span> <span class="title">extractTimestamp</span><span class="params">(T element, <span class="keyword">long</span> recordTimestamp)</span></span>;</span><br></pre></td></tr></table></figure>
<h3 id="周期性水印"><a href="#周期性水印" class="headerlink" title="周期性水印"></a>周期性水印</h3><p>分配器为元素分配事件时间时间戳，并生成表示流中事件时间进度的低水位线。这些时间戳和水印由对事件时间进行操作的函数和运算符使用，例如事件时间窗口。</p>
<p>使用此类以周期性间隔生成水印。最多每隔I毫秒(通过<code>ExecutionConfig.getautowerMarkinterval()</code>)配置一次，系统将调用<code>getCurrentWatermark()</code>方法来探测下一个水印值。<br>如果探测到的值为非空，并且时间戳大于前一个水印的时间戳，系统将生成新的水印(以保留升序水印的约定)。 如果自上次调用<code>getCurrentWatermark()</code>方法后没有新元素到达，系统调用该方法的频率可能会低于每I毫秒一次。<br>时间戳和水印被定义为代表自纪元(世界协调时1970年1月1日午夜)以来的毫秒的长度。某个值为t的水印表示不会再出现带有事件时间戳x的元素，其中x小于或等于t。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">* 返回当前水印。系统定期调用此方法来检索当前水印。该方法可能返回空值，表示没有新的水印可用。</span></span><br><span class="line"><span class="comment">* 只有当返回的水印为非空且其时间戳大于之前发出的水印的时间戳时，才会发出该水印(以保留升序水印的约定)。</span></span><br><span class="line"><span class="comment">* 如果当前水印仍然与前一个相同，则自上一次调用此方法以来，事件时间没有任何进展。如果返回空值，或者返回的水印的时间戳小于上次发出的时间戳，则不会生成新的水印。</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line">Watermark  AssignerWithPeriodicWatermarks.getCurrentWatermark()</span><br></pre></td></tr></table></figure>
<h3 id="非连续性水印"><a href="#非连续性水印" class="headerlink" title="非连续性水印"></a>非连续性水印</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">* 询问此实现是否要发出水印。这个方法是在extractTimestamp(对象，长)方法之后调用的。</span></span><br><span class="line"><span class="comment">* 只有当返回的水印为非空且其时间戳大于之前发出的水印的时间戳时，才会发出该水印(以保留升序水印的约定)。</span></span><br><span class="line"><span class="comment">* 如果返回空值，或者返回的水印的时间戳小于上次发出的时间戳，则不会生成新的水印。 有关如何使用此方法的示例，请参见此类的分配器带标点水印的文档。</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line">Watermark AssignerWithPunctuatedWatermarks.checkAndGetNextWatermark(T lastElement, <span class="keyword">long</span> extractedTimestamp)</span><br></pre></td></tr></table></figure>
<p><img src="_v_images/20201207105022608_1990981860.png"></p>
<h3 id="升序水印-AscendingTimestampExtractor"><a href="#升序水印-AscendingTimestampExtractor" class="headerlink" title="升序水印 AscendingTimestampExtractor"></a>升序水印 AscendingTimestampExtractor</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line">org.apache.flink.streaming.api.functions.timestamps.AscendingTimestampExtractor</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">AscendingTimestampExtractor</span>&lt;<span class="title">T</span>&gt; <span class="keyword">implements</span> <span class="title">AssignerWithPeriodicWatermarks</span>&lt;<span class="title">T</span>&gt; </span>&#123;</span><br><span class="line">	<span class="comment">/** The current timestamp. */</span></span><br><span class="line">	<span class="keyword">private</span> <span class="keyword">long</span> currentTimestamp = Long.MIN_VALUE;</span><br><span class="line">	<span class="comment">/** Handler that is called when timestamp monotony is violated. */</span></span><br><span class="line">	<span class="keyword">private</span> MonotonyViolationHandler violationHandler = <span class="keyword">new</span> LoggingHandler();</span><br><span class="line">	<span class="comment">/**</span></span><br><span class="line"><span class="comment">	 * Extracts the timestamp from the given element. The timestamp must be monotonically increasing.</span></span><br><span class="line"><span class="comment">	 *</span></span><br><span class="line"><span class="comment">	 * <span class="doctag">@param</span> element The element that the timestamp is extracted from.</span></span><br><span class="line"><span class="comment">	 * <span class="doctag">@return</span> The new timestamp.</span></span><br><span class="line"><span class="comment">	 */</span></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">abstract</span> <span class="keyword">long</span> <span class="title">extractAscendingTimestamp</span><span class="params">(T element)</span></span>;</span><br><span class="line">	<span class="comment">/**</span></span><br><span class="line"><span class="comment">	 * Sets the handler for violations to the ascending timestamp order.</span></span><br><span class="line"><span class="comment">	 *</span></span><br><span class="line"><span class="comment">	 * <span class="doctag">@param</span> handler The violation handler to use.</span></span><br><span class="line"><span class="comment">	 * <span class="doctag">@return</span> This extractor.</span></span><br><span class="line"><span class="comment">	 */</span></span><br><span class="line">	<span class="function"><span class="keyword">public</span> AscendingTimestampExtractor&lt;T&gt; <span class="title">withViolationHandler</span><span class="params">(MonotonyViolationHandler handler)</span> </span>&#123;</span><br><span class="line">		<span class="keyword">this</span>.violationHandler = requireNonNull(handler);</span><br><span class="line">		<span class="keyword">return</span> <span class="keyword">this</span>;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="comment">// ------------------------------------------------------------------------</span></span><br><span class="line">    <span class="comment">// 调用</span></span><br><span class="line">	<span class="meta">@Override</span></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">final</span> <span class="keyword">long</span> <span class="title">extractTimestamp</span><span class="params">(T element, <span class="keyword">long</span> elementPrevTimestamp)</span> </span>&#123;</span><br><span class="line">		<span class="keyword">final</span> <span class="keyword">long</span> newTimestamp = extractAscendingTimestamp(element);</span><br><span class="line">		<span class="keyword">if</span> (newTimestamp &gt;= <span class="keyword">this</span>.currentTimestamp) &#123;</span><br><span class="line">			<span class="keyword">this</span>.currentTimestamp = newTimestamp;</span><br><span class="line">			<span class="keyword">return</span> newTimestamp;</span><br><span class="line">		&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">			violationHandler.handleViolation(newTimestamp, <span class="keyword">this</span>.currentTimestamp);</span><br><span class="line">			<span class="keyword">return</span> newTimestamp;</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="meta">@Override</span></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">final</span> Watermark <span class="title">getCurrentWatermark</span><span class="params">()</span> </span>&#123;</span><br><span class="line">		<span class="keyword">return</span> <span class="keyword">new</span> Watermark(currentTimestamp == Long.MIN_VALUE ? Long.MIN_VALUE : currentTimestamp - <span class="number">1</span>);</span><br><span class="line">	&#125;</span><br><span class="line">	...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>AscendingTimestampExtractor抽象类实现了ssignerWithPeriodicWatermarks接口的extractTimestamp及getCurrentWatermark方法，同时声明抽象方法extractAscendingTimestamp供子类实现<br>水印的生产策略为：getCurrentWatermark 方法在currentTimestamp不为Long.MIN_VALUE时返回Watermark(currentTimestamp - 1)</p>
<p>综上：<br>发现AscendingTimestampExtractor适用于elements的时间在每个并行task里面事单调递增的，（timestamp monotony）数据场景。</p>
<h3 id="无序水印-BoundedOutOfOrdernessTimestampExtractor"><a href="#无序水印-BoundedOutOfOrdernessTimestampExtractor" class="headerlink" title="无序水印 BoundedOutOfOrdernessTimestampExtractor"></a>无序水印 BoundedOutOfOrdernessTimestampExtractor</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">BoundedOutOfOrdernessTimestampExtractor</span>&lt;<span class="title">T</span>&gt; <span class="keyword">implements</span> <span class="title">AssignerWithPeriodicWatermarks</span>&lt;<span class="title">T</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">	<span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> serialVersionUID = <span class="number">1L</span>;</span><br><span class="line"></span><br><span class="line">	<span class="comment">/** The current maximum timestamp seen so far. */</span></span><br><span class="line">	<span class="keyword">private</span> <span class="keyword">long</span> currentMaxTimestamp;</span><br><span class="line"></span><br><span class="line">	<span class="comment">/** The timestamp of the last emitted watermark. */</span></span><br><span class="line">	<span class="keyword">private</span> <span class="keyword">long</span> lastEmittedWatermark = Long.MIN_VALUE;</span><br><span class="line"></span><br><span class="line">	<span class="comment">/**</span></span><br><span class="line"><span class="comment">	 * The (fixed) interval between the maximum seen timestamp seen in the records</span></span><br><span class="line"><span class="comment">	 * and that of the watermark to be emitted.</span></span><br><span class="line"><span class="comment">	 */</span></span><br><span class="line">	<span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">long</span> maxOutOfOrderness;</span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="title">BoundedOutOfOrdernessTimestampExtractor</span><span class="params">(Time maxOutOfOrderness)</span> </span>&#123;</span><br><span class="line">		<span class="keyword">if</span> (maxOutOfOrderness.toMilliseconds() &lt; <span class="number">0</span>) &#123;</span><br><span class="line">			<span class="keyword">throw</span> <span class="keyword">new</span> RuntimeException(<span class="string">&quot;Tried to set the maximum allowed &quot;</span> +</span><br><span class="line">				<span class="string">&quot;lateness to &quot;</span> + maxOutOfOrderness + <span class="string">&quot;. This parameter cannot be negative.&quot;</span>);</span><br><span class="line">		&#125;</span><br><span class="line">		<span class="keyword">this</span>.maxOutOfOrderness = maxOutOfOrderness.toMilliseconds();</span><br><span class="line">		<span class="keyword">this</span>.currentMaxTimestamp = Long.MIN_VALUE + <span class="keyword">this</span>.maxOutOfOrderness;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">long</span> <span class="title">getMaxOutOfOrdernessInMillis</span><span class="params">()</span> </span>&#123;</span><br><span class="line">		<span class="keyword">return</span> maxOutOfOrderness;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="comment">/**</span></span><br><span class="line"><span class="comment">	 * Extracts the timestamp from the given element.</span></span><br><span class="line"><span class="comment">	 *</span></span><br><span class="line"><span class="comment">	 * <span class="doctag">@param</span> element The element that the timestamp is extracted from.</span></span><br><span class="line"><span class="comment">	 * <span class="doctag">@return</span> The new timestamp.</span></span><br><span class="line"><span class="comment">	 */</span></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">abstract</span> <span class="keyword">long</span> <span class="title">extractTimestamp</span><span class="params">(T element)</span></span>;</span><br><span class="line"></span><br><span class="line">	<span class="meta">@Override</span></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">final</span> Watermark <span class="title">getCurrentWatermark</span><span class="params">()</span> </span>&#123;</span><br><span class="line">		<span class="comment">// this guarantees that the watermark never goes backwards.</span></span><br><span class="line">		<span class="keyword">long</span> potentialWM = currentMaxTimestamp - maxOutOfOrderness;</span><br><span class="line">		<span class="keyword">if</span> (potentialWM &gt;= lastEmittedWatermark) &#123;</span><br><span class="line">			lastEmittedWatermark = potentialWM;</span><br><span class="line">		&#125;</span><br><span class="line">		<span class="keyword">return</span> <span class="keyword">new</span> Watermark(lastEmittedWatermark);</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="meta">@Override</span></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">final</span> <span class="keyword">long</span> <span class="title">extractTimestamp</span><span class="params">(T element, <span class="keyword">long</span> previousElementTimestamp)</span> </span>&#123;</span><br><span class="line">		<span class="keyword">long</span> timestamp = extractTimestamp(element);</span><br><span class="line">		<span class="keyword">if</span> (timestamp &gt; currentMaxTimestamp) &#123;</span><br><span class="line">			currentMaxTimestamp = timestamp;</span><br><span class="line">		&#125;</span><br><span class="line">		<span class="keyword">return</span> timestamp;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><code>BoundedOutOfOrdernessTimestampExtractor</code> 抽象类实现<code>AssignerWithPeriodicWatermarks</code>接口的<code>extractTimestamp及getCurrentWatermark</code>方法，同时声明抽象方法<code>extractAscendingTimestamp</code>供子类实现<br><code>BoundedOutOfOrdernessTimestampExtractor</code> 的构造器接收<code>maxOutOfOrderness</code>参数用于指定element允许滞后（<code>t</code>~`t_w<code>，</code>t<code>为element的eventTime，</code>t_w<code>为前一次watermark的时间）的最大时间，在计算窗口数据时，如果超过该值则会被忽略。 </code>BoundedOutOfOrdernessTimestampExtractor<code>的</code>extractTimestamp<code>方法会调用子类的</code>extractTimestamp<code>方法抽取时间，如果该时间大于</code>currentMaxTimestamp<code>，则更新</code>currentMaxTimestamp<code>； </code>getCurrentWatermark<code>先计算</code>potentialWM<code>，如果</code>potentialWM<code>大于等于</code>lastEmittedWatermark<code>则更新</code>lastEmittedWatemakr<code>(</code>currentMaxTimestamp - lastEmittedWatermark &gt;= maxOutOfOrderness<code>， 这里表示</code>lastEmittedWatermark<code>太小了，所以差值超过了</code>maxOutOfOrderness<code>，因此会调大</code>lastEmittedWatermark<code>)，最后返回</code>watermark` </p>
<p>具体逻辑参考：就是判断如果新的水印时间大于上次最后一次发出水印时间，选择新的水印时间，否则选择上次最后发送水印时间戳</p>
<h3 id="TimeStamp分配器和Watermark生成器-Timestamp-Assigners-Watermark-Generators"><a href="#TimeStamp分配器和Watermark生成器-Timestamp-Assigners-Watermark-Generators" class="headerlink" title="TimeStamp分配器和Watermark生成器(Timestamp Assigners / Watermark Generators)"></a>TimeStamp分配器和Watermark生成器(Timestamp Assigners / Watermark Generators)</h3><h2 id="数据延迟"><a href="#数据延迟" class="headerlink" title="数据延迟"></a>数据延迟</h2><p>主要的办法是给定一个允许延迟的时间，在该时间范围内仍可以接受处理延迟数据</p>
<p>设置允许延迟的时间是通过<code>allowedLateness(lateness: Time)</code>设置</p>
<p>保存延迟数据则是通过<code>sideOutputLateData(outputTag: OutputTag[T])</code>保存</p>
<p>获取延迟数据是通过<code>DataStream.getSideOutput(tag: OutputTag[X])</code>获取</p>
<h3 id="allowedLateness-lateness-Time"><a href="#allowedLateness-lateness-Time" class="headerlink" title="allowedLateness(lateness: Time)"></a>allowedLateness(lateness: Time)</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">allowedLateness</span></span>(lateness: <span class="type">Time</span>): <span class="type">WindowedStream</span>[<span class="type">T</span>, <span class="type">K</span>, <span class="type">W</span>] = &#123;</span><br><span class="line">    javaStream.allowedLateness(lateness)</span><br><span class="line">    <span class="keyword">this</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>该方法传入一个Time值，设置允许数据迟到的时间，这个时间和waterMark中的时间概念不同。再来回顾一下，</p>
<p>waterMark=数据的事件时间-允许乱序时间值</p>
<p>随着新数据的到来，waterMark的值会更新为最新数据事件时间-允许乱序时间值，但是如果这时候来了一条历史数据，waterMark值则不会更新。总的来说，waterMark是为了能接收到尽可能多的乱序数据。</p>
<p>那这里的Time值呢？主要是为了等待迟到的数据，在一定时间范围内，如果属于该窗口的数据到来，仍会进行计算，后面会对计算方式仔细说明</p>
<p><strong>allowedLateness 是等待数据进入窗口的最大时间，未进入的数据，可以通过偏流导出</strong></p>
<p>注意：该方法只针对于基于event-time的窗口，如果是基于processing-time，并且指定了非零的time值则会抛出异常</p>
<h3 id="sideOutputLateData-outputTag-OutputTag-T"><a href="#sideOutputLateData-outputTag-OutputTag-T" class="headerlink" title="sideOutputLateData(outputTag: OutputTag[T])"></a>sideOutputLateData(outputTag: OutputTag[T])</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sideOutputLateData</span></span>(outputTag: <span class="type">OutputTag</span>[<span class="type">T</span>]): <span class="type">WindowedStream</span>[<span class="type">T</span>, <span class="type">K</span>, <span class="type">W</span>] = &#123;</span><br><span class="line">    javaStream.sideOutputLateData(outputTag)</span><br><span class="line">    <span class="keyword">this</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>该方法是将迟来的数据保存至给定的outputTag参数，而OutputTag则是用来标记延迟数据的一个对象。</p>
<h3 id="DataStream-getSideOutput-tag-OutputTag-X"><a href="#DataStream-getSideOutput-tag-OutputTag-X" class="headerlink" title="DataStream.getSideOutput(tag: OutputTag[X])"></a>DataStream.getSideOutput(tag: OutputTag[X])</h3><p>通过window等操作返回的DataStream调用该方法，传入标记延迟数据的对象来获取延迟的数据</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">waterStream.keyBy(<span class="number">0</span>)</span><br><span class="line">      .window(<span class="type">TumblingEventTimeWindows</span>.of(<span class="type">Time</span>.seconds(<span class="number">5</span>L)))</span><br><span class="line">  </span><br><span class="line">      .allowedLateness(<span class="type">Time</span>.seconds(<span class="number">2</span>L))</span><br><span class="line">      .sideOutputLateData(lateData)</span><br><span class="line">      .apply(<span class="keyword">new</span> <span class="type">WindowFunction</span>[(<span class="type">String</span>, <span class="type">Long</span>), <span class="type">String</span>, <span class="type">Tuple</span>, <span class="type">TimeWindow</span>] &#123;</span><br><span class="line">        <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">apply</span></span>(key: <span class="type">Tuple</span>, window: <span class="type">TimeWindow</span>, input: <span class="type">Iterable</span>[(<span class="type">String</span>, <span class="type">Long</span>)], out: <span class="type">Collector</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">          <span class="keyword">val</span> timeArr = <span class="type">ArrayBuffer</span>[<span class="type">String</span>]()</span><br><span class="line">          <span class="keyword">val</span> iterator = input.iterator</span><br><span class="line">          <span class="keyword">while</span> (iterator.hasNext) &#123;</span><br><span class="line">            <span class="keyword">val</span> tup2 = iterator.next()</span><br><span class="line">            timeArr.append(sdf.format(tup2._2))</span><br><span class="line">          &#125;</span><br><span class="line">          <span class="keyword">val</span> outData = <span class="type">String</span>.format(<span class="string">&quot;key: %s    data: %s    startTime:  %s    endTime:  %s&quot;</span>,</span><br><span class="line">            key.toString,</span><br><span class="line">            timeArr.mkString(<span class="string">&quot;-&quot;</span>),</span><br><span class="line">            sdf.format(window.getStart),</span><br><span class="line">            sdf.format(window.getEnd))</span><br><span class="line">          out.collect(outData)</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;)</span><br><span class="line">    result.print(<span class="string">&quot;window计算结果:&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> late = result.getSideOutput(lateData)</span><br><span class="line">    late.print(<span class="string">&quot;迟到的数据:&quot;</span>)</span><br></pre></td></tr></table></figure>
<hr>
<p>【参考文献】</p>
<ol>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/103519313">Flink时间系列-EventTime下数据延迟处理</a></li>
<li><a target="_blank" rel="noopener" href="https://www.infoq.cn/article/zvc15xotpp5x5brxhwuz">Flink: 时间属性深度解析</a></li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://example.com/bigdata/Flink/Flink-SQL-active/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="aaronzhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Guadazi">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/bigdata/Flink/Flink-SQL-active/" class="post-title-link" itemprop="url">Flink-SQL实践</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2019-08-10 14:58:00" itemprop="dateCreated datePublished" datetime="2019-08-10T14:58:00+08:00">2019-08-10</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-06-01 20:52:39" itemprop="dateModified" datetime="2021-06-01T20:52:39+08:00">2021-06-01</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/bigdata/" itemprop="url" rel="index"><span itemprop="name">bigdata</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="Flink-SQL-active"><a href="#Flink-SQL-active" class="headerlink" title="Flink-SQL-active"></a>Flink-SQL-active</h1><p>维表同步脚本:</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">insert</span></span><br><span class="line"><span class="keyword">into</span></span><br><span class="line">    dim_result_lct_activy_config</span><br><span class="line">    <span class="keyword">select</span></span><br><span class="line">        Fact_id,</span><br><span class="line">        <span class="built_in">LAST_VALUE</span>(Fact_name),</span><br><span class="line">        FROM_UNIXTIME(UNIX_TIMESTAMP(<span class="built_in">LAST_VALUE</span>(Fact_start_time)),</span><br><span class="line">        <span class="string">&#x27;yyyyMMddHHmmss&#x27;</span>) <span class="keyword">as</span> startTime,</span><br><span class="line">        FROM_UNIXTIME(UNIX_TIMESTAMP(<span class="built_in">LAST_VALUE</span>(Fact_end_time)),</span><br><span class="line">        <span class="string">&#x27;yyyyMMddHHmmss&#x27;</span>) <span class="keyword">as</span> endTime,</span><br><span class="line">        <span class="built_in">LAST_VALUE</span>(Fstate)</span><br><span class="line">    <span class="keyword">from</span></span><br><span class="line">        db_act_config_t_act_logic_config</span><br><span class="line">    <span class="keyword">group</span> <span class="keyword">by</span></span><br><span class="line">        Fact_id</span><br></pre></td></tr></table></figure>
<p><a target="_blank" rel="noopener" href="http://apache-flink.147419.n8.nabble.com/FlinkSQL-Upsert-Retraction-MySQL-td2785.html">FlinkSQL Upsert/Retraction 写入 MySQL 的问题 </a></p>
<p> retract流是算子的特性，不同的算子不同，比如group by聚合算子天生就是会发retract流的，比如你之前算出来的是a, 1, 后面变化了，变成了a,2，所以group by算子会发-a,1, +a,2.</p>
<p>像join的话也是有retract流的，比如outer join</p>
<p>当然如果下游sink支持upsert, 这块也会有优化，会优化为update。</p>
<blockquote>
<p>INSERT INTO  mysql_sink SELECT  f1, count(*) FROM kafka_src GROUP BY f1<br>每从 kafka 过来一条新的记录，会生成两条记录 Tuple2&lt;Row, Boolean&gt;, 旧的被删除，新的会添加上。这是query是会一个会产生retract stream的query，可以简单理解成每条kafka的数据过来会产生两条记录，但是最终写入下游的系统。需要看下游的系统支持和实现的sink(现在有三种sink AppendStreamSink, UpsertStreamSink, RetractStreamSink)</p>
</blockquote>
<blockquote>
<p>我看 <a target="_blank" rel="noopener" href="https://github.com/apache/flink/tree/master/flink-connectors/flink-jdbc/src/main/java/org/apache/flink/api/java/io/jdbc">https://github.com/apache/flink/tree/master/flink-connectors/flink-jdbc/src/main/java/org/apache/flink/api/java/io/jdbc</a> 没有 Retract 方式<br>实际上使用了 JDBCUpsertTableSink.java 的代码写入 MySQL 吗？<br>现有的sink中，kafka是实现的AppendStreamSink，所以只支持insert 的记录，不支持retract.<br>你用DDL声明的mysql表，对应的jdbc sink 是JDBCUpsertTableSink，所以会按照Upsert的逻辑处理, 也不支持retract。</p>
</blockquote>
<blockquote>
<p>如若不带 group by 直接：<br>INSERT INTO  mysql_sink SELECT  f1,  f2 FROM kafka_src<br>主键冲突写入 mysql 是会出错的，怎么可以用 Upsert 的方式直接覆盖呢？</p>
</blockquote>
<p>不带 group by时无法推导出query的 unique key，没法做按照unique key的更新，<br>只需要将 query的 key （你这里是group by 后的字段）和db中主键保持一致即可</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://example.com/Java/metric/00_docker%E6%9E%84%E5%BB%BAJava%E6%B5%8B%E8%AF%95%E7%8E%AF%E5%A2%83/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="aaronzhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Guadazi">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/Java/metric/00_docker%E6%9E%84%E5%BB%BAJava%E6%B5%8B%E8%AF%95%E7%8E%AF%E5%A2%83/" class="post-title-link" itemprop="url">Docker构建Java调试环境</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2019-08-10 14:58:00" itemprop="dateCreated datePublished" datetime="2019-08-10T14:58:00+08:00">2019-08-10</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-06-01 20:52:39" itemprop="dateModified" datetime="2021-06-01T20:52:39+08:00">2021-06-01</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/java/" itemprop="url" rel="index"><span itemprop="name">java</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="Docker构建Java调试环境"><a href="#Docker构建Java调试环境" class="headerlink" title="Docker构建Java调试环境"></a>Docker构建Java调试环境</h1><p><a target="_blank" rel="noopener" href="https://c.163yun.com/hub#/home">网易镜像仓库</a></p>
<p>Docker动态给容器Container暴露端口</p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/lsziri/article/details/69396990">https://blog.csdn.net/lsziri/article/details/69396990</a></p>
<h2 id="Docker-container"><a href="#Docker-container" class="headerlink" title="Docker container"></a>Docker container</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">docker pull hub.c.163.com/public/centos:6.7-tools</span><br><span class="line">docker tag hub.c.163.com/public/centos:6.7-tools  centos</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 增加参数解决不能获取ptrace的问题</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> https://blog.csdn.net/russle/article/details/99708261</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 这种不可用 docker run --name centos-one --cap-add=SYS_PTRACE  -d -P centos</span></span><br><span class="line">docker run --cap-add=SYS_PTRACE --security-opt seccomp:unconfined --name centos-two  -d -P centos</span><br><span class="line">➜  ~ docker container ls -a</span><br><span class="line">CONTAINER ID        IMAGE               COMMAND                  CREATED              STATUS              PORTS                   NAMES</span><br><span class="line">3ae403bf463c        centos              &quot;/usr/bin/supervisord&quot;   About a minute ago   Up About a minute   0.0.0.0:32772-&gt;22/tcp   centos-one</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 进入容器</span></span><br><span class="line">➜  ~ docker exec -it centos-one su deploy</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>


<h2 id="Java环境"><a href="#Java环境" class="headerlink" title="Java环境"></a>Java环境</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 安装全部 yum install -y java-1.8.0-openjdk*</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 只安装需要的</span></span><br><span class="line">yum install -y java-1.8.0-openjdk.x86_64 java-1.8.0-openjdk-devel.x86_64</span><br></pre></td></tr></table></figure>
<p> install git</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">yum install gcc gcc-c++ autoconf make automake -y</span><br><span class="line">yum install curl-devel expat-devel gettext-devel openssl-devel zlib-devel -y</span><br><span class="line">yum install perl docbook2X texinfo sgml2xml openjade perl-ExtUtils-MakeMaker -y</span><br><span class="line">yum install asciidoc xmlto cpio expat-devel gettext-devel  -y</span><br><span class="line">yum install perl-ExtUtils-MakeMaker</span><br><span class="line"></span><br><span class="line">yum install -y tk zlib-devel openssl-devel perl cpio expat-devel gettext-devel  asciidoc xmlto autoconf gcc</span><br><span class="line">wget https://mirrors.edge.kernel.org/pub/software/scm/git/git-2.9.5.tar.gz</span><br><span class="line"></span><br><span class="line">tar zxvf git-2.9.5.tar.gz</span><br><span class="line">cd git-2.9.5</span><br><span class="line"></span><br><span class="line">make configure</span><br><span class="line">./configure --prefix=/usr/local/git --with-iconv=/usr/local/libiconv</span><br><span class="line"><span class="meta">#</span><span class="bash"> 配置安装路径</span></span><br><span class="line">make all doc</span><br><span class="line"><span class="meta">#</span><span class="bash"> 编译</span></span><br><span class="line">make install install-doc install-html</span><br><span class="line"><span class="meta">#</span><span class="bash"> 安装</span></span><br><span class="line"></span><br><span class="line">修改环境变量</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> <span class="built_in">echo</span> -e <span class="string">&quot;# git\nexport PATH=/usr/local/git/bin:\$PATH&quot;</span>&gt; /etc/profile.d/git.sh</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> cat /etc/profile.d/git.sh</span></span><br><span class="line">\# git        // 文件内容</span><br><span class="line">export PATH=/usr/local/git/bin:$PATH</span><br><span class="line"><span class="meta">#</span><span class="bash"> <span class="built_in">source</span> /etc/profile</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>安装oh-my-zsh</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line">yum install -y zsh</span><br><span class="line">chsh -s /bin/zsh</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">sh -c &quot;$(curl -fsSL https://raw.githubusercontent.com/robbyrussell/oh-my-zsh/master/tools/install.sh)&quot;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">git clone https://github.com/zsh-users/zsh-autosuggestions $&#123;ZSH_CUSTOM:-~/.oh-my-zsh/custom&#125;/plugins/zsh-autosuggestions</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>编辑~/.zshrc文件</p>
<p>找到plugins=(git)这一行，然后再添加autosuggestions，最后为：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">plugins&#x3D;(git zsh-autosuggestions)</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker container cp target/demo-0.0.1-SNAPSHOT.jar centos-one:/home/deploy/</span><br></pre></td></tr></table></figure>



<h3 id="增加端口，做远程debug"><a href="#增加端口，做远程debug" class="headerlink" title="增加端口，做远程debug"></a>增加端口，做远程debug</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">docker commit 95c6d4eed5f0 jdk8-debug</span><br><span class="line"></span><br><span class="line">docker run --cap-add=SYS_PTRACE --security-opt seccomp:unconfined --name centos-jdk8-debug  -d -p 8000:8000 -p  8001:8001 -p 8002:8002  -P jdk8-debug</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">docker exec -it centos-jdk8-debug zsh</span><br></pre></td></tr></table></figure>
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://example.com/bigdata/Flink/Flink-checkpoint-savepoint-2pc/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="aaronzhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Guadazi">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/bigdata/Flink/Flink-checkpoint-savepoint-2pc/" class="post-title-link" itemprop="url">Flink-checkpoint与高可用</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2019-08-10 14:58:00" itemprop="dateCreated datePublished" datetime="2019-08-10T14:58:00+08:00">2019-08-10</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-06-01 20:52:39" itemprop="dateModified" datetime="2021-06-01T20:52:39+08:00">2021-06-01</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/bigdata/" itemprop="url" rel="index"><span itemprop="name">bigdata</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>Flink Checkpoint 受 Chandy-Lamport 分布式快照启发，可以保证数据的高可用。但是有些情况下，不见得一定有效:</p>
<p>Flink On Yarn 模式，某个 Container 发生 OOM 异常，这种情况程序直接变成失败状态，此时 Flink 程序虽然开启 Checkpoint 也无法恢复，因为程序已经变成失败状态，所以此时可以借助外部参与启动程序，比如外部程序检测到实时任务失败时，从新对实时任务进行拉起。</p>
<h2 id="1-1-2PC"><a href="#1-1-2PC" class="headerlink" title="1.1. 2PC"></a>1.1. 2PC</h2><h3 id="1-1-1-Exactly-once-VS-At-least-once"><a href="#1-1-1-Exactly-once-VS-At-least-once" class="headerlink" title="1.1.1. Exactly-once VS At-least-once"></a>1.1.1. Exactly-once VS At-least-once</h3><p>算子做快照时，如果等所有输入端的barrier都到了才开始做快照，可保证算子的exactly-once；如果为了降低延时而跳过对齐，从而继续处理数据，那么等barrier都到齐后做快照就是at-least-once了，因为这次的快照掺杂了下一次快照的数据，当作业失败恢复的时候，这些数据会重复作用系统，就好像这些数据被消费了两遍。</p>
<p>注：对齐只会发生在算子的上端是join操作以及上游存在partition或者shuffle的情况，对于直连操作类似map、flatMap、filter等还是会保证exactly-once的语义。</p>
<h3 id="1-1-2-端到端的Exactly-once实现"><a href="#1-1-2-端到端的Exactly-once实现" class="headerlink" title="1.1.2. 端到端的Exactly once实现"></a>1.1.2. 端到端的Exactly once实现</h3><p>2PC分为几个阶段: 开始事务-&gt;预提交-&gt;提交(或回滚)</p>
<p>为了保证Exactly once, source和sink必须支持flink的2PC</p>
<p>当状态涉及到外部系统时，需要外部系统支持事务操作来配合flink实现2PC协议，从而保证数据的exatly-once。<br>这个时候，sink算子除了将自己的state写到后段，还必须准备好事务提交。</p>
<ul>
<li>一旦所有的算子完成了它们的pre-commit，它们会要求一个commit。</li>
<li>如果存在一个算子pre-commit失败了，本次事务失败，我们回滚到上次的checkpoint。</li>
<li>一旦master做出了commit的决定，那么这个commit必须得到执行，就算宕机恢复也有继续执行。</li>
</ul>
<h4 id="1-1-2-1-pre-commit"><a href="#1-1-2-1-pre-commit" class="headerlink" title="1.1.2.1. pre-commit"></a>1.1.2.1. pre-commit</h4><p>pre-commit阶段起始于一次快照的开始，即master节点将checkpoint的barrier注入source端，barrier随着数据向下流动直到sink端。barrier每到一个算子，都会出发算子做本地快照。</p>
<p><img src="_v_images/20200710154629243_751269158.png" alt="precommit"></p>
<p>当所有的算子都做完了本地快照并且回复到master节点时，pre-commit阶段才算结束。这个时候，checkpoint已经成功，并且包含了外部系统的状态。如果作业失败，可以进行恢复。</p>
<p><img src="_v_images/20200710154750060_1524377793.png" alt="precommit-success"></p>
<h4 id="1-1-2-2-commit"><a href="#1-1-2-2-commit" class="headerlink" title="1.1.2.2. commit"></a>1.1.2.2. commit</h4><p>通知所有的算子这次checkpoint成功了，即2PC的commit阶段。source节点和window节点没有外部状态，所以这时它们不需要做任何操作。<br>而对于sink节点，需要commit这次事务，将数据写到外部系统。</p>
<p><img src="_v_images/20200710154844838_737658241.png" alt="commit"></p>
<h4 id="1-1-2-3-rollback"><a href="#1-1-2-3-rollback" class="headerlink" title="1.1.2.3. rollback"></a>1.1.2.3. rollback</h4><p>一旦任何一个算子的快照保存失败，则触发回滚，同样的sink算子也需要取消写入外部的数据</p>
<h3 id="1-1-3-TwoPhaseCommitSinkFunction"><a href="#1-1-3-TwoPhaseCommitSinkFunction" class="headerlink" title="1.1.3. TwoPhaseCommitSinkFunction"></a>1.1.3. TwoPhaseCommitSinkFunction</h3><p>为了简化2PC的实现成本，flink抽象了TwoPhaseCommitSinkFunction</p>
<ul>
<li>beginTransaction。开始一次事务，在目的文件系统创建一个临时文件。接下来我们就可以将数据写到这个文件。</li>
<li>preCommit。在这个阶段，将文件flush掉，同时重起一个文件写入，作为下一次事务的开始。</li>
<li>commit。这个阶段，将文件写到真正的目的目录。值得注意的是，这会增加数据可视的延时。</li>
<li>abort。如果回滚，那么删除临时文件。</li>
</ul>
<p>如果pre-commit成功了，但是commit没有到达算子旧宕机了，flink会将算子恢复到pre-commit时的状态，然后继续commit。</p>
<p>我们需要做的还有就是保证commit的幂等性，这可以通过检查临时文件是否还在来实现。</p>
<h2 id="1-2-checkpoint"><a href="#1-2-checkpoint" class="headerlink" title="1.2. checkpoint"></a>1.2. checkpoint</h2><p><strong>保留策略</strong>:</p>
<ul>
<li>DELETE_ON_CANCELLATION 表示当程序取消时，删除 Checkpoint 存储文件。</li>
<li>RETAIN_ON_CANCELLATION 表示当程序取消时，保存之前的 Checkpoint 存储文件</li>
</ul>
<p>默认情况下，Flink不会触发一次 Checkpoint 当系统有其他 Checkpoint 在进行时，也就是说 Checkpoint 默认的并发为1。</p>
<p><strong>CheckpointCoordinator</strong> :</p>
<p>针对 Flink DataStream 任务，程序需要经历从 StreamGraph -&gt; JobGraph -&gt; ExecutionGraph -&gt; 物理执行图四个步骤，其中在 ExecutionGraph 构建时，会初始化 CheckpointCoordinator。ExecutionGraph通过ExecutionGraphBuilder.buildGraph方法构建，在构建完时，会调用 ExecutionGraph 的enableCheckpointing方法创建CheckpointCoordinator</p>
<p><strong>Flink Checkpoint 参数配置及建议</strong>:</p>
<ul>
<li>当 Checkpoint 时间比设置的 Checkpoint 间隔时间要长时，可以设置 Checkpoint 间最小时间间隔 。这样在上次 Checkpoint 完成时，不会立马进行下一次 Checkpoint，而是会等待一个最小时间间隔，然后在进行该次 Checkpoint。否则，每次 Checkpoint 完成时，就会立马开始下一次 Checkpoint，系统会有很多资源消耗 Checkpoint。</li>
<li>如果Flink状态很大，在进行恢复时，需要从远程存储读取状态恢复，此时可能导致任务恢复很慢，可以设置 Flink Task 本地状态恢复。任务状态本地恢复默认没有开启，可以设置参数state.backend.local-recovery值为true进行激活。</li>
<li>Checkpoint保存数，Checkpoint 保存数默认是1，也就是保存最新的 Checkpoint 文件，当进行状态恢复时，如果最新的Checkpoint文件不可用时(比如HDFS文件所有副本都损坏或者其他原因)，那么状态恢复就会失败，如果设置 Checkpoint 保存数2，即使最新的Checkpoint恢复失败，那么Flink 会回滚到之前那一次Checkpoint进行恢复。考虑到这种情况，用户可以增加 Checkpoint 保存数。</li>
<li>建议设置的 Checkpoint 的间隔时间最好大于 Checkpoint 的完成时间。</li>
</ul>
<p>下图是不设置 Checkpoint 最小时间间隔示例图，可以看到，系统一致在进行 Checkpoint，可能对运行的任务产生一定影响：<br><img src="_v_images/20200714095846469_121820659.png"></p>
<h2 id="1-3-savepoint"><a href="#1-3-savepoint" class="headerlink" title="1.3. savepoint"></a>1.3. savepoint</h2><blockquote>
<p>注意:<br>使用DataStream进行开发，建议为每个算子定义一个 uid，这样我们在修改作业时，即使导致程序拓扑图改变，由于相关算子 uid 没有变，那么这些算子还能够继续使用之前的状态，如果用户没有定义 uid ， Flink 会为每个算子自动生成 uid，如果用户修改了程序，可能导致之前的状态程序不能再进行复用。</p>
</blockquote>
<p>Flink 在触发Savepoint 或者 Checkpoint时，会根据这次触发的类型计算出在HDFS上面的目录:</p>
<p>如果类型是 Savepoint，那么 其 HDFS 上面的目录为：Savepoint 根目录+savepoint-jobid前六位+随机数字，具体如下格式：</p>
<p><img src="_v_images/20200714100459823_887900222.png"></p>
<p>Checkpoint 目录为 chk-checkpoint ID,具体格式如下：</p>
<p><img src="_v_images/20200714100516223_630729421.png"></p>
<ul>
<li>使用 flink cancel -s 命令取消作业同时触发 Savepoint 时，会有一个问题，可能存在触发 Savepoint 失败。比如实时程序处于异常状态(比如 Checkpoint失败)，而此时你停止作业，同时触发 Savepoint,这次 Savepoint 就会失败，这种情况会导致，在实时平台上面看到任务已经停止，但是实际实时作业在 Yarn 还在运行。针对这种情况，需要捕获触发 Savepoint 失败的异常，当抛出异常时，可以直接在 Yarn 上面 Kill 掉该任务。</li>
<li>使用 DataStream 程序开发时，最好为每个算子分配 uid,这样即使作业拓扑图变了，相关算子还是能够从之前的状态进行恢复，默认情况下，Flink 会为每个算子分配 uid,这种情况下，当你改变了程序的某些逻辑时，可能导致算子的 uid 发生改变，那么之前的状态数据，就不能进行复用，程序在启动的时候，就会报错。</li>
<li>由于 Savepoint 是程序的全局状态，对于某些状态很大的实时任务，当我们触发 Savepoint，可能会对运行着的实时任务产生影响，个人建议如果对于状态过大的实时任务，触发 Savepoint 的时间，不要太过频繁。根据状态的大小，适当的设置触发时间。</li>
<li>当我们从 Savepoint 进行恢复时，需要检查这次 Savepoint 目录文件是否可用。可能存在你上次触发 Savepoint 没有成功，导致 HDFS 目录上面 Savepoint 文件不可用或者缺少数据文件等，这种情况下，如果在指定损坏的 Savepoint 的状态目录进行状态恢复，任务会启动不起来。</li>
</ul>
<h2 id="1-4-snapshot保存到哪里-应该需要汇总到jobManager？"><a href="#1-4-snapshot保存到哪里-应该需要汇总到jobManager？" class="headerlink" title="1.4. snapshot保存到哪里? 应该需要汇总到jobManager？"></a>1.4. snapshot保存到哪里? 应该需要汇总到jobManager？</h2><h2 id="1-5-state-backend"><a href="#1-5-state-backend" class="headerlink" title="1.5. state backend"></a>1.5. state backend</h2><p><img src="_v_images/20200713183839429_2053265091.png"></p>
<h3 id="FsStateBackend"><a href="#FsStateBackend" class="headerlink" title="FsStateBackend"></a>FsStateBackend</h3><p>构造方法:<br><code>FsStateBackend(URI checkpointDataUri,boolean asynchronousSnapshots)</code></p>
<p>1 基于文件系统的状态管理器<br>2 如果使用，默认是异步<br>3 比较稳定，3个副本，比较安全。不会出现任务无法恢复等问题<br>4 状态大小受磁盘容量限制</p>
<p>存储方式:</p>
<ul>
<li>State: TaskManager内存</li>
<li>checkpoint: 外部文件系统(本地或HDFS)</li>
</ul>
<p>容量限制:</p>
<ul>
<li>单TaskManager上State总量不超过它的内存</li>
<li>总大小不超过配置的文件系统容量</li>
</ul>
<p>推荐使用场景:</p>
<ul>
<li>常规使用状态的作业，例如分钟级窗口聚合、join、窗口比较长、kv状态大；需要开启HA的作业</li>
<li>可以用于生产场景</li>
</ul>
<h3 id="RocksDBStateBackend"><a href="#RocksDBStateBackend" class="headerlink" title="RocksDBStateBackend"></a>RocksDBStateBackend</h3><p>状态数据先写入RocksDB，然后异步的将状态数据写入文件系统。正在进行计算的热数据存储在RocksDB，长时间才更新的数据写入磁盘中（文件系统）存储，体量比较小的元数据状态写入JobManager内存中（将工作state保存在RocksDB中，并且默认将checkpoint数据存在文件系统中）</p>
<p>目前唯一支持incremental的checkpoints的策略</p>
<p>构造方法:<br><code>RocksDBStateBackend(URI checkpointDataUri,boolean enableIncrementalCheckpointing)</code></p>
<p>存储方式:</p>
<ul>
<li>State: TaskManager上的KV数据库(实际使用内存+硬盘)</li>
<li>Checkpoint: 外部文件系统(本地或HDFS)</li>
</ul>
<p>容量限制:</p>
<ul>
<li>单TaskManager上的State总量不超过他的内存+磁盘</li>
<li>单key最大2G</li>
<li>总大小不超过配置的文件系统容量</li>
</ul>
<p>推荐使用的场景:</p>
<ul>
<li>超大状态的作业，例如天级别窗口聚合；需要开启HA的作业；对状态读写性能要求不高的作业</li>
<li>可以在生产环境使用</li>
</ul>
<h3 id="MemoryStateBackend"><a href="#MemoryStateBackend" class="headerlink" title="MemoryStateBackend"></a>MemoryStateBackend</h3><p>构造方法:<br><code>MemoryStateBackend(int maxStateSize, boolean asynchronousSnapshots)</code></p>
<p>主机内存中的数据可能会丢失，任务可能无法恢复</p>
<p>存储方式:</p>
<ul>
<li>State: TaskManager内存</li>
<li>Checkpoint: JobManager内存</li>
</ul>
<p>容量限制</p>
<ul>
<li>单个State maxStateSize默认5M</li>
<li>maxStateSize &lt;= akka.frameSize 默认10M</li>
<li>总大小不超过JobManager的内存</li>
</ul>
<p>推荐使用场景：</p>
<ul>
<li>本地测试；几乎无状态的作业，比如ETL；JobManager不容易挂，或挂掉影响不大的情况</li>
<li>不推荐在生产环境使用</li>
</ul>
<h2 id="1-6-checkpoint-与-savepoint"><a href="#1-6-checkpoint-与-savepoint" class="headerlink" title="1.6. checkpoint 与 savepoint"></a>1.6. checkpoint 与 savepoint</h2><p>Checkpoint指定触发生成时间间隔后，每当需要触发Checkpoint时，会向Flink程序运行时的多个分布式的Stream Source中插入一个Barrier标记，这些Barrier会根据Stream中的数据记录一起流向下游的各个Operator。<br>当一个Operator接收到一个Barrier时，它会暂停处理Steam中新接收到的数据记录。<br>因为一个Operator可能存在多个输入的Stream，而每个Stream中都会存在对应的Barrier，该Operator要等到所有的输入Stream中的Barrier都到达。(<strong>对齐</strong>)<br>当所有Stream中的Barrier都已经到达该Operator，这时所有的Barrier在时间上看来是同一个时刻点（表示已经对齐），在等待所有Barrier到达的过程中，<br>Operator的Buffer中可能已经缓存了一些比Barrier早到达Operator的数据记录（Outgoing Records），这时该Operator会将数据记录（Outgoing Records）发射（Emit）出去，作为下游Operator的输入，<br>最后将Barrier对应Snapshot发射（Emit）出去作为此次Checkpoint的结果数据。</p>
<p>Checkpoint 是增量做的，每次的时间较短，数据量较小，只要在程序里面启用后会自动触发，用户无须感知；Checkpoint 是作业 failover 的时候自动使用，不需要用户指定。</p>
<p>Savepoint 是全量做的，每次的时间较长，数据量较大，需要用户主动去触发。Savepoint 一般用于程序的版本更新（详见文档），Bug 修复，A/B Test 等场景，需要用户指定。</p>
<p><strong>保存的内容</strong></p>
<ul>
<li>首先，Savepoint 包含了一个目录，其中包含（通常很大的）二进制文件，这些文件表示了整个流应用在 Checkpoint/Savepoint 时的状态。</li>
<li>以及一个（相对较小的）元数据文件，包含了指向 Savapoint 各个文件的指针，并存储在所选的分布式文件系统或数据存储中。</li>
</ul>
<p><strong>目标</strong></p>
<p>Savepoint 和 Checkpoint 的不同之处很像传统数据库中备份与恢复日志之间的区别。Checkpoint 的主要目标是充当 Flink 中的恢复机制，确保能从潜在的故障中恢复。相反，Savepoint 的主要目标是充当手动备份、恢复暂停作业的方法。</p>
<p><strong>实现</strong></p>
<p>Checkpoint 被设计成轻量和快速的机制。它们可能（但不一定必须）利用底层状态后端的不同功能尽可能快速地恢复数据。例如，基于 RocksDB 状态后端的增量检查点，能够加速 RocksDB 的 checkpoint 过程，这使得 checkpoint 机制变得更加轻量。相反，Savepoint 旨在更多地关注数据的可移植性，并支持对作业做任何更改而状态能保持兼容，这使得生成和恢复的成本更高</p>
<p><strong>状态文件保留策略</strong></p>
<p>Checkpoint默认程序删除，可以设置CheckpointConfig中的参数进行保留 。Savepoint会一直保存，除非用户删除 。</p>
<p><strong>应用</strong></p>
<ul>
<li>部署流应用的一个新版本，包括新功能、BUG 修复、或者一个更好的机器学习模型</li>
<li>引入 A/B 测试，使用相同的源数据测试程序的不同版本，从同一时间点开始测试而不牺牲先前的状态</li>
<li>在需要更多资源时扩容应用程序</li>
<li>迁移流应用程序到 Flink 的新版本上，或者迁移到另一个集群</li>
</ul>
<h1 id="Flink数据一致性"><a href="#Flink数据一致性" class="headerlink" title="Flink数据一致性"></a>Flink数据一致性</h1><h2 id="一、综述"><a href="#一、综述" class="headerlink" title="一、综述"></a>一、综述</h2><p><strong>flink 通过内部依赖checkpoint 并且可以通过设置其参数exactly-once 实现其内部的一致性</strong>。但要实现其端到端的一致性，还必须保证<br>1、source 外部数据源可重设数据的读取位置<br>2、sink端 需要保证数据从故障恢复时，数据不会重复写入外部系统（或者可以逻辑实现写入多次，但只有一次生效的数据sink端）</p>
<h2 id="二、sink-端到端实现方式"><a href="#二、sink-端到端实现方式" class="headerlink" title="二、sink 端到端实现方式"></a>二、sink 端到端实现方式</h2><p><strong>幂等操作：</strong><br>一个操作，可以重复执行多次，但只导致一次结果更改，豁免重复操作执行就不起作用了，他的瑕疵 （在系统恢复的过程中，如果这段时间内多个更新或者插入导致状态不一致，但当数据追上就可以了）<br>（逻辑与、逻辑或等）具体理解参照自己以前写的文章。<br><strong>事务写入：</strong><br>事务应该具有四个属性：原子性、一致性、隔离性、持久性等。其具体的实现方式有两种<br><strong>（1）、预写日志</strong><br>简单易于实现，由于数据提前在状态后端中做了缓存，所以无论什么sink系统，都能用这种方式一批搞定，DataStream API提供了一个模板类：GenericWriteAheadSink，来实现这种事务性sink；<br>缺点：<br>1）、sink系统没说他支持事务。有可能出现一部分写入集群了。一部分没有写进去（如果实表，再写一次就写重复了）<br>2）、checkpoint做完了sink才去真正的写入（但其实得等sink都写完checkpoint才能生效，所以WAL这个机制jobmanager确定它写完还不算真正写完，还得有一个外部系统已经确认 完成的checkpoint）<br>（<strong>2）、两阶段提交。 flink 真正实现exactle-once</strong><br>对于每个checkpoint,sink 任务会启动一个事务，并将接下来所有接收的数据添加到事务中，然后将这些数据写入外部sink系统，但不提交他们（这里是预提交）。当checkpoint完成时的通知，它才正式提交事务，实现结果的真正写入；这种方式真正实现了exactly-once,它需要一个提供事务支持的外部sink系统，Flink提供了其具体实现（TwoPhaseCommitSinkFunction接口）</p>
<h2 id="三、-2pc-对外部-sink的要求"><a href="#三、-2pc-对外部-sink的要求" class="headerlink" title="三、 2pc 对外部 sink的要求"></a>三、 2pc 对外部 sink的要求</h2><p>1、外部sink系统必须事务支持，或者sink任务必须能够模拟外部系统上的事务；<br>2、在checkpoint的间隔期间里，必须能够开启一个事务，并接受数据写入。<br>3、在收到checkpoint完成通知之前，事务必须是“等待提交”的状态，在故障恢复的情况线，这可能需要一些时间。如果个时候sink系统关闭事务（例如超时了），那么未提交的数据就会丢失；<br>4、四年任务必选能够在进程失败后恢复事务<br>5、提交事务必须是幂等操作；</p>
<p>四、综上不同Source和sink的一致性保证：<br><img src="_v_images/20201208154240979_1471214802.png" alt="在这里插入图片描述"></p>
<h2 id="五、应用（flinK-kafka-端到端一致性保证）"><a href="#五、应用（flinK-kafka-端到端一致性保证）" class="headerlink" title="五、应用（flinK+kafka 端到端一致性保证）"></a>五、应用（flinK+kafka 端到端一致性保证）</h2><p>flink 和kafka 端到端一致性(kafka(source+flink+kafka(sink)))<br>1、内部 – 利用checkpoint机制，把状态存盘，发生故障的时候可以恢复，保证内部的状态一致性<br>2、source – kafka consumer作为source，可以将偏移量保存下来，如果后续任务出现了故障，恢复的时候可以由连接器重置偏移量，重新消费数据，保证一致性；</p>
<figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-tag">kafka</span> 0.8 和<span class="selector-tag">kafka</span> 0.11 之后 通过以下配置将偏移量保存，恢复时候重新消费</span><br><span class="line"> <span class="selector-tag">kafka</span><span class="selector-class">.setStartFromLatest</span>();</span><br><span class="line"> <span class="selector-tag">kafka</span><span class="selector-class">.setCommitOffsetsOnCheckpoints</span>(<span class="selector-tag">false</span>);</span><br><span class="line"> <span class="selector-tag">kafka</span> 0.9 和<span class="selector-tag">kafka0</span>.10 未验证是否支持这两个参数(<span class="selector-tag">todo</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>3、sink FlinkkafkaProducer作为Sink，采用两阶段提交的sink，由下图可以看出flink 0.11 已经默认继承了TwoPhaseCommitSinkFunction<br><img src="_v_images/20201208154240548_914126344.png" alt="在这里插入图片描述"><br>但我们需要在参数种传入指定语义，它默认时还是at-least-once<br>此外我们还需要进行一些producer的容错配置：<br>（1）除了启用Flink的检查点之外，还可以通过将适当的semantic参数传递给FlinkKafkaProducer011（FlinkKafkaProducer对于Kafka&gt; = 1.0.0版本）<br>（2）来选择三种不同的操作模式<br>1）、Semantic.NONE 代表at-mostly-once语义<br>2）、Semantic.AT_LEAST_ONCE（Flink默认设置<br>3）、Semantic.EXACTLY_ONCE 使用Kafka事务提供一次精确的语义，每当您使用事务写入Kafka时<br>（3）、请不要忘记消费kafka记录任何应用程序设置所需的设置isolation.leva（read_committed 或者read_uncommitted-后者是默认）<br>read_committed，只是读取已经提交的数据。</p>
<p>应用；<br>Semantic.EXACTLY_ONCE依赖与下游系统能支持事务操作.以0.11kafka为例.<br>transaction.max.timeout.ms 最大超市时长，默认15分钟，如果需要用exactly语义，需要增加这个值。（因为它小于transaction.timeout.ms ）<br>isolation.level 如果需要用到exactly语义，需要在下级consumerConfig中设置read-commited [read-uncommited(默认值)]<br>transaction.timeout.ms 默认为1hour</p>
<p><strong>其参数对应关系为 和一些报错问题<br>checkpoint间隔&lt;transaction.timeout.ms&lt;transaction.max.timeout.ms</strong></p>
<p><strong>参考：<a target="_blank" rel="noopener" href="https://www.cnblogs.com/createweb/p/11971846.html">https://www.cnblogs.com/createweb/p/11971846.html</a></strong></p>
<p>注意：<br>1、Semantic.EXACTLY_ONCE 模式每个FlinkKafkaProducer011实例使用一个固定大小的KafkaProducers池。每个检查点使用这些生产者中的每一个。如果并发检查点的数量超过池大小，FlinkKafkaProducer011 将引发异常，并使整个应用程序失败。请相应地配置最大池大小和最大并发检查点数。</p>
<p>2、Semantic.EXACTLY_ONCE采取所有可能的措施，不要留下任何挥之不去的数据，否则这将有碍于消费者更多地阅读Kafka主题。但是，如果flink应用程序在第一个检查点之前失败，则在重新启动此类应用程序后，系统种将没有有关先前池大小信息，因此，在第一个检查点完成前按比例缩小Flink应用程序的FlinkKafkaProducer011.SAFE_SCALE_DOWN_FACTOR</p>
<figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">//1。设置最大允许的并行<span class="selector-tag">checkpoint</span>数，防止超过<span class="selector-tag">producer</span>池的个数发生异常</span><br><span class="line"><span class="selector-tag">env</span><span class="selector-class">.getCheckpointConfig</span><span class="selector-class">.setMaxConcurrentCheckpoints</span>(5) </span><br><span class="line">//2。设置<span class="selector-tag">producer</span>的<span class="selector-tag">ack</span>传输配置</span><br><span class="line">// 设置超市时长，默认15分钟，建议1个小时以上</span><br><span class="line"><span class="selector-tag">producerConfig</span><span class="selector-class">.put</span>(<span class="selector-tag">ProducerConfig</span><span class="selector-class">.ACKS_CONFIG</span>, 1) </span><br><span class="line"><span class="selector-tag">producerConfig</span><span class="selector-class">.put</span>(<span class="selector-tag">ProducerConfig</span><span class="selector-class">.TRANSACTION_TIMEOUT_CONFIG</span>, 15000) </span><br><span class="line"></span><br><span class="line">//3。在下一个<span class="selector-tag">kafka</span> <span class="selector-tag">consumer</span>的配置文件，或者代码中设置<span class="selector-tag">ISOLATION_LEVEL_CONFIG-read-commited</span></span><br><span class="line">//<span class="selector-tag">Note</span>:必须在下一个<span class="selector-tag">consumer</span>中指定，当前指定是没用用的</span><br><span class="line"><span class="selector-tag">kafkaonfigs</span><span class="selector-class">.setProperty</span>(<span class="selector-tag">ConsumerConfig</span><span class="selector-class">.ISOLATION_LEVEL_CONFIG</span>,&quot;<span class="selector-tag">read_commited</span>&quot;)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>完整应用代码：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.shufang.flink.connectors</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.Properties</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.serialization.SimpleStringSchema</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.TimeCharacteristic</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.functions.timestamps.BoundedOutOfOrdernessTimestampExtractor</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala._</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.windowing.time.Time</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer.Semantic</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.connectors.kafka._</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.util.serialization.KeyedSerializationSchemaWrapper</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.ConsumerConfig</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.ProducerConfig</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.common.serialization.StringDeserializer</span><br><span class="line"></span><br><span class="line">object KafkaSource01 &#123;</span><br><span class="line">  <span class="function">def <span class="title">main</span><span class="params">(args: Array[String])</span>: Unit </span>= &#123;</span><br><span class="line">    val env: StreamExecutionEnvironment = StreamExecutionEnvironment.getExecutionEnvironment</span><br><span class="line"></span><br><span class="line">    env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//这是checkpoint的超时时间</span></span><br><span class="line">    <span class="comment">//env.getCheckpointConfig.setCheckpointTimeout()</span></span><br><span class="line">    <span class="comment">//设置最大并行的chekpoint</span></span><br><span class="line">    env.getCheckpointConfig.setMaxConcurrentCheckpoints(<span class="number">5</span>)</span><br><span class="line">    env.getCheckpointConfig.setCheckpointInterval(<span class="number">1000</span>) <span class="comment">//增加checkpoint的中间时长，保证可靠性</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 为了保证数据的一致性，我们开启Flink的checkpoint一致性检查点机制，保证容错</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    env.enableCheckpointing(<span class="number">60000</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 从kafka获取数据，一定要记得添加checkpoint，能保证offset的状态可以重置，从数据源保证数据的一致性</span></span><br><span class="line"><span class="comment">     * 保证kafka代理的offset与checkpoint备份中保持状态一致</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line"></span><br><span class="line">    val kafkaonfigs = <span class="keyword">new</span> Properties()</span><br><span class="line"></span><br><span class="line">    <span class="comment">//指定kafka的启动集群</span></span><br><span class="line">    kafkaonfigs.setProperty(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, <span class="string">&quot;localhost:9092&quot;</span>)</span><br><span class="line">    <span class="comment">//指定消费者组</span></span><br><span class="line">    kafkaonfigs.setProperty(ConsumerConfig.GROUP_ID_CONFIG, <span class="string">&quot;flinkConsumer&quot;</span>)</span><br><span class="line">    <span class="comment">//指定key的反序列化类型</span></span><br><span class="line">    kafkaonfigs.setProperty(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, classOf[StringDeserializer].getName)</span><br><span class="line">    <span class="comment">//指定value的反序列化类型</span></span><br><span class="line">    kafkaonfigs.setProperty(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, classOf[StringDeserializer].getName)</span><br><span class="line">    <span class="comment">//指定自动消费offset的起点配置</span></span><br><span class="line">    <span class="comment">//    kafkaonfigs.setProperty(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, &quot;latest&quot;)</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 自定义kafkaConsumer，同时可以指定从哪里开始消费</span></span><br><span class="line"><span class="comment">     * 开启了Flink的检查点之后，我们还要开启kafka-offset的检查点，通过kafkaConsumer.setCommitOffsetsOnCheckpoints(true)开启，</span></span><br><span class="line"><span class="comment">     * 一旦这个检查点开启，那么之前配置的 auto-commit-enable = true的配置就会自动失效</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    val kafkaConsumer = <span class="keyword">new</span> FlinkKafkaConsumer[String](</span><br><span class="line">      <span class="string">&quot;console-topic&quot;</span>,</span><br><span class="line">      <span class="keyword">new</span> SimpleStringSchema(), <span class="comment">// 这个schema是将kafka的数据应设成Flink中的String类型</span></span><br><span class="line">      kafkaonfigs</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 开启kafka-offset检查点状态保存机制</span></span><br><span class="line">    kafkaConsumer.setCommitOffsetsOnCheckpoints(<span class="keyword">true</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//    kafkaConsumer.setStartFromEarliest()//</span></span><br><span class="line">    <span class="comment">//    kafkaConsumer.setStartFromTimestamp(1010003794)</span></span><br><span class="line">    <span class="comment">//    kafkaConsumer.setStartFromLatest()</span></span><br><span class="line">    <span class="comment">//    kafkaConsumer.setStartFromSpecificOffsets(Map[KafkaTopicPartition,Long]()</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 添加source数据源</span></span><br><span class="line">    val kafkaStream: DataStream[String] = env.addSource(kafkaConsumer)</span><br><span class="line"></span><br><span class="line">    kafkaStream.print()</span><br><span class="line"></span><br><span class="line">    val sinkStream: DataStream[String] = kafkaStream.assignTimestampsAndWatermarks(<span class="keyword">new</span> BoundedOutOfOrdernessTimestampExtractor[String](Time.seconds(<span class="number">5</span>)) &#123;</span><br><span class="line">      <span class="function">override def <span class="title">extractTimestamp</span><span class="params">(element: String)</span>: Long </span>= &#123;</span><br><span class="line">        element.split(<span class="string">&quot;,&quot;</span>)(<span class="number">1</span>).toLong</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 通过FlinkkafkaProduccer API将stream的数据写入到kafka的&#x27;sink-topic&#x27;中</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="comment">//    val brokerList = &quot;localhost:9092&quot;</span></span><br><span class="line">    val topic = <span class="string">&quot;sink-topic&quot;</span></span><br><span class="line">    val producerConfig = <span class="keyword">new</span> Properties()</span><br><span class="line">    producerConfig.put(ProducerConfig.ACKS_CONFIG, <span class="keyword">new</span> Integer(<span class="number">1</span>)) <span class="comment">// 设置producer的ack传输配置</span></span><br><span class="line">    producerConfig.put(ProducerConfig.TRANSACTION_TIMEOUT_CONFIG, Time.hours(<span class="number">2</span>)) <span class="comment">//设置超市时长，默认1小时，建议1个小时以上</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 自定义producer，可以通过不同的构造器创建</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    val producer: FlinkKafkaProducer[String] = <span class="keyword">new</span> FlinkKafkaProducer[String](</span><br><span class="line">      topic,</span><br><span class="line">      <span class="keyword">new</span> KeyedSerializationSchemaWrapper[String](SimpleStringSchema),</span><br><span class="line">      producerConfig,</span><br><span class="line">      Semantic.EXACTLY_ONCE</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment">//    FlinkKafkaProducer.SAFE_SCALE_DOWN_FACTOR</span></span><br><span class="line">    <span class="comment">/** *****************************************************************************************************************</span></span><br><span class="line"><span class="comment">     * * 出了要开启flink的checkpoint功能，同时还要设置相关配置功能。</span></span><br><span class="line"><span class="comment">     * * 因在0.9或者0.10，默认的FlinkKafkaProducer只能保证at-least-once语义，假如需要满足at-least-once语义，我们还需要设置</span></span><br><span class="line"><span class="comment">     * * setLogFailuresOnly(boolean)    默认false</span></span><br><span class="line"><span class="comment">     * * setFlushOnCheckpoint(boolean)  默认true</span></span><br><span class="line"><span class="comment">     * * come from 官网 below：</span></span><br><span class="line"><span class="comment">     * * Besides enabling Flink’s checkpointing，you should also configure the setter methods setLogFailuresOnly(boolean)</span></span><br><span class="line"><span class="comment">     * * and setFlushOnCheckpoint(boolean) appropriately.</span></span><br><span class="line"><span class="comment">     * ******************************************************************************************************************/</span></span><br><span class="line"></span><br><span class="line">    producer.setLogFailuresOnly(<span class="keyword">false</span>) <span class="comment">//默认是false</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 除了启用Flink的检查点之外，还可以通过将适当的semantic参数传递给FlinkKafkaProducer011（FlinkKafkaProducer对于Kafka&gt; = 1.0.0版本）</span></span><br><span class="line"><span class="comment">     * 来选择三种不同的操作模式：</span></span><br><span class="line"><span class="comment">     * Semantic.NONE  代表at-mostly-once语义</span></span><br><span class="line"><span class="comment">     * Semantic.AT_LEAST_ONCE（Flink默认设置）</span></span><br><span class="line"><span class="comment">     * Semantic.EXACTLY_ONCE：使用Kafka事务提供一次精确的语义，每当您使用事务写入Kafka时，</span></span><br><span class="line"><span class="comment">     * 请不要忘记为使用Kafka记录的任何应用程序设置所需的设置isolation.level（read_committed 或read_uncommitted-后者是默认值)</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line"></span><br><span class="line">    sinkStream.addSink(producer)</span><br><span class="line"></span><br><span class="line">    env.execute(<span class="string">&quot;kafka source &amp; sink&quot;</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="[参考文献]"></a>[参考文献]</h2><ol>
<li><a target="_blank" rel="noopener" href="http://shiyanjun.cn/archives/1855.html">Flink Checkpoint、Savepoint配置与实践</a></li>
<li><a target="_blank" rel="noopener" href="http://wuchong.me/blog/2018/11/04/how-apache-flink-manages-kafka-consumer-offsets/">Flink 小贴士 (2)：Flink 如何管理 Kafka 消费位点</a></li>
<li><a target="_blank" rel="noopener" href="https://www.jianshu.com/p/4bcbcda0e2f4">Flink实时计算-深入理解Checkpoint和Savepoint</a></li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1506.08603">Lightweight Asynchronous Snapshots for Distributed Dataflows: 分布式数据流轻量级异步快照</a></li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://example.com/bigdata/Flink/Flink-TableAPI/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="aaronzhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Guadazi">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/bigdata/Flink/Flink-TableAPI/" class="post-title-link" itemprop="url">Flink:Table API</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2019-08-10 14:58:00" itemprop="dateCreated datePublished" datetime="2019-08-10T14:58:00+08:00">2019-08-10</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-06-01 20:52:39" itemprop="dateModified" datetime="2021-06-01T20:52:39+08:00">2021-06-01</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/bigdata/" itemprop="url" rel="index"><span itemprop="name">bigdata</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="flink-table-api"><a href="#flink-table-api" class="headerlink" title="flink-table-api"></a>flink-table-api</h1><p><a target="_blank" rel="noopener" href="https://github.com/crestofwave1/oneFlink/blob/master/doc/table/Concept%20%26%20Common%20API.md">官方文档翻译</a></p>
<h2 id="Concept-amp-Common-API"><a href="#Concept-amp-Common-API" class="headerlink" title="Concept  &amp; Common API"></a>Concept  &amp; Common API</h2><p>Table API和SQL集成在一个联合的API中。这个API核心概念是Table，<br>Table可以作为查询的输入和输出。这篇文章展示了使用Table API和SQL查询的通用结构，<br>如何去进行表的注册，如何去进行表的查询，并且展示如何去进行表的输出。</p>
<h2 id="1-Structure-of-Table-API-and-SQL-Programs"><a href="#1-Structure-of-Table-API-and-SQL-Programs" class="headerlink" title="1. Structure of Table API and SQL Programs"></a>1. Structure of Table API and SQL Programs</h2><p>​    所有使用批量和流式相关的Table API和SQL的程序都有以下相同模式。下面的代码实例展示了Table API和SQL程序的通用结构。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 在批处理程序中使用ExecutionEnvironment代替StreamExecutionEnvironment</span></span><br><span class="line"><span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line"></span><br><span class="line"><span class="comment">// 创建TableEnvironment对象</span></span><br><span class="line"><span class="keyword">val</span> tableEnv = <span class="type">TableEnvironment</span>.getTableEnvironment(env)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 注册表</span></span><br><span class="line">tableEnv.registerTable(<span class="string">&quot;table1&quot;</span>, ...)           <span class="comment">// or</span></span><br><span class="line">tableEnv.registerTableSource(<span class="string">&quot;table2&quot;</span>, ...)     <span class="comment">// or</span></span><br><span class="line">tableEnv.registerExternalCatalog(<span class="string">&quot;extCat&quot;</span>, ...) </span><br><span class="line"></span><br><span class="line"><span class="comment">// 基于Table API的查询创建表</span></span><br><span class="line"><span class="keyword">val</span> tapiResult = tableEnv.scan(<span class="string">&quot;table1&quot;</span>).select(...)</span><br><span class="line"><span class="comment">// 从SQL查询创建表</span></span><br><span class="line"><span class="keyword">val</span> sqlResult  = tableEnv.sqlQuery(<span class="string">&quot;SELECT ... FROM table2 ...&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 将表操作API查询到的结果表输出到TableSink，SQL查询到的结果一样如此</span></span><br><span class="line">tapiResult.writeToSink(...)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 执行</span></span><br><span class="line">env.execute()</span><br></pre></td></tr></table></figure>
<p>注意：Table API和SQL查询很容易集成并被嵌入到DataStream或者DataSet程序中。查看<a target="_blank" rel="noopener" href="https://ci.apache.org/projects/flink/flink-docs-release-1.6/dev/table/common.html#integration-with-datastream-and-dataset-api">将DataStream和DataSet API进行整合</a>章节<br>学习DataSteams和DataSets是如何转换成Table以及Table是如何转换为DataStream或DataSet</p>
<h2 id="2-Create-a-TableEnvironment"><a href="#2-Create-a-TableEnvironment" class="headerlink" title="2. Create a TableEnvironment"></a>2. Create a TableEnvironment</h2><p>TableEnvironment是Table API与SQL整合的核心概念之一，它主要有如下功能：</p>
<ul>
<li>在internal catalog注册表</li>
<li>注册external catalog</li>
<li>执行SQL查询</li>
<li>注册UDF函数（user-defined function)，例如 标量, 表或聚合</li>
<li>将DataStream或者DataSet转换为表</li>
<li>保持ExecutionEnvironment或者StreamExecutionEnvironment的引用指向</li>
</ul>
<p>一个表总是与一个特定的TableEnvironment绑定在一块，<br>相同的查询不同的TableEnvironment是无法通过join、union合并在一起。</p>
<p>创建TableEnvironment的方法通常是通过StreamExecutionEnvironment，ExecutionEnvironment对象调用其中的静态方法TableEnvironment.getTableEnvironment()，或者是TableConfig来创建。<br>TableConfig可以用作配置TableEnvironment或是对自定义查询优化器或者是编译过程进行优化(详情查看<a target="_blank" rel="noopener" href="https://ci.apache.org/projects/flink/flink-docs-release-1.6/dev/table/common.html#query-optimization">查询优化</a>)</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// ***************</span></span><br><span class="line"><span class="comment">// 流式查询</span></span><br><span class="line"><span class="comment">// ***************</span></span><br><span class="line"><span class="keyword">val</span> sEnv = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line"><span class="comment">// 为流式查询创建一个TableEnvironment对象</span></span><br><span class="line"><span class="keyword">val</span> sTableEnv = <span class="type">TableEnvironment</span>.getTableEnvironment(sEnv)</span><br><span class="line"></span><br><span class="line"><span class="comment">// ***********</span></span><br><span class="line"><span class="comment">// 批量查询</span></span><br><span class="line"><span class="comment">// ***********</span></span><br><span class="line"><span class="keyword">val</span> bEnv = <span class="type">ExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line"><span class="comment">// 为批量查询创建一个TableEnvironment对象</span></span><br><span class="line"><span class="keyword">val</span> bTableEnv = <span class="type">TableEnvironment</span>.getTableEnvironment(bEnv)</span><br></pre></td></tr></table></figure>
<h2 id="Register-Tables-in-the-Catalog"><a href="#Register-Tables-in-the-Catalog" class="headerlink" title="Register Tables in the Catalog"></a>Register Tables in the Catalog</h2><p>TableEnvironment包含了通过名称注册表时的表的catalog信息。通常情况下有两种表，一种为输入表，<br>一种为输出表。输入表主要是在使用Table API和SQL查询时提供输入数据，输出表主要是将Table API和<br>SQL查询的结果作为输出结果对接到外部系统。</p>
<p>输入表有多种不同的输入源进行注册：</p>
<ul>
<li>已经存在的Table对象，通常是是作为Table API和SQL查询的结果</li>
<li>TableSource，可以访问外部数据如文件，数据库或者是消息系统</li>
<li>来自DataStream或是DataSet程序中的DataStream或DataSet，讨论DataStream或是DataSet<br>可以<a target="_blank" rel="noopener" href="https://ci.apache.org/projects/flink/flink-docs-release-1.6/dev/table/common.html#integration-with-datastream-and-dataset-api">整合DataStream和DataSet API</a>了解到</li>
</ul>
<p>输出表可使用TableSink进行注册</p>
<h2 id="Register-a-Table"><a href="#Register-a-Table" class="headerlink" title="Register a Table"></a>Register a Table</h2><p>Table是如何注册到TableEnvironment中如下所示：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 获取(创建)TableEnvironment对象</span></span><br><span class="line"><span class="keyword">val</span> tableEnv = <span class="type">TableEnvironment</span>.getTableEnvironment(env)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 从简单的查询结果中作为表</span></span><br><span class="line"><span class="keyword">val</span> projTable: <span class="type">Table</span> = tableEnv.scan(<span class="string">&quot;X&quot;</span>).select(...)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 将创建的表projTable命名为projectedTable注册到TableEnvironment中</span></span><br><span class="line">tableEnv.registerTable(<span class="string">&quot;projectedTable&quot;</span>, projTable)</span><br></pre></td></tr></table></figure>
<p>注意：一张注册过的Table就跟关系型数据库中的视图性质相同，定义表的查询未进行优化，但在另一个查询引用已注册的表时将进行内联。<br>如果多表查询引用了相同的Table，它就会将每一个引用进行内联并且多次执行，已注册的Table的结果之间不会进行共享。</p>
<h2 id="Register-a-TableSource"><a href="#Register-a-TableSource" class="headerlink" title="Register a TableSource"></a>Register a TableSource</h2><p>TableSource可以访问外部系统存储例如数据库（Mysql,HBase），特殊格式编码的文件(CSV, Apache [Parquet, Avro, ORC], …)<br>或者是消息系统 (Apache Kafka, RabbitMQ, …)中的数据。</p>
<p>Flink旨在为通用数据格式和存储系统提供TableSource。请查看<a target="_blank" rel="noopener" href="https://ci.apache.org/projects/flink/flink-docs-release-1.6/dev/table/sourceSinks.html">此处</a><br>了解支持的TableSource类型与如何去自定义TableSour。</p>
<p>TableSource是如何注册到TableEnvironment中如下所示：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 获取(创建)TableEnvironment对象</span></span><br><span class="line"><span class="keyword">val</span> tableEnv = <span class="type">TableEnvironment</span>.getTableEnvironment(env)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 创建TableSource对象</span></span><br><span class="line"><span class="keyword">val</span> csvSource: <span class="type">TableSource</span> = <span class="keyword">new</span> <span class="type">CsvTableSource</span>(<span class="string">&quot;/path/to/file&quot;</span>, ...)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 将创建的TableSource作为表并命名为csvTable注册到TableEnvironment中</span></span><br><span class="line">tableEnv.registerTableSource(<span class="string">&quot;CsvTable&quot;</span>, csvSource)</span><br></pre></td></tr></table></figure>
<h2 id="Register-a-TableSink"><a href="#Register-a-TableSink" class="headerlink" title="Register a TableSink"></a>Register a TableSink</h2><p>注册过的TableSink可以将SQL查询的结果以表的形式输出到外部的存储系统，例如关系型数据库，<br>Key-Value数据库(Nosql)，消息队列，或者是其他文件系统(使用不同的编码, 例如CSV, Apache [Parquet, Avro, ORC], …)</p>
<p>Flink使用TableSink的目的是为了将常用的数据进行清洗转换然后存储到不同的存储介质中。详情请查看<a target="_blank" rel="noopener" href="https://ci.apache.org/projects/flink/flink-docs-release-1.6/dev/table/sourceSinks.html">此处</a><br>去深入了解哪些sinks是可用的，并且如何去自定义TableSink。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 获取(创建)TableEnvironment对象</span></span><br><span class="line"><span class="keyword">val</span> tableEnv = <span class="type">TableEnvironment</span>.getTableEnvironment(env)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 创建TableSink对象</span></span><br><span class="line"><span class="keyword">val</span> csvSink: <span class="type">TableSink</span> = <span class="keyword">new</span> <span class="type">CsvTableSink</span>(<span class="string">&quot;/path/to/file&quot;</span>, ...)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 定义字段的名称和类型</span></span><br><span class="line"><span class="keyword">val</span> fieldNames: <span class="type">Array</span>[<span class="type">String</span>] = <span class="type">Array</span>(<span class="string">&quot;a&quot;</span>, <span class="string">&quot;b&quot;</span>, <span class="string">&quot;c&quot;</span>)</span><br><span class="line"><span class="keyword">val</span> fieldTypes: <span class="type">Array</span>[<span class="type">TypeInformation</span>[_]] = <span class="type">Array</span>(<span class="type">Types</span>.<span class="type">INT</span>, <span class="type">Types</span>.<span class="type">STRING</span>, <span class="type">Types</span>.<span class="type">LONG</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 将创建的TableSink作为表并命名为CsvSinkTable注册到TableEnvironment中</span></span><br><span class="line">tableEnv.registerTableSink(<span class="string">&quot;CsvSinkTable&quot;</span>, fieldNames, fieldTypes, csvSink)</span><br></pre></td></tr></table></figure>
<h2 id="Register-an-External-Catalog"><a href="#Register-an-External-Catalog" class="headerlink" title="Register an External Catalog"></a>Register an External Catalog</h2><p>外部目录可以提供有关外部数据库和表的信息，<br>例如其名称，模式，统计以及有关如何访问存储在外部数据库，表或文件中的数据的信息。</p>
<p>外部目录的创建方式可以通过实现ExternalCatalog接口，并且注册到TableEnvironment中，详情如下所示:</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 获取(创建)TableEnvironment对象</span></span><br><span class="line"><span class="keyword">val</span> tableEnv = <span class="type">TableEnvironment</span>.getTableEnvironment(env)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 创建一个External Catalog目录对象</span></span><br><span class="line"><span class="keyword">val</span> catalog: <span class="type">ExternalCatalog</span> = <span class="keyword">new</span> <span class="type">InMemoryExternalCatalog</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 将ExternalCatalog注册到TableEnvironment中</span></span><br><span class="line">tableEnv.registerExternalCatalog(<span class="string">&quot;InMemCatalog&quot;</span>, catalog)</span><br></pre></td></tr></table></figure>
<p>一旦将External Catalog注册到TableEnvironment中，所有在ExternalCatalog中<br>定义的表可以通过完整的路径如catalog.database.table进行Table API和SQL的查询操作 </p>
<p>目前，Flink提供InMemoryExternalCatalog对象用来做demo和测试，然而，<br>ExternalCatalog对象还可用作Table API来连接catalogs，例如HCatalog 或 Metastore</p>
<h2 id="Query-a-Table"><a href="#Query-a-Table" class="headerlink" title="Query a Table"></a>Query a Table</h2><h3 id="Table-API"><a href="#Table-API" class="headerlink" title="Table API"></a>Table API</h3><p>Table API是Scala和Java语言集成查询的API，与SQL查询不同之处在于，它的查询不是像<br>SQL一样使用字符串进行查询，而是在语言中使用语法进行逐步组合使用</p>
<p>Table API是基于展示表（流或批处理）的Table类，它提供一些列操作应用相关的操作。<br>这些方法返回一个新的Table对象，该对象表示在输入表上关系运算的结果。一些关系运算是<br>由多个方法组合而成的，例如 table.groupBy(…).select()，其中groupBy()指定<br>表的分组，select()表示在分组的结果上进行查询。</p>
<p><a target="_blank" rel="noopener" href="https://ci.apache.org/projects/flink/flink-docs-release-1.6/dev/table/tableApi.html">Table API</a><br>描述了所有支持表的流式或者批处理相关的操作。</p>
<p>下面给出一个简单的实例去说明如何去使用Table API进行聚合查询：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 获取(创建)TableEnvironment对象</span></span><br><span class="line"><span class="keyword">val</span> tableEnv = <span class="type">TableEnvironment</span>.getTableEnvironment(env)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 注册Orders表</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 扫描注册过的Orders表</span></span><br><span class="line"><span class="keyword">val</span> orders = tableEnv.scan(<span class="string">&quot;Orders&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 计算表中所有来自法国的客户的收入</span></span><br><span class="line"><span class="keyword">val</span> revenue = orders</span><br><span class="line">  .filter(<span class="symbol">&#x27;cCountry</span> === <span class="string">&quot;FRANCE&quot;</span>)</span><br><span class="line">  .groupBy(<span class="symbol">&#x27;cID</span>, <span class="symbol">&#x27;cName</span>)</span><br><span class="line">  .select(<span class="symbol">&#x27;cID</span>, <span class="symbol">&#x27;cName</span>, <span class="symbol">&#x27;revenue</span>.sum <span class="type">AS</span> <span class="symbol">&#x27;revSum</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 将结果输出成一张表或者是转换表</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 执行查询</span></span><br></pre></td></tr></table></figure>
<p>注意：Scala的Table API使用Scala符号，它使用单引号加字段(‘cID)来表示表的属性的引用，<br>如果使用Scala的隐式转换的话，确保引入了org.apache.flink.api.scala._ 和 org.apache.flink.table.api.scala._<br>来确保它们之间的转换。</p>
<h3 id="SQL"><a href="#SQL" class="headerlink" title="SQL"></a>SQL</h3><p>Flink的SQL操作基于实现了SQL标准的<a target="_blank" rel="noopener" href="https://calcite.apache.org/">Apache Calcite</a>，SQL查询通常是使用特殊且有规律的字符串。<br><a target="_blank" rel="noopener" href="https://ci.apache.org/projects/flink/flink-docs-release-1.6/dev/table/sql.html">SQL</a><br>描述了所有支持表的流式或者批处理相关的SQL操作。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 获取(创建)TableEnvironment对象</span></span><br><span class="line"><span class="keyword">val</span> tableEnv = <span class="type">TableEnvironment</span>.getTableEnvironment(env)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 注册Orders表</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 计算表中所有来自法国的客户的收入</span></span><br><span class="line"><span class="keyword">val</span> revenue = tableEnv.sqlQuery(<span class="string">&quot;&quot;</span><span class="string">&quot;</span></span><br><span class="line"><span class="string">  |SELECT cID, cName, SUM(revenue) AS revSum</span></span><br><span class="line"><span class="string">  |FROM Orders</span></span><br><span class="line"><span class="string">  |WHERE cCountry = &#x27;FRANCE&#x27;</span></span><br><span class="line"><span class="string">  |GROUP BY cID, cName</span></span><br><span class="line"><span class="string">  &quot;</span><span class="string">&quot;&quot;</span>.stripMargin)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 将结果输出成一张表或者是转换表</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 执行查询</span></span><br></pre></td></tr></table></figure>
<p>下面的例子展示了如何去使用更新查询去插入数据到已注册的表中</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 获取(创建)TableEnvironment对象</span></span><br><span class="line"><span class="keyword">val</span> tableEnv = <span class="type">TableEnvironment</span>.getTableEnvironment(env)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 注册&quot;Orders&quot;表</span></span><br><span class="line"><span class="comment">// 注册&quot;RevenueFrance&quot;输出表</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 计算表中所有来自法国的客户的收入并且将结果作为结果输出到&quot;RevenueFrance&quot;中</span></span><br><span class="line">tableEnv.sqlUpdate(<span class="string">&quot;&quot;</span><span class="string">&quot;</span></span><br><span class="line"><span class="string">  |INSERT INTO RevenueFrance</span></span><br><span class="line"><span class="string">  |SELECT cID, cName, SUM(revenue) AS revSum</span></span><br><span class="line"><span class="string">  |FROM Orders</span></span><br><span class="line"><span class="string">  |WHERE cCountry = &#x27;FRANCE&#x27;</span></span><br><span class="line"><span class="string">  |GROUP BY cID, cName</span></span><br><span class="line"><span class="string">  &quot;</span><span class="string">&quot;&quot;</span>.stripMargin)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 执行查询</span></span><br></pre></td></tr></table></figure>
<h2 id="Mixing-Table-API-and-SQL"><a href="#Mixing-Table-API-and-SQL" class="headerlink" title="Mixing Table API and SQL"></a>Mixing Table API and SQL</h2><p>Table API和SQL可以很轻松的混合使用因为他们两者返回的结果都为Table对象：</p>
<ul>
<li>可以在SQL查询返回的Table对象上定义Table API查询</li>
<li>通过在TableEnvironment中注册结果表并在SQL查询的FROM子句中引用它，<br>可以在Table API查询的结果上定义SQL查询。</li>
</ul>
<h2 id="Emit-a-Table"><a href="#Emit-a-Table" class="headerlink" title="Emit a Table"></a>Emit a Table</h2><p>通过将Table写入到TableSink来作为一张表的输出，TableSink是做为多种文件类型 (CSV, Apache Parquet, Apache Avro),<br>存储系统(JDBC, Apache HBase, Apache Cassandra, Elasticsearch), 或者是消息系统 (Apache Kafka, RabbitMQ).输出的通用接口，</p>
<p>Batch Table只能通过BatchTableSink来进行数据写入，而Streaming Table可以<br>选择AppendStreamTableSink，RetractStreamTableSink，UpsertStreamTableSink<br>中的任意一个来进行。</p>
<p>请查看<a target="_blank" rel="noopener" href="https://ci.apache.org/projects/flink/flink-docs-release-1.6/dev/table/sourceSinks.html">Table Source &amp; Sinks</a><br>来更详细的了解支持的Sinks并且如何去实现自定义的TableSink。</p>
<p>可以使用两种方式来输出一张表：</p>
<ul>
<li>Table.writeToSink(TableSink sink)方法使用提供的TableSink自动配置的表的schema来<br>进行表的输出</li>
<li>Table.insertInto（String sinkTable）方法查找在TableEnvironment目录中提供的名称下使用特定模式注册的TableSink。<br>将输出表的模式将根据已注册的TableSink的模式进行验证</li>
</ul>
<p>下面的例子展示了如何去查询结果作为一张表输出</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 获取(创建)TableEnvironment对象</span></span><br><span class="line"><span class="keyword">val</span> tableEnv = <span class="type">TableEnvironment</span>.getTableEnvironment(env)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 使用Table API或者SQL 查询来查找结果</span></span><br><span class="line"><span class="keyword">val</span> result: <span class="type">Table</span> = ...</span><br><span class="line"><span class="comment">// 创建TableSink对象</span></span><br><span class="line"><span class="keyword">val</span> sink: <span class="type">TableSink</span> = <span class="keyword">new</span> <span class="type">CsvTableSink</span>(<span class="string">&quot;/path/to/file&quot;</span>, fieldDelim = <span class="string">&quot;|&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 方法1: 使用TableSink的writeToSink()方法来将结果输出为一张表</span></span><br><span class="line">result.writeToSink(sink)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 方法2: 注册特殊schema的TableSink</span></span><br><span class="line"><span class="keyword">val</span> fieldNames: <span class="type">Array</span>[<span class="type">String</span>] = <span class="type">Array</span>(<span class="string">&quot;a&quot;</span>, <span class="string">&quot;b&quot;</span>, <span class="string">&quot;c&quot;</span>)</span><br><span class="line"><span class="keyword">val</span> fieldTypes: <span class="type">Array</span>[<span class="type">TypeInformation</span>] = <span class="type">Array</span>(<span class="type">Types</span>.<span class="type">INT</span>, <span class="type">Types</span>.<span class="type">STRING</span>, <span class="type">Types</span>.<span class="type">LONG</span>)</span><br><span class="line">tableEnv.registerTableSink(<span class="string">&quot;CsvSinkTable&quot;</span>, fieldNames, fieldTypes, sink)</span><br><span class="line"><span class="comment">// 调用注册过的TableSink中insertInto() 方法来将结果输出为一张表</span></span><br><span class="line">result.insertInto(<span class="string">&quot;CsvSinkTable&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 执行</span></span><br></pre></td></tr></table></figure>
<h2 id="Translate-and-Execute-a-Query"><a href="#Translate-and-Execute-a-Query" class="headerlink" title="Translate and Execute a Query"></a>Translate and Execute a Query</h2><p>Table API和SQL查询的结果转换为<a target="_blank" rel="noopener" href="https://ci.apache.org/projects/flink/flink-docs-release-1.6/dev/datastream_api.html">DataStream</a><br>或是<a target="_blank" rel="noopener" href="https://ci.apache.org/projects/flink/flink-docs-release-1.6/dev/batch/">DataSet</a><br>取决于它的输入是流式输入还是批处理输入。查询逻辑在内部表示为逻辑执行计划，并分为两个阶段进行转换：</p>
<ul>
<li>优化逻辑执行计划</li>
<li>转换为DataStream或DataSet</li>
</ul>
<p>Table API或SQL查询在下面请看下进行转换：</p>
<ul>
<li>当调用Table.writeToSink() 或 Table.insertInto()进行查询结果表输出的时候</li>
<li>当调用TableEnvironment.sqlUpdate()进行SQL更新查询时</li>
<li>当表转换为DataSteam或DataSet时，详情查看<a target="_blank" rel="noopener" href="https://ci.apache.org/projects/flink/flink-docs-release-1.6/dev/table/common.html#integration-with-dataStream-and-dataSet-api">Integration with DataStream and DataSet API</a></li>
</ul>
<p>一旦进行转换后，Table API或SQL查询的结果就会在StreamExecutionEnvironment.execute() 或 ExecutionEnvironment.execute()<br>被调用时被当做DataStream或DataSet一样被进行处理</p>
<h2 id="Integration-with-DataStream-and-DataSet-API"><a href="#Integration-with-DataStream-and-DataSet-API" class="headerlink" title="Integration with DataStream and DataSet API"></a>Integration with DataStream and DataSet API</h2><p>Table API或SQL查询的结果很容易被<a target="_blank" rel="noopener" href="https://ci.apache.org/projects/flink/flink-docs-release-1.6/dev/datastream_api.html">DataStream</a><br>或是<a target="_blank" rel="noopener" href="https://ci.apache.org/projects/flink/flink-docs-release-1.6/dev/batch/">DataSet</a>内嵌整合。举个例子，<br>我们会进行外部表的查询(像关系型数据库)，然后做像过滤，映射，聚合或者是元数据关联的一些预处理。<br>然后使用DataStream或是DataSet API(或者是基于这些基础库开发的上层API库, 例如CEP或Gelly)进一步对数据进行处理。<br>同样，Table API或SQL查询也可以应用于DataStream或DataSet程序的结果。</p>
<p>##implicit Conversion for Scala<br>Scala Table API具有DataSet，DataStream和Table Class之间的隐式转换，流式操作API中只要引入org.apache.flink.table.api.scala._<br>和 org.apache.flink.api.scala._ 便可以进行相应的隐式转换</p>
<h2 id="Register-a-DataStream-or-DataSet-as-Table"><a href="#Register-a-DataStream-or-DataSet-as-Table" class="headerlink" title="Register a DataStream or DataSet as Table"></a>Register a DataStream or DataSet as Table</h2><p>DataStream或DataSet也可以作为Table注册到TableEnvironment中。结果表的模式取决于已注册的DataStream或DataSet的数据类型，<br>详情请查看<a target="_blank" rel="noopener" href="https://ci.apache.org/projects/flink/flink-docs-release-1.6/dev/table/common.html#mapping-of-data-types-to-table-schema">mapping of data types to table schema</a></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 获取(创建)TableEnvironment对象</span></span><br><span class="line"><span class="comment">// 注册如表一样的DataSet</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> tableEnv = <span class="type">TableEnvironment</span>.getTableEnvironment(env)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> stream: <span class="type">DataStream</span>[(<span class="type">Long</span>, <span class="type">String</span>)] = ...</span><br><span class="line"></span><br><span class="line"><span class="comment">// 将DataStream作为具有&quot;f0&quot;, &quot;f1&quot;字段的&quot;myTable&quot;表注册到TableEnvironment中</span></span><br><span class="line">tableEnv.registerDataStream(<span class="string">&quot;myTable&quot;</span>, stream)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 将DataStream作为具有&quot;myLong&quot;, &quot;myString&quot;字段的&quot;myTable2&quot;表注册到TableEnvironment中</span></span><br><span class="line">tableEnv.registerDataStream(<span class="string">&quot;myTable2&quot;</span>, stream, <span class="symbol">&#x27;myLong</span>, <span class="symbol">&#x27;myString</span>)</span><br></pre></td></tr></table></figure>
<p>注意：DataStream表的名称必须与^ <em>DataStreamTable</em> [0-9] +模式不匹配，<br>并且DataSet表的名称必须与^ <em>DataSetTable</em> [0-9] +模式不匹配。<br>这些模式仅供内部使用。</p>
<h2 id="Convert-a-DataStream-or-DataSet-into-a-Table"><a href="#Convert-a-DataStream-or-DataSet-into-a-Table" class="headerlink" title="Convert a DataStream or DataSet into a Table"></a>Convert a DataStream or DataSet into a Table</h2><p>如果你使用Table API或是SQL查询，你可以直接将DataStream或DataSet直接转换为表而不需要<br>再将它们注册到TableEnvironment中。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 获取(创建)TableEnvironment对象</span></span><br><span class="line"><span class="comment">// 注册如表一样的DataSet</span></span><br><span class="line"><span class="keyword">val</span> tableEnv = <span class="type">TableEnvironment</span>.getTableEnvironment(env)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> stream: <span class="type">DataStream</span>[(<span class="type">Long</span>, <span class="type">String</span>)] = ...</span><br><span class="line"></span><br><span class="line"><span class="comment">// 使用默认的字段&#x27;_1, &#x27;_2将DataStram转换为Table</span></span><br><span class="line"><span class="keyword">val</span> table1: <span class="type">Table</span> = tableEnv.fromDataStream(stream)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 使用默认的字段&#x27;myLong, &#x27;myString将DataStram转换为Table</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> table2: <span class="type">Table</span> = tableEnv.fromDataStream(stream, <span class="symbol">&#x27;myLong</span>, <span class="symbol">&#x27;myString</span>)</span><br></pre></td></tr></table></figure>
<h2 id="Convert-a-Table-into-a-DataStream-or-DataSet"><a href="#Convert-a-Table-into-a-DataStream-or-DataSet" class="headerlink" title="Convert a Table into a DataStream or DataSet"></a>Convert a Table into a DataStream or DataSet</h2><p>表可以转换为DataStream或DataSet，通过这种方式，自定义DataStream或DataSet<br>同样也可以作为Table API或SQL查询结果的结果。<br>当把表转换为DataStream或DataSet时，你需要指定生成的DataStream或DataSet的数据类型。<br>例如，表格行所需转换的数据类型，通常最方便的转换类型也最常用的是Row。<br>以下列表概述了不同选项的功能：</p>
<ul>
<li>Row：字段按位置，任意数量的字段映射，支持空值，无类型安全访问。</li>
<li>POJO：字段按名称(POJO字段必须与Table字段保持一致)，任意数量的字段映射，支持空值，类型安全访问。</li>
<li>Case Class：字段按位置，任意数量的字段映射，不支持空值，类型安全访问。</li>
<li>Tuple：字段按位置，Scala支持22个字段，Java 25个字段映射，不支持空值，类型安全访问。</li>
<li>Atomic Type：表必须具有单个字段，不支持空值，类型安全访问。<h3 id="Convert-a-Table-into-a-DataStream"><a href="#Convert-a-Table-into-a-DataStream" class="headerlink" title="Convert a Table into a DataStream"></a>Convert a Table into a DataStream</h3>作为流式查询结果的表将动态更新，它随着新记录到达查询的输入流而改变，于是，转换到这样的动态查询DataStream<br>需要对表的更新进行编码。<br>将表转换为DataStream有两种模式：</li>
<li>Append Mode：这种模式仅用于动态表仅仅通过INSERT来进行表的更新，它是仅可追加模式，<br>并且之前输出的表不会进行更改</li>
<li>Retract Mode：这种模式经常用到。它使用布尔值的变量来对INSERT和DELETE对表的更新做标记<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 获取(创建)TableEnvironment对象 </span></span><br><span class="line"><span class="comment">// 注册如表一样的DataSet</span></span><br><span class="line"><span class="keyword">val</span> tableEnv = <span class="type">TableEnvironment</span>.getTableEnvironment(env)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 表中有两个字段(String name, Integet age)</span></span><br><span class="line"><span class="keyword">val</span> table: <span class="type">Table</span> = ...</span><br><span class="line"></span><br><span class="line"><span class="comment">// 将表转换为列的 append DataStream</span></span><br><span class="line"><span class="keyword">val</span> dsRow: <span class="type">DataStream</span>[<span class="type">Row</span>] = tableEnv.toAppendStream[<span class="type">Row</span>](table)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 将表转换为Tubple2[String,Int]的 append DataStream</span></span><br><span class="line"><span class="comment">// convert the Table into an append DataStream of Tuple2[String, Int]</span></span><br><span class="line"><span class="keyword">val</span> dsTuple: <span class="type">DataStream</span>[(<span class="type">String</span>, <span class="type">Int</span>)] dsTuple = </span><br><span class="line">  tableEnv.toAppendStream[(<span class="type">String</span>, <span class="type">Int</span>)](table)</span><br><span class="line"></span><br><span class="line"><span class="comment">// convert the Table into a retract DataStream of Row.</span></span><br><span class="line"><span class="comment">// Retract Mode下将表转换为列的 append DataStream</span></span><br><span class="line"><span class="comment">// 判断A retract stream X是否为DataStream[(Boolean, X)]</span></span><br><span class="line"><span class="comment">//  布尔只表示数据类型的变化,True代表为INSERT，false表示为删除</span></span><br><span class="line"><span class="keyword">val</span> retractStream: <span class="type">DataStream</span>[(<span class="type">Boolean</span>, <span class="type">Row</span>)] = tableEnv.toRetractStream[<span class="type">Row</span>](table)</span><br></pre></td></tr></table></figure>
注意：关于动态表和它的属性详情参考<a target="_blank" rel="noopener" href="https://ci.apache.org/projects/flink/flink-docs-release-1.6/dev/table/streaming.html">Streaming Queries</a></li>
</ul>
<h3 id="Convert-a-Table-into-a-DataSet"><a href="#Convert-a-Table-into-a-DataSet" class="headerlink" title="Convert a Table into a DataSet"></a>Convert a Table into a DataSet</h3><p>表转换为DataSet如下所示：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 获取(创建)TableEnvironment对象 </span></span><br><span class="line"><span class="comment">// 注册如表一样的DataSet</span></span><br><span class="line"><span class="keyword">val</span> tableEnv = <span class="type">TableEnvironment</span>.getTableEnvironment(env)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 表中有两个字段(String name, Integet age)</span></span><br><span class="line"><span class="keyword">val</span> table: <span class="type">Table</span> = ...</span><br><span class="line"></span><br><span class="line"><span class="comment">// 将表转换为列的DataSet</span></span><br><span class="line"><span class="keyword">val</span> dsRow: <span class="type">DataSet</span>[<span class="type">Row</span>] = tableEnv.toDataSet[<span class="type">Row</span>](table)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 将表转换为Tubple2[String,Int]的DataSet</span></span><br><span class="line"><span class="keyword">val</span> dsTuple: <span class="type">DataSet</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = tableEnv.toDataSet[(<span class="type">String</span>, <span class="type">Int</span>)](table)</span><br></pre></td></tr></table></figure>
<h3 id="Mapping-of-Data-Types-to-Table-Schema"><a href="#Mapping-of-Data-Types-to-Table-Schema" class="headerlink" title="Mapping of Data Types to Table Schema"></a>Mapping of Data Types to Table Schema</h3><p>Flink的DataStream和DataSet API支持多种类型。组合类型像Tuple(内置Scala元组和Flink Java元组)，<br>POJOs，Scala case classes和Flink中具有可在表表达式中访问的多个字段允许嵌套数据结构的Row类型，<br>其他类型都被视为原子类型。接下来，我们将会描述Table API是如何将这些类型转换为内部的列展现并且<br>举例说明如何将DataStream转换为Table</p>
<h4 id="Position-based-Mapping"><a href="#Position-based-Mapping" class="headerlink" title="Position-based Mapping"></a>Position-based Mapping</h4><p>基于位置的映射通常在保持顺序的情况下给字段一个更有意义的名称，这种映射可用于有固定顺序的组合数据类型，<br>也可用于原子类型。复合数据类型（如元组，行和Case Class）具有此类字段顺序.然而，POJO的字段必须与映射的<br>表的字段名相同。</p>
<p>当定义基于位置的映射，输入的数据类型不得存在指定的名称，不然API会认为这些映射应该按名称来进行映射。<br>如果未指定字段名称，则使用复合类型的默认字段名称和字段顺序，或者使用f0作为原子类型。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 获取(创建)TableEnvironment对象 </span></span><br><span class="line"><span class="keyword">val</span> tableEnv = <span class="type">TableEnvironment</span>.getTableEnvironment(env)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> stream: <span class="type">DataStream</span>[(<span class="type">Long</span>, <span class="type">Int</span>)] = ...</span><br><span class="line"></span><br><span class="line"><span class="comment">// 使用默认的字段&#x27;_1, &#x27;_2将DataStram转换为Table</span></span><br><span class="line"><span class="keyword">val</span> table1: <span class="type">Table</span> = tableEnv.fromDataStream(stream)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 使用默认的字段&#x27;myLong, &#x27;myInt将DataStram转换为Table</span></span><br><span class="line"><span class="keyword">val</span> table: <span class="type">Table</span> = tableEnv.fromDataStream(stream, <span class="symbol">&#x27;myLong</span> <span class="symbol">&#x27;myInt</span>)</span><br></pre></td></tr></table></figure>
<h4 id="Name-based-Mapping"><a href="#Name-based-Mapping" class="headerlink" title="Name-based Mapping"></a>Name-based Mapping</h4><p>基于名称的映射可用于一切数据类型包括POJOs，它是定义表模式映射最灵活的一种方式。虽然查询结果的字段可能会使用别名，但<br>这种模式下所有的字段都是使用名称进行映射的。使用别名的情况下会进行重排序。<br>如果未指定字段名称，则使用复合类型的默认字段名称和字段顺序，或者使用f0作为原子类型。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 获取(创建)TableEnvironment对象 </span></span><br><span class="line"><span class="keyword">val</span> tableEnv = <span class="type">TableEnvironment</span>.getTableEnvironment(env)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> stream: <span class="type">DataStream</span>[(<span class="type">Long</span>, <span class="type">Int</span>)] = ...</span><br><span class="line"></span><br><span class="line"><span class="comment">// 使用默认的字段&#x27;_1 和 &#x27;_2将DataStram转换为Table</span></span><br><span class="line"><span class="keyword">val</span> table: <span class="type">Table</span> = tableEnv.fromDataStream(stream)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 只使用&#x27;_2字段将DataStream转换为Table</span></span><br><span class="line"><span class="keyword">val</span> table: <span class="type">Table</span> = tableEnv.fromDataStream(stream, <span class="symbol">&#x27;_2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 交换字段将DataStream转换为Table</span></span><br><span class="line"><span class="keyword">val</span> table: <span class="type">Table</span> = tableEnv.fromDataStream(stream, <span class="symbol">&#x27;_2</span>, <span class="symbol">&#x27;_1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 交换后的字段给予别名&#x27;myInt, &#x27;myLong将DataStream转换为Table</span></span><br><span class="line"><span class="keyword">val</span> table: <span class="type">Table</span> = tableEnv.fromDataStream(stream, <span class="symbol">&#x27;_2</span> as <span class="symbol">&#x27;myInt</span>, <span class="symbol">&#x27;_1</span> as <span class="symbol">&#x27;myLong</span>)</span><br></pre></td></tr></table></figure>
<h4 id="Atomic-Types"><a href="#Atomic-Types" class="headerlink" title="Atomic Types"></a>Atomic Types</h4><p>Flink将基础类型(Integer, Double, String)和通用类型(不能被分析和拆分的类型)视为原子类型。<br>原子类型的DataStream或DataSet转换为只有单个属性的表。从原子类型推断属性的类型，并且可以指定属性的名称。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 获取(创建)TableEnvironment对象</span></span><br><span class="line"><span class="keyword">val</span> tableEnv = <span class="type">TableEnvironment</span>.getTableEnvironment(env)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> stream: <span class="type">DataStream</span>[<span class="type">Long</span>] = ...</span><br><span class="line"></span><br><span class="line"><span class="comment">// 将DataStream转换为带默认字段&quot;f0&quot;的表</span></span><br><span class="line"><span class="keyword">val</span> table: <span class="type">Table</span> = tableEnv.fromDataStream(stream)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 将DataStream转换为带字段&quot;myLong&quot;的表</span></span><br><span class="line"><span class="keyword">val</span> table: <span class="type">Table</span> = tableEnv.fromDataStream(stream, <span class="symbol">&#x27;myLong</span>)</span><br></pre></td></tr></table></figure>
<h4 id="Tuples-Scala-and-Java-and-Case-Classes-Scala-only"><a href="#Tuples-Scala-and-Java-and-Case-Classes-Scala-only" class="headerlink" title="Tuples (Scala and Java) and Case Classes (Scala only)"></a>Tuples (Scala and Java) and Case Classes (Scala only)</h4><p>Flink支持内建的Tuples并且提供了自己的Tuple类给Java进行使用。DataStreams和DataSet这两种<br>Tuple都可以转换为表。提供所有字段的名称(基于位置的映射)字段可以被重命名。如果没有指定字段的名称，<br>就使用默认的字段名称。如果原始字段名(f0, f1, … for Flink Tuples and _1, _2, … for Scala Tuples)被引用了的话，<br>API就会使用基于名称的映射来代替位置的映射。基于名称的映射可以起别名并且会进行重排序。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 获取(创建)TableEnvironment对象 </span></span><br><span class="line"><span class="keyword">val</span> tableEnv = <span class="type">TableEnvironment</span>.getTableEnvironment(env)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> stream: <span class="type">DataStream</span>[(<span class="type">Long</span>, <span class="type">String</span>)] = ...</span><br><span class="line"></span><br><span class="line"><span class="comment">// 将默认的字段重命名为&#x27;_1，&#x27;_2的DataStream转换为Table</span></span><br><span class="line"><span class="keyword">val</span> table: <span class="type">Table</span> = tableEnv.fromDataStream(stream)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 将字段名为&#x27;myLong，&#x27;myString的DataStream转换为Table(基于位置)</span></span><br><span class="line"><span class="keyword">val</span> table: <span class="type">Table</span> = tableEnv.fromDataStream(stream, <span class="symbol">&#x27;myLong</span>, <span class="symbol">&#x27;myString</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 将重排序后字段为&#x27;_2，&#x27;_1 的DataStream转换为Table(基于名称)</span></span><br><span class="line"><span class="keyword">val</span> table: <span class="type">Table</span> = tableEnv.fromDataStream(stream, <span class="symbol">&#x27;_2</span>, <span class="symbol">&#x27;_1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 将映射字段&#x27;_2的DataStream转换为Table(基于名称)</span></span><br><span class="line"><span class="keyword">val</span> table: <span class="type">Table</span> = tableEnv.fromDataStream(stream, <span class="symbol">&#x27;_2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 将重排序后字段为&#x27;_2给出别名&#x27;myString，&#x27;_1给出别名&#x27;myLong 的DataStream转换为Table(基于名称)</span></span><br><span class="line"><span class="keyword">val</span> table: <span class="type">Table</span> = tableEnv.fromDataStream(stream, <span class="symbol">&#x27;_2</span> as <span class="symbol">&#x27;myString</span>, <span class="symbol">&#x27;_1</span> as <span class="symbol">&#x27;myLong</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 定义 case class</span></span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Person</span>(<span class="params">name: <span class="type">String</span>, age: <span class="type">Int</span></span>)</span></span><br><span class="line"><span class="class"><span class="title">val</span> <span class="title">streamCC</span></span>: <span class="type">DataStream</span>[<span class="type">Person</span>] = ...</span><br><span class="line"></span><br><span class="line"><span class="comment">// 将默认字段&#x27;name, &#x27;age的DataStream转换为Table</span></span><br><span class="line"><span class="keyword">val</span> table = tableEnv.fromDataStream(streamCC)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 将字段名为&#x27;myName，&#x27;myAge的DataStream转换为Table(基于位置)</span></span><br><span class="line"><span class="keyword">val</span> table = tableEnv.fromDataStream(streamCC, <span class="symbol">&#x27;myName</span>, <span class="symbol">&#x27;myAge</span>)</span><br><span class="line"></span><br><span class="line">将重排序后字段为<span class="symbol">&#x27;_age</span>给出别名<span class="symbol">&#x27;myAge</span>，<span class="symbol">&#x27;_name</span>给出别名<span class="symbol">&#x27;myName</span> 的<span class="type">DataStream</span>转换为<span class="type">Table</span>(基于名称)</span><br><span class="line"><span class="keyword">val</span> table: <span class="type">Table</span> = tableEnv.fromDataStream(stream, <span class="symbol">&#x27;age</span> as <span class="symbol">&#x27;myAge</span>, <span class="symbol">&#x27;name</span> as <span class="symbol">&#x27;myName</span>)</span><br></pre></td></tr></table></figure>
<h4 id="POJO-Java-and-Scala"><a href="#POJO-Java-and-Scala" class="headerlink" title="POJO (Java and Scala)"></a>POJO (Java and Scala)</h4><p>Flink支持POJO作为符合类型。决定POJO规则的文档请参考<a target="_blank" rel="noopener" href="https://ci.apache.org/projects/flink/flink-docs-release-1.6/dev/api_concepts.html#pojos">这里</a></p>
<p>当将一个POJO类型的DataStream或者DataSet转换为Table而不指定字段名称时，Table的字段名称将采用JOPO原生的字段名称作为字段名称。<br>重命名原始的POJO字段需要关键字AS，因为POJO没有固定的顺序，名称映射需要原始名称并且不能通过位置来完成。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 获取(创建)TableEnvironment对象</span></span><br><span class="line"><span class="keyword">val</span> tableEnv = <span class="type">TableEnvironment</span>.getTableEnvironment(env)</span><br><span class="line"></span><br><span class="line"><span class="comment">// Person 是一个有两个字段&quot;name&quot;和&quot;age&quot;的POJO</span></span><br><span class="line"><span class="keyword">val</span> stream: <span class="type">DataStream</span>[<span class="type">Person</span>] = ...</span><br><span class="line"></span><br><span class="line"><span class="comment">// 将 DataStream 转换为带字段 &quot;age&quot;, &quot;name&quot; 的Table(字段通过名称进行排序)</span></span><br><span class="line"><span class="keyword">val</span> table: <span class="type">Table</span> = tableEnv.fromDataStream(stream)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 将DataStream转换为重命名为&quot;myAge&quot;, &quot;myName&quot;的Table(基于名称)</span></span><br><span class="line"><span class="keyword">val</span> table: <span class="type">Table</span> = tableEnv.fromDataStream(stream, <span class="symbol">&#x27;age</span> as <span class="symbol">&#x27;myAge</span>, <span class="symbol">&#x27;name</span> as <span class="symbol">&#x27;myName</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 将带映射字段&#x27;name的DataStream转换为Table(基于名称)</span></span><br><span class="line"><span class="keyword">val</span> table: <span class="type">Table</span> = tableEnv.fromDataStream(stream, <span class="symbol">&#x27;name</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 将带映射字段&#x27;name并重命名为&#x27;myName的DataStream转换为Table(基于名称)</span></span><br><span class="line"><span class="keyword">val</span> table: <span class="type">Table</span> = tableEnv.fromDataStream(stream, <span class="symbol">&#x27;name</span> as <span class="symbol">&#x27;myName</span>)</span><br></pre></td></tr></table></figure>
<h4 id="Row"><a href="#Row" class="headerlink" title="Row"></a>Row</h4><p>Row数据类型可以支持任意数量的字段，并且这些字段支持null值。当进行Row DataStream或Row DataSet<br>转换为Table时可以通过RowTypeInfo来指定字段的名称。Row Type支持基于位置和名称的两种映射方式。<br>通过提供所有字段的名称可以进行字段的重命名(基于位置)，或者是单独选择列来进行映射/重排序/重命名(基于名称)</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 获取(创建)TableEnvironment对象</span></span><br><span class="line"><span class="keyword">val</span> tableEnv = <span class="type">TableEnvironment</span>.getTableEnvironment(env)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 在`RowTypeInfo`中指定字段&quot;name&quot; 和 &quot;age&quot;的Row类型DataStream</span></span><br><span class="line"><span class="keyword">val</span> stream: <span class="type">DataStream</span>[<span class="type">Row</span>] = ...</span><br><span class="line"></span><br><span class="line"><span class="comment">// 将 DataStream 转换为带默认字段 &quot;age&quot;, &quot;name&quot; 的Table</span></span><br><span class="line"><span class="keyword">val</span> table: <span class="type">Table</span> = tableEnv.fromDataStream(stream)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 将 DataStream 转换为重命名字段 &#x27;myName, &#x27;myAge 的Table(基于位置)</span></span><br><span class="line"><span class="keyword">val</span> table: <span class="type">Table</span> = tableEnv.fromDataStream(stream, <span class="symbol">&#x27;myName</span>, <span class="symbol">&#x27;myAge</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 将 DataStream 转换为重命名字段 &#x27;myName, &#x27;myAge 的Table(基于名称)</span></span><br><span class="line"><span class="keyword">val</span> table: <span class="type">Table</span> = tableEnv.fromDataStream(stream, <span class="symbol">&#x27;name</span> as <span class="symbol">&#x27;myName</span>, <span class="symbol">&#x27;age</span> as <span class="symbol">&#x27;myAge</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 将 DataStream 转换为映射字段 &#x27;name的Table(基于名称)</span></span><br><span class="line"><span class="keyword">val</span> table: <span class="type">Table</span> = tableEnv.fromDataStream(stream, <span class="symbol">&#x27;name</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 将 DataStream 转换为映射字段 &#x27;name并重命名为&#x27;myName的Table(基于名称)</span></span><br><span class="line"><span class="keyword">val</span> table: <span class="type">Table</span> = tableEnv.fromDataStream(stream, <span class="symbol">&#x27;name</span> as <span class="symbol">&#x27;myName</span>)</span><br></pre></td></tr></table></figure>
<h4 id="Query-Optimization"><a href="#Query-Optimization" class="headerlink" title="Query Optimization"></a>Query Optimization</h4><p>Apache Flink 基于 Apache Calcite 来做转换和查询优化。当前的查询优化包括投影、过滤下推、<br>相关子查询和各种相关的查询重写。Flink不去做join优化，但是会让他们去顺序执行(FROM子句中表的顺序或者WHERE子句中连接谓词的顺序)</p>
<p>可以通过提供一个CalciteConfig对象来调整在不同阶段应用的优化规则集，<br>这个可以通过调用CalciteConfig.createBuilder())获得的builder来创建，<br>并且可以通过调用tableEnv.getConfig.setCalciteConfig(calciteConfig)来提供给TableEnvironment。</p>
<h4 id="Explaining-a-Table"><a href="#Explaining-a-Table" class="headerlink" title="Explaining a Table"></a>Explaining a Table</h4><p>Table API为计算Table提供了一个机制来解析逻辑和优化查询计划，这个可以通过TableEnvironment.explain(table)<br>来完成。它返回描述三个计划的字符串信息：</p>
<ul>
<li>关联查询抽象语法树，即未优化过的逻辑执行计划</li>
<li>优化过的逻辑执行计划</li>
<li>物理执行计划</li>
</ul>
<p>下面的实例展示了相应的输出：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line"><span class="keyword">val</span> tEnv = <span class="type">TableEnvironment</span>.getTableEnvironment(env)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> table1 = env.fromElements((<span class="number">1</span>, <span class="string">&quot;hello&quot;</span>)).toTable(tEnv, <span class="symbol">&#x27;count</span>, <span class="symbol">&#x27;word</span>)</span><br><span class="line"><span class="keyword">val</span> table2 = env.fromElements((<span class="number">1</span>, <span class="string">&quot;hello&quot;</span>)).toTable(tEnv, <span class="symbol">&#x27;count</span>, <span class="symbol">&#x27;word</span>)</span><br><span class="line"><span class="keyword">val</span> table = table1</span><br><span class="line">  .where(<span class="symbol">&#x27;word</span>.like(<span class="string">&quot;F%&quot;</span>))</span><br><span class="line">  .unionAll(table2)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> explanation: <span class="type">String</span> = tEnv.explain(table)</span><br><span class="line">println(explanation)</span><br></pre></td></tr></table></figure>
<p>对应的输出如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">&#x3D;&#x3D; 抽象语法树 &#x3D;&#x3D;</span><br><span class="line">LogicalUnion(all&#x3D;[true])</span><br><span class="line">  LogicalFilter(condition&#x3D;[LIKE($1, &#39;F%&#39;)])</span><br><span class="line">    LogicalTableScan(table&#x3D;[[_DataStreamTable_0]])</span><br><span class="line">  LogicalTableScan(table&#x3D;[[_DataStreamTable_1]])</span><br><span class="line"></span><br><span class="line">&#x3D;&#x3D; 优化后的逻辑执行计划 &#x3D;&#x3D;</span><br><span class="line">DataStreamUnion(union&#x3D;[count, word])</span><br><span class="line">  DataStreamCalc(select&#x3D;[count, word], where&#x3D;[LIKE(word, &#39;F%&#39;)])</span><br><span class="line">    DataStreamScan(table&#x3D;[[_DataStreamTable_0]])</span><br><span class="line">  DataStreamScan(table&#x3D;[[_DataStreamTable_1]])</span><br><span class="line"></span><br><span class="line">&#x3D;&#x3D; 物理执行计划 &#x3D;&#x3D;</span><br><span class="line">Stage 1 : Data Source</span><br><span class="line">  content : collect elements with CollectionInputFormat</span><br><span class="line"></span><br><span class="line">Stage 2 : Data Source</span><br><span class="line">  content : collect elements with CollectionInputFormat</span><br><span class="line"></span><br><span class="line">  Stage 3 : Operator</span><br><span class="line">    content : from: (count, word)</span><br><span class="line">    ship_strategy : REBALANCE</span><br><span class="line"></span><br><span class="line">    Stage 4 : Operator</span><br><span class="line">      content : where: (LIKE(word, &#39;F%&#39;)), select: (count, word)</span><br><span class="line">      ship_strategy : FORWARD</span><br><span class="line"></span><br><span class="line">      Stage 5 : Operator</span><br><span class="line">        content : from: (count, word)</span><br><span class="line">        ship_strategy : REBALANCE</span><br></pre></td></tr></table></figure>

<h1 id="Flink用户自定义函数"><a href="#Flink用户自定义函数" class="headerlink" title="Flink用户自定义函数"></a>Flink用户自定义函数</h1><p>用户自定义函数是非常重要的一个特征，因为他极大地扩展了查询的表达能力。</p>
<p>在大多数场景下，用户自定义函数在使用之前是必须要注册的。对于Scala的Table API，udf是不需要注册的。<br>调用TableEnvironment的registerFunction()方法来实现注册。Udf注册成功之后，会被插入TableEnvironment的function catalog，这样table API和sql就能解析他了。<br>本文会主要讲三种udf：</p>
<ul>
<li>ScalarFunction</li>
<li>TableFunction</li>
<li>AggregateFunction</li>
</ul>
<h2 id="1-Scalar-Functions-标量函数"><a href="#1-Scalar-Functions-标量函数" class="headerlink" title="1. Scalar Functions 标量函数"></a>1. Scalar Functions 标量函数</h2><p>标量函数，是指指返回一个值的函数。标量函数是实现讲0，1，或者多个标量值转化为一个新值。</p>
<p>实现一个标量函数需要继承ScalarFunction，并且实现一个或者多个evaluation方法。标量函数的行为就是通过evaluation方法来实现的。evaluation方法必须定义为public，命名为eval。evaluation方法的输入参数类型和返回值类型决定着标量函数的输入参数类型和返回值类型。evaluation方法也可以被重载实现多个eval。同时evaluation方法支持变参数，例如：eval(String… strs)。</p>
<p>下面给出一个标量函数的例子。例子实现的事一个hashcode方法。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">HashCode</span> <span class="keyword">extends</span> <span class="title">ScalarFunction</span> </span>&#123;</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">int</span> factor = <span class="number">12</span>;</span><br><span class="line">  </span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="title">HashCode</span><span class="params">(<span class="keyword">int</span> factor)</span> </span>&#123;</span><br><span class="line">      <span class="keyword">this</span>.factor = factor;</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">eval</span><span class="params">(String s)</span> </span>&#123;</span><br><span class="line">      <span class="keyword">return</span> s.hashCode() * factor;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">BatchTableEnvironment tableEnv = TableEnvironment.getTableEnvironment(env);</span><br><span class="line"></span><br><span class="line"><span class="comment">// register the function</span></span><br><span class="line">tableEnv.registerFunction(<span class="string">&quot;hashCode&quot;</span>, <span class="keyword">new</span> HashCode(<span class="number">10</span>));</span><br><span class="line"></span><br><span class="line"><span class="comment">// use the function in Java Table API</span></span><br><span class="line">myTable.select(<span class="string">&quot;string, string.hashCode(), hashCode(string)&quot;</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// use the function in SQL API</span></span><br><span class="line">tableEnv.sqlQuery(<span class="string">&quot;SELECT string, HASHCODE(string) FROM MyTable&quot;</span>);</span><br><span class="line"></span><br><span class="line">````</span><br><span class="line"></span><br><span class="line">默认情况下evaluation方法的返回值类型是由flink类型抽取工具决定。对于基础类型，简单的POJOS是足够的，但是更复杂的类型，自定义类型，组合类型，会报错。这种情况下，返回值类型的TypeInformation，需要手动指定，方法是重载</span><br><span class="line">ScalarFunction#getResultType()。</span><br><span class="line"></span><br><span class="line">下面给一个例子，通过复写ScalarFunction#getResultType()，将long型的返回值在代码生成的时候翻译成Types.TIMESTAMP。</span><br><span class="line"></span><br><span class="line">```java</span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">TimestampModifier</span> <span class="keyword">extends</span> <span class="title">ScalarFunction</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">long</span> <span class="title">eval</span><span class="params">(<span class="keyword">long</span> t)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> t % <span class="number">1000</span>;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">public</span> TypeInformation&lt;?&gt; getResultType(signature: Class&lt;?&gt;[]) &#123;</span><br><span class="line">    <span class="keyword">return</span> Types.TIMESTAMP;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="2-Table-Functions-表函数"><a href="#2-Table-Functions-表函数" class="headerlink" title="2. Table Functions 表函数"></a>2. Table Functions 表函数</h2><p>与标量函数相似之处是输入可以0，1，或者多个参数，但是不同之处可以输出任意数目的行数。返回的行也可以包含一个或者多个列。</p>
<p>为了自定义表函数，需要继承TableFunction，实现一个或者多个evaluation方法。表函数的行为定义在这些evaluation方法内部，函数名为eval并且必须是public。TableFunction可以重载多个eval方法。Evaluation方法的输入参数类型，决定着表函数的输入类型。Evaluation方法也支持变参，例如：eval(String… strs)。返回表的类型取决于TableFunction的基本类型。Evaluation方法使用collect(T)发射输出的rows。</p>
<p>在Table API中，表函数在scala语言中使用方法如下：.join(Expression) 或者 .leftOuterJoin(Expression)，在java语言中使用方法如下：.join(String) 或者.leftOuterJoin(String)。</p>
<p>Join操作算子会使用表值函数(操作算子右边的表)产生的所有行进行(cross) join 外部表(操作算子左边的表)的每一行。</p>
<p>leftOuterJoin操作算子会使用表值函数(操作算子右边的表)产生的所有行进行(cross) join 外部表(操作算子左边的表)的每一行，并且在表函数返回一个空表的情况下会保留所有的outer rows。</p>
<p>在sql语法中稍微有点区别：<br>cross join用法是LATERAL TABLE(<TableFunction>)。<br>LEFT JOIN用法是在join条件中加入ON TRUE。</p>
<p>下面的理智讲的是如何使用表值函数。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// The generic type &quot;Tuple2&lt;String, Integer&gt;&quot; determines the schema of the returned table as (String, Integer).</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Split</span> <span class="keyword">extends</span> <span class="title">TableFunction</span>&lt;<span class="title">Tuple2</span>&lt;<span class="title">String</span>, <span class="title">Integer</span>&gt;&gt; </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> String separator = <span class="string">&quot; &quot;</span>;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">Split</span><span class="params">(String separator)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.separator = separator;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">eval</span><span class="params">(String str)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">for</span> (String s : str.split(separator)) &#123;</span><br><span class="line">            <span class="comment">// use collect(...) to emit a row</span></span><br><span class="line">            collect(<span class="keyword">new</span> Tuple2&lt;String, Integer&gt;(s, s.length()));</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">BatchTableEnvironment tableEnv = TableEnvironment.getTableEnvironment(env);</span><br><span class="line">Table myTable = ...         <span class="comment">// table schema: [a: String]</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Register the function.</span></span><br><span class="line">tableEnv.registerFunction(<span class="string">&quot;split&quot;</span>, <span class="keyword">new</span> Split(<span class="string">&quot;#&quot;</span>));</span><br><span class="line"></span><br><span class="line"><span class="comment">// Use the table function in the Java Table API. &quot;as&quot; specifies the field names of the table.</span></span><br><span class="line">myTable.join(<span class="string">&quot;split(a) as (word, length)&quot;</span>).select(<span class="string">&quot;a, word, length&quot;</span>);</span><br><span class="line">myTable.leftOuterJoin(<span class="string">&quot;split(a) as (word, length)&quot;</span>).select(<span class="string">&quot;a, word, length&quot;</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// Use the table function in SQL with LATERAL and TABLE keywords.</span></span><br><span class="line">join.md</span><br><span class="line">tableEnv.sqlQuery(<span class="string">&quot;SELECT a, word, length FROM MyTable, LATERAL TABLE(split(a)) as T(word, length)&quot;</span>);</span><br><span class="line"><span class="comment">// LEFT JOIN a table function (equivalent to &quot;leftOuterJoin&quot; in Table API).</span></span><br><span class="line">tableEnv.sqlQuery(<span class="string">&quot;SELECT a, word, length FROM MyTable LEFT JOIN LATERAL TABLE(split(a)) as T(word, length) ON TRUE&quot;</span>);</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>需要注意的是PROJO类型不需要一个确定的字段顺序。意味着你不能使用as修改表函数返回的pojo的字段的名字。</p>
<p>默认情况下TableFunction返回值类型是由flink类型抽取工具决定。对于基础类型，简单的POJOS是足够的，但是更复杂的类型，自定义类型，组合类型，会报错。这种情况下，返回值类型的TypeInformation，需要手动指定，方法是重载<br>TableFunction#getResultType()。</p>
<p>下面的例子，我们通过复写TableFunction#getResultType()方法使得表返回类型是RowTypeInfo(String, Integer)。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CustomTypeSplit</span> <span class="keyword">extends</span> <span class="title">TableFunction</span>&lt;<span class="title">Row</span>&gt; </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">eval</span><span class="params">(String str)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">for</span> (String s : str.split(<span class="string">&quot; &quot;</span>)) &#123;</span><br><span class="line">            Row row = <span class="keyword">new</span> Row(<span class="number">2</span>);</span><br><span class="line">            row.setField(<span class="number">0</span>, s);</span><br><span class="line">            row.setField(<span class="number">1</span>, s.length);</span><br><span class="line">            collect(row);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> TypeInformation&lt;Row&gt; <span class="title">getResultType</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> Types.ROW(Types.STRING(), Types.INT());</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="3-Aggregation-Functions-聚合函数"><a href="#3-Aggregation-Functions-聚合函数" class="headerlink" title="3. Aggregation Functions 聚合函数"></a>3. Aggregation Functions 聚合函数</h2><p>用户自定义聚合函数聚合一张表(一行或者多行，一行有一个或者多个属性)为一个标量的值。</p>
<p>上图中是讲的一张饮料的表这个表有是那个字段五行数据，现在要做的事求出所有饮料的最高价。</p>
<p>聚合函数需要继承AggregateFunction。聚合函数工作方式如下：<br>首先，需要一个accumulator，这个是保存聚合中间结果的数据结构。调用AggregateFunction函数的createAccumulator()方法来创建一个空的accumulator.<br>随后，每个输入行都会调用accumulate()方法来更新accumulator。一旦所有的行被处理了，getValue()方法就会被调用，计算和返回最终的结果。</p>
<p>对于每个AggregateFunction，下面三个方法都是比不可少的：<br>createAccumulator()<br>accumulate()<br>getValue()</p>
<p>flink的类型抽取机制不能识别复杂的数据类型，比如，数据类型不是基础类型或者简单的pojos类型。所以，类似于ScalarFunction 和TableFunction，AggregateFunction提供了方法去指定返回结果类型的TypeInformation，用的是AggregateFunction#getResultType()。Accumulator类型用的是AggregateFunction#getAccumulatorType()。</p>
<p>除了上面的方法，这里有一些可选的方法。尽管有些方法是让系统更加高效的执行查询，另外的一些在特定的场景下是必须的。例如，merge()方法在会话组窗口上下文中是必须的。当一行数据是被视为跟两个回话窗口相关的时候，两个会话窗口的accumulators需要被join。</p>
<p>AggregateFunction的下面几个方法，根据使用场景的不同需要被实现：<br>retract()：在bounded OVER窗口的聚合方法中是需要实现的。<br>merge()：在很多batch 聚合和会话窗口聚合是必须的。<br>resetAccumulator(): 在大多数batch聚合是必须的。</p>
<p>AggregateFunction的所有方法都是需要被声明为public，而不是static。定义聚合函数需要实现org.apache.flink.table.functions.AggregateFunction同时需要实现一个或者多个accumulate方法。该方法可以被重载为不同的数据类型，并且支持变参。</p>
<p>在这里就不贴出来AggregateFunction的源码了。</p>
<p>下面举个求加权平均的栗子<br>为了计算加权平均值，累加器需要存储已累积的所有数据的加权和及计数。在栗子中定义一个WeightedAvgAccum类作为accumulator。尽管，retract(), merge(), 和resetAccumulator()方法在很多聚合类型是不需要的，这里也给出了栗子。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Accumulator for WeightedAvg.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">WeightedAvgAccum</span> </span>&#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">long</span> sum = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">int</span> count = <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Weighted Average user-defined aggregate function.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">WeightedAvg</span> <span class="keyword">extends</span> <span class="title">AggregateFunction</span>&lt;<span class="title">Long</span>, <span class="title">WeightedAvgAccum</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> WeightedAvgAccum <span class="title">createAccumulator</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> WeightedAvgAccum();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Long <span class="title">getValue</span><span class="params">(WeightedAvgAccum acc)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (acc.count == <span class="number">0</span>) &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="keyword">return</span> acc.sum / acc.count;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">accumulate</span><span class="params">(WeightedAvgAccum acc, <span class="keyword">long</span> iValue, <span class="keyword">int</span> iWeight)</span> </span>&#123;</span><br><span class="line">        acc.sum += iValue * iWeight;</span><br><span class="line">        acc.count += iWeight;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">retract</span><span class="params">(WeightedAvgAccum acc, <span class="keyword">long</span> iValue, <span class="keyword">int</span> iWeight)</span> </span>&#123;</span><br><span class="line">        acc.sum -= iValue * iWeight;</span><br><span class="line">        acc.count -= iWeight;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">merge</span><span class="params">(WeightedAvgAccum acc, Iterable&lt;WeightedAvgAccum&gt; it)</span> </span>&#123;</span><br><span class="line">        Iterator&lt;WeightedAvgAccum&gt; iter = it.iterator();</span><br><span class="line">        <span class="keyword">while</span> (iter.hasNext()) &#123;</span><br><span class="line">            WeightedAvgAccum a = iter.next();</span><br><span class="line">            acc.count += a.count;</span><br><span class="line">            acc.sum += a.sum;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">resetAccumulator</span><span class="params">(WeightedAvgAccum acc)</span> </span>&#123;</span><br><span class="line">        acc.count = <span class="number">0</span>;</span><br><span class="line">        acc.sum = <span class="number">0L</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// register function</span></span><br><span class="line">StreamTableEnvironment tEnv = ...</span><br><span class="line">tEnv.registerFunction(<span class="string">&quot;wAvg&quot;</span>, <span class="keyword">new</span> WeightedAvg());</span><br><span class="line"></span><br><span class="line"><span class="comment">// use function</span></span><br><span class="line">tEnv.sqlQuery(<span class="string">&quot;SELECT user, wAvg(points, level) AS avgPoints FROM userScores GROUP BY user&quot;</span>);</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="4-实现udf的最佳实践经验"><a href="#4-实现udf的最佳实践经验" class="headerlink" title="4. 实现udf的最佳实践经验"></a>4. 实现udf的最佳实践经验</h2><p>Table API和SQL 代码生成器内部会尽可能多的尝试使用原生值。用户定义的函数可能通过对象创建、强制转换(casting)和拆装箱((un)boxing)引入大量开销。因此，强烈推荐参数和返回值的类型定义为原生类型而不是他们包装类型(boxing class)。Types.DATE 和Types.TIME可以用int代替。Types.TIMESTAMP可以用long代替。</p>
<p>我们建议用户自定义函数使用java编写而不是scala编写，因为scala的类型可能会有不被flink类型抽取器兼容。</p>
<p>用Runtime集成UDFs</p>
<p>有时候udf需要获取全局runtime信息或者在进行实际工作之前做一些设置和清除工作。Udf提供了open()和close()方法，可以被复写，功能类似Dataset和DataStream API的RichFunction方法。</p>
<p>Open()方法是在evaluation方法调用前调用一次。Close()是在evaluation方法最后一次调用后调用。</p>
<p>Open()方法提共一个FunctionContext，FunctionContext包含了udf执行环境的上下文，比如，metric group，分布式缓存文件，全局的job参数。</p>
<p>通过调用FunctionContext的相关方法，可以获取到相关的信息：</p>
<p>方法描述</p>
<ul>
<li>getMetricGroup() - 并行子任务的指标组</li>
<li>getCachedFile(name) -分布式缓存文件的本地副本</li>
<li>getJobParameter(name, defaultValue) - 给定key全局job参数。</li>
</ul>
<p>下面，给出的例子就是通过FunctionContext在一个标量函数中获取全局job的参数。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">HashCode</span> <span class="keyword">extends</span> <span class="title">ScalarFunction</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">int</span> factor = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">open</span><span class="params">(FunctionContext context)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="comment">// access &quot;hashcode_factor&quot; parameter</span></span><br><span class="line">        <span class="comment">// &quot;12&quot; would be the default value if parameter does not exist</span></span><br><span class="line">        factor = Integer.valueOf(context.getJobParameter(<span class="string">&quot;hashcode_factor&quot;</span>, <span class="string">&quot;12&quot;</span>)); </span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">eval</span><span class="params">(String s)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> s.hashCode() * factor;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">BatchTableEnvironment tableEnv = TableEnvironment.getTableEnvironment(env);</span><br><span class="line"></span><br><span class="line"><span class="comment">// set job parameter</span></span><br><span class="line">Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">conf.setString(<span class="string">&quot;hashcode_factor&quot;</span>, <span class="string">&quot;31&quot;</span>);</span><br><span class="line">env.getConfig().setGlobalJobParameters(conf);</span><br><span class="line"></span><br><span class="line"><span class="comment">// register the function</span></span><br><span class="line">tableEnv.registerFunction(<span class="string">&quot;hashCode&quot;</span>, <span class="keyword">new</span> HashCode());</span><br><span class="line"></span><br><span class="line"><span class="comment">// use the function in Java Table API</span></span><br><span class="line">myTable.select(<span class="string">&quot;string, string.hashCode(), hashCode(string)&quot;</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// use the function in SQL</span></span><br><span class="line">tableEnv.sqlQuery(<span class="string">&quot;SELECT string, HASHCODE(string) FROM MyTable&quot;</span>);</span><br></pre></td></tr></table></figure>
<h1 id="内置函数"><a href="#内置函数" class="headerlink" title="内置函数"></a>内置函数</h1><h2 id="scala"><a href="#scala" class="headerlink" title="scala"></a>scala</h2><h3 id="三元运算符"><a href="#三元运算符" class="headerlink" title="三元运算符"></a>三元运算符</h3><p>sql或者table API筛选数据，必须保证每个字段不为空，<br>Flink内部，中间结果都是通过case class传递，而case class的字段必须保证不能为空</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">BOOLEAN</span>.?(<span class="type">VALUE1</span>, <span class="type">VALUE2</span>)</span><br><span class="line"><span class="symbol">&#x27;is_active_user</span>.isNull.?(<span class="string">&quot;0&quot;</span>, <span class="string">&quot;1&quot;</span>)</span><br></pre></td></tr></table></figure>
<h3 id="等值判断"><a href="#等值判断" class="headerlink" title="等值判断"></a>等值判断</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="symbol">&#x27;Fuin</span> === <span class="symbol">&#x27;active_user</span></span><br></pre></td></tr></table></figure>
<p>scala中的<code>===</code>是运算符重构</p>
<ol>
<li><a target="_blank" rel="noopener" href="https://www.cnblogs.com/Springmoon-venn/p/11826359.html">Flink Table Api &amp; SQL 翻译目录</a></li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/9/"><i class="fa fa-angle-left" aria-label="Previous page"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/9/">9</a><span class="page-number current">10</span><a class="page-number" href="/page/11/">11</a><span class="space">&hellip;</span><a class="page-number" href="/page/25/">25</a><a class="extend next" rel="next" href="/page/11/"><i class="fa fa-angle-right" aria-label="Next page"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">aaronzhang</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">242</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">16</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">128</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">aaronzhang</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  

</body>
</html>
