<!DOCTYPE html>
<html lang="zh-Hans">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta property="og:type" content="website">
<meta property="og:title" content="Guadazi">
<meta property="og:url" content="http://example.com/page/10/index.html">
<meta property="og:site_name" content="Guadazi">
<meta property="og:locale">
<meta property="article:author" content="aaronzhang">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://example.com/page/10/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'zh-Hans'
  };
</script>

  <title>Guadazi</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Guadazi</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://example.com/bigdata/Flink/Flink-checkpoint-savepoint-2pc/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="aaronzhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Guadazi">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/bigdata/Flink/Flink-checkpoint-savepoint-2pc/" class="post-title-link" itemprop="url">Flink-checkpoint与高可用</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2019-08-10 14:58:00" itemprop="dateCreated datePublished" datetime="2019-08-10T14:58:00+08:00">2019-08-10</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-04-11 11:49:10" itemprop="dateModified" datetime="2021-04-11T11:49:10+08:00">2021-04-11</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/bigdata/" itemprop="url" rel="index"><span itemprop="name">bigdata</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>Flink Checkpoint 受 Chandy-Lamport 分布式快照启发，可以保证数据的高可用。但是有些情况下，不见得一定有效:</p>
<p>Flink On Yarn 模式，某个 Container 发生 OOM 异常，这种情况程序直接变成失败状态，此时 Flink 程序虽然开启 Checkpoint 也无法恢复，因为程序已经变成失败状态，所以此时可以借助外部参与启动程序，比如外部程序检测到实时任务失败时，从新对实时任务进行拉起。</p>
<h2 id="1-1-2PC"><a href="#1-1-2PC" class="headerlink" title="1.1. 2PC"></a>1.1. 2PC</h2><h3 id="1-1-1-Exactly-once-VS-At-least-once"><a href="#1-1-1-Exactly-once-VS-At-least-once" class="headerlink" title="1.1.1. Exactly-once VS At-least-once"></a>1.1.1. Exactly-once VS At-least-once</h3><p>算子做快照时，如果等所有输入端的barrier都到了才开始做快照，可保证算子的exactly-once；如果为了降低延时而跳过对齐，从而继续处理数据，那么等barrier都到齐后做快照就是at-least-once了，因为这次的快照掺杂了下一次快照的数据，当作业失败恢复的时候，这些数据会重复作用系统，就好像这些数据被消费了两遍。</p>
<p>注：对齐只会发生在算子的上端是join操作以及上游存在partition或者shuffle的情况，对于直连操作类似map、flatMap、filter等还是会保证exactly-once的语义。</p>
<h3 id="1-1-2-端到端的Exactly-once实现"><a href="#1-1-2-端到端的Exactly-once实现" class="headerlink" title="1.1.2. 端到端的Exactly once实现"></a>1.1.2. 端到端的Exactly once实现</h3><p>2PC分为几个阶段: 开始事务-&gt;预提交-&gt;提交(或回滚)</p>
<p>为了保证Exactly once, source和sink必须支持flink的2PC</p>
<p>当状态涉及到外部系统时，需要外部系统支持事务操作来配合flink实现2PC协议，从而保证数据的exatly-once。<br>这个时候，sink算子除了将自己的state写到后段，还必须准备好事务提交。</p>
<ul>
<li>一旦所有的算子完成了它们的pre-commit，它们会要求一个commit。</li>
<li>如果存在一个算子pre-commit失败了，本次事务失败，我们回滚到上次的checkpoint。</li>
<li>一旦master做出了commit的决定，那么这个commit必须得到执行，就算宕机恢复也有继续执行。</li>
</ul>
<h4 id="1-1-2-1-pre-commit"><a href="#1-1-2-1-pre-commit" class="headerlink" title="1.1.2.1. pre-commit"></a>1.1.2.1. pre-commit</h4><p>pre-commit阶段起始于一次快照的开始，即master节点将checkpoint的barrier注入source端，barrier随着数据向下流动直到sink端。barrier每到一个算子，都会出发算子做本地快照。</p>
<p><img src="_v_images/20200710154629243_751269158.png" alt="precommit"></p>
<p>当所有的算子都做完了本地快照并且回复到master节点时，pre-commit阶段才算结束。这个时候，checkpoint已经成功，并且包含了外部系统的状态。如果作业失败，可以进行恢复。</p>
<p><img src="_v_images/20200710154750060_1524377793.png" alt="precommit-success"></p>
<h4 id="1-1-2-2-commit"><a href="#1-1-2-2-commit" class="headerlink" title="1.1.2.2. commit"></a>1.1.2.2. commit</h4><p>通知所有的算子这次checkpoint成功了，即2PC的commit阶段。source节点和window节点没有外部状态，所以这时它们不需要做任何操作。<br>而对于sink节点，需要commit这次事务，将数据写到外部系统。</p>
<p><img src="_v_images/20200710154844838_737658241.png" alt="commit"></p>
<h4 id="1-1-2-3-rollback"><a href="#1-1-2-3-rollback" class="headerlink" title="1.1.2.3. rollback"></a>1.1.2.3. rollback</h4><p>一旦任何一个算子的快照保存失败，则触发回滚，同样的sink算子也需要取消写入外部的数据</p>
<h3 id="1-1-3-TwoPhaseCommitSinkFunction"><a href="#1-1-3-TwoPhaseCommitSinkFunction" class="headerlink" title="1.1.3. TwoPhaseCommitSinkFunction"></a>1.1.3. TwoPhaseCommitSinkFunction</h3><p>为了简化2PC的实现成本，flink抽象了TwoPhaseCommitSinkFunction</p>
<ul>
<li>beginTransaction。开始一次事务，在目的文件系统创建一个临时文件。接下来我们就可以将数据写到这个文件。</li>
<li>preCommit。在这个阶段，将文件flush掉，同时重起一个文件写入，作为下一次事务的开始。</li>
<li>commit。这个阶段，将文件写到真正的目的目录。值得注意的是，这会增加数据可视的延时。</li>
<li>abort。如果回滚，那么删除临时文件。</li>
</ul>
<p>如果pre-commit成功了，但是commit没有到达算子旧宕机了，flink会将算子恢复到pre-commit时的状态，然后继续commit。</p>
<p>我们需要做的还有就是保证commit的幂等性，这可以通过检查临时文件是否还在来实现。</p>
<h2 id="1-2-checkpoint"><a href="#1-2-checkpoint" class="headerlink" title="1.2. checkpoint"></a>1.2. checkpoint</h2><p><strong>保留策略</strong>:</p>
<ul>
<li>DELETE_ON_CANCELLATION 表示当程序取消时，删除 Checkpoint 存储文件。</li>
<li>RETAIN_ON_CANCELLATION 表示当程序取消时，保存之前的 Checkpoint 存储文件</li>
</ul>
<p>默认情况下，Flink不会触发一次 Checkpoint 当系统有其他 Checkpoint 在进行时，也就是说 Checkpoint 默认的并发为1。</p>
<p><strong>CheckpointCoordinator</strong> :</p>
<p>针对 Flink DataStream 任务，程序需要经历从 StreamGraph -&gt; JobGraph -&gt; ExecutionGraph -&gt; 物理执行图四个步骤，其中在 ExecutionGraph 构建时，会初始化 CheckpointCoordinator。ExecutionGraph通过ExecutionGraphBuilder.buildGraph方法构建，在构建完时，会调用 ExecutionGraph 的enableCheckpointing方法创建CheckpointCoordinator</p>
<p><strong>Flink Checkpoint 参数配置及建议</strong>:</p>
<ul>
<li>当 Checkpoint 时间比设置的 Checkpoint 间隔时间要长时，可以设置 Checkpoint 间最小时间间隔 。这样在上次 Checkpoint 完成时，不会立马进行下一次 Checkpoint，而是会等待一个最小时间间隔，然后在进行该次 Checkpoint。否则，每次 Checkpoint 完成时，就会立马开始下一次 Checkpoint，系统会有很多资源消耗 Checkpoint。</li>
<li>如果Flink状态很大，在进行恢复时，需要从远程存储读取状态恢复，此时可能导致任务恢复很慢，可以设置 Flink Task 本地状态恢复。任务状态本地恢复默认没有开启，可以设置参数state.backend.local-recovery值为true进行激活。</li>
<li>Checkpoint保存数，Checkpoint 保存数默认是1，也就是保存最新的 Checkpoint 文件，当进行状态恢复时，如果最新的Checkpoint文件不可用时(比如HDFS文件所有副本都损坏或者其他原因)，那么状态恢复就会失败，如果设置 Checkpoint 保存数2，即使最新的Checkpoint恢复失败，那么Flink 会回滚到之前那一次Checkpoint进行恢复。考虑到这种情况，用户可以增加 Checkpoint 保存数。</li>
<li>建议设置的 Checkpoint 的间隔时间最好大于 Checkpoint 的完成时间。</li>
</ul>
<p>下图是不设置 Checkpoint 最小时间间隔示例图，可以看到，系统一致在进行 Checkpoint，可能对运行的任务产生一定影响：<br><img src="_v_images/20200714095846469_121820659.png"></p>
<h2 id="1-3-savepoint"><a href="#1-3-savepoint" class="headerlink" title="1.3. savepoint"></a>1.3. savepoint</h2><blockquote>
<p>注意:<br>使用DataStream进行开发，建议为每个算子定义一个 uid，这样我们在修改作业时，即使导致程序拓扑图改变，由于相关算子 uid 没有变，那么这些算子还能够继续使用之前的状态，如果用户没有定义 uid ， Flink 会为每个算子自动生成 uid，如果用户修改了程序，可能导致之前的状态程序不能再进行复用。</p>
</blockquote>
<p>Flink 在触发Savepoint 或者 Checkpoint时，会根据这次触发的类型计算出在HDFS上面的目录:</p>
<p>如果类型是 Savepoint，那么 其 HDFS 上面的目录为：Savepoint 根目录+savepoint-jobid前六位+随机数字，具体如下格式：</p>
<p><img src="_v_images/20200714100459823_887900222.png"></p>
<p>Checkpoint 目录为 chk-checkpoint ID,具体格式如下：</p>
<p><img src="_v_images/20200714100516223_630729421.png"></p>
<ul>
<li>使用 flink cancel -s 命令取消作业同时触发 Savepoint 时，会有一个问题，可能存在触发 Savepoint 失败。比如实时程序处于异常状态(比如 Checkpoint失败)，而此时你停止作业，同时触发 Savepoint,这次 Savepoint 就会失败，这种情况会导致，在实时平台上面看到任务已经停止，但是实际实时作业在 Yarn 还在运行。针对这种情况，需要捕获触发 Savepoint 失败的异常，当抛出异常时，可以直接在 Yarn 上面 Kill 掉该任务。</li>
<li>使用 DataStream 程序开发时，最好为每个算子分配 uid,这样即使作业拓扑图变了，相关算子还是能够从之前的状态进行恢复，默认情况下，Flink 会为每个算子分配 uid,这种情况下，当你改变了程序的某些逻辑时，可能导致算子的 uid 发生改变，那么之前的状态数据，就不能进行复用，程序在启动的时候，就会报错。</li>
<li>由于 Savepoint 是程序的全局状态，对于某些状态很大的实时任务，当我们触发 Savepoint，可能会对运行着的实时任务产生影响，个人建议如果对于状态过大的实时任务，触发 Savepoint 的时间，不要太过频繁。根据状态的大小，适当的设置触发时间。</li>
<li>当我们从 Savepoint 进行恢复时，需要检查这次 Savepoint 目录文件是否可用。可能存在你上次触发 Savepoint 没有成功，导致 HDFS 目录上面 Savepoint 文件不可用或者缺少数据文件等，这种情况下，如果在指定损坏的 Savepoint 的状态目录进行状态恢复，任务会启动不起来。</li>
</ul>
<h2 id="1-4-snapshot保存到哪里-应该需要汇总到jobManager？"><a href="#1-4-snapshot保存到哪里-应该需要汇总到jobManager？" class="headerlink" title="1.4. snapshot保存到哪里? 应该需要汇总到jobManager？"></a>1.4. snapshot保存到哪里? 应该需要汇总到jobManager？</h2><h2 id="1-5-state-backend"><a href="#1-5-state-backend" class="headerlink" title="1.5. state backend"></a>1.5. state backend</h2><p><img src="_v_images/20200713183839429_2053265091.png"></p>
<h3 id="FsStateBackend"><a href="#FsStateBackend" class="headerlink" title="FsStateBackend"></a>FsStateBackend</h3><p>构造方法:<br><code>FsStateBackend(URI checkpointDataUri,boolean asynchronousSnapshots)</code></p>
<p>1 基于文件系统的状态管理器<br>2 如果使用，默认是异步<br>3 比较稳定，3个副本，比较安全。不会出现任务无法恢复等问题<br>4 状态大小受磁盘容量限制</p>
<p>存储方式:</p>
<ul>
<li>State: TaskManager内存</li>
<li>checkpoint: 外部文件系统(本地或HDFS)</li>
</ul>
<p>容量限制:</p>
<ul>
<li>单TaskManager上State总量不超过它的内存</li>
<li>总大小不超过配置的文件系统容量</li>
</ul>
<p>推荐使用场景:</p>
<ul>
<li>常规使用状态的作业，例如分钟级窗口聚合、join、窗口比较长、kv状态大；需要开启HA的作业</li>
<li>可以用于生产场景</li>
</ul>
<h3 id="RocksDBStateBackend"><a href="#RocksDBStateBackend" class="headerlink" title="RocksDBStateBackend"></a>RocksDBStateBackend</h3><p>状态数据先写入RocksDB，然后异步的将状态数据写入文件系统。正在进行计算的热数据存储在RocksDB，长时间才更新的数据写入磁盘中（文件系统）存储，体量比较小的元数据状态写入JobManager内存中（将工作state保存在RocksDB中，并且默认将checkpoint数据存在文件系统中）</p>
<p>目前唯一支持incremental的checkpoints的策略</p>
<p>构造方法:<br><code>RocksDBStateBackend(URI checkpointDataUri,boolean enableIncrementalCheckpointing)</code></p>
<p>存储方式:</p>
<ul>
<li>State: TaskManager上的KV数据库(实际使用内存+硬盘)</li>
<li>Checkpoint: 外部文件系统(本地或HDFS)</li>
</ul>
<p>容量限制:</p>
<ul>
<li>单TaskManager上的State总量不超过他的内存+磁盘</li>
<li>单key最大2G</li>
<li>总大小不超过配置的文件系统容量</li>
</ul>
<p>推荐使用的场景:</p>
<ul>
<li>超大状态的作业，例如天级别窗口聚合；需要开启HA的作业；对状态读写性能要求不高的作业</li>
<li>可以在生产环境使用</li>
</ul>
<h3 id="MemoryStateBackend"><a href="#MemoryStateBackend" class="headerlink" title="MemoryStateBackend"></a>MemoryStateBackend</h3><p>构造方法:<br><code>MemoryStateBackend(int maxStateSize, boolean asynchronousSnapshots)</code></p>
<p>主机内存中的数据可能会丢失，任务可能无法恢复</p>
<p>存储方式:</p>
<ul>
<li>State: TaskManager内存</li>
<li>Checkpoint: JobManager内存</li>
</ul>
<p>容量限制</p>
<ul>
<li>单个State maxStateSize默认5M</li>
<li>maxStateSize &lt;= akka.frameSize 默认10M</li>
<li>总大小不超过JobManager的内存</li>
</ul>
<p>推荐使用场景：</p>
<ul>
<li>本地测试；几乎无状态的作业，比如ETL；JobManager不容易挂，或挂掉影响不大的情况</li>
<li>不推荐在生产环境使用</li>
</ul>
<h2 id="1-6-checkpoint-与-savepoint"><a href="#1-6-checkpoint-与-savepoint" class="headerlink" title="1.6. checkpoint 与 savepoint"></a>1.6. checkpoint 与 savepoint</h2><p>Checkpoint指定触发生成时间间隔后，每当需要触发Checkpoint时，会向Flink程序运行时的多个分布式的Stream Source中插入一个Barrier标记，这些Barrier会根据Stream中的数据记录一起流向下游的各个Operator。<br>当一个Operator接收到一个Barrier时，它会暂停处理Steam中新接收到的数据记录。<br>因为一个Operator可能存在多个输入的Stream，而每个Stream中都会存在对应的Barrier，该Operator要等到所有的输入Stream中的Barrier都到达。(<strong>对齐</strong>)<br>当所有Stream中的Barrier都已经到达该Operator，这时所有的Barrier在时间上看来是同一个时刻点（表示已经对齐），在等待所有Barrier到达的过程中，<br>Operator的Buffer中可能已经缓存了一些比Barrier早到达Operator的数据记录（Outgoing Records），这时该Operator会将数据记录（Outgoing Records）发射（Emit）出去，作为下游Operator的输入，<br>最后将Barrier对应Snapshot发射（Emit）出去作为此次Checkpoint的结果数据。</p>
<p>Checkpoint 是增量做的，每次的时间较短，数据量较小，只要在程序里面启用后会自动触发，用户无须感知；Checkpoint 是作业 failover 的时候自动使用，不需要用户指定。</p>
<p>Savepoint 是全量做的，每次的时间较长，数据量较大，需要用户主动去触发。Savepoint 一般用于程序的版本更新（详见文档），Bug 修复，A/B Test 等场景，需要用户指定。</p>
<p><strong>保存的内容</strong></p>
<ul>
<li>首先，Savepoint 包含了一个目录，其中包含（通常很大的）二进制文件，这些文件表示了整个流应用在 Checkpoint/Savepoint 时的状态。</li>
<li>以及一个（相对较小的）元数据文件，包含了指向 Savapoint 各个文件的指针，并存储在所选的分布式文件系统或数据存储中。</li>
</ul>
<p><strong>目标</strong></p>
<p>Savepoint 和 Checkpoint 的不同之处很像传统数据库中备份与恢复日志之间的区别。Checkpoint 的主要目标是充当 Flink 中的恢复机制，确保能从潜在的故障中恢复。相反，Savepoint 的主要目标是充当手动备份、恢复暂停作业的方法。</p>
<p><strong>实现</strong></p>
<p>Checkpoint 被设计成轻量和快速的机制。它们可能（但不一定必须）利用底层状态后端的不同功能尽可能快速地恢复数据。例如，基于 RocksDB 状态后端的增量检查点，能够加速 RocksDB 的 checkpoint 过程，这使得 checkpoint 机制变得更加轻量。相反，Savepoint 旨在更多地关注数据的可移植性，并支持对作业做任何更改而状态能保持兼容，这使得生成和恢复的成本更高</p>
<p><strong>状态文件保留策略</strong></p>
<p>Checkpoint默认程序删除，可以设置CheckpointConfig中的参数进行保留 。Savepoint会一直保存，除非用户删除 。</p>
<p><strong>应用</strong></p>
<ul>
<li>部署流应用的一个新版本，包括新功能、BUG 修复、或者一个更好的机器学习模型</li>
<li>引入 A/B 测试，使用相同的源数据测试程序的不同版本，从同一时间点开始测试而不牺牲先前的状态</li>
<li>在需要更多资源时扩容应用程序</li>
<li>迁移流应用程序到 Flink 的新版本上，或者迁移到另一个集群</li>
</ul>
<h1 id="Flink数据一致性"><a href="#Flink数据一致性" class="headerlink" title="Flink数据一致性"></a>Flink数据一致性</h1><h2 id="一、综述"><a href="#一、综述" class="headerlink" title="一、综述"></a>一、综述</h2><p><strong>flink 通过内部依赖checkpoint 并且可以通过设置其参数exactly-once 实现其内部的一致性</strong>。但要实现其端到端的一致性，还必须保证<br>1、source 外部数据源可重设数据的读取位置<br>2、sink端 需要保证数据从故障恢复时，数据不会重复写入外部系统（或者可以逻辑实现写入多次，但只有一次生效的数据sink端）</p>
<h2 id="二、sink-端到端实现方式"><a href="#二、sink-端到端实现方式" class="headerlink" title="二、sink 端到端实现方式"></a>二、sink 端到端实现方式</h2><p><strong>幂等操作：</strong><br>一个操作，可以重复执行多次，但只导致一次结果更改，豁免重复操作执行就不起作用了，他的瑕疵 （在系统恢复的过程中，如果这段时间内多个更新或者插入导致状态不一致，但当数据追上就可以了）<br>（逻辑与、逻辑或等）具体理解参照自己以前写的文章。<br><strong>事务写入：</strong><br>事务应该具有四个属性：原子性、一致性、隔离性、持久性等。其具体的实现方式有两种<br><strong>（1）、预写日志</strong><br>简单易于实现，由于数据提前在状态后端中做了缓存，所以无论什么sink系统，都能用这种方式一批搞定，DataStream API提供了一个模板类：GenericWriteAheadSink，来实现这种事务性sink；<br>缺点：<br>1）、sink系统没说他支持事务。有可能出现一部分写入集群了。一部分没有写进去（如果实表，再写一次就写重复了）<br>2）、checkpoint做完了sink才去真正的写入（但其实得等sink都写完checkpoint才能生效，所以WAL这个机制jobmanager确定它写完还不算真正写完，还得有一个外部系统已经确认 完成的checkpoint）<br>（<strong>2）、两阶段提交。 flink 真正实现exactle-once</strong><br>对于每个checkpoint,sink 任务会启动一个事务，并将接下来所有接收的数据添加到事务中，然后将这些数据写入外部sink系统，但不提交他们（这里是预提交）。当checkpoint完成时的通知，它才正式提交事务，实现结果的真正写入；这种方式真正实现了exactly-once,它需要一个提供事务支持的外部sink系统，Flink提供了其具体实现（TwoPhaseCommitSinkFunction接口）</p>
<h2 id="三、-2pc-对外部-sink的要求"><a href="#三、-2pc-对外部-sink的要求" class="headerlink" title="三、 2pc 对外部 sink的要求"></a>三、 2pc 对外部 sink的要求</h2><p>1、外部sink系统必须事务支持，或者sink任务必须能够模拟外部系统上的事务；<br>2、在checkpoint的间隔期间里，必须能够开启一个事务，并接受数据写入。<br>3、在收到checkpoint完成通知之前，事务必须是“等待提交”的状态，在故障恢复的情况线，这可能需要一些时间。如果个时候sink系统关闭事务（例如超时了），那么未提交的数据就会丢失；<br>4、四年任务必选能够在进程失败后恢复事务<br>5、提交事务必须是幂等操作；</p>
<p>四、综上不同Source和sink的一致性保证：<br><img src="_v_images/20201208154240979_1471214802.png" alt="在这里插入图片描述"></p>
<h2 id="五、应用（flinK-kafka-端到端一致性保证）"><a href="#五、应用（flinK-kafka-端到端一致性保证）" class="headerlink" title="五、应用（flinK+kafka 端到端一致性保证）"></a>五、应用（flinK+kafka 端到端一致性保证）</h2><p>flink 和kafka 端到端一致性(kafka(source+flink+kafka(sink)))<br>1、内部 – 利用checkpoint机制，把状态存盘，发生故障的时候可以恢复，保证内部的状态一致性<br>2、source – kafka consumer作为source，可以将偏移量保存下来，如果后续任务出现了故障，恢复的时候可以由连接器重置偏移量，重新消费数据，保证一致性；</p>
<figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-tag">kafka</span> 0.8 和<span class="selector-tag">kafka</span> 0.11 之后 通过以下配置将偏移量保存，恢复时候重新消费</span><br><span class="line"> <span class="selector-tag">kafka</span><span class="selector-class">.setStartFromLatest</span>();</span><br><span class="line"> <span class="selector-tag">kafka</span><span class="selector-class">.setCommitOffsetsOnCheckpoints</span>(<span class="selector-tag">false</span>);</span><br><span class="line"> <span class="selector-tag">kafka</span> 0.9 和<span class="selector-tag">kafka0</span>.10 未验证是否支持这两个参数(<span class="selector-tag">todo</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>3、sink FlinkkafkaProducer作为Sink，采用两阶段提交的sink，由下图可以看出flink 0.11 已经默认继承了TwoPhaseCommitSinkFunction<br><img src="_v_images/20201208154240548_914126344.png" alt="在这里插入图片描述"><br>但我们需要在参数种传入指定语义，它默认时还是at-least-once<br>此外我们还需要进行一些producer的容错配置：<br>（1）除了启用Flink的检查点之外，还可以通过将适当的semantic参数传递给FlinkKafkaProducer011（FlinkKafkaProducer对于Kafka&gt; = 1.0.0版本）<br>（2）来选择三种不同的操作模式<br>1）、Semantic.NONE 代表at-mostly-once语义<br>2）、Semantic.AT_LEAST_ONCE（Flink默认设置<br>3）、Semantic.EXACTLY_ONCE 使用Kafka事务提供一次精确的语义，每当您使用事务写入Kafka时<br>（3）、请不要忘记消费kafka记录任何应用程序设置所需的设置isolation.leva（read_committed 或者read_uncommitted-后者是默认）<br>read_committed，只是读取已经提交的数据。</p>
<p>应用；<br>Semantic.EXACTLY_ONCE依赖与下游系统能支持事务操作.以0.11kafka为例.<br>transaction.max.timeout.ms 最大超市时长，默认15分钟，如果需要用exactly语义，需要增加这个值。（因为它小于transaction.timeout.ms ）<br>isolation.level 如果需要用到exactly语义，需要在下级consumerConfig中设置read-commited [read-uncommited(默认值)]<br>transaction.timeout.ms 默认为1hour</p>
<p><strong>其参数对应关系为 和一些报错问题<br>checkpoint间隔&lt;transaction.timeout.ms&lt;transaction.max.timeout.ms</strong></p>
<p><strong>参考：<a target="_blank" rel="noopener" href="https://www.cnblogs.com/createweb/p/11971846.html">https://www.cnblogs.com/createweb/p/11971846.html</a></strong></p>
<p>注意：<br>1、Semantic.EXACTLY_ONCE 模式每个FlinkKafkaProducer011实例使用一个固定大小的KafkaProducers池。每个检查点使用这些生产者中的每一个。如果并发检查点的数量超过池大小，FlinkKafkaProducer011 将引发异常，并使整个应用程序失败。请相应地配置最大池大小和最大并发检查点数。</p>
<p>2、Semantic.EXACTLY_ONCE采取所有可能的措施，不要留下任何挥之不去的数据，否则这将有碍于消费者更多地阅读Kafka主题。但是，如果flink应用程序在第一个检查点之前失败，则在重新启动此类应用程序后，系统种将没有有关先前池大小信息，因此，在第一个检查点完成前按比例缩小Flink应用程序的FlinkKafkaProducer011.SAFE_SCALE_DOWN_FACTOR</p>
<figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">//1。设置最大允许的并行<span class="selector-tag">checkpoint</span>数，防止超过<span class="selector-tag">producer</span>池的个数发生异常</span><br><span class="line"><span class="selector-tag">env</span><span class="selector-class">.getCheckpointConfig</span><span class="selector-class">.setMaxConcurrentCheckpoints</span>(5) </span><br><span class="line">//2。设置<span class="selector-tag">producer</span>的<span class="selector-tag">ack</span>传输配置</span><br><span class="line">// 设置超市时长，默认15分钟，建议1个小时以上</span><br><span class="line"><span class="selector-tag">producerConfig</span><span class="selector-class">.put</span>(<span class="selector-tag">ProducerConfig</span><span class="selector-class">.ACKS_CONFIG</span>, 1) </span><br><span class="line"><span class="selector-tag">producerConfig</span><span class="selector-class">.put</span>(<span class="selector-tag">ProducerConfig</span><span class="selector-class">.TRANSACTION_TIMEOUT_CONFIG</span>, 15000) </span><br><span class="line"></span><br><span class="line">//3。在下一个<span class="selector-tag">kafka</span> <span class="selector-tag">consumer</span>的配置文件，或者代码中设置<span class="selector-tag">ISOLATION_LEVEL_CONFIG-read-commited</span></span><br><span class="line">//<span class="selector-tag">Note</span>:必须在下一个<span class="selector-tag">consumer</span>中指定，当前指定是没用用的</span><br><span class="line"><span class="selector-tag">kafkaonfigs</span><span class="selector-class">.setProperty</span>(<span class="selector-tag">ConsumerConfig</span><span class="selector-class">.ISOLATION_LEVEL_CONFIG</span>,&quot;<span class="selector-tag">read_commited</span>&quot;)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>完整应用代码：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.shufang.flink.connectors</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.Properties</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.serialization.SimpleStringSchema</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.TimeCharacteristic</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.functions.timestamps.BoundedOutOfOrdernessTimestampExtractor</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala._</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.windowing.time.Time</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer.Semantic</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.connectors.kafka._</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.util.serialization.KeyedSerializationSchemaWrapper</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.ConsumerConfig</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.ProducerConfig</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.common.serialization.StringDeserializer</span><br><span class="line"></span><br><span class="line">object KafkaSource01 &#123;</span><br><span class="line">  <span class="function">def <span class="title">main</span><span class="params">(args: Array[String])</span>: Unit </span>= &#123;</span><br><span class="line">    val env: StreamExecutionEnvironment = StreamExecutionEnvironment.getExecutionEnvironment</span><br><span class="line"></span><br><span class="line">    env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//这是checkpoint的超时时间</span></span><br><span class="line">    <span class="comment">//env.getCheckpointConfig.setCheckpointTimeout()</span></span><br><span class="line">    <span class="comment">//设置最大并行的chekpoint</span></span><br><span class="line">    env.getCheckpointConfig.setMaxConcurrentCheckpoints(<span class="number">5</span>)</span><br><span class="line">    env.getCheckpointConfig.setCheckpointInterval(<span class="number">1000</span>) <span class="comment">//增加checkpoint的中间时长，保证可靠性</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 为了保证数据的一致性，我们开启Flink的checkpoint一致性检查点机制，保证容错</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    env.enableCheckpointing(<span class="number">60000</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 从kafka获取数据，一定要记得添加checkpoint，能保证offset的状态可以重置，从数据源保证数据的一致性</span></span><br><span class="line"><span class="comment">     * 保证kafka代理的offset与checkpoint备份中保持状态一致</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line"></span><br><span class="line">    val kafkaonfigs = <span class="keyword">new</span> Properties()</span><br><span class="line"></span><br><span class="line">    <span class="comment">//指定kafka的启动集群</span></span><br><span class="line">    kafkaonfigs.setProperty(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, <span class="string">&quot;localhost:9092&quot;</span>)</span><br><span class="line">    <span class="comment">//指定消费者组</span></span><br><span class="line">    kafkaonfigs.setProperty(ConsumerConfig.GROUP_ID_CONFIG, <span class="string">&quot;flinkConsumer&quot;</span>)</span><br><span class="line">    <span class="comment">//指定key的反序列化类型</span></span><br><span class="line">    kafkaonfigs.setProperty(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, classOf[StringDeserializer].getName)</span><br><span class="line">    <span class="comment">//指定value的反序列化类型</span></span><br><span class="line">    kafkaonfigs.setProperty(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, classOf[StringDeserializer].getName)</span><br><span class="line">    <span class="comment">//指定自动消费offset的起点配置</span></span><br><span class="line">    <span class="comment">//    kafkaonfigs.setProperty(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, &quot;latest&quot;)</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 自定义kafkaConsumer，同时可以指定从哪里开始消费</span></span><br><span class="line"><span class="comment">     * 开启了Flink的检查点之后，我们还要开启kafka-offset的检查点，通过kafkaConsumer.setCommitOffsetsOnCheckpoints(true)开启，</span></span><br><span class="line"><span class="comment">     * 一旦这个检查点开启，那么之前配置的 auto-commit-enable = true的配置就会自动失效</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    val kafkaConsumer = <span class="keyword">new</span> FlinkKafkaConsumer[String](</span><br><span class="line">      <span class="string">&quot;console-topic&quot;</span>,</span><br><span class="line">      <span class="keyword">new</span> SimpleStringSchema(), <span class="comment">// 这个schema是将kafka的数据应设成Flink中的String类型</span></span><br><span class="line">      kafkaonfigs</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 开启kafka-offset检查点状态保存机制</span></span><br><span class="line">    kafkaConsumer.setCommitOffsetsOnCheckpoints(<span class="keyword">true</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//    kafkaConsumer.setStartFromEarliest()//</span></span><br><span class="line">    <span class="comment">//    kafkaConsumer.setStartFromTimestamp(1010003794)</span></span><br><span class="line">    <span class="comment">//    kafkaConsumer.setStartFromLatest()</span></span><br><span class="line">    <span class="comment">//    kafkaConsumer.setStartFromSpecificOffsets(Map[KafkaTopicPartition,Long]()</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 添加source数据源</span></span><br><span class="line">    val kafkaStream: DataStream[String] = env.addSource(kafkaConsumer)</span><br><span class="line"></span><br><span class="line">    kafkaStream.print()</span><br><span class="line"></span><br><span class="line">    val sinkStream: DataStream[String] = kafkaStream.assignTimestampsAndWatermarks(<span class="keyword">new</span> BoundedOutOfOrdernessTimestampExtractor[String](Time.seconds(<span class="number">5</span>)) &#123;</span><br><span class="line">      <span class="function">override def <span class="title">extractTimestamp</span><span class="params">(element: String)</span>: Long </span>= &#123;</span><br><span class="line">        element.split(<span class="string">&quot;,&quot;</span>)(<span class="number">1</span>).toLong</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 通过FlinkkafkaProduccer API将stream的数据写入到kafka的&#x27;sink-topic&#x27;中</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="comment">//    val brokerList = &quot;localhost:9092&quot;</span></span><br><span class="line">    val topic = <span class="string">&quot;sink-topic&quot;</span></span><br><span class="line">    val producerConfig = <span class="keyword">new</span> Properties()</span><br><span class="line">    producerConfig.put(ProducerConfig.ACKS_CONFIG, <span class="keyword">new</span> Integer(<span class="number">1</span>)) <span class="comment">// 设置producer的ack传输配置</span></span><br><span class="line">    producerConfig.put(ProducerConfig.TRANSACTION_TIMEOUT_CONFIG, Time.hours(<span class="number">2</span>)) <span class="comment">//设置超市时长，默认1小时，建议1个小时以上</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 自定义producer，可以通过不同的构造器创建</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    val producer: FlinkKafkaProducer[String] = <span class="keyword">new</span> FlinkKafkaProducer[String](</span><br><span class="line">      topic,</span><br><span class="line">      <span class="keyword">new</span> KeyedSerializationSchemaWrapper[String](SimpleStringSchema),</span><br><span class="line">      producerConfig,</span><br><span class="line">      Semantic.EXACTLY_ONCE</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment">//    FlinkKafkaProducer.SAFE_SCALE_DOWN_FACTOR</span></span><br><span class="line">    <span class="comment">/** *****************************************************************************************************************</span></span><br><span class="line"><span class="comment">     * * 出了要开启flink的checkpoint功能，同时还要设置相关配置功能。</span></span><br><span class="line"><span class="comment">     * * 因在0.9或者0.10，默认的FlinkKafkaProducer只能保证at-least-once语义，假如需要满足at-least-once语义，我们还需要设置</span></span><br><span class="line"><span class="comment">     * * setLogFailuresOnly(boolean)    默认false</span></span><br><span class="line"><span class="comment">     * * setFlushOnCheckpoint(boolean)  默认true</span></span><br><span class="line"><span class="comment">     * * come from 官网 below：</span></span><br><span class="line"><span class="comment">     * * Besides enabling Flink’s checkpointing，you should also configure the setter methods setLogFailuresOnly(boolean)</span></span><br><span class="line"><span class="comment">     * * and setFlushOnCheckpoint(boolean) appropriately.</span></span><br><span class="line"><span class="comment">     * ******************************************************************************************************************/</span></span><br><span class="line"></span><br><span class="line">    producer.setLogFailuresOnly(<span class="keyword">false</span>) <span class="comment">//默认是false</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 除了启用Flink的检查点之外，还可以通过将适当的semantic参数传递给FlinkKafkaProducer011（FlinkKafkaProducer对于Kafka&gt; = 1.0.0版本）</span></span><br><span class="line"><span class="comment">     * 来选择三种不同的操作模式：</span></span><br><span class="line"><span class="comment">     * Semantic.NONE  代表at-mostly-once语义</span></span><br><span class="line"><span class="comment">     * Semantic.AT_LEAST_ONCE（Flink默认设置）</span></span><br><span class="line"><span class="comment">     * Semantic.EXACTLY_ONCE：使用Kafka事务提供一次精确的语义，每当您使用事务写入Kafka时，</span></span><br><span class="line"><span class="comment">     * 请不要忘记为使用Kafka记录的任何应用程序设置所需的设置isolation.level（read_committed 或read_uncommitted-后者是默认值)</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line"></span><br><span class="line">    sinkStream.addSink(producer)</span><br><span class="line"></span><br><span class="line">    env.execute(<span class="string">&quot;kafka source &amp; sink&quot;</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="[参考文献]"></a>[参考文献]</h2><ol>
<li><a target="_blank" rel="noopener" href="http://shiyanjun.cn/archives/1855.html">Flink Checkpoint、Savepoint配置与实践</a></li>
<li><a target="_blank" rel="noopener" href="http://wuchong.me/blog/2018/11/04/how-apache-flink-manages-kafka-consumer-offsets/">Flink 小贴士 (2)：Flink 如何管理 Kafka 消费位点</a></li>
<li><a target="_blank" rel="noopener" href="https://www.jianshu.com/p/4bcbcda0e2f4">Flink实时计算-深入理解Checkpoint和Savepoint</a></li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1506.08603">Lightweight Asynchronous Snapshots for Distributed Dataflows: 分布式数据流轻量级异步快照</a></li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://example.com/bigdata/Impala/01.Impala/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="aaronzhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Guadazi">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/bigdata/Impala/01.Impala/" class="post-title-link" itemprop="url">Impala基础与痛点</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2019-08-10 14:58:00" itemprop="dateCreated datePublished" datetime="2019-08-10T14:58:00+08:00">2019-08-10</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-04-11 11:49:10" itemprop="dateModified" datetime="2021-04-11T11:49:10+08:00">2021-04-11</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/bigdata/" itemprop="url" rel="index"><span itemprop="name">bigdata</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="01-Impala"><a href="#01-Impala" class="headerlink" title="01.Impala"></a>01.Impala</h1><h2 id="components"><a href="#components" class="headerlink" title="components"></a>components</h2><p><a target="_blank" rel="noopener" href="https://impala.apache.org/docs/build/html/topics/impala_components.html">impala components</a></p>
<h3 id="impalad的部署"><a href="#impalad的部署" class="headerlink" title="impalad的部署"></a>impalad的部署</h3><p>Impala daemons can be deployed in one of the following ways:</p>
<ul>
<li>HDFS and Impala are co-located, and each Impala daemon runs on the same host as a DataNode.</li>
<li>Impala is deployed separately in a compute cluster and reads remotely from HDFS, S3, ADLS, etc.</li>
</ul>
<h3 id="hive连接表需要刷新impalad"><a href="#hive连接表需要刷新impalad" class="headerlink" title="hive连接表需要刷新impalad"></a>hive连接表需要刷新impalad</h3><p>The catalog service avoids the need to issue REFRESH and INVALIDATE METADATA statements when the metadata changes are performed by statements issued through Impala. When you create a table, load data, and so on through Hive, you do need to issue REFRESH or INVALIDATE METADATA on an Impala daemon before executing a query there.</p>
<p>impala 自己执行修改元数据的请求时，不需要刷新和重载元数据，当通过hive创建表和加载数据时，在执行查询之前，需要在Impala守护进程上发出REFRESH或INVALIDATE元数据。</p>
<p>The REFRESH and INVALIDATE METADATA statements are not needed when the CREATE TABLE, INSERT, or other table-changing or data-changing operation is performed through Impala. These statements are still needed if such operations are done through Hive or by manipulating data files directly in HDFS, but in those cases the statements only need to be issued on one Impala daemon rather than on all daemons. </p>
<p>当通过Impala执行创建表、插入或其他表更改或数据更改操作时，不需要刷新和无效元数据语句。<br>如果通过Hive或直接在HDFS中操作数据文件，仍然需要，但是在这种情况下，只需要在一个Impala守护进程上发出这些语句，而不是在所有守护进程上。</p>
<h3 id="元数据加载与对查询的影响"><a href="#元数据加载与对查询的影响" class="headerlink" title="元数据加载与对查询的影响"></a>元数据加载与对查询的影响</h3><p><code>‑‑load_catalog_in_background</code> option to control when the metadata of a table is loaded.<br>If set to false, the metadata of a table is loaded when it is referenced for the first time. This means that the first run of a particular query can be slower than subsequent runs. Starting in Impala 2.2, the default for ‑‑load_catalog_in_background is false.</p>
<p>表的元数据在第一次引用时加载。这意味着特定查询的第一次运行可能比后续运行慢</p>
<p>If set to true, the catalog service attempts to load metadata for a table even if no query needed that metadata. So metadata will possibly be already loaded when the first query that would need it is run. However, for the following reasons, we recommend not to set the option to true.</p>
<p>catalogd尝试加载表的元数据，即使没有查询需要该元数据。因此，在运行第一个需要元数据的查询时，元数据可能已经被加载</p>
<p>Background load can interfere with query-specific metadata loading. This can happen on startup or after invalidating metadata, with a duration depending on the amount of metadata, and can lead to a seemingly random long running queries that are difficult to diagnose.</p>
<p>后台加载可能会干扰特定查询的元数据加载。这种情况可能在启动时发生，也可能在元数据失效后发生，持续时间取决于元数据的数量，并可能导致看似随机的长时间运行查询，而这些查询很难诊断。</p>
<p>Impala may load metadata for tables that are possibly never used, potentially increasing catalog size and consequently memory usage for both catalog service and Impala Daemon.</p>
<p>Impala可以为可能从未使用过的表加载元数据，这可能会增加目录大小，从而增加目录服务和Impala守护进程的内存使用量。</p>
<h3 id="元数据刷新耗时"><a href="#元数据刷新耗时" class="headerlink" title="元数据刷新耗时"></a>元数据刷新耗时</h3><p>For tables with a large volume of data and/or many partitions, retrieving all the metadata for a table can be time-consuming, taking minutes in some cases. Thus, each Impala node caches all of this metadata to reuse for future queries against the same table.<br>对于具有大量数据和/或许多分区的表，检索表的所有元数据可能很耗时，在某些情况下会花费几分钟。<br>因此，每个Impala节点都会缓存所有这些元数据，以供将来针对同一表的查询重用。</p>
<p>If the table definition or the data in the table is updated, all other Impala daemons in the cluster must receive the latest metadata, replacing the obsolete cached metadata, before issuing a query against that table</p>
<p>For DDL and DML issued through Hive, or changes made manually to files in HDFS, you still use the REFRESH statement (when new data files are added to existing tables) or the INVALIDATE METADATA statement (for entirely new tables, or after dropping a table, performing an HDFS rebalance operation, or deleting data files). Issuing INVALIDATE METADATA by itself retrieves metadata for all the tables tracked by the metastore. If you know that only specific tables have been changed outside of Impala, you can issue REFRESH table_name for each affected table to only retrieve the latest metadata for those tables.</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://example.com/bigdata/Flink/metrics/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="aaronzhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Guadazi">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/bigdata/Flink/metrics/" class="post-title-link" itemprop="url">Flink:指标监控</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2019-08-10 14:58:00" itemprop="dateCreated datePublished" datetime="2019-08-10T14:58:00+08:00">2019-08-10</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-04-11 11:49:10" itemprop="dateModified" datetime="2021-04-11T11:49:10+08:00">2021-04-11</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/bigdata/" itemprop="url" rel="index"><span itemprop="name">bigdata</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="flink-metrics"><a href="#flink-metrics" class="headerlink" title="flink-metrics"></a>flink-metrics</h1><p>[参考文献]</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_21653785/article/details/79625601">Flink源码系列-指标监控</a></li>
<li><a target="_blank" rel="noopener" href="https://www.cnblogs.com/0x12345678/p/10561039.html">自定义metric-report</a></li>
<li><a target="_blank" rel="noopener" href="http://www.mamicode.com/info-detail-2317943.html">深入理解Flink之metrics</a></li>
<li><a target="_blank" rel="noopener" href="https://my.oschina.net/go4it/blog/3023586">聊聊Flink的MertricsQueryServiceGateway</a></li>
<li><a target="_blank" rel="noopener" href="https://www.jianshu.com/p/e50586fff515">Flink指标</a></li>
</ul>
<p>Flink Metrics是通过引入<code>com.codahale.metrics</code>包实现的，它将收集的metrics分为四大类：<code>Counter</code>，<code>Gauge</code>，<code>Histogram</code>和<code>Meter</code>下面分别说明：</p>
<ul>
<li><code>Counter计数器</code><br>  用来统计一个metrics的总量。<br>  拿flink中的指标来举例，像Task/Operator中的numRecordsIn（此task或者operator接收到的record总量）和numRecordsOut（此task或者operator发送的record总量）就属于Counter。</li>
<li><code>Gauge指标值</code><br>  用来记录一个metrics的瞬间值。<br>  拿flink中的指标举例，像JobManager或者TaskManager中的<code>JVM.Heap.Used</code>就属于<code>Gauge</code>，记录某个时刻JobManager或者TaskManager所在机器的JVM堆使用量。</li>
<li><code>Histogram直方图</code><br>  有的时候我们不满足于只拿到metrics的总量或者瞬时值，当想得到metrics的最大值，最小值，中位数等信息时，我们就能用到Histogram了。<br>  Flink中属于Histogram的指标很少，但是最重要的一个是属于operator的latency。此项指标会记录数据处理的延迟信息，对任务监控起到很重要的作用。</li>
<li><code>Meter平均值</code><br>   用来记录一个metrics某个时间段内平均值。<br>   flink中类似指标有task/operator中的numRecordsInPerSecond，字面意思就可以理解，指的是此task或者operator每秒接收的记录数。</li>
</ul>
<h3 id="com-tencent-oceanus-metastore-metrics-CustomMetricsRegistry"><a href="#com-tencent-oceanus-metastore-metrics-CustomMetricsRegistry" class="headerlink" title="com.tencent.oceanus.metastore.metrics.CustomMetricsRegistry"></a>com.tencent.oceanus.metastore.metrics.CustomMetricsRegistry</h3>
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://example.com/bigdata/Flink/Flink-metrics/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="aaronzhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Guadazi">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/bigdata/Flink/Flink-metrics/" class="post-title-link" itemprop="url">Flink:指标监控</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2019-08-10 14:58:00" itemprop="dateCreated datePublished" datetime="2019-08-10T14:58:00+08:00">2019-08-10</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-04-11 11:49:10" itemprop="dateModified" datetime="2021-04-11T11:49:10+08:00">2021-04-11</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/bigdata/" itemprop="url" rel="index"><span itemprop="name">bigdata</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="flink-metrics"><a href="#flink-metrics" class="headerlink" title="flink-metrics"></a>flink-metrics</h1><p>[参考文献]</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_21653785/article/details/79625601">Flink源码系列-指标监控</a></li>
<li><a target="_blank" rel="noopener" href="https://www.cnblogs.com/0x12345678/p/10561039.html">自定义metric-report</a></li>
<li><a target="_blank" rel="noopener" href="http://www.mamicode.com/info-detail-2317943.html">深入理解Flink之metrics</a></li>
<li><a target="_blank" rel="noopener" href="https://my.oschina.net/go4it/blog/3023586">聊聊Flink的MertricsQueryServiceGateway</a></li>
<li><a target="_blank" rel="noopener" href="https://www.jianshu.com/p/e50586fff515">Flink指标</a></li>
</ul>
<p>Flink Metrics是通过引入<code>com.codahale.metrics</code>包实现的，它将收集的metrics分为四大类：<code>Counter</code>，<code>Gauge</code>，<code>Histogram</code>和<code>Meter</code>下面分别说明：</p>
<ul>
<li><code>Counter计数器</code><br>  用来统计一个metrics的总量。<br>  拿flink中的指标来举例，像Task/Operator中的numRecordsIn（此task或者operator接收到的record总量）和numRecordsOut（此task或者operator发送的record总量）就属于Counter。</li>
<li><code>Gauge指标值</code><br>  用来记录一个metrics的瞬间值。<br>  拿flink中的指标举例，像JobManager或者TaskManager中的<code>JVM.Heap.Used</code>就属于<code>Gauge</code>，记录某个时刻JobManager或者TaskManager所在机器的JVM堆使用量。</li>
<li><code>Histogram直方图</code><br>  有的时候我们不满足于只拿到metrics的总量或者瞬时值，当想得到metrics的最大值，最小值，中位数等信息时，我们就能用到Histogram了。<br>  Flink中属于Histogram的指标很少，但是最重要的一个是属于operator的latency。此项指标会记录数据处理的延迟信息，对任务监控起到很重要的作用。</li>
<li><code>Meter平均值</code><br>   用来记录一个metrics某个时间段内平均值。<br>   flink中类似指标有task/operator中的numRecordsInPerSecond，字面意思就可以理解，指的是此task或者operator每秒接收的记录数。</li>
</ul>
<h3 id="com-tencent-oceanus-metastore-metrics-CustomMetricsRegistry"><a href="#com-tencent-oceanus-metastore-metrics-CustomMetricsRegistry" class="headerlink" title="com.tencent.oceanus.metastore.metrics.CustomMetricsRegistry"></a>com.tencent.oceanus.metastore.metrics.CustomMetricsRegistry</h3><h3 id="org-apache-flink-metrics-MeterView"><a href="#org-apache-flink-metrics-MeterView" class="headerlink" title="org.apache.flink.metrics.MeterView"></a>org.apache.flink.metrics.MeterView</h3>
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://example.com/Java/metric/Java%E7%94%9F%E4%BA%A7%E7%8E%AF%E5%A2%83%E4%B8%8B%E6%80%A7%E8%83%BD%E7%9B%91%E6%8E%A7%E4%B8%8E%E8%B0%83%E4%BC%98%E8%AF%A6%E8%A7%A3%E7%AC%94%E8%AE%B0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="aaronzhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Guadazi">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/Java/metric/Java%E7%94%9F%E4%BA%A7%E7%8E%AF%E5%A2%83%E4%B8%8B%E6%80%A7%E8%83%BD%E7%9B%91%E6%8E%A7%E4%B8%8E%E8%B0%83%E4%BC%98%E8%AF%A6%E8%A7%A3%E7%AC%94%E8%AE%B0/" class="post-title-link" itemprop="url">【转载】Java生产环境下性能监控与调优详解笔记</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2019-08-10 14:58:00" itemprop="dateCreated datePublished" datetime="2019-08-10T14:58:00+08:00">2019-08-10</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-04-11 11:49:10" itemprop="dateModified" datetime="2021-04-11T11:49:10+08:00">2021-04-11</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/java/" itemprop="url" rel="index"><span itemprop="name">java</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="Java生产环境下性能监控与调优详解笔记"><a href="#Java生产环境下性能监控与调优详解笔记" class="headerlink" title="Java生产环境下性能监控与调优详解笔记"></a>Java生产环境下性能监控与调优详解笔记</h1><p>另一个整理<a target="_blank" rel="noopener" href="http://alanhou.org/java-optimization/">http://alanhou.org/java-optimization/</a></p>
<h2 id="1：JVM字节码指令与-javap"><a href="#1：JVM字节码指令与-javap" class="headerlink" title="1：JVM字节码指令与 javap"></a>1：JVM字节码指令与 javap</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">javap &lt;options&gt; &lt;classes&gt;</span><br><span class="line"><span class="built_in">cd</span> monitor\_tuning/target/classes/org/alanhou/monitor\_tuning/chapter8/</span><br><span class="line">javap -verbose Test1.class &gt; Test1.txt</span><br></pre></td></tr></table></figure>
<p>即可保存字节码文件<br>会有三个部分组成<br>操作数栈<br>LineNumberTable<br>LocalVariableTable</p>
<p>i++和++i 的执行效果完全相同 多了一个压入栈顶操作<br>for(int i=0;i&lt;10;i++) {}<br>for(int i=0;i&lt;10;++i) {} 执行效果一样</p>
<p>2：</p>
<p>public static void f1() {<br>String src = “”;<br>for(int i=0;i&lt;10;i++) {<br>//每一次循环都会new一个StringBuilder 然后在src.append(“A”);<br>src = src + “A”;<br>}<br>System.out.println(src);<br>}<br>public static void f2() {<br>//只要一个StringBuilder<br>StringBuilder src = new StringBuilder();<br>for(int i=0;i&lt;10;i++) {<br>src.append(“A”);<br>}<br>System.out.println(src);<br>}</p>
<p>3：</p>
<p>public static String f1() {<br>String str = “hello”;<br>try{<br>return str;<br>}<br>finally{<br>str = “imooc”;<br>}<br>} 返回 hello 但会执行finally 中的代码</p>
<p>4：字符串拼接都会在编译阶段转换成stringbuilder</p>
<p>5:字符串去重</p>
<p>字符串在任何应用中都占用了大量的内存。尤其数包含独立UTF-16字符的char[]数组对JVM内存的消耗贡献最多——因为每个字符占用2位。</p>
<p>内存的30%被字符串消耗其实是很常见的，不仅是因为字符串是与我们互动的最好的格式，而且是由于流行的HTTP API使用了大量的字符串。使用Java 8 Update 20，我们现在可以接触到一个新特性，叫做字符串去重，该特性需要G1垃圾回收器，该垃圾回收器默认是被关闭的。</p>
<p>字符串去重利用了字符串内部实际是char数组，并且是final的特性，所以JVM可以任意的操纵他们。</p>
<p>对于字符串去重，开发者考虑了大量的策略，但最终的实现采用了下面的方式：</p>
<p>无论何时垃圾回收器访问了String对象，它会对char数组进行一个标记。它获取char数组的hash value并把它和一个对数组的弱引用存在一起。只要垃圾回收器发现另一个字符串，而这个字符串和char数组具有相同的hash code，那么就会对两者进行一个字符一个字符的比对。</p>
<p>如果他们恰好匹配，那么一个字符串就会被修改，指向第二个字符串的char数组。第一个char数组就不再被引用，也就可以被回收了。</p>
<p>这整个过程当然带来了一些开销，但是被很紧实的上限控制了。例如，如果一个字符未发现有重复，那么一段时间之内，它会不再被检查。</p>
<p>那么该特性实际上是怎么工作的呢？首先，你需要刚刚发布的Java 8 Update 20，然后按照这个配置: -Xmx256m -XX:+UseG1GC 去运行下列的代码:</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">LotsOfStrings</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> LinkedList LOTS_OF_STRINGS = <span class="keyword">new</span> LinkedList&lt;&gt;();</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> iteration = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line">            <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">100</span>; i++) &#123;</span><br><span class="line">                <span class="keyword">for</span> (<span class="keyword">int</span> j = <span class="number">0</span>; j &lt; <span class="number">1000</span>; j++) &#123;</span><br><span class="line">                    LOTS_OF_STRINGS.add(<span class="keyword">new</span> String(<span class="string">&quot;String &quot;</span> + j));</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">            iteration++;</span><br><span class="line">            System.out.println(<span class="string">&quot;Survived Iteration: &quot;</span> + iteration);</span><br><span class="line">            Thread.sleep(<span class="number">100</span>);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这段代码会执行30个迭代之后报OutOfMemoryError。</p>
<p>现在，开启字符串去重，使用如下配置去跑上述代码：</p>
<p>-Xmx256m -XX:+UseG1GC -XX:+UseStringDeduplication -XX:+PrintStringDeduplicationStatistics</p>
<p>此时它已经可以运行更长的时间，而且在50个迭代之后才终止。</p>
<p>6:</p>
<p>ArrayLIst  底层是数组  扩容会拷贝<br>hashmap   底层也是数组+ 链表 扩容 重新计算key 负载因子是 0.75  </p>
<p>linklist底层是双向链表<br>1. 尽量重用对象，不要循环创建对象，比如:for 循环字符串拼接(不在 for中使用+拼接，先new 一个StringBuilder再在 for 里 append)  </p>
<p>2. 容器类初始化的地时候指定长度  </p>
<p>List<String> collection = new ArrayLIst<String>(5);  </p>
<p>Map&lt;String, String&gt; map = new HashMap&lt;String, String&gt;(32);  </p>
<p>3. ArrayList（底层数组）随机遍历快，LinkedList（底层双向链表）添加删除快  </p>
<p>4. 集合遍历尽量减少重复计算  </p>
<p>5. 使用 Entry 遍历 Map可以同时取出key和value  </p>
<p>6. 大数组复制使用System.arraycopy 底层是native实现的  </p>
<p>7. 尽量使用基本类型而不是包装类型  </p>
<p>public class Test03 {  </p>
<p>  public static void main(String[] args) {<br>  Integer f1 = 100, f2 = 100, f3 = 150, f4 = 150;  </p>
<p>  System.out.println(f1 == f2);<br>  System.out.println(f3 == f4);<br>}<br>}  </p>
<p>如果不明就里很容易认为两个输出要么都是true要么都是false。首先需要注意的是f1、f2、f3、f4四个变量都是Integer对象引用，所以下面的==运算比较的不是值而是引用。装箱的本质是什么呢？当我们给一个Integer对象赋一个int值的时候，会调用Integer类的静态方法valueOf，如果看看valueOf的源代码就知道发生了什么。<br>public static Integer valueOf(int i) {<br>  if (i &gt;= IntegerCache.low &amp;&amp; i &lt;= IntegerCache.high)<br>    return IntegerCache.cache[i + (-IntegerCache.low)];<br>  return new Integer(i);<br>}<br>简单的说，如果整型字面量的值在-128到127之间，那么不会new新的Integer对象，而是直接引用常量池中的Integer对象，所以上面的面试题中f1==f2的结果是true，而f3==f4的结果是false。<br>8. 不要手动调用 System.gc()  </p>
<p>9. 及时消除过期对象的引用，防止内存泄漏<br>public string pop()<br>{<br>  string currentValue=object[size];<br>  //object[size]=null;如果不添加这句话就会造成内存泄漏<br>  size–;<br>  return currentValue;<br>}  </p>
<p>10. 尽量使用局部变量，减小变量的作用域 方便出了作用域尽快垃圾回收  </p>
<p>11. 尽量使用非同步的容器ArraryList vs. Vector  </p>
<p>12. 尽量减小同步作用范围, synchronized 方法 vs. 代码块  </p>
<p>public class SynchronizedTest {<br>  public static void main(String[] args) {<br>}<br>public synchronized void f1() {//在this對象上加鎖<br>  System.out.println(“f1”);<br>}<br>public  void f2() {//在this對象上加鎖<br>  synchronized(this) {<br>    System.out.println(“f2”);<br>  }<br>}<br>public static synchronized void f3() {//在类上加鎖<br>  System.out.println(“f3”);<br>}<br>public static void f4() {//在类上加鎖<br>  synchronized(SynchronizedTest.class) {<br>    System.out.println(“f4”);<br>  }<br>}<br>}  </p>
<p>13. 用ThreadLocal 缓存线程不安全的对象，SimpleDateFormat 缓存重量的对象避免重新构造<br>@SuppressWarnings(“rawtypes”)<br>    private static ThreadLocal threadLocal = new ThreadLocal() {<br>        protected synchronized Object initialValue() {<br>            return new SimpleDateFormat(DATE_FORMAT);<br>        }<br>    };  </p>
<p>14. 尽量使用延迟加载  </p>
<p>15. 尽量减少使用反射，必须用加缓存，反射比较影响性能  </p>
<p>16. 尽量使用连接池、线程池、对象池、缓存  </p>
<p>17. 及时释放资源， I/O 流、Socket、数据库连接  </p>
<p>18. 慎用异常，不要用抛异常来表示正常的业务逻辑，异常也是比较重的对象要记录堆栈信息  </p>
<p>19. String 操作尽量少用正则表达式 比如replaceAll是用正则 比较耗费性能 replace就不是用正则  </p>
<p>20. 日志输出注意使用不同的级别  </p>
<p>21. 日志中参数拼接使用占位符<br>log.info(“orderId:” + orderId); 不推荐 会用字符串拼接<br>log.info(“orderId:{}”, orderId); 推荐 用占位符 不会进行字符串拼接  </p>
<p>7：JVM的参数类型</p>
<p>标准参数（各版本中保持稳定）</p>
<p>-help</p>
<p>-server -client</p>
<p>-version -showversion</p>
<p>-cp -classpath</p>
<p>X 参数（非标准化参数）</p>
<p>-Xint：解释执行</p>
<p>-Xcomp：第一次使用就编译成本地代码</p>
<p>-Xmixed：混合模式，JVM 自己决定是否编译成本地代码</p>
<p>示例：</p>
<p>java -version（默认是混合模式）</p>
<p>Java HotSpot(TM) 64-Bit Server VM (build 25.40-b25, mixed mode)</p>
<p>java -Xint -version</p>
<p>Java HotSpot(TM) 64-Bit Server VM (build 25.40-b25, interpreted mode)</p>
<p>XX 参数（非标准化参数）</p>
<p>主要用于 JVM调优和 debug</p>
<ul>
<li>Boolean类型</li>
</ul>
<p>格式：-XX:[+-]<name>表示启用或禁用 name 属性<br>如：-XX:+UseConcMarkSweepGC<br>-XX:+UseG1GC</p>
<ul>
<li>非Boolean类型</li>
</ul>
<p>格式：-XX:<name>=<value>表示 name 属性的值是 value<br>如：-XX:MaxGCPauseMillis=500<br>-xx:GCTimeRatio=19<br>-Xmx -Xms属于 XX 参数<br>-Xms 等价于-XX:InitialHeapSize<br>-Xmx 等价于-XX:MaxHeapSize<br>-xss 等价于-XX:ThreadStackSize</p>
<h3 id="查看"><a href="#查看" class="headerlink" title="查看"></a>查看</h3><p>-XX:+PrintFlagsInitial 查看jvm初始值</p>
<p>-XX:+PrintFlagsFinal 查看jvm最终值</p>
<p>-XX:+UnlockExperimentalVMOptions 解锁实验参数</p>
<p>-XX:+UnlockDiagnosticVMOptions 解锁诊断参数</p>
<p>-XX:+PrintCommandLineFlags 打印命令行参数</p>
<p>输出结果中=表示默认值，:=表示被用户或 JVM 修改后的值</p>
<p>示例：java -XX:+PrintFlagsFinal -version</p>
<p>补充：测试中需要用到 Tomcat，CentOS 7安装示例如下</p>
<p><code>sudo </code>yum -y ``install java-1.8.0-openjdk*<br>wget  <a target="_blank" rel="noopener" href="http://mirror.bit.edu.cn/apache/tomcat/tomcat-8/v8.5.32/bin/apache-tomcat-8.5.32.tar.gz">http://mirror.bit.edu.cn/apache/tomcat/tomcat-8/v8.5.32/bin/apache-tomcat-8.5.32.tar.gz</a><br>tar -zxvf apache-tomcat-8.5.32.tar.gz<br>mv apache-tomcat-8.5.32 tomcat<br>cd tomcat/bin/sh startup.sh</p>
<p>pid 可通过类似 ps -ef|grep tomcat或 jps来进行查看</p>
<h3 id="jps"><a href="#jps" class="headerlink" title="jps"></a>jps</h3><p>查看java进程 -l 可以知道完全类名</p>
<h3 id="jinfo"><a href="#jinfo" class="headerlink" title="jinfo"></a>jinfo</h3><p>jinfo -flag MaxHeapSize <pid></p>
<p>jinfo -flags <pid>  手动赋过值的参数</p>
<h3 id="jstat"><a href="#jstat" class="headerlink" title="jstat"></a>jstat</h3><p>可以查看jvm的统计信息 如类加载。垃圾回收信息，jit编译信息</p>
<p>详情参考 <a target="_blank" rel="noopener" href="https://docs.oracle.com/javase/8/docs/technotes/tools/unix/jstat.html">jstat 官方文档</a></p>
<p><img src="_v_images/20200117100110226_7723.jpg" alt="jstat 使用示例"></p>
<p>类加载</p>
<h1 id="以下1000表每隔1000ms-即1秒，共输出10次"><a href="#以下1000表每隔1000ms-即1秒，共输出10次" class="headerlink" title="以下1000表每隔1000ms 即1秒，共输出10次"></a>以下1000表每隔1000ms 即1秒，共输出10次</h1><p>jstat -class <pid> 1000 10</p>
<p>垃圾收集</p>
<p>-gc, -gcutil, -gccause, -gcnew, -gcold</p>
<p>jstat -gc <pid> 1000 10</p>
<p>以下大小的单位均为 KB</p>
<p>![](_v_images/20200117100110111_28215.png =800x600)</p>
<p>S0C, S1C, S0U, S1U: S0和 S1的总量和使用量</p>
<p>EC, EU: Eden区总量与使用量</p>
<p>OC, OU: Old区总量与使用量</p>
<p>MC, MU: Metacspace区(jdk1.8前为 PermGen)总量与使用量</p>
<p>CCSC, CCSU: 压缩类区总量与使用量</p>
<p>YGC, YGCT: YoungGC 的次数与时间</p>
<p>FGC, FGCT: FullGC 的次数与时间</p>
<p>GCT: 总的 GC 时间</p>
<p>JIT 编译</p>
<p>-compiler, -printcompilation</p>
<p>一个对象默认分配在堆上面 但是有个指针指向class默认是64位长指针，可以设置为用32位存储在压缩类空间</p>
<p>非堆区 即对应于虚拟机规范中的方法区 是操作系统本地内存 独立于jvm堆区之外 jdk8后面叫metaspace jdk8前面叫performancespace</p>
<p>codecache 存储的是jit即时编译的代码 以及native代码</p>
<h3 id="jmap-MAT"><a href="#jmap-MAT" class="headerlink" title="jmap+MAT"></a>jmap+MAT</h3><p>详情参考<a target="_blank" rel="noopener" href="https://docs.oracle.com/javase/8/docs/technotes/tools/unix/jmap.html">jmap 官方文档</a></p>
<p>内存溢出演示：</p>
<p><a target="_blank" rel="noopener" href="https://start.spring.io/%E7%94%9F%E6%88%90%E5%88%9D%E5%A7%8B%E4%BB%A3%E7%A0%81">https://start.spring.io/生成初始代码</a></p>
<p>最终代码：<a target="_blank" rel="noopener" href="https://github.com/alanhou7/java-codes/tree/master/monitor_tuning">monitor_tuning</a></p>
<p>为快速产生内存溢出，右击 Run As&gt;Run Configurations, Arguments 标签VM arguments 中填入</p>
<p>-Xmx32M -Xms32M</p>
<p><img src="_v_images/20200117100109881_9995.png"></p>
<p>访问 <a target="_blank" rel="noopener" href="http://localhost:8080/heap">http://localhost:8080/heap</a></p>
<p>Exception in thread “http-nio-8080-exec-2” Exception in thread “http-nio-8080-exec-1” java.lang.OutOfMemoryError: Java heap space<br>java.lang.OutOfMemoryError: Java heap space</p>
<p>-XX:MetaspaceSize=32M -XX:MaxMetaspaceSize=32M（同时在 pom.xml 中加入 asm 的依赖）</p>
<p><img src="_v_images/20200117100109670_1795.png"></p>
<p>访问 <a target="_blank" rel="noopener" href="http://localhost:8080/nonheap">http://localhost:8080/nonheap</a></p>
<p>Exception in thread “main” java.lang.OutOfMemoryError: Metaspace<br>Exception in thread “ContainerBackgroundProcessor[StandardEngine[Tomcat]]“ java.lang.OutOfMemoryError: Metaspace</p>
<p>内存溢出自动导出</p>
<p>-XX:+HeapDumpOnOutOfMemoryError</p>
<p>-XX:HeapDumpPath=./</p>
<p>右击 Run As&gt;Run Configurations, Arguments 标签VM arguments 中填入</p>
<p>-Xmx32M -Xms32M -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=./</p>
<p>可以看到自动在当前目录中生成了一个java_pid660.hprof文件</p>
<p>java.lang.OutOfMemoryError: GC overhead limit exceeded<br>Dumping heap to ./java_pid660.hprof …</p>
<p>另一种导出溢出也更推荐的方式是jmap</p>
<p>option: -heap, -clstats, -dump:<dump-options>, -F</p>
<p>jmap -dump:format=b,file=heap.hprof <pid></p>
<p><img src="_v_images/20200117100109462_23769.jpg" alt="jmap 导出溢出文件"></p>
<p>MAT下载地址：<a target="_blank" rel="noopener" href="http://www.eclipse.org/mat/">http://www.eclipse.org/mat/</a></p>
<p>找开上述导出的内存溢出文件即可进行分析，如下图的溢出源头分析：</p>
<p><img src="_v_images/20200117100109352_8610.jpg" alt="Memory Analyzer 内存溢出分析"></p>
<ol>
<li>Histogram可以列出内存中的对象，对象的个数以及大小。</li>
<li>Dominator Tree可以列出那个线程，以及线程下面的那些对象占用的空间。</li>
</ol>
<p>Histogram</p>
<pre><code>[![](_v_images/20200117100109236_4753.png)](http://static.oschina.net/uploads/space/2014/0702/120039_qSi5_1767531.png)</code></pre>
<ul>
<li><p>Class Name ： 类名称，java类名</p>
</li>
<li><p>Objects ： 类的对象的数量，这个对象被创建了多少个</p>
</li>
<li><p>Shallow Heap ：一个对象内存的消耗大小，不包含对其他对象的引用</p>
</li>
</ul>
<ul>
<li>Retained Heap ：是shallow Heap的总和，也就是该对象被GC之后所能回收到内存的总和</li>
</ul>
<p>Dominator Tree</p>
<p><a target="_blank" rel="noopener" href="http://static.oschina.net/uploads/space/2014/0702/145926_K3ET_1767531.png"><img src="_v_images/20200117100109129_22861.png"></a></p>
<p>我们可以看到ibatis占了较多内存</p>
<p>快速找出某个实例没被释放的原因，可以右健 Path to GC Roots–&gt;exclue all phantom/weak/soft etc. reference :</p>
<p> <img src="_v_images/20200117100108918_9987.png"></p>
<p>得到的结果是：</p>
<p><img src="_v_images/20200117100108703_17828.png"></p>
<p>从表中可以看出 PreferenceManager -&gt; … -&gt;HomePage这条线路就引用着这个 HomePage实例。用这个方法可以快速找到某个对象的 <strong>GC Root</strong>,一个存在 GC Root的对象是不会被 GC回收掉的.</p>
<h3 id="jstack"><a href="#jstack" class="headerlink" title="jstack"></a>jstack</h3><p>详情参考 <a target="_blank" rel="noopener" href="https://docs.oracle.com/javase/8/docs/technotes/tools/unix/jstack.html">jstack 官方文档</a></p>
<p>jstack <pid>  打印jvm内部所有的线程</p>
<p> <em>jstack 15672 &gt;15673.txt  导出当前进程文件</em></p>
<p>可查看其中包含java.lang.Thread.State: WAITING (parking)，JAVA 线程包含的状态有：</p>
<p>NEW：线程尚未启动</p>
<p>RUNNABLE：线程正在 JVM 中执行</p>
<p>BLOCKED：线程在等待监控锁(monitor lock)</p>
<p>WAITING：线程在等待另一个线程进行特定操作（时间不确定）</p>
<p>TIMED_WAITING：线程等待另一个线程进行限时操作</p>
<p>TERMINATED：线程已退出</p>
<p>此时会生成一个monitor_tuning-0.0.1-SNAPSHOT.jar的 jar包，为避免本地的 CPU 消耗过多导致死机，建议上传上传到虚拟机进行测试</p>
<p>nohup java -jar monitor_tuning-0.0.1-SNAPSHOT.jar &amp;</p>
<p>访问 <a target="_blank" rel="noopener" href="http://xx.xx.xx.xx:12345/loop(%E7%AB%AF%E5%8F%A312345%E5%9C%A8application.properties%E6%96%87%E4%BB%B6%E4%B8%AD%E5%AE%9A%E4%B9%89)">http://xx.xx.xx.xx:12345/loop(端口12345在application.properties文件中定义)</a></p>
<p>top 是查询所有进程的cpu 占用率<br>top还可以用来显示一个进程中各个线程CPU的占用率：top -p <pid> -H<br>top命令如下  </p>
<p><img src="_v_images/20200117100108489_10785.png"></p>
<p>top -p <pid>  -H 命令如下 看的是7930的进程</p>
<p><img src="_v_images/20200117100108169_20096.png"></p>
<p>使用 jstack <pid>可以导出追踪文件，文件中 PID 在 jstack 中显示的对应 nid 为十六进制(命令行可执行 print ‘%x’ <pid>可以进行转化，如1640对应的十六进制为668)</p>
<p>“http-nio-12345-exec-3” #18 daemon prio=5 os_prio=0 tid=0x00007f10003fb000 nid=0x668 runnable [0x00007f0fcf8f9000]<br>   java.lang.Thread.State: RUNNABLE<br>    at org.alanhou.monitor_tuning.chapter2.CpuController.getPartneridsFromJson(CpuController.java:77)<br>…</p>
<p>访问<a target="_blank" rel="noopener" href="http://xx.xx.xx.xx:12345/deadlock">http://xx.xx.xx.xx:12345/deadlock</a>(如上jstack <pid>导出追踪记录会发现如下这样的记录)</p>
<p> ![](_v_images/20200117100107951_17762.png =800x500)</p>
<h1 id="Java-stack-information-for-the-threads-listed-above"><a href="#Java-stack-information-for-the-threads-listed-above" class="headerlink" title="Java stack information for the threads listed above:"></a>Java stack information for the threads listed above:</h1><p>“Thread-5”:<br>    at org.alanhou.monitor_tuning.chapter2.CpuController.lambda$deadlock$1(CpuController.java:41)<br>    - waiting to lock &lt;0x00000000edcf3470&gt; (a java.lang.Object)<br>    - locked &lt;0x00000000edcf3480&gt; (a java.lang.Object)<br>    at org.alanhou.monitor_tuning.chapter2.CpuController$$Lambda$337/547045985.run(Unknown Source)<br>    at java.lang.Thread.run(Thread.java:748)<br>“Thread-4”:<br>    at org.alanhou.monitor_tuning.chapter2.CpuController.lambda$deadlock$0(CpuController.java:33)<br>    - waiting to lock &lt;0x00000000edcf3480&gt; (a java.lang.Object)<br>    - locked &lt;0x00000000edcf3470&gt; (a java.lang.Object)<br>    at org.alanhou.monitor_tuning.chapter2.CpuController$$Lambda$336/1704575158.run(Unknown Source)<br>    at java.lang.Thread.run(Thread.java:748)</p>
<p>Found 1 deadlock.</p>
<p>查看后台日志，都是使用tail -f catalina.out命令来查看  </p>
<p>jvisualvm 图形化工具<br>插件安装Tools&gt;Plugins&gt;Settings根据自身版本(java -version)更新插件中心地址，各版本查询地址：<br><a target="_blank" rel="noopener" href="http://visualvm.github.io/pluginscenters.html">http://visualvm.github.io/pluginscenters.html</a><br> 建议安装：Visual GC, BTrace Workbench<br>概述 监控可以堆dump 线程可以线程dump 抽样器可以对cpu和内存进行抽样调查</p>
<p>以上是本地的JAVA进程监控，还可以进行远程的监控，在上图左侧导航的 Applications 下的 Remote 处右击Add Remote Host…，输入主机 IP 即可添加，在 IP 上右击会发现有两种连接 JAVA 进程进行监控的方式:JMX, jstatd</p>
<p>bin/catalina.sh(以192.168.0.5为例)</p>
<p>JAVA_OPTS=”$JAVA_OPTS -Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.port=9004 -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false -Djava.net.preferIPv4Stack=true -Djava.rmi.server.hostname=192.168.0.5”</p>
<p>启动tomcat，</p>
<p>启动tomcat服务<br>方式一：直接启动 ./startup.sh<br>方式二：作为服务启动 nohup ./startup.sh &amp;<br>查看tomcat运行日志<br>tail -f catalina.out</p>
<p>tomcat设置jvm参数<br>修改文件 apache-tomcat-9.0.10/bin下catalina.bat文件</p>
<p>以 JMX 为例，在 IP 上右击点击Add JMX Connection…，输入 IP:PORT</p>
<p><img src="_v_images/20200117100107732_2132.jpg" alt="Add JMX Connection"></p>
<p>以上为 Tomcat，其它 JAVA 进程也是类似的，如：</p>
<p>nohup java -Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.port=9005 -Dcom.sun.management.jmxremote.local.only=false -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false -Djava.net.preferIPv4Stack=true -Djava.rmi.server.hostname=192.168.0.5 -jar monitor_tuning-0.0.1-SNAPSHOT.jar &amp;</p>
<h3 id="BTrace"><a href="#BTrace" class="headerlink" title="BTrace"></a>BTrace</h3><p><a target="_blank" rel="noopener" href="https://github.com/jbachorik/btrace/releases/latest">BTrace</a> 可以动态地向目标应用程序的字节码注入追踪代码，使用的技术有 JavaCompilerApi, JVMTI, Agent, Instrumentation+ASM</p>
<p>使用方法：JVisualVM中添加 BTrace 插件</p>
<p>方法二：btrace <pid> <trace_script></p>
<p>btrace只能调试本地进程<br>btrace修改后的字节码不能被还原</p>
<p>pom.xml 中添加 btrace-agent, btrace-boot, btrace-client的依赖</p>
<p><img src="_v_images/20200117100107618_23079.png"></p>
<p><img src="_v_images/20200117100107506_24487.png"></p>
<p>拦截构造方法</p>
<p><img src="_v_images/20200117100107296_11386.png"></p>
<p>拦截同名方法  </p>
<p><img src="_v_images/20200117100107084_31796.png"></p>
<p>拦截返回值  </p>
<p><img src="_v_images/20200117100106870_26027.png"></p>
<p>拦截行号</p>
<p><img src="_v_images/20200117100106659_4260.png"></p>
<p>拦截异常信息</p>
<p><img src="_v_images/20200117100106449_4493.png"></p>
<p>拦截复杂类型</p>
<p><img src="_v_images/20200117100106216_21011.png"></p>
<p>拦截正则表达式</p>
<p><img src="_v_images/20200117100105902_16999.png"></p>
<p>拦截环境参数信息  </p>
<p><img src="_v_images/20200117100105692_8879.png">  </p>
<p>常用参数：  </p>
<p>-Xms -Xmx  </p>
<p>-XX:NewSize -XX:MaxNewSize  </p>
<p>-XX:NewRatio -XX:SurvivorRatio  </p>
<p>-XX:MetaspaceSize -XX:MaxMetaspaceSize 以下几个参数通常这样只设置这个值即可  </p>
<p>-XX:+UseCompressedClassPointers  </p>
<p>-XX:CompressedClassSpaceSize  </p>
<p>-XX:InitialCodeCacheSize  </p>
<p>-XX:ReservedCodeCacheSize</p>
<p>Tomcat 远程 Debug</p>
<p>JDWP</p>
<p>bin/startup.sh 修改最后一行(添加 jpda)</p>
<p>exec “$PRGDIR”/“$EXECUTABLE” jpda start “$@”</p>
<p>bin/catalina.sh 为便于远程调试进行如下修改</p>
<p>JPDA_ADDRESS=”localhost:8000”</p>
<h1 id="修改为"><a href="#修改为" class="headerlink" title="修改为"></a>修改为</h1><p>JPDA_ADDRESS=”54321”</p>
<p>若发现54321端口启动存在问题可尝试bin/catalina.sh jpda start</p>
<p>使用 Eclipse 远程调试，右击 Debug As &gt; Debug Configurations… &gt; Remote Java Application &gt; 右击 New 新建</p>
<p>普通java进程可以这样配置<br>java -jar -Xdebug -Xrunjdwp:transport=dt_socket,server=y,suspend=n,address=10001 access-10000.jar</p>
<p>tomcat-manager 监控</p>
<p>1.conf/tomcat-users.xml添加用户</p>
  <role rolename="tomcat"/>
  <role rolename="manager-status"/>
  <role rolename="manager-gui"/>
  <user username="tomcat" password="123456" roles="tomcat,manager-gui,manager-status"/>

<p>2.conf/Catalina/localhost/manager.xml配置允许的远程连接</p>
<?xml version="1.0" encoding="UTF-8"?>
<p><Context privileged="true" antiResourceLocking="false"
        docBase="$(catalina.home)/webapps/manager"><br>  <Valve className="org.apache.catalina.valves.RemoteAddrValve"
        allow="127\\.0\\.0\\.1" /><br></Context></p>
<p>远程连接将allow=”127\.0\.0\.1”修改为allow=”^.*$”，浏览器中输入<a target="_blank" rel="noopener" href="http://127.0.0.1:8080/manage%E6%88%96%E5%AF%B9%E5%BA%94%E7%9A%84">http://127.0.0.1:8080/manage或对应的</a> IP，用户名密码为tomcat-users.xml中所设置的</p>
<p>3.重启 Tomcat 服务</p>
<p><img src="_v_images/20200117100105380_4818.jpg" alt="Tomcat Manager"></p>
<p>psi-probe 监控</p>
<p>下载地址：<a target="_blank" rel="noopener" href="https://github.com/psi-probe/psi-probe%EF%BC%8C">https://github.com/psi-probe/psi-probe，</a></p>
<p>下载后进入psi-probe-master目录，执行：</p>
<p>mvn clean package -Dmaven.test.skip</p>
<p>将 web/target/probe.war放到 Tomcat 的 webapps 目录下，同样需要conf/tomcat-users.xml和conf/Catalina/localhost/manager.xml中的配置（可保持不变），启动 Tomcat 服务</p>
<p>浏览器中输入<a target="_blank" rel="noopener" href="http://127.0.0.1:8080/probe%E6%88%96%E5%AF%B9%E5%BA%94%E7%9A%84">http://127.0.0.1:8080/probe或对应的</a> IP，用户名密码为tomcat-users.xml中所设置的</p>
<p><img src="_v_images/20200117100105268_9524.jpg" alt="PSI Probe演示"></p>
<p>Tomcat 调优</p>
<p>线程优化（webapps/docs/config/http.html）：</p>
<p>maxConnections</p>
<p>acceptCount</p>
<p>maxThreads</p>
<p>minSpareThreads</p>
<p>配置优化（webapps/docs/config/host.html）：</p>
<p>autoDeploy</p>
<p>enableLookups（http.html）</p>
<p>reloadable（context.html）</p>
<p>protocol=”org.apache.coyote.http11.Http11AprProtocol”</p>
<p>Session 优化：</p>
<p>如果是 JSP, 可以禁用 Session</p>
<h3 id="Nginx-性能监控与调优"><a href="#Nginx-性能监控与调优" class="headerlink" title="Nginx 性能监控与调优"></a>Nginx 性能监控与调优</h3><p>Nginx 安装</p>
<p>添加 yum 源（/etc/yum.repos.d/nginx.repo）</p>
<p>[nginx]<br>name=nginx repo<br>baseurl=<a target="_blank" rel="noopener" href="http://nginx.org/packages/centos/7/$basesearch/">http://nginx.org/packages/centos/7/$basesearch/</a><br>gpgcheck=0<br>enabled=1</p>
<p>安装及常用命令</p>
<p>yum install -y nginx</p>
<p>systemctl status|start|stop|reload|restart nginx<br>nginx -s stop|reload|quit|reopen  nginx  启动nginx<br>cat default.conf | grep -v “#’ &gt; default2.conf  移除配置文件中的注释 并生成新的配置文件<br>nginx -V<br>nginx -t</p>
<p>配置反向代理 setenforce 0</p>
<p>ngx_http_stub_status 监控连接信息</p>
<p>location = /nginx_status {<br>    stub_status on;<br>    access_log off;<br>    allow 127.0.0.1;<br>    deny all;<br>}</p>
<p>可通过curl <a target="_blank" rel="noopener" href="http://127.0.0.1/nginx_status">http://127.0.0.1/nginx_status</a> 进行查看或注释掉 allow 和 deny 两行使用 IP 进行访问</p>
<p>ngxtop监控请求信息</p>
<p>查看官方使用方法：<a target="_blank" rel="noopener" href="https://github.com/lebinh/ngxtop">https://github.com/lebinh/ngxtop</a></p>
<h1 id="安装-python-pip"><a href="#安装-python-pip" class="headerlink" title="安装 python-pip"></a>安装 python-pip</h1><p>yum install epel-release<br>yum install python-pip</p>
<h1 id="安装-ngxtop"><a href="#安装-ngxtop" class="headerlink" title="安装 ngxtop"></a>安装 ngxtop</h1><p>pip install ngxtop</p>
<p>使用示例</p>
<p>指定配置文件：ngxtop -c /etc/nginx/nginx.conf</p>
<p>查询状态是200：ngxtop -c /etc/nginx/nginx.conf -i ‘status == 200’</p>
<p>查询访问最多 ip：ngxtop -c /etc/nginx/nginx.conf -g remote_addr</p>
<p><img src="_v_images/20200117100103032_6684.jpg" alt="ngxtop查询访问最多 ip"></p>
<p>Nginx 优化</p>
<p>增加工作线程数和并发连接数</p>
<p>worker_processes  4; # 一般CPU 是几核就设置为几<br>events {<br>    worker_connections  1024; # 每个进程打开的最大连接数，包含了 Nginx 与客户端和 Nginx 与 upstream 之间的连接<br>    multi_accept on; # 可以一次建立多个连接<br>    use epoll;<br>}</p>
<p>启用长连接</p>
<p>upstream server_pool{<br>    server localhost:8080 weight=1 max_fails=2 fail_timeout=30s;<br>    server localhost:8081 weight=1 max_fails=2 fail_timeout=30s;<br>    keepalive 300; # 300个长连接<br>}<br>location / {<br>    proxy_http_version 1.1;<br>    proxy_set_header Upgrade $http_upgrade;<br>    proxy_set_header Connection “upgrade”;<br>    proxy_pass <a target="_blank" rel="noopener" href="http://server/_pool">http://server\_pool</a>;<br>}</p>
<p>启用缓存压缩</p>
<p>gzip on;<br>gzip_http_version 1.1;<br>gzip_disable “MSIE [1-6]\.(?!.*SV1)”;<br>gzip_proxied any;<br>gzip_types text/plain text/css application/javascript application/x-javascript application/json application/xml application/vnd.ms-fontobject application/x-font-ttf application/svg+xml application/x-icon;<br>gzip_vary on;<br>gzip_static on;</p>
<p>操作系统优化</p>
<h1 id="配置文件-etc-sysctl-conf"><a href="#配置文件-etc-sysctl-conf" class="headerlink" title="配置文件/etc/sysctl.conf"></a>配置文件/etc/sysctl.conf</h1><p>sysctl -w net.ipv4.tcp_syncookies=1 # 防止一个套接字在有过多试图连接到时引起过载<br>sysctl -w net.core.somaxconn=1024 # 默认128，连接队列<br>sysctl -w net.ipv4.tcp_fin_timeout=10 # timewait 的超时时间<br>sysctl -w net.ipv4.tcp_tw_reuse=1 # os 直接使用 timewait的连接<br>sysctl -w net.ipv4.tcp_tw_recycle=0 # 回收禁用</p>
<h1 id="etc-security-limits-conf"><a href="#etc-security-limits-conf" class="headerlink" title="/etc/security/limits.conf"></a>/etc/security/limits.conf</h1><ul>
<li><pre><code>          hard    nofile            204800</code></pre>
</li>
<li><pre><code>          soft    nofile             204800</code></pre>
</li>
<li><pre><code>          soft    core             unlimited</code></pre>
</li>
<li><pre><code>          soft    stack             204800</code></pre>
</li>
</ul>
<p>其它优化</p>
<p>sendfile    on; # 减少文件在应用和内核之间拷贝<br>tcp_nopush    on; # 当数据包达到一定大小再发送<br>tcp_nodelay    off; # 有数据随时发送</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://example.com/bigdata/Flink/flink-on-yarn/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="aaronzhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Guadazi">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/bigdata/Flink/flink-on-yarn/" class="post-title-link" itemprop="url">Flink on yarn</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2019-08-10 14:58:00" itemprop="dateCreated datePublished" datetime="2019-08-10T14:58:00+08:00">2019-08-10</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-04-11 11:49:10" itemprop="dateModified" datetime="2021-04-11T11:49:10+08:00">2021-04-11</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/bigdata/" itemprop="url" rel="index"><span itemprop="name">bigdata</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="flink-on-yarn部署"><a href="#flink-on-yarn部署" class="headerlink" title="flink on yarn部署"></a>flink on yarn部署</h2><p>flink on yarn需要的组件与版本如下</p>
<ol>
<li>Zookeeper 3.4.9 用于做Flink的JobManager的HA服务</li>
<li>hadoop 2.7.2 搭建HDFS和Yarn</li>
<li>flink 1.3.2 或者 1.4.1版本（scala 2.11）</li>
</ol>
<p>Zookeeper, HDFS 和 Yarn 的组件的安装可以参照网上的教程。</p>
<p>在zookeeper，HDFS 和Yarn的组件的安装好的前提下，在客户机上提交Flink任务，具体流程如下：</p>
<ul>
<li>在启动Yarn-Session 之前， 设置好HADOOP_HOME,YARN_CONF_DIR ， HADOOP_CONF_DIR环境变量中三者的一个。如下所示， 根据具体的hadoop 路径来设置<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">export</span> HADOOP_HOME=/usr/<span class="built_in">local</span>/hadoop-current</span></span><br></pre></td></tr></table></figure></li>
<li>配置flink 目录下的flink-conf.yaml, 如下所示<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">jobmanager.rpc.address:</span> <span class="string">localhost</span></span><br><span class="line"><span class="attr">jobmanager.rpc.port:</span> <span class="number">6123</span></span><br><span class="line"><span class="attr">jobmanager.heap.mb:</span> <span class="number">256</span></span><br><span class="line"><span class="attr">taskmanager.heap.mb:</span> <span class="number">512</span></span><br><span class="line"><span class="attr">taskmanager.numberOfTaskSlots:</span> <span class="number">1</span></span><br><span class="line"><span class="attr">taskmanager.memory.preallocate:</span> <span class="literal">false</span></span><br><span class="line"><span class="attr">parallelism.default:</span> <span class="number">1</span></span><br><span class="line"><span class="attr">jobmanager.web.port:</span> <span class="number">8081</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># yarn</span></span><br><span class="line"><span class="attr">yarn.maximum-failed-containers:</span> <span class="number">99999</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#akka config</span></span><br><span class="line"><span class="attr">akka.watch.heartbeat.interval:</span> <span class="number">5</span> <span class="string">s</span></span><br><span class="line"><span class="attr">akka.watch.heartbeat.pause:</span> <span class="number">20</span> <span class="string">s</span></span><br><span class="line"><span class="attr">akka.ask.timeout:</span> <span class="number">60</span> <span class="string">s</span></span><br><span class="line"><span class="attr">akka.framesize:</span> <span class="string">20971520b</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#high-avaliability</span></span><br><span class="line"><span class="attr">high-availability:</span> <span class="string">zookeeper</span></span><br><span class="line"><span class="comment">## 根据安装的zookeeper信息填写</span></span><br><span class="line"><span class="attr">high-availability.zookeeper.quorum:</span> <span class="number">10.141</span><span class="number">.61</span><span class="number">.226</span><span class="string">:2181,10.141.53.244:2181,10.141.18.219:2181</span></span><br><span class="line"><span class="attr">high-availability.zookeeper.path.root:</span> <span class="string">/flink</span></span><br><span class="line"><span class="comment">## HA 信息存储到HDFS的目录，根据各自的Hdfs情况修改</span></span><br><span class="line"><span class="attr">high-availability.zookeeper.storageDir:</span> <span class="string">hdfs://hdcluster/flink/recovery/</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#checkpoint config</span></span><br><span class="line"><span class="attr">state.backend:</span> <span class="string">rocksdb</span></span><br><span class="line"><span class="comment">## checkpoint到HDFS的目录 根据各自安装的HDFS情况修改</span></span><br><span class="line"><span class="attr">state.backend.fs.checkpointdir:</span> <span class="string">hdfs://hdcluster/flink/checkpoint</span></span><br><span class="line"><span class="comment">## 对外checkpoint到HDFS的目录</span></span><br><span class="line"><span class="attr">state.checkpoints.dir:</span> <span class="string">hdfs://hdcluster/flink/savepoint</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#memory config</span></span><br><span class="line"><span class="attr">env.java.opts:</span> <span class="string">-XX:+UseConcMarkSweepGC</span> <span class="string">-XX:CMSInitiatingOccupancyFraction=75</span> <span class="string">-XX:+UseCMSInitiatingOccupancyOnly</span> <span class="string">-XX:+AlwaysPreTouch</span> <span class="string">-server</span> <span class="string">-XX:+HeapDumpOnOutOfMemoryError</span></span><br><span class="line"><span class="attr">yarn.heap-cutoff-ratio:</span> <span class="number">0.2</span></span><br><span class="line"><span class="attr">taskmanager.memory.off-heap:</span> <span class="literal">true</span></span><br><span class="line"></span><br></pre></td></tr></table></figure></li>
<li>提交Yarn-Session，切换到flink的bin 目录下,提交命令如下<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> ./yarn-session.sh -n 2 -s 6 -jm 3072 -tm 6144 -nm <span class="built_in">test</span> -d</span></span><br></pre></td></tr></table></figure>
启动yarn-session的参数解释如下</li>
</ul>
<table>
<thead>
<tr>
<th>参数</th>
<th>参数解释</th>
<th>设置推荐</th>
</tr>
</thead>
<tbody><tr>
<td>-n(–container)</td>
<td>taskmanager的数量</td>
<td></td>
</tr>
<tr>
<td>-s(–slots)</td>
<td>用启动应用所需的slot数量/ -s 的值向上取整，有时可以多一些taskmanager，做冗余 每个taskmanager的slot数量，默认一个slot一个core，默认每个taskmanager的slot的个数为1</td>
<td>6～10</td>
</tr>
<tr>
<td>-jm</td>
<td>jobmanager的内存（单位MB)</td>
<td>3072</td>
</tr>
<tr>
<td>-tm</td>
<td>每个taskmanager的内存（单位MB)</td>
<td>根据core 与内存的比例来设置，-s的值＊ （core与内存的比）来算</td>
</tr>
<tr>
<td>-nm</td>
<td>yarn 的appName(现在yarn的ui上的名字)｜</td>
<td></td>
</tr>
<tr>
<td>-d</td>
<td>后台执行</td>
<td></td>
</tr>
</tbody></table>
<ul>
<li>提交yarn－session 后，可以在yarn的ui上看到一个应用（应用有一个appId）, 切换到flink的bin目录下，提交flink 应用。命令如下<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> ./flink -run file:///home/yarn/test.jar -a 1 -p 12 -yid appId -nm flink-test -d</span></span><br></pre></td></tr></table></figure>
启动flink 应用的参数解释如下</li>
</ul>
<table>
<thead>
<tr>
<th>参数</th>
<th>参数解释</th>
</tr>
</thead>
<tbody><tr>
<td>-j</td>
<td>运行flink 应用的jar所在的目录</td>
</tr>
<tr>
<td>-a</td>
<td>运行flink 应用的主方法的参数</td>
</tr>
<tr>
<td>-p</td>
<td>运行flink应用的并行度</td>
</tr>
<tr>
<td>-c</td>
<td>运行flink应用的主类, 可以通过在打包设置主类</td>
</tr>
<tr>
<td>-nm</td>
<td>flink 应用名字，在flink-ui 上面展示</td>
</tr>
<tr>
<td>-d</td>
<td>后台执行</td>
</tr>
<tr>
<td>–fromsavepoint</td>
<td>flink 应用启动的状态恢复点</td>
</tr>
</tbody></table>
<ul>
<li>启动flink应用成功，即可在yarn ui 点击对应应用的ApplicationMaster链接,既可以查看flink-ui ，并查看flink 应用运行情况。</li>
</ul>
<p>注：在安装部署遇到任何问题，可以在小象问答，微信群以及私聊提出，我们一般会在晚上作答（由于白天要上班，作答不及时请谅解。）</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://example.com/bigdata/Flink/Flink-StateManagement/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="aaronzhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Guadazi">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/bigdata/Flink/Flink-StateManagement/" class="post-title-link" itemprop="url">Flink状态管理</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2019-08-10 14:58:00" itemprop="dateCreated datePublished" datetime="2019-08-10T14:58:00+08:00">2019-08-10</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-04-11 11:49:10" itemprop="dateModified" datetime="2021-04-11T11:49:10+08:00">2021-04-11</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/bigdata/" itemprop="url" rel="index"><span itemprop="name">bigdata</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="state-management"><a href="#state-management" class="headerlink" title="state-management"></a>state-management</h1><h2 id="org-apache-flink-streaming-api-checkpoint-CheckpointedFunction"><a href="#org-apache-flink-streaming-api-checkpoint-CheckpointedFunction" class="headerlink" title="org.apache.flink.streaming.api.checkpoint.CheckpointedFunction"></a>org.apache.flink.streaming.api.checkpoint.CheckpointedFunction</h2><ul>
<li>CheckpointedFunction是stateful transformation functions的核心接口，用于跨stream维护state<ul>
<li>snapshotState 在checkpoint的时候会被调用，用于snapshot state，通常用于flush、commit、synchronize外部系统</li>
<li>initializeState 在parallel function初始化的时候(<strong>第一次初始化或者从前一次checkpoint recover的时候</strong>)被调用，通常用来初始化state，以及处理state recovery的逻辑</li>
</ul>
</li>
</ul>
<p>从checkpoint中恢复数据时，需要判断snapshot当前的情况，</p>
<p>FunctionSnapshotContext实现了ManagedSnapshotContext, 父类中的方法: <code>getCheckpointId</code>,<code>getCheckpointTimestamp</code><br>FunctionInitializationContext实现了ManagedInitializationContext接口, 实现了<code>isRestored</code>、<code>getOperatorStateStore</code>、<code>getKeyedStateStore</code>方法</p>
<p>在初始化容器之后，我们使用上下文的<code>isrestore()</code>方法检查失败后是否正在恢复。如果是true，即正在恢复，则应用恢复逻辑。</p>
<blockquote>
<p>样例: HBase写入OutPutFormat</p>
</blockquote>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br></pre></td><td class="code"><pre><span class="line"> <span class="class"><span class="keyword">class</span> <span class="title">PortraitOutputFormat</span> <span class="keyword">extends</span> <span class="title">RichOutputFormat</span>&lt;<span class="title">EventItem</span>&gt; <span class="keyword">implements</span> <span class="title">CheckpointedFunction</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 输出阈值，批量写入的条数</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">int</span> threshold;</span><br><span class="line">    <span class="comment">// 维护在状态中的数据</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">transient</span> ListState&lt;EventItem&gt; checkpointState;</span><br><span class="line">    <span class="comment">// 内存中的数据</span></span><br><span class="line">    <span class="keyword">private</span> List&lt;EventItem&gt; bufferedEventItem;</span><br><span class="line">    <span class="comment">// HBase客户端</span></span><br><span class="line">    <span class="keyword">private</span> HBaseClient hbaseClient;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">PortraitOutputFormat</span><span class="params">(HBaseClient hbaseClient)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.hbaseClient = hbaseClient;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">    * checkpoint时调用</span></span><br><span class="line"><span class="comment">    * 执行snapshot操作，将内存中的数据写入到内存</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">snapshotState</span><span class="params">(FunctionSnapshotContext functionSnapshotContext)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        checkpointState.clear();</span><br><span class="line">        <span class="keyword">for</span> (EventItem eventItem : bufferedEventItem) &#123;</span><br><span class="line">            checkpointState.add(eventItem);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">    * 创建state，判断是否存在需要恢复的状态，如果有则需要恢复到bufferedEventItem</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">initializeState</span><span class="params">(FunctionInitializationContext context)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        ListStateDescriptor&lt;EventItem&gt; descriptor = <span class="keyword">new</span> ListStateDescriptor&lt;&gt;(<span class="string">&quot;buf-p&quot;</span>, EventItem.class);</span><br><span class="line">        checkpointState = context.getOperatorStateStore().getListState(descriptor);</span><br><span class="line">        <span class="keyword">if</span> (context.isRestored()) &#123;</span><br><span class="line">            <span class="keyword">for</span> (EventItem eventItem : checkpointState.get()) &#123;</span><br><span class="line">                bufferedEventItem.add(eventItem);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">    *</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">configure</span><span class="params">(Configuration configuration)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">    *</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">open</span><span class="params">(<span class="keyword">int</span> taskNumber, <span class="keyword">int</span> numTasks)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">    * 将新消息写入到缓存bufferedEventItem，缓存个数大约threshold,则执行sink写入，然后清空bufferedEventItem</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">writeRecord</span><span class="params">(EventItem value)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (value.getAttachUserId() == <span class="keyword">null</span>) &#123;</span><br><span class="line">            <span class="keyword">return</span>;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        bufferedEventItem.add(value);</span><br><span class="line">        <span class="keyword">int</span> size = bufferedEventItem.size();</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (size &gt;= threshold) &#123;</span><br><span class="line">            List&lt;Put&gt; puts = bufferedEventItem</span><br><span class="line">                    .stream()</span><br><span class="line">                    .map(eventItem -&gt; &#123;</span><br><span class="line">                        String rowKey1 = portraitDataGenerator.rowKey(eventItem);</span><br><span class="line">                        Map&lt;String, String&gt; data = portraitDataGenerator.data(eventItem);</span><br><span class="line">                        Put put = <span class="keyword">new</span> Put(rowKey1.getBytes());</span><br><span class="line">                        <span class="keyword">for</span> (String cfc : data.keySet()) &#123;</span><br><span class="line">                            String[] cfcs = cfc.split(<span class="string">&quot;:&quot;</span>);</span><br><span class="line">                            String cf = cfcs[<span class="number">0</span>];</span><br><span class="line">                            String c = cfcs[<span class="number">1</span>];</span><br><span class="line">                            String dataOne = data.get(cfc);</span><br><span class="line">                            put.addColumn(cf.getBytes(), c.getBytes(), dataOne.getBytes());</span><br><span class="line">                        &#125;</span><br><span class="line">                        <span class="keyword">return</span> put;</span><br><span class="line">                    &#125;)</span><br><span class="line">                    .collect(Collectors.toList());</span><br><span class="line">            <span class="keyword">try</span> &#123;</span><br><span class="line">                hbaseClient.putAndFlush(puts);</span><br><span class="line">            &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">                e.printStackTrace();</span><br><span class="line">            &#125;</span><br><span class="line">            bufferedEventItem.clear();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">    *</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">close</span><span class="params">()</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (hTable != <span class="keyword">null</span>) &#123;</span><br><span class="line">            hTable.flushCommits();</span><br><span class="line">            hTable.close();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> (connection != <span class="keyword">null</span>) &#123;</span><br><span class="line">            connection.close();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>




<h2 id="org-apache-flink-runtime-state-CheckpointListener"><a href="#org-apache-flink-runtime-state-CheckpointListener" class="headerlink" title="org.apache.flink.runtime.state.CheckpointListener"></a>org.apache.flink.runtime.state.CheckpointListener</h2><p>一旦所有checkpoint参与者确认完全，该接口必须由想要接收提交通知的功能/操作来实现。</p>
<h1 id="TTL"><a href="#TTL" class="headerlink" title="TTL"></a>TTL</h1><p>1.8 自动清理原理</p>
<p>Apache Flink的1.6.0版本引入了State TTL功能。它使流处理应用程序的开发人员配置过期时间，并在定义时间超时（Time to Live）之后进行清理。在Flink 1.8.0中，该功能得到了扩展，包括对RocksDB和堆状态后端（FSStateBackend和MemoryStateBackend）的历史数据进行持续清理，从而实现旧条目的连续清理过程（根据TTL设置）。</p>
<p>RocksDB后台压缩可以过滤掉过期状态<br>如果你的Flink应用程序使用RocksDB作为状态后端存储，则可以启用另一个基于Flink特定压缩过滤器的清理策略。RocksDB定期运行异步压缩以合并状态更新并减少存储。Flink压缩过滤器使用TTL检查状态条目的到期时间戳，并丢弃所有过期值。</p>
<p>激活此功能的第一步是通过设置以下Flink配置选项来配置RocksDB状态后端：</p>
<p>state.backend.rocksdb.ttl.compaction.filter.enabled</p>
<p>配置RocksDB状态后端后，将为状态启用压缩清理策略，如以下代码示例所示：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">StateTtlConfig ttlConfig &#x3D; StateTtlConfig</span><br><span class="line">    .newBuilder(Time.days(7))</span><br><span class="line">    .cleanupInRocksdbCompactFilter()</span><br><span class="line">    .build();</span><br></pre></td></tr></table></figure>


<h2 id="backend-状态后端"><a href="#backend-状态后端" class="headerlink" title="backend 状态后端"></a>backend 状态后端</h2><table>
<thead>
<tr>
<th>state</th>
<th>保存</th>
<th>snapshot与restore</th>
<th>大小</th>
</tr>
</thead>
<tbody><tr>
<td>keyed state</td>
<td>堆内或堆外(RocksDB)</td>
<td>backend自行实现，用户不关心</td>
<td>大</td>
</tr>
<tr>
<td>operator state</td>
<td>堆内</td>
<td>用户自行实现</td>
<td>小</td>
</tr>
</tbody></table>
<p><img src="_v_images/20201208101748835_1926021392.png"></p>
<p>Flink 的 keyed state 本质上来说就是一个键值对，所以与 RocksDB 的数据模型是吻合的。下图分别是 “window state” 和 “value state” 在 RocksDB 中的存储格式，所有存储的 key，value 均被序列化成 bytes 进行存储。</p>
<p><img src="_v_images/20201208113819762_751391291.png"></p>
<p>在 RocksDB 中，每个 state 独享一个 Column Family，而每个 Column family 使用各自独享的 write buffer 和 block cache，上图中的 window state 和 value state实际上分属不同的 column family。</p>
<h3 id="最佳实践"><a href="#最佳实践" class="headerlink" title="最佳实践"></a>最佳实践</h3><h4 id="operator-state"><a href="#operator-state" class="headerlink" title="operator state"></a>operator state</h4><h5 id="慎重使用长-list"><a href="#慎重使用长-list" class="headerlink" title="慎重使用长 list"></a>慎重使用长 list</h5><p>下图展示的是目前 task 端 operator state 在执行完 checkpoint 返回给 job master 端的 StateMetaInfo 的代码片段。</p>
<p><img src="_v_images/20201208113527702_980726558.png"></p>
<p>由于 operator state 没有 key group 的概念，所以为了实现改并发恢复的功能，需要对 operator state 中的每一个序列化后的元素存储一个位置偏移 offset，也就是构成了上图红框中的 offset 数组。  </p>
<p>那么如果你的 operator state 中的 list 长度达到一定规模时，这个 offset 数组就可能会有几十 MB 的规模，关键这个数组是会返回给 job master，当 operator 的并发数目很大时，很容易触发 job master 的内存超用问题。我们遇到过用户把 operator state 当做黑名单存储，结果这个黑名单规模很大，导致一旦开始执行 checkpoint，job master 就会因为收到 task 发来的“巨大”的 offset 数组，而内存不断增长直到超用无法正常响应。</p>
<h5 id="正确使用-UnionListState"><a href="#正确使用-UnionListState" class="headerlink" title="正确使用 UnionListState"></a>正确使用 UnionListState</h5><p>union list state 目前被广泛使用在 kafka connector 中，不过可能用户日常开发中较少遇到，他的语义是从检查点恢复之后每个并发 task 内拿到的是原先所有operator 上的 state，如下图所示：</p>
<p><img src="_v_images/20201208113559017_2066750174.png"></p>
<p>kafka connector 使用该功能，为的是从检查点恢复时，可以拿到之前的全局信息，如果用户需要使用该功能，需要切记恢复的 task 只取其中的一部分进行处理和用于下一次 snapshot，否则有可能随着作业不断的重启而导致 state 规模不断增长。</p>
<h4 id="Keyed-state-使用建议"><a href="#Keyed-state-使用建议" class="headerlink" title="Keyed state 使用建议"></a>Keyed state 使用建议</h4><h5 id="如何正确清空当前的-state"><a href="#如何正确清空当前的-state" class="headerlink" title="如何正确清空当前的 state"></a>如何正确清空当前的 state</h5><p>state.clear() 实际上只能清理当前 key 对应的 value 值，如果想要清空整个 state，需要借助于 applyToAllKeys 方法，具体代码片段如下：</p>
<p><img src="_v_images/20201208113620034_1338097160.png"></p>
<p>如果你的需求中只是对 state 有过期需求，借助于 state TTL 功能来清理会是一个性能更好的方案。</p>
<h5 id="RocksDB-中考虑-value-值很大的极限场景"><a href="#RocksDB-中考虑-value-值很大的极限场景" class="headerlink" title="RocksDB 中考虑 value 值很大的极限场景"></a>RocksDB 中考虑 value 值很大的极限场景</h5><p>受限于 JNI bridge API 的限制，单个 value 只支持 2^31 bytes 大小，如果存在很极限的情况，可以考虑使用 MapState 来替代 ListState 或者 ValueState，因为RocksDB 的 map state 并不是将整个 map 作为 value 进行存储，而是将 map 中的一个条目作为键值对进行存储。</p>
<h5 id="如何知道当前-RocksDB-的运行情况"><a href="#如何知道当前-RocksDB-的运行情况" class="headerlink" title="如何知道当前 RocksDB 的运行情况"></a>如何知道当前 RocksDB 的运行情况</h5><p>比较直观的方式是打开 RocksDB 的 native metrics ，在默认使用 Flink managed memory 方式的情况下，state.backend.rocksdb.metrics.block-cache-usage ，state.backend.rocksdb.metrics.mem-table-flush-pending，state.backend.rocksdb.metrics.num-running-compactions 以及 state.backend.rocksdb.metrics.num-running-flushes 是比较重要的相关 metrics。</p>
<h4 id="使用-checkpoint-的使用建议"><a href="#使用-checkpoint-的使用建议" class="headerlink" title="使用 checkpoint 的使用建议"></a>使用 checkpoint 的使用建议</h4><h5 id="Checkpoint-间隔不要太短"><a href="#Checkpoint-间隔不要太短" class="headerlink" title="Checkpoint 间隔不要太短"></a>Checkpoint 间隔不要太短</h5><p>虽然理论上 Flink 支持很短的 checkpoint 间隔，但是在实际生产中，过短的间隔对于底层分布式文件系统而言，会带来很大的压力。另一方面，由于检查点的语义，所以实际上 Flink 作业处理 record 与执行 checkpoint 存在互斥锁，过于频繁的 checkpoint，可能会影响整体的性能。当然，这个建议的出发点是底层分布式文件系统的压力考虑。 </p>
<h5 id="合理设置超时时间"><a href="#合理设置超时时间" class="headerlink" title="合理设置超时时间"></a>合理设置超时时间</h5><p>默认的超时时间是 10min，如果 state 规模大，则需要合理配置。最坏情况是分布式地创建速度大于单点（job master 端）的删除速度，导致整体存储集群可用空间压力较大。建议当检查点频繁因为超时而失败时，增大超时时间。</p>
<p>【参考文献】</p>
<ol>
<li><a target="_blank" rel="noopener" href="https://www.jianshu.com/p/6ed0ef5e2b74">Flink Streaming状态处理（Working with State）</a></li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://example.com/bigdata/Flink/Flink-SQL/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="aaronzhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Guadazi">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/bigdata/Flink/Flink-SQL/" class="post-title-link" itemprop="url">FlinkSQL与动态表</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2019-08-10 14:58:00" itemprop="dateCreated datePublished" datetime="2019-08-10T14:58:00+08:00">2019-08-10</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-04-11 11:49:10" itemprop="dateModified" datetime="2021-04-11T11:49:10+08:00">2021-04-11</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/bigdata/" itemprop="url" rel="index"><span itemprop="name">bigdata</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="Flink-SQL"><a href="#Flink-SQL" class="headerlink" title="Flink SQL"></a>Flink SQL</h1><h2 id="实现原理"><a href="#实现原理" class="headerlink" title="实现原理"></a>实现原理</h2><p><img src="_v_images/20201030204716034_449053674"></p>
<p>新版本的Table &amp; SQL API在原有的Table API基础上，由Calcite提供SQL解析和优化能力，将Table API调用和SQL查询统一转换成Calcite逻辑执行计划（Calcite RelNode树），并对此进行优化和代码生成，最终同样转化成Flink DataStream/DataSet API调用代码。</p>
<p>一般在使用的时候需要分别注册Source表 和 Sink表，分别对应数据的输入和输出。<br>对于注册Source表，可以从内部的catalog注册；也可以从TableSource注册；还可以通过DataSet转换注册。<br>对于SInk表，一般就直接通过TableSink注册了。查询时可以通过Table API执行select或者filter之类，也可以通过env.sqlQuery执行查询。<br>写入时可以通过table.insertInto()执行写操作，也可以通过env.sqlUpdate()执行写入。<br>这里还要吐槽下：弄一个sql()自动判断查询和写入不好么，为什么要区分update和insert?</p>
<h2 id="SQL原理"><a href="#SQL原理" class="headerlink" title="SQL原理"></a>SQL原理</h2><p>Table &amp; SQL API基于scala和java编写，内部基于calcite实现标准sql的解析和校验。跟spark不一样，flink直接基于开源的calcite编写。<br>calcite本身是一个apache的开源项目，它独立于存储和执行，专门负责sql的解析优化、语法树的校验等，并且通过插件的方式可以很方便的扩展优化规则，广泛的应用在hive、solr、flink等中。</p>
<p><img src="_v_images/20201030205455658_1363203717.jpg"></p>
<p>在Flink中通过tableEnv.sqlQuery和tableEnv.sqlUpdate可以看到具体的calcite使用流程。query与update的操作其实内部差不多，都是解析、校验、转换，不过sqlUpdate最后会基于内部的Table增加一个insertInto的操作。</p>
<p><img src="_v_images/20201030205455452_457126200.jpg"></p>
<p>以sqlQuery为例，先来看看整体的流程：</p>
<p><img src="_v_images/20201030205455347_1666287840.jpg"></p>
<p>首先创建FlinkPlannerImpl的执行计划，然后调用parse方法，内部直接使用calcite的SqlParser形成语法树。此时的语法树其实是一个个的SqlNode，这个SqlNode是calcite中定义，不同的sql有不同的sqlNode实现。比如最常见的SqlSelect，SqlJoin，SqlInsert等。每个类中会有自己的一些组件，比如SqlSelect会有group by, from, where, selectList等等。</p>
<p>获得语法树后，会通过一个简单的校验，判断是否为QUERY或者INSERT。然后经过一个通用的validate校验，粗略的看了下有catalog、表达式等的校验。最后通过rel把calcite的SqlNode转换成RelNode即逻辑执行计划。</p>
<p><img src="_v_images/20201030205455242_164668059.png"></p>
<p>Table后续在使用时会通过translate转换成一个DataSet，内部会先进行优化（优化过程既包括calcite提供的默认优化规则，也有Flink扩展的规则），最后生成物理执行计划。物理执行计划会按照node类型的不同将node转换成dataset或datastream的API。</p>
<p><img src="_v_images/20201030205455038_55047340.jpg"></p>
<p>总结来说，Flink SQL通过calcite实现：</p>
<ul>
<li>解析（字符串SQL转AST抽象语法树）</li>
<li>校验（语法、表达式、表信息）</li>
<li>优化（剪枝、谓词下推）</li>
<li>转换（逻辑计划转换成物理执行计划=Node转换成DataSet\DataStream API）</li>
<li>最终把SQL转换成DataSet或DataStream的API。</li>
</ul>
<h2 id="Flink-SQL-的编译及优化过程"><a href="#Flink-SQL-的编译及优化过程" class="headerlink" title="Flink SQL 的编译及优化过程"></a>Flink SQL 的编译及优化过程</h2><ul>
<li>Flink SQL 利用 Apache Calcite 将 SQL 翻译为关系代数表达式，使用表达式折叠（Expression Reduce），下推优化（Predicate / Projection Pushdown ）等优化技术生成物理执行计划（Physical Plan），利用 Codegen 技术生成高效执行代码。</li>
<li>Flink SQL 使用高效的二进制数据存储结构 BinaryRow 加速计算性能；使用 Mini-batch 攒批提高吞吐，降低两层聚合时由 Retraction 引起的数据抖动；聚合场景下数据倾斜处理和 Top-N 排序的优化原理。</li>
</ul>
<h2 id="表注册"><a href="#表注册" class="headerlink" title="表注册"></a>表注册</h2><h3 id="虚表注册"><a href="#虚表注册" class="headerlink" title="虚表注册"></a>虚表注册</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 获取一个TableEnvironment</span></span><br><span class="line"><span class="type">TableEnvironment</span> tableEnv = ...; </span><br><span class="line"><span class="comment">// table对象，查询的结果集</span></span><br><span class="line"><span class="type">Table</span> projTable = tableEnv.from(<span class="string">&quot;X&quot;</span>).select(...);</span><br><span class="line"><span class="comment">// 注册一个表，名称为 &quot;projectedTable&quot;</span></span><br><span class="line">tableEnv.createTemporaryView(<span class="string">&quot;projectedTable&quot;</span>, projTable);</span><br></pre></td></tr></table></figure>
<h3 id="外部表注册"><a href="#外部表注册" class="headerlink" title="外部表注册"></a>外部表注册</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">tableEnvironment</span><br><span class="line">  .connect(...)</span><br><span class="line">  .withFormat(...)</span><br><span class="line">  .withSchema(...)</span><br><span class="line">  .inAppendMode()</span><br><span class="line">  .createTemporaryTable(<span class="string">&quot;MyTable&quot;</span>)</span><br></pre></td></tr></table></figure>
<h3 id="catalog与db是两个概念"><a href="#catalog与db是两个概念" class="headerlink" title="catalog与db是两个概念"></a>catalog与db是两个概念</h3><p>表的注册总是包含三部分标识属性：catalog、数据库、表名。用户可以在内部设置一个catalog和一个数据库作为当前的catalog和数据库，所以对于catalog和数据库这两个标识属性是可选的，即如果不指定，默认使用的是“current catalog”和 “current database”。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">TableEnvironment tEnv = ...;</span><br><span class="line">tEnv.useCatalog(<span class="string">&quot;custom_catalog&quot;</span>);<span class="comment">//设置catalog</span></span><br><span class="line">tEnv.useDatabase(<span class="string">&quot;custom_database&quot;</span>);<span class="comment">//设置数据库</span></span><br><span class="line">Table table = ...;</span><br><span class="line"><span class="comment">// 注册一个名为exampleView的视图，catalog名为custom_catalog</span></span><br><span class="line"><span class="comment">// 数据库的名为custom_database</span></span><br><span class="line">tableEnv.createTemporaryView(<span class="string">&quot;exampleView&quot;</span>, table);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 注册一个名为exampleView的视图，catalog的名为custom_catalog</span></span><br><span class="line"><span class="comment">// 数据库的名为other_database</span></span><br><span class="line">tableEnv.createTemporaryView(<span class="string">&quot;other_database.exampleView&quot;</span>, table);</span><br><span class="line"> </span><br><span class="line"><span class="comment">// 注册一个名为&#x27;View&#x27;的视图，catalog的名称为custom_catalog</span></span><br><span class="line"><span class="comment">// 数据库的名为custom_database，&#x27;View&#x27;是保留关键字，需要使用``(反引号)</span></span><br><span class="line">tableEnv.createTemporaryView(<span class="string">&quot;`View`&quot;</span>, table);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 注册一个名为example.View的视图，catalog的名为custom_catalog，</span></span><br><span class="line"><span class="comment">// 数据库名为custom_database</span></span><br><span class="line">tableEnv.createTemporaryView(<span class="string">&quot;`example.View`&quot;</span>, table);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 注册一个名为&#x27;exampleView&#x27;的视图， catalog的名为&#x27;other_catalog&#x27;</span></span><br><span class="line"><span class="comment">// 数据库名为other_database&#x27; </span></span><br><span class="line">tableEnv.createTemporaryView(<span class="string">&quot;other_catalog.other_database.exampleView&quot;</span>, table);</span><br></pre></td></tr></table></figure>


<h2 id="案例分析"><a href="#案例分析" class="headerlink" title="案例分析"></a>案例分析</h2><p>以WordCount为例，为了增加sql的复杂度，在外层增加了filter：</p>
<p><img src="_v_images/20201030205601002_1446863552.jpg"></p>
<p>使用System.out.println(tEnv.explain(table));可以输出执行计划：</p>
<p><img src="_v_images/20201030205600897_1720675340.jpg"></p>
<p>通过parse方法获得到抽象语法树，显示一个filter节点，然后跟着Agg和scan。经过优化后，查询条件优化到最底层。最后转换生成真正的物理执行计划。</p>
<p>后续会继续研究下calcite以及optimize部分，到时再做分享。</p>
<h2 id="Retract-mode-和-Append-mode"><a href="#Retract-mode-和-Append-mode" class="headerlink" title="Retract mode 和 Append mode"></a>Retract mode 和 Append mode</h2><p>toAppendStream  只支持insert<br>toRetractStream  其余模式都可以</p>
<p>如果动态表仅只有Insert操作，即之前输出的结果不会被更新，则使用该模式。如果更新或删除操作使用追加模式会失败报错，始终可以使用此模式。返回值是boolean类型。它用true或false来标记数据的插入和撤回，返回true代表数据插入，false代表数据的撤回。</p>
<p>使用flinkSQL处理实时数据当我们把表转化成流的时候，需要用toAppendStream与toRetractStream这两个方法。稍不注意可能直接选择了toAppendStream。</p>
<p>始终可以使用此模式。返回值是boolean类型。它用true或false来标记数据的插入和撤回，返回true代表数据插入，false代表数据的撤回。</p>
<p>当我们使用的sql语句包含：count() group by时，必须使用缩进模式</p>
<p><img src="_v_images/20201111111954370_1165111473.png"><br><img src="_v_images/20201111112036578_1272633365.png"></p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 获取StreamTableEnvironment.</span></span><br><span class="line">StreamTableEnvironment tableEnv = ...;</span><br><span class="line"><span class="comment">// 包含两个字段的表(String name, Integer age)</span></span><br><span class="line">Table table = ...</span><br><span class="line"><span class="comment">// 将表转为DataStream，使用Append Mode追加模式，数据类型为Row</span></span><br><span class="line">DataStream&lt;Row&gt; dsRow = tableEnv.toAppendStream(table, Row.class);</span><br><span class="line"><span class="comment">// 将表转为DataStream，使用Append Mode追加模式，数据类型为定义好的TypeInformation</span></span><br><span class="line">TupleTypeInfo&lt;Tuple2&lt;String, Integer&gt;&gt; tupleType = <span class="keyword">new</span> TupleTypeInfo&lt;&gt;(</span><br><span class="line">  Types.STRING(),</span><br><span class="line">  Types.INT());</span><br><span class="line">  DataStream&lt;Tuple2&lt;String, Integer&gt;&gt; dsTuple =</span><br><span class="line">  tableEnv.toAppendStream(table, tupleType);</span><br><span class="line"><span class="comment">// 将表转为DataStream，使用的模式为Retract Mode撤回模式，类型为Row</span></span><br><span class="line"><span class="comment">// 对于转换后的DataStream&lt;Tuple2&lt;Boolean, X&gt;&gt;，X表示流的数据类型，</span></span><br><span class="line"><span class="comment">// boolean值表示数据改变的类型，其中INSERT返回true，DELETE返回的是false</span></span><br><span class="line">DataStream&lt;Tuple2&lt;Boolean, Row&gt;&gt; retractStream</span><br><span class="line">  tableEnv.toRetractStream(table, Row.class);</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h3 id="如何实现回退更新"><a href="#如何实现回退更新" class="headerlink" title="如何实现回退更新?"></a>如何实现回退更新?</h3><p>flink-connector-jdbc 最终使用的是SQL引擎的upsert语法:</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> tbl() <span class="keyword">values</span> ( ),( ) <span class="keyword">on</span> duplicate key <span class="keyword">update</span></span><br></pre></td></tr></table></figure>
<p>可以看下 MySQL/upsert 一节</p>
<p>对应flink 源码</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">org.apache.flink.connector.jdbc.dialect.MySQLDialect#getUpsertStatement</span><br></pre></td></tr></table></figure>



<h2 id="keyedBy与group-by区别"><a href="#keyedBy与group-by区别" class="headerlink" title="keyedBy与group by区别"></a>keyedBy与group by区别</h2><h2 id="SQL解析工具"><a href="#SQL解析工具" class="headerlink" title="SQL解析工具"></a>SQL解析工具</h2><p>hive使用了antlr3实现了自己的HQL,<br>Flink使用Apache Calcite,<br>而Calcite的解析器是使用JavaCC实现的,<br>Spark2.x以后采用了antlr4实现自己的解析器,<br>Presto也是使用antlr4。</p>
<h2 id="通过Table-api创建表"><a href="#通过Table-api创建表" class="headerlink" title="通过Table api创建表"></a>通过Table api创建表</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># create a <span class="type">Table</span> from a <span class="type">Table</span> <span class="type">API</span> query</span><br><span class="line">tapi_result = table_env.from_path(<span class="string">&quot;table1&quot;</span>).select(...)</span><br></pre></td></tr></table></figure>
<h2 id="视图-view"><a href="#视图-view" class="headerlink" title="视图 view"></a>视图 view</h2><h2 id="window-aggregate与group-aggregate区别"><a href="#window-aggregate与group-aggregate区别" class="headerlink" title="window aggregate与group aggregate区别"></a>window aggregate与group aggregate区别</h2><p>参考自<a target="_blank" rel="noopener" href="https://ververica.cn/developers/flink-sql-programming-practice/">Apache Flink 零基础入门（九）：Flink SQL 编程实践</a></p>
<h3 id="Group-Aggregate的例子"><a href="#Group-Aggregate的例子" class="headerlink" title="Group Aggregate的例子"></a>Group Aggregate的例子</h3><p>这是一个group aggregate, 内存中累计每个分类的数据，有变化时，update sink</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> psgCnt, <span class="built_in">COUNT</span>(<span class="operator">*</span>) <span class="keyword">AS</span> cnt </span><br><span class="line"><span class="keyword">FROM</span> Rides </span><br><span class="line"><span class="keyword">WHERE</span> isInNYC(lon, lat)</span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> psgCnt;</span><br></pre></td></tr></table></figure>
<h3 id="Window-Aggregate"><a href="#Window-Aggregate" class="headerlink" title="Window Aggregate"></a>Window Aggregate</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> </span><br><span class="line">  toAreaId(lon, lat) <span class="keyword">AS</span> area, <span class="comment">-- toAreaId为UDF</span></span><br><span class="line">  TUMBLE_END(rideTime, <span class="type">INTERVAL</span> <span class="string">&#x27;5&#x27;</span> <span class="keyword">MINUTE</span>) <span class="keyword">AS</span> window_end, </span><br><span class="line">  <span class="comment">--滚动窗口的结束时间: </span></span><br><span class="line">  <span class="comment">-- ① 可以时间不同吗?</span></span><br><span class="line">  <span class="comment">-- ② 窗口可以别名吗?</span></span><br><span class="line">  <span class="built_in">COUNT</span>(<span class="operator">*</span>) <span class="keyword">AS</span> cnt </span><br><span class="line"><span class="keyword">FROM</span> Rides </span><br><span class="line"><span class="keyword">WHERE</span> isInNYC(lon, lat) <span class="keyword">and</span> isStart</span><br><span class="line"><span class="comment">-- isStart 为boolean</span></span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> </span><br><span class="line">  toAreaId(lon, lat), </span><br><span class="line">  TUMBLE(rideTime, <span class="type">INTERVAL</span> <span class="string">&#x27;5&#x27;</span> <span class="keyword">MINUTE</span>) </span><br><span class="line">  <span class="comment">-- 定义窗口</span></span><br><span class="line"><span class="keyword">HAVING</span> <span class="built_in">COUNT</span>(<span class="operator">*</span>) <span class="operator">&gt;=</span> <span class="number">5</span>;</span><br><span class="line"><span class="comment">-- 5分钟超过5次才输出</span></span><br></pre></td></tr></table></figure>
<h3 id="Window-Aggregate-与-Group-Aggregate-的区别"><a href="#Window-Aggregate-与-Group-Aggregate-的区别" class="headerlink" title="Window Aggregate 与 Group Aggregate 的区别"></a>Window Aggregate 与 Group Aggregate 的区别</h3><p>Window Aggregate 是当window结束时才输出，其输出的结果是最终值，不会再进行修改，其输出流是一个 Append 流。而 Group Aggregate 是每处理一条数据，就输出最新的结果，其结果是在不断更新的，就好像数据库中的数据一样，其输出流是一个 Update 流。</p>
<p>window 由于有 watermark ，可以精确知道哪些窗口已经过期了，所以可以及时清理过期状态，保证状态维持在稳定的大小。而 Group Aggregate 因为不知道哪些数据是过期的，所以状态会无限增长，这对于生产作业来说不是很稳定，所以建议对 Group Aggregate 的作业配上 State TTL 的配置。</p>
<p><img src="_v_images/20210410180758564_1166368853.png"></p>
<p>例如统计每个店铺每天的实时PV，那么就可以将 TTL 配置成 24+ 小时，因为一天前的状态一般来说就用不到了。</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span>  DATE_FORMAT(ts, <span class="string">&#x27;yyyy-MM-dd&#x27;</span>), shop_id, <span class="built_in">COUNT</span>(<span class="operator">*</span>) <span class="keyword">as</span> pv</span><br><span class="line"><span class="keyword">FROM</span> T</span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> DATE_FORMAT(ts, <span class="string">&#x27;yyyy-MM-dd&#x27;</span>), shop_id</span><br></pre></td></tr></table></figure>
<p>当然，如果 TTL 配置地太小，可能会清除掉一些有用的状态和数据，从而导致数据精确性地问题。这也是用户需要权衡地一个参数。</p>
<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="[参考文献]"></a>[参考文献]</h2><ol>
<li><p><a target="_blank" rel="noopener" href="http://www.10tiao.com/html/157/201707/2653162664/1.html">在数据流中使用SQL查询：Apache Flink中的动态表的持续查询</a></p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://jiamaoxiang.top/2020/05/25/Flink-Table-API-SQL%E7%BC%96%E7%A8%8B%E6%8C%87%E5%8D%97/">Flink Table API &amp; SQL编程指南(1)</a></p>
</li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://example.com/bigdata/Flink/DataTypesAndSerialization/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="aaronzhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Guadazi">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/bigdata/Flink/DataTypesAndSerialization/" class="post-title-link" itemprop="url">Flink:数据类型与序列化</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2019-08-10 14:58:00" itemprop="dateCreated datePublished" datetime="2019-08-10T14:58:00+08:00">2019-08-10</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-04-11 11:49:10" itemprop="dateModified" datetime="2021-04-11T11:49:10+08:00">2021-04-11</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/bigdata/" itemprop="url" rel="index"><span itemprop="name">bigdata</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="Flink-Data-Types-amp-Serialization"><a href="#Flink-Data-Types-amp-Serialization" class="headerlink" title="Flink Data Types &amp; Serialization"></a>Flink Data Types &amp; Serialization</h1><h2 id="使用case-class的坑"><a href="#使用case-class的坑" class="headerlink" title="使用case class的坑"></a>使用case class的坑</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Event</span>(<span class="params">id: <span class="type">Int</span></span>) </span>&#123;</span><br><span class="line">  <span class="keyword">val</span> lb = <span class="keyword">new</span> <span class="type">ListBuffer</span>[<span class="type">Int</span>]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">13:00:43,342 INFO  org.apache.flink.api.java.typeutils.TypeExtractor             - class org.myorg.quickstart.Event does not contain a setter for field id</span><br><span class="line">13:00:43,343 INFO  org.apache.flink.api.java.typeutils.TypeExtractor             - Class class org.myorg.quickstart.Event cannot be used as a POJO type because not all fields are valid POJO fields, and must be processed as GenericType. Please read the Flink documentation on &quot;Data Types &amp; Serialization&quot; for details of the effect on performance.</span><br></pre></td></tr></table></figure>
<p><strong>提示信息：找不到setter，对于POJO类型必须所有的字段必须要有setter和getter</strong><br>命名是case class啊</p>
<p>再看生产环境的例子： 折腾了一下午</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Event</span>(<span class="params">uin: <span class="type">String</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">                 sPid: <span class="type">String</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">                 applyId: <span class="type">String</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">                 bankType: <span class="type">Long</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">                 transactionId: <span class="type">String</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">                 amount: <span class="type">Long</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">                 createTime: <span class="type">Long</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">                 bizType: <span class="type">Long</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">                 modifyTime: <span class="type">String</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">                 equ: <span class="type">ListBuffer</span>[<span class="type">Int</span>]</span></span></span><br><span class="line"><span class="class"><span class="params">                </span>) </span>&#123;</span><br><span class="line"></span><br><span class="line">   <span class="function"><span class="keyword">def</span> <span class="title">this</span></span>(uin: <span class="type">String</span>,</span><br><span class="line">           sPid: <span class="type">String</span>,</span><br><span class="line">           applyId: <span class="type">String</span>,</span><br><span class="line">           bankType: <span class="type">Long</span>,</span><br><span class="line">           transactionId: <span class="type">String</span>,</span><br><span class="line">           amount: <span class="type">Long</span>,</span><br><span class="line">           createTime: <span class="type">Long</span>,</span><br><span class="line">           bizType: <span class="type">Long</span>,</span><br><span class="line">           modifyTime: <span class="type">String</span>,</span><br><span class="line">           equ: <span class="type">List</span>[<span class="type">Int</span>]</span><br><span class="line">          ) = &#123;</span><br><span class="line">    <span class="keyword">this</span>(uin, sPid, applyId, bankType, transactionId, amount, createTime, bizType, modifyTime)</span><br><span class="line">    <span class="keyword">if</span> (equ != <span class="literal">null</span>) &#123;</span><br><span class="line">      equities.appendAll(equ)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">var</span> _active = <span class="literal">false</span></span><br><span class="line">  <span class="keyword">val</span> equities: <span class="type">ListBuffer</span>[<span class="type">Int</span>] = <span class="keyword">new</span> <span class="type">ListBuffer</span>[<span class="type">Int</span>]</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">addEquity</span></span>(id: <span class="type">Int</span>): <span class="type">Event</span> = &#123;</span><br><span class="line">    equities.append(id)</span><br><span class="line">    <span class="keyword">this</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">equityString</span></span>(separator: <span class="type">String</span>): <span class="type">String</span> = &#123;</span><br><span class="line">    equities.mkString(separator)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">setActive</span></span>(): <span class="type">Event</span> = &#123;</span><br><span class="line">    _active = <span class="literal">true</span></span><br><span class="line">    <span class="keyword">this</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">setActive</span></span>(active: <span class="type">String</span>): <span class="type">Event</span> = &#123;</span><br><span class="line">    _active = <span class="string">&quot;1&quot;</span>.equals(active)</span><br><span class="line">    <span class="keyword">this</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">isActive</span></span>() = &#123;</span><br><span class="line">    _active</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>使用的是flink 1.6版本的，case class识别出来了，但是equities没有传递到下一个算子中，始终没有值</p>
<p>老老实实的修改成普通类</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Event</span>(<span class="params">_uin: <span class="type">String</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">            _sPid: <span class="type">String</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">            _applyId: <span class="type">String</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">            _bankType: <span class="type">Long</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">            _transactionId: <span class="type">String</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">            _amount: <span class="type">Long</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">            _createTime: <span class="type">Long</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">            _bizType: <span class="type">Long</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">            _modifyTime: <span class="type">String</span></span></span></span><br><span class="line"><span class="class"><span class="params">           </span>) <span class="keyword">extends</span> <span class="title">Serializable</span> </span>&#123;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">var</span> equities: <span class="type">ListBuffer</span>[<span class="type">Int</span>] = <span class="keyword">new</span> <span class="type">ListBuffer</span>[<span class="type">Int</span>]</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">var</span> uin: <span class="type">String</span> = _uin</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">var</span> sPid: <span class="type">String</span> = _sPid</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">var</span> applyId: <span class="type">String</span> = _applyId</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">var</span> bankType: <span class="type">Long</span> = _bankType</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">var</span> transactionId: <span class="type">String</span> = _transactionId</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">var</span> amount: <span class="type">Long</span> = _amount</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">var</span> createTime: <span class="type">Long</span> = _createTime</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">var</span> bizType: <span class="type">Long</span> = _bizType</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">var</span> modifyTime: <span class="type">String</span> = _modifyTime</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">var</span> _active = <span class="literal">false</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">this</span></span>(uin: <span class="type">String</span>,</span><br><span class="line">           sPid: <span class="type">String</span>,</span><br><span class="line">           applyId: <span class="type">String</span>,</span><br><span class="line">           bankType: <span class="type">Long</span>,</span><br><span class="line">           transactionId: <span class="type">String</span>,</span><br><span class="line">           amount: <span class="type">Long</span>,</span><br><span class="line">           createTime: <span class="type">Long</span>,</span><br><span class="line">           bizType: <span class="type">Long</span>,</span><br><span class="line">           modifyTime: <span class="type">String</span>,</span><br><span class="line">           equ: <span class="type">List</span>[<span class="type">Int</span>]</span><br><span class="line">          ) = &#123;</span><br><span class="line">    <span class="keyword">this</span>(uin, sPid, applyId, bankType, transactionId, amount, createTime, bizType, modifyTime)</span><br><span class="line">    <span class="keyword">if</span> (equ != <span class="literal">null</span>) &#123;</span><br><span class="line">      equities.appendAll(equ)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">getUin</span></span>: <span class="type">String</span> = uin</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">setUin</span></span>(<span class="type">Uin</span>: <span class="type">String</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">this</span>.uin = <span class="type">Uin</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">getSPid</span></span>: <span class="type">String</span> = sPid</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">setSPid</span></span>(<span class="type">SPid</span>: <span class="type">String</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">this</span>.sPid = <span class="type">SPid</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">getApplyId</span></span>: <span class="type">String</span> = applyId</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">setApplyId</span></span>(<span class="type">ApplyId</span>: <span class="type">String</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">this</span>.applyId = <span class="type">ApplyId</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">getBankType</span></span>: <span class="type">Long</span> = bankType</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">setBankType</span></span>(<span class="type">BankType</span>: <span class="type">Long</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">this</span>.bankType = <span class="type">BankType</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">getTransactionId</span></span>: <span class="type">String</span> = transactionId</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">setTransactionId</span></span>(<span class="type">TransactionId</span>: <span class="type">String</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">this</span>.transactionId = <span class="type">TransactionId</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">getAmount</span></span>: <span class="type">Long</span> = amount</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">setAmount</span></span>(<span class="type">Amount</span>: <span class="type">Long</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">this</span>.amount = <span class="type">Amount</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">getCreateTime</span></span>: <span class="type">Long</span> = createTime</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">setCreateTime</span></span>(<span class="type">CreateTime</span>: <span class="type">Long</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">this</span>.createTime = <span class="type">CreateTime</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">getBizType</span></span>: <span class="type">Long</span> = bizType</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">setBizType</span></span>(<span class="type">BizType</span>: <span class="type">Long</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">this</span>.bizType = <span class="type">BizType</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">getModifyTime</span></span>: <span class="type">String</span> = modifyTime</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">setModifyTime</span></span>(<span class="type">ModifyTime</span>: <span class="type">String</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">this</span>.modifyTime = <span class="type">ModifyTime</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">addEquity</span></span>(id: <span class="type">Int</span>): <span class="type">Event</span> = &#123;</span><br><span class="line">    equities.append(id)</span><br><span class="line">    <span class="keyword">this</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">equityString</span></span>(separator: <span class="type">String</span>): <span class="type">String</span> = &#123;</span><br><span class="line">    equities.mkString(separator)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">setActive</span></span>(): <span class="type">Event</span> = &#123;</span><br><span class="line">    _active = <span class="literal">true</span></span><br><span class="line">    <span class="keyword">this</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">setActive</span></span>(active: <span class="type">String</span>): <span class="type">Event</span> = &#123;</span><br><span class="line">    _active = <span class="string">&quot;1&quot;</span>.equals(active)</span><br><span class="line">    <span class="keyword">this</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">isActive</span></span>() = &#123;</span><br><span class="line">    _active</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">rights</span></span>(split: <span class="type">String</span>) = &#123;</span><br><span class="line">    <span class="type">RightEvent</span>(<span class="keyword">this</span>, equities.mkString(split))</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">getEquities</span> </span>= equities</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">setEquities</span></span>(equities: <span class="type">ListBuffer</span>[<span class="type">Int</span>]) = &#123;</span><br><span class="line">    <span class="keyword">this</span>.equities = equities</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>可以了</p>
<p>初步估计，序列化除了问题</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://example.com/bigdata/Flink/HBaseConnectors/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="aaronzhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Guadazi">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/bigdata/Flink/HBaseConnectors/" class="post-title-link" itemprop="url">Flink:HBaseConnectors</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2019-08-10 14:58:00" itemprop="dateCreated datePublished" datetime="2019-08-10T14:58:00+08:00">2019-08-10</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-04-11 11:49:10" itemprop="dateModified" datetime="2021-04-11T11:49:10+08:00">2021-04-11</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/bigdata/" itemprop="url" rel="index"><span itemprop="name">bigdata</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="Flink-HBase-connectors"><a href="#Flink-HBase-connectors" class="headerlink" title="Flink HBase connectors"></a>Flink HBase connectors</h1><h2 id="flighting"><a href="#flighting" class="headerlink" title="flighting"></a>flighting</h2><p>奇怪，没有依赖HBase-Client，而是依赖了HBase-server</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hbase<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hbase-hadoop2-compat<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;hbase.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">scope</span>&gt;</span>test<span class="tag">&lt;/<span class="name">scope</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">type</span>&gt;</span>test-jar<span class="tag">&lt;/<span class="name">type</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hbase<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hbase-server<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;hbase.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>
<h3 id="添加文件到ClassLoader的classpath"><a href="#添加文件到ClassLoader的classpath" class="headerlink" title="添加文件到ClassLoader的classpath"></a>添加文件到ClassLoader的classpath</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Get the classloader actually used by HBaseConfiguration</span></span><br><span class="line">ClassLoader classLoader = HBaseConfiguration.create().getClassLoader();</span><br><span class="line"><span class="keyword">if</span> (!(classLoader <span class="keyword">instanceof</span> URLClassLoader)) &#123;</span><br><span class="line">	fail(<span class="string">&quot;We should get a URLClassLoader&quot;</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Make the addURL method accessible</span></span><br><span class="line">Method method = URLClassLoader.class.getDeclaredMethod(<span class="string">&quot;addURL&quot;</span>, URL.class);</span><br><span class="line">method.setAccessible(<span class="keyword">true</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// Add the directory where we put the hbase-site.xml to the classpath</span></span><br><span class="line">method.invoke(classLoader, directory.toURI().toURL());</span><br></pre></td></tr></table></figure>








      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/9/"><i class="fa fa-angle-left" aria-label="Previous page"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/9/">9</a><span class="page-number current">10</span><a class="page-number" href="/page/11/">11</a><span class="space">&hellip;</span><a class="page-number" href="/page/25/">25</a><a class="extend next" rel="next" href="/page/11/"><i class="fa fa-angle-right" aria-label="Next page"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">aaronzhang</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">242</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">17</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">123</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">aaronzhang</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  

</body>
</html>
